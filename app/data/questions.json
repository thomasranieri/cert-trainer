[
  {
    "id": "64abd78acb42432d7f81537ddccec54e5dd5596ec9e86017c336f98aa3d0e018",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A company processes IoT sensor data streams at 200 MB/s for ML feature engineering. The data arrives in JSON format and must be ingested into SageMaker Data Wrangler in near-real time. The ingestion solution must minimize operational overhead and storage costs while enabling efficient downstream transformations. Which solution meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy Amazon Kinesis Data Streams to buffer JSON records. Configure an AWS Lambda function to batch and convert records into Parquet, then write to S3. Use Data Wrangler to read from S3.",
      "B": "Use Amazon Kinesis Data Firehose to ingest JSON data directly into an S3 bucket with Parquet conversion and GZIP compression. Configure Data Wrangler to read from that S3 bucket.",
      "C": "Set up an Amazon Managed Streaming for Apache Kafka cluster. Use Kafka Connect to stream data into an Amazon Redshift table. Use Data Wrangler to query Redshift.",
      "D": "Ingest streams into an AWS Glue Streaming ETL job. Write output to Amazon DynamoDB. Use Data Wrangler to fetch items from DynamoDB."
    },
    "explanation": "Kinesis Data Firehose natively converts JSON to Parquet with compression and writes to S3, minimizing operational overhead and cost. The other options add complexity or incur higher cost/management burden."
  },
  {
    "id": "d3fb774cf7de594eb516a06e5b11c84b17cde1b0d6ec9fa1eaeab537ec679da7",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A dataset consists of structured CSV transaction logs, semi-structured JSON user event logs, and tens of millions of small binary image files. The ML pipeline uses Spark on EMR for preprocessing and SageMaker for training. The dataset has grown to multi-terabyte scale and ingestion performance is suffering. Which storage configuration should the ML engineer choose to optimize throughput, cost, and simplicity?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Store CSV and JSON in S3. Store images in Amazon FSx for Lustre mounted to EMR. Use Spark on EMR to read both sources.",
      "B": "Import all data into Amazon Redshift with Spectrum. Store CSV/JSON in tables, images as BLOBs. UNLOAD data for SageMaker training.",
      "C": "Store all data in Amazon S3 as partitioned, SNAPPY-compressed Parquet files. Configure EMR to read via the S3A connector and SageMaker to use S3 input mode.",
      "D": "Use Amazon EFS to store all raw files. Mount EFS to EMR clusters and SageMaker training jobs as a unified data source."
    },
    "explanation": "Partitioned Parquet on S3 offers cost-effective, high-throughput storage, simple management, and native support in Spark and SageMaker. FSx adds cost/ops, Redshift BLOBs and Spectrum incur complexity, and EFS suffers lower throughput and higher cost for large-scale data."
  },
  {
    "id": "a57ba9d9c8c20e854779e3d258fe6935f67001f5b9455ed544e25ef0cd02e69f",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An ML engineer runs a SageMaker processing job to merge and feature-engineer 50 million small JSON files in Amazon S3. The job fails due to S3 request throttling from excessive LIST and GET operations. Which solution will reduce operational overhead and ensure reliable job completion?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure a VPC endpoint for S3 in the SageMaker processing job to reduce throttling. Increase parallelism to maximize throughput.",
      "B": "Enable S3 Transfer Acceleration on the bucket to speed up GET requests. Use the Transfer Acceleration endpoint in the processing job.",
      "C": "Use AWS Glue to run a job that aggregates the small JSON files into larger SNAPPY-compressed Parquet files in S3. Update the SageMaker processing job to read the Parquet files.",
      "D": "Copy all JSON files to an Amazon EFS file system using AWS DataSync. Configure the SageMaker processing job to read directly from EFS."
    },
    "explanation": "Aggregating small files into larger Parquet files reduces S3 request count and improves throughput with minimal ongoing operations. VPC endpoints and Transfer Acceleration do not change S3 request rate limits; EFS adds complexity and cost."
  },
  {
    "id": "e226a3774426f3fb013986e7aa3175ecdf49454355d5ea97fdfef01619c1bc32",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "An ML engineer needs to encode a high-cardinality categorical feature (10 000 unique values) for a tree-based model. They must minimize memory footprint, avoid one-hot explosion, and preserve ordering where meaningful. Which approach and AWS tool should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use one-hot encoding in SageMaker Data Wrangler to produce sparse binary columns.",
      "B": "Use label encoding in AWS Glue DataBrew to assign integer codes to categories.",
      "C": "Use binary encoding in SageMaker Data Wrangler to compress categories into \u2308log\u2082(10000)\u2309 binary features.",
      "D": "Use frequency encoding in an AWS Glue ETL job to replace each category with its occurrence count."
    },
    "explanation": "Binary encoding (also called bit hashing) reduces dimensionality to \u2308log\u2082(n)\u2309 columns and preserves some ordinal information. SageMaker Data Wrangler supports a built-in binary encoding transform, minimizing memory and avoiding one-hot explosion."
  },
  {
    "id": "327dac087f3fbad33b0c8a3a59982372709ec19d1cb6ed81120c7d63a6bb91f3",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A data scientist is building a cleaning pipeline in SageMaker Data Wrangler for a numerical dataset with extreme outliers and missing values. They need to: (1) cap outliers at the 1st and 99th percentiles, (2) impute remaining missing values with the column median, and (3) standardize features to zero mean and unit variance. Which sequence of Wrangler nodes achieves this with minimal custom code?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Standardize \u2192 percentile clip \u2192 median impute",
      "B": "Median impute \u2192 percentile clip \u2192 standardize",
      "C": "Percentile clip \u2192 median impute \u2192 standardize",
      "D": "Median impute \u2192 standardize \u2192 percentile clip"
    },
    "explanation": "Clipping outliers first avoids imputing capped values. Next, median imputation handles any remaining missing data. Finally, standardization yields zero mean/unit variance on the cleaned data."
  },
  {
    "id": "4611994ae01a2b04c30d1e6eeef8f11a11b96e231bd961f1e6bc21e3a7c9f371",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A real-time recommendation engine requires streaming user events from Kinesis Data Streams to be transformed (JSON flattening, type conversion) and ingested into SageMaker Feature Store with under-second latency. Which solution minimizes operational overhead and meets latency SLAs?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Implement an AWS Lambda function triggered by Kinesis Data Streams that transforms each record and calls the SageMaker Feature Store PutRecord API.",
      "B": "Run an AWS Glue streaming ETL job on Kinesis Data Streams with PySpark to transform and write directly to Feature Store.",
      "C": "Deploy an Apache Flink application on Amazon EMR to consume Kinesis, perform transformations, write to S3, then batch-load into Feature Store.",
      "D": "Use Amazon Kinesis Data Analytics (Flink SQL) to transform the stream and send to a SageMaker Batch Transform job for periodic ingestion."
    },
    "explanation": "A Lambda function on Kinesis provides sub-second processing with minimal infrastructure to manage. It can call PutRecord directly. Glue streaming and EMR/Flink introduce greater complexity and latency; Batch Transform cannot meet real-time SLAs."
  },
  {
    "id": "e01227cfd058bc88bc06778b4fc10f4d2dc758f463485ee7fddb089fc7d85db6",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A health-tech startup has a dataset in Amazon S3 with PII fields (names, SSNs) and binary labels. They must prepare the data for SageMaker training by removing or masking PII, validating data quality, computing pre-training bias metrics (class imbalance, difference in proportions), and mitigating identified bias before training. Which sequence of AWS services and actions fulfills these requirements with minimal operational overhead?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon Macie to discover PII \u2192 apply client-side KMS encryption \u2192 run a SageMaker Processing job with a custom script to compute bias metrics.",
      "B": "Use AWS Glue Data Quality to profile dataset \u2192 use AWS Glue ETL to anonymize PII \u2192 run SageMaker Model Monitor to compute pre-training bias metrics.",
      "C": "Use SageMaker Ground Truth to label PII columns \u2192 use SageMaker Data Wrangler to transform data \u2192 use SageMaker Clarify ModelBias to compute post-training bias.",
      "D": "Use SageMaker Data Wrangler to detect and mask PII with built-in transforms \u2192 launch a SageMaker Clarify DataBias job to calculate pre-training CI and DPL metrics \u2192 apply synthetic oversampling for the minority class via Data Wrangler or a SageMaker Processing job."
    },
    "explanation": "Option D leverages Data Wrangler\u2019s built-in PII masking, uses Clarify\u2019s DataBias monitoring to compute pre-training bias metrics, and then mitigates imbalance via a native transform or Processing job, minimizing custom code and meeting compliance and bias-mitigation requirements."
  },
  {
    "id": "a06cf190c48b95cdd9cea127d3c5c0602e3dbb1c6c7c5b4190079f38619d932c",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "During preprocessing, an ML engineer discovers a significant class imbalance: one protected class comprises only 1% of the training data. They need to generate additional synthetic samples for that class without introducing label noise and then validate the augmented dataset\u2019s quality. Which approach aligns best with AWS-recommended bias mitigation and data validation practices?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Clarify\u2019s synthetic data generator to produce minority-class examples and merge with original data.",
      "B": "Launch a SageMaker Processing job that uses scikit-learn\u2019s SMOTE to create synthetic minority-class samples, then run AWS Glue Data Quality jobs to validate data integrity.",
      "C": "Use AWS Glue DataBrew\u2019s \u201cGenerate rows\u201d transform to duplicate minority-class records until balance is achieved.",
      "D": "Ingest data into SageMaker Feature Store and use record augmentation in Feature Store to rebalance classes."
    },
    "explanation": "Option B applies SMOTE in a managed Processing job to generate realistic synthetic samples for the minority class, then validates the augmented dataset with Glue Data Quality, aligning with best practices for bias mitigation and data integrity."
  },
  {
    "id": "ac578519d83e14ae02b7825b68e79dd8ae4a014d8b0233ab527313c99ba077ad",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "An ML engineer must deliver preprocessed, bias-mitigated training data to a SageMaker training job. The data must be encrypted at rest and in transit, and the training dataset split must be stratified by a protected attribute to avoid introducing bias. How should the engineer configure the data pipeline to meet these requirements?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Store the CSVs in S3 with SSE-S3 encryption, use File input mode in SageMaker training, and rely on built-in random shuffle for splits.",
      "B": "Provision an encrypted FSx for Lustre file system, copy preprocessed files there, and configure the training job with File mode and a JSON split manifest.",
      "C": "Mount an encrypted Amazon EFS volume in the training container, copy data via a Processing job that shuffles and splits, then train using File mode.",
      "D": "Store preprocessed data in S3 with SSE-KMS encryption, use Pipe input mode for the training job, and implement a stratified shuffle-and-split routine inside the training container to ensure splits respect the protected attribute."
    },
    "explanation": "Option D ensures end-to-end encryption with SSE-KMS, leverages Pipe mode to stream data efficiently, and uses a custom stratified shuffle-and-split routine in the container to maintain fairness across protected groups."
  },
  {
    "id": "9b67490da6c419011a48aa78c91ff19b414cb17dc0c7d6aff39e25b058f74528",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A fintech company is building a credit-scoring model to classify loan applicants as high or low risk. The dataset consists of 1 million rows of structured tabular data with a mix of numerical and categorical features of moderate cardinality. Regulatory requirements mandate that the model be highly interpretable for audit, and the application demands low-latency inference. Data scientists want to minimize manual feature engineering. Which SageMaker built-in algorithm should the ML engineer choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use the XGBoost built-in algorithm with default hyperparameters.",
      "B": "Use the Factorization Machines built-in algorithm to capture feature interactions.",
      "C": "Use the Linear Learner built-in algorithm configured for logistic regression.",
      "D": "Use the K-Nearest Neighbors built-in algorithm for classification."
    },
    "explanation": "Linear Learner in logistic regression mode provides a fully interpretable model with low inference latency and minimal feature engineering. XGBoost offers higher accuracy but less interpretability. Factorization Machines capture interactions but are harder to audit. KNN has high latency and is not suited for large datasets."
  },
  {
    "id": "0f0ef5d884acbd92cd803629fc26462606c4edc3af6b85e4a5080eb236ef3a36",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A legal analytics startup needs to automate abstractive summarization of large contract documents. The team lacks expertise in training sequence-to-sequence models and requires a fully managed, high-quality solution that supports few-shot prompting. Which modeling approach should the ML engineer select?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon Comprehend\u2019s extractive summarization API on the contracts.",
      "B": "Fine-tune a T5 sequence-to-sequence model on SageMaker using the Hugging Face framework from JumpStart.",
      "C": "Invoke a foundation model via Amazon Bedrock (for example, Titan) with a prompt template for abstractive summarization.",
      "D": "Build a custom TensorFlow sequence-to-sequence model from scratch on SageMaker training instances."
    },
    "explanation": "Amazon Bedrock provides managed foundation models that excel at few-shot abstractive summarization with minimal ML expertise. Comprehend only supports extractive summarization. Fine-tuning on SageMaker or building from scratch incurs significant development and infrastructure overhead."
  },
  {
    "id": "f8dbef2b93f3febae2872c6c9479ed2cd2f245680fe8f3f6bdc852e9b0b2257a",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "An industrial IoT provider collects streaming time-series sensor data from thousands of devices and needs to detect anomalies in near real time. The provider wants a fully managed service requiring minimal ML development and configuration. Which AWS service or approach should the ML engineer choose?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Train and deploy a Random Cut Forest (RCF) model on SageMaker for anomaly detection.",
      "B": "Develop and train a custom LSTM-based anomaly detector on SageMaker using TensorFlow.",
      "C": "Use Amazon Lookout for Metrics to automatically detect anomalies in the time-series data.",
      "D": "Use the anomaly-detection recipe in SageMaker Studio JumpStart and deploy the generated pipeline."
    },
    "explanation": "Amazon Lookout for Metrics is a fully managed anomaly-detection service that automatically tracks time-series metrics with minimal configuration. Training an RCF or LSTM on SageMaker requires substantial ML development and ongoing maintenance. JumpStart recipes simplify development but still require managing model training and pipelines."
  },
  {
    "id": "98179541959f6bb8a7f57f24c7a66e74b8016f2091f2ff20e89fd6f7a3f76556",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An ML engineer previously ran a hyperparameter tuning job on a deep learning model and wants to optimize a new tuning job by reusing the results of that earlier job. Additionally, to reduce overall training cost and time, the engineer needs to automatically stop underperforming training jobs during tuning. Which configuration will meet these requirements?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run a new hyperparameter tuning job using BayesianOptimization search, with MaxNumberOfTrainingJobs set to 100, and leave EarlyStoppingType unset.",
      "B": "Run a warm start tuning job with type TRANSFER_LEARNING and disable early stopping to leverage prior results only.",
      "C": "Run a warm start hyperparameter tuning job with type IDENTICAL_DATA_AND_ALGORITHM and set EarlyStoppingType to Auto.",
      "D": "Run a random search tuning job with MaxNumberOfTrainingJobs set to 50 and enable EarlyStoppingType to OfflineStopping."
    },
    "explanation": "A warm start of type IDENTICAL_DATA_AND_ALGORITHM reuses prior tuning results for the same algorithm and search space, and setting EarlyStoppingType to Auto applies SageMaker\u2019s median stopping rule to halt underperforming jobs early."
  },
  {
    "id": "8a309bfbb9e632d8a20d68ec4a4fab48d2f04d117ef037346f687df213f20bef",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A data scientist has fine-tuned a 2 GB PyTorch BERT model for text classification on a GPU instance. The model must now be deployed to a CPU-based inference endpoint with a memory footprint below 500 MB while maintaining at least 95% of its original accuracy. Which approach best meets these requirements with the least development effort?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Export the model to ONNX, write custom quantization code to convert weights to int8, and then deploy the ONNX model on the CPU endpoint.",
      "B": "Compile and optimize the trained PyTorch model using Amazon SageMaker Neo to perform graph optimizations and int8 quantization, then deploy the Neo-compiled model to the CPU endpoint.",
      "C": "Rewrite the model architecture to reduce hidden layer sizes by 50%, retrain from scratch on CPU, and deploy the smaller model.",
      "D": "Use AWS Lambda with a custom container to dynamically load and prune model weights at inference time to reduce memory usage."
    },
    "explanation": "SageMaker Neo automates model graph optimization and precision quantization (e.g., int8), reducing size and improving CPU performance with minimal code changes while preserving accuracy."
  },
  {
    "id": "32d545f0a18abaeebd520595bf1296c3d283c7f0f9f0a9ca3dcbbbd5f4a03f54",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A research team has trained a custom XGBoost model locally and now wants to integrate it into Amazon SageMaker for managed hosting, hyperparameter tuning, and CI/CD pipelines without rewriting the training code. What is the most appropriate way to import and serve this externally trained model in SageMaker?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Package the model artifact into a model.tar.gz, build a custom inference Docker container (BYOC) that loads the artifact, push it to Amazon ECR, and create a SageMaker Model referencing that container and S3 artifact.",
      "B": "Translate the local training code into a SageMaker Script Mode training script, run it in a built-in XGBoost container, and register the new model.",
      "C": "Use the SageMaker SDK to call CreateTrainingJob with LocalMode enabled to import and train the model artifact directly.",
      "D": "Upload the model artifact to Amazon SageMaker Model Registry without a container and deploy it to a serverless endpoint."
    },
    "explanation": "Bringing an externally trained model into SageMaker requires a Bring-Your-Own-Container (BYOC) that contains the inference logic; SageMaker Model Registry alone cannot host artifacts without a container."
  },
  {
    "id": "31fc0a4cb71c0e1ec420dbaf8159e28fbf50846e0f05572d5aa809a583d025f5",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "A financial services company is building a fraud detection model. The base fraud rate in production is 1%. On a validation set of 10,000 transactions (100 frauds, 9,900 non-frauds), two candidate models yield the following metrics:\n\nModel A: precision = 0.667, recall = 0.8\nModel B: precision = 0.8, recall = 0.6\n\nThe business cost of a false positive (FP) is $1 and the cost of a false negative (FN) is $10. Which model has the lower expected cost per 10,000 transactions, and which should the company choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Choose Model A: expected cost = (FP=40\u00d7$1)+(FN=20\u00d7$10) = $240",
      "B": "Choose Model B: expected cost = (FP=15\u00d7$1)+(FN=40\u00d7$10) = $415",
      "C": "Both models have the same cost",
      "D": "Cannot decide without additional metrics"
    },
    "explanation": "Compute TP, FP, FN for each: Model A: TP=0.8\u00d7100=80, FP=80\u00d7(1/0.667\u22121)=40, FN=20 \u2192 cost=40+200=240. Model B: TP=60, FP=60\u00d7(1/0.8\u22121)=15, FN=40 \u2192 cost=15+400=415. Model A yields lower cost."
  },
  {
    "id": "f95f490c946a069e6391883efadb6dbff02866ec0e3b713dcac48e3f1496ffa8",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "An ML engineer needs to detect if a production classification model begins to rely on different features over time due to data drift. Which SageMaker Clarify monitor class should the engineer configure to track changes in feature contributions (for example, SHAP values) between a baseline and production data?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ModelBiasMonitor",
      "B": "ModelExplainabilityMonitor",
      "C": "ModelQualityMonitor",
      "D": "DataQualityMonitor"
    },
    "explanation": "ModelExplainabilityMonitor tracks explainability metrics such as SHAP feature attributions over time. The other monitors handle bias metrics (ModelBiasMonitor), overall prediction quality (ModelQualityMonitor), or input data distributions (DataQualityMonitor)."
  },
  {
    "id": "a488fe097019d57282e57c1a0b635b1058221b0c80b864cf391ff09f94d5ed26",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "A deep neural network trained on SageMaker displays stagnating training loss and high validation loss. The engineer enabled SageMaker Debugger\u2019s default rules and then enabled the vanishing gradient rule, which flagged anomalies in the earliest layers. Which action best addresses the vanishing gradient problem in these layers?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase the global learning rate to accelerate gradient propagation",
      "B": "Replace sigmoid/tanh activations with ReLU (and use He initialization)",
      "C": "Add L2 weight regularization to penalize large weights",
      "D": "Reduce batch size to increase gradient noise and variability"
    },
    "explanation": "Vanishing gradients in early layers are mitigated by using activation functions with constant gradients (ReLU) and appropriate weight initializations (He). Other options (higher learning rate, L2 penalty, smaller batches) do not directly solve vanishing gradient issues."
  },
  {
    "id": "df675febc7b0afeb04848c065d2d356bee58dd57f82adab0ef9ec475c6f2368b",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A fintech firm has developed a credit-scoring ML model (<50 MB) that must serve real-time API requests with no more than 20 ms 95th-percentile latency. Traffic patterns are unpredictable, ranging from 5 to 500 requests per minute. The firm wants to minimize infrastructure management overhead and pay only for the compute capacity it uses, while ensuring consistent low-latency performance. Which SageMaker deployment infrastructure should the ML engineer select?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Deploy the model to a SageMaker serverless endpoint with default concurrency limits.",
      "B": "Deploy the model to a multi-model real-time SageMaker endpoint on a single ml.m5.large instance with auto scaling.",
      "C": "Deploy the model to a SageMaker real-time endpoint on ml.c5.large instances with a provisioned concurrency configuration and target-tracking auto scaling.",
      "D": "Deploy the model to a SageMaker asynchronous inference endpoint with default VCPU provisioning."
    },
    "explanation": "A serverless endpoint can cold-start and breach latency requirements. A single-instance multi-model endpoint cannot guarantee sub-20 ms under unpredictable load. Asynchronous inference is batch-oriented. A real-time endpoint with provisioned concurrency keeps containers warm and uses target-tracking scaling to elastically adjust capacity while maintaining low latency."
  },
  {
    "id": "abd6e1af4756debb4d46ea7ae5974818e339441b96230cae705e00122815c9d5",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An industrial IoT company needs to deploy a vision ML model to 1 000 ARM-based edge cameras with intermittent connectivity. The model must run local inference with latency <50 ms and accept periodic updates without manual intervention. Which deployment infrastructure should the ML engineer choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Containerize the model and deploy it to AWS Lambda functions on AWS Greengrass Core.",
      "B": "Use SageMaker Edge Manager to package and deploy the model directly to AWS IoT devices.",
      "C": "Compile and optimize the model with SageMaker Neo for the target ARM architecture and deploy it to devices using AWS IoT Greengrass.",
      "D": "Deploy the model to a SageMaker real-time endpoint and configure the devices to call the endpoint when connected."
    },
    "explanation": "Lambda on Greengrass incurs container startup and may not meet <50 ms. Edge Manager provides monitoring but still requires a runtime-optimized model. A real-time endpoint requires connectivity. SageMaker Neo compiles and optimizes the model for ARM, and AWS IoT Greengrass handles offline deployment and periodic updates with minimal ops overhead."
  },
  {
    "id": "de1a8a2b42f8a688f0fde115afba1b2bdadf0983d67dbdf58dd8d7ce8d416f4c",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A retail company requires an automated, end-to-end CI/CD pipeline for their ML workflow: data preprocessing, training with hyperparameter tuning, model registration and approval, and blue/green deployment to production endpoints. The pipeline must integrate with a Git repository for version control, provide step-level observability, and support easy rollback. Which orchestrator should the ML engineer select to meet these requirements with minimal custom infrastructure?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS CodePipeline with custom AWS Lambda functions for each ML workflow step.",
      "B": "Amazon Managed Workflows for Apache Airflow (MWAA) with DAGs defining each stage.",
      "C": "AWS Step Functions orchestrating AWS Batch jobs for training and deploying Step Functions tasks.",
      "D": "Amazon SageMaker Pipelines with integrated steps for data processing, training, model registry, approval, and deployment."
    },
    "explanation": "CodePipeline requires significant custom Lambda code for ML steps. MWAA is general-purpose and needs custom operators. Step Functions plus Batch require building and managing additional compute and monitoring. SageMaker Pipelines natively integrates preprocessing, training, tuning, registry, approval, and deployment, supports Git integration, lineage, monitoring, and rollback with minimal custom infrastructure."
  },
  {
    "id": "5ed341518430eea1f5821c35ca570706bd99f702a427409d40a46a32833270c4",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "A machine learning team must deploy a SageMaker real-time multi-model endpoint in private subnets with no internet access. The models are packaged as custom Docker images stored in Amazon ECR. The team wants to automate provisioning of all networking, ECR, and SageMaker resources using infrastructure as code (IaC) with minimal operational overhead, and to enforce coding best practices and unit tests. Which approach best meets these requirements?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "Write an AWS CloudFormation template that defines the VPC, private subnets, ECR repository, SageMaker model, endpoint configuration, and endpoint resources.",
      "B": "Develop an AWS CDK application (in a supported language) that defines the VPC with interface endpoints, ECR repository, SageMaker model, endpoint configuration, and auto scaling policies, and run unit tests against the CDK constructs.",
      "C": "Use an AWS SAM template to define a Lambda function that creates the VPC and provisions the SageMaker endpoint when invoked.",
      "D": "Author a Terraform module to provision the VPC, ECR repository, and SageMaker resources, and manage state in an S3 backend."
    },
    "explanation": "AWS CDK provides a programmable IaC framework with built-in support for unit testing of constructs, reduces YAML/JSON boilerplate compared to raw CloudFormation, and automates resource provisioning including VPC interface endpoints and SageMaker auto scaling policies with minimal operational overhead."
  },
  {
    "id": "a383b9260f95869478cee83d323b0f00217b40faf88e21bc5b2d693cf15a9790",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "A company\u2019s SageMaker real-time endpoint experiences sudden traffic spikes, causing increased latency. The current target-tracking auto scaling policy uses CPUUtilization. The operations team wants to trigger scale-out more quickly on incoming inference requests while avoiding over-provisioning during lulls. Which metric should they use in the target-tracking policy?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CPUUtilization",
      "B": "ModelLatency",
      "C": "InvocationsPerInstance",
      "D": "MemoryUtilization"
    },
    "explanation": "InvocationsPerInstance measures the number of inference requests handled per instance, providing a direct signal of request load and enabling faster scaling on bursts, whereas CPUUtilization and MemoryUtilization lag and ModelLatency may not correlate directly with load volume."
  },
  {
    "id": "9f6104e1f3c0a037f8defe2938f10416325aac255c46eccc7328b16ca2cb0aae",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "An ML engineer needs an automated, end-to-end pipeline that builds custom inference container images on code changes, pushes them to ECR, and updates a SageMaker endpoint to use the new image\u2014all defined as code. Which design has the least operational overhead?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use EventBridge to trigger a CodeBuild project on Git push; CodeBuild builds and pushes the image to ECR; a CloudFormation stack update is triggered manually to point the endpoint to the new image.",
      "B": "Define a CodePipeline pipeline (in AWS CDK) that uses CodeCommit, a CodeBuild action to build/push the image, and a CloudFormation action to update the SageMaker endpoint; deploy the pipeline via the CDK app.",
      "C": "Install AWS CLI scripts on an EC2 instance to poll CodeCommit for changes, build and push the container, then run AWS CLI to update the endpoint.",
      "D": "Use CodePipeline with CodeCommit and a Lambda step to build the container image, then invoke an AWS SAM deployment to update the endpoint."
    },
    "explanation": "A CDK-defined CodePipeline with CodeCommit, CodeBuild, and CloudFormation actions provides a fully managed, declarative CI/CD pipeline with minimal custom code, built-in integration, and automated endpoint updates, reducing operational overhead compared to scripting or Lambda-based workarounds."
  },
  {
    "id": "7ed2f1c92264addcbdd31c7809553cc350d1589f54c7ee345ece69e1334654d2",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "An ML engineer is tasked with building a fully automated CI/CD pipeline for an Amazon SageMaker\u2013based model. The pipeline must: 1) pull code and configuration from a Git repository, 2) run unit tests and data validation, 3) execute data transformation in a SageMaker Processing job, 4) train the model, 5) register the trained model in the SageMaker Model Registry, and 6) perform a canary deployment to production with traffic shifting. The solution must minimize custom Lambda code and leverage native AWS services. Which pipeline architecture best meets these requirements?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS CodePipeline with stages: Source (GitHub), CodeBuild (run tests and data validation), CloudFormation (provision Processing and Training jobs and register model), Manual approval, CloudFormation (update endpoint).",
      "B": "Use AWS CodePipeline with stages: Source (GitHub), CodeBuild (run tests, invoke direct SDK calls to Processing and Training, register model), Manual approval, CodeBuild (use AWS CLI to update endpoint).",
      "C": "Implement a SageMaker Pipeline that includes Processing, Training, Evaluation, and Model Registry steps, and trigger it via EventBridge on git push. Use SageMaker Pipeline\u2019s built-in endpoint update step with canary configuration.",
      "D": "Use AWS CodePipeline with stages: Source (GitHub), CodeBuild (invoke a SageMaker Pipeline via AWS CLI to run Processing, Training, Evaluation, and register the model), Approval, CodeDeploy (configured for SageMaker endpoint canary deployment and traffic shifting)."
    },
    "explanation": "Option D minimizes custom code by delegating ML workflow orchestration to SageMaker Pipelines and uses native CodePipeline stages and CodeDeploy for canary traffic shifting. It cleanly separates CI (CodeBuild) from CD (CodeDeploy) without custom Lambdas."
  },
  {
    "id": "ac8c719fd5d41d19bcbd13ff23a674b7c94cc7df56f92c14f0e86cc9409d4e10",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A new CodePipeline contains a CodeBuild action that uses the AWS CLI to start SageMaker Training and Model Deployment jobs. The CodeBuild project\u2019s IAM role has permissions for sagemaker:CreateTrainingJob, sagemaker:CreateModel, and sagemaker:CreateEndpoint, but the build fails with an AccessDenied error stating that the role cannot be passed. What is the LEAST-privilege IAM change required to allow the CodeBuild stage to succeed?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add iam:PassRole permission for the SageMaker execution role to the CodeBuild project's IAM role policy.",
      "B": "Update the SageMaker execution role trust policy to allow sts:AssumeRole from codepipeline.amazonaws.com.",
      "C": "Add iam:PassRole permission for the CodeBuild project role to the CodePipeline service role policy.",
      "D": "Add sts:AssumeRole permission for the SageMaker execution role in the CodeBuild project's trust policy."
    },
    "explanation": "When CodeBuild calls SageMaker, it must pass the SageMaker service execution role. Granting iam:PassRole on that role in the CodeBuild project's IAM role policy satisfies the least-privilege requirement."
  },
  {
    "id": "88d119e03af0efc4959f7f1404a900ef72b267dabc286ffa65c5fb029a465e8f",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A financial services team uses SageMaker Model Monitor to emit CloudWatch metrics when data drift exceeds thresholds. They need to automatically retrain the model end-to-end (processing, training, evaluation, registry) and roll out the new model with a linear traffic shift as soon as drift is detected. Which combination of AWS services and configurations will satisfy these requirements with minimal custom code?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create a CloudWatch alarm on the Model Monitor drift metric and an EventBridge rule that triggers an AWS CodePipeline pipeline. In CodePipeline, invoke a SageMaker Pipeline (processing, training, evaluation, model registration) and add a CodeDeploy stage configured for SageMaker endpoint linear traffic shifting.",
      "B": "Subscribe an SNS topic to the Model Monitor violation notification and use an AWS Lambda function to start a SageMaker Pipeline and update the endpoint via SDK with gradual traffic shifting.",
      "C": "Configure Model Monitor to directly invoke a SageMaker Pipeline on drift and include a built-in traffic shifting step in the SageMaker Pipeline definition.",
      "D": "Schedule a daily CodeBuild job to query Model Monitor metrics, and if drift is detected, run a training job and invoke a CloudFormation change set to update the endpoint."
    },
    "explanation": "Option A uses native integration: CloudWatch\u2192EventBridge to trigger CodePipeline, SageMaker Pipelines for retraining, and CodeDeploy for linear traffic shifting. This minimizes custom code and leverages managed services end-to-end."
  },
  {
    "id": "ceda3ff25531ba4eb97c2ffe35e4cdec940a00d8f9e554059e848839e38a46d0",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An e-commerce company deployed a recommendation model on SageMaker and needs to monitor both input data drift and model bias in production. They have a baseline training dataset for drift detection and fairness metrics for bias detection. To minimize operational overhead and leverage fully managed monitoring, which solution meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a single SageMaker Model Monitor job with default settings to detect both drift and bias; schedule hourly.",
      "B": "Configure SageMaker Model Monitor (DefaultModelMonitor) to run a data quality monitoring job with baseline statistics and constraints for input features, and SageMaker Clarify ModelBiasMonitor to run a bias monitoring job with baseline fairness metrics; configure both on a daily schedule.",
      "C": "Use SageMaker Clarify's ModelExplainabilityMonitor to detect both data drift and bias by specifying a SHAP baseline, and schedule it hourly.",
      "D": "Use AWS Lambda triggered every hour to run custom Python scripts that compute drift metrics and bias metrics against baselines; send alerts via Amazon SNS."
    },
    "explanation": "SageMaker Model Monitor (DefaultModelMonitor) is optimized for data drift and quality monitoring using baseline statistics and constraints. SageMaker Clarify\u2019s ModelBiasMonitor is designed to monitor bias against fairness baselines. Combining these two fully managed monitors meets both drift and bias requirements with minimal custom code and operational overhead. Options A and C misuse or over-simplify the services, and D introduces unnecessary custom code."
  },
  {
    "id": "f10a29ffd6553875af3890a77a0a59cdff3fff3085da74b9445e34a7074b8dfe",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A SaaS provider hosts a real-time image classification endpoint on Amazon SageMaker. They have observed intermittent spikes in 5XX invocation errors and increased inference latency impacting user experience. The operations team needs a solution with the least operational overhead to automatically detect and alert on these issues in near real time. Which approach should the team implement?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Enable Amazon CloudWatch Logs for the endpoint; configure a metric filter to count '5XX' errors; create CloudWatch alarms for '5XX' error count and for 'ModelLatency' metric.",
      "B": "Configure SageMaker Model Monitor to capture inference requests and responses; schedule a data quality monitoring job every 5 minutes with a custom script to check for errors and latency; publish custom metrics to CloudWatch.",
      "C": "Configure DataCaptureConfig on the endpoint to capture all invocations to S3; create an AWS Lambda function triggered by S3 events that calculates error rates and latency; publish custom CloudWatch metrics and alarms.",
      "D": "Use SageMaker Clarify ModelExplainabilityMonitor to detect anomalies in output embeddings, which will indirectly detect errors and latency issues; schedule it on an hourly basis."
    },
    "explanation": "SageMaker endpoints emit built-in CloudWatch metrics such as Invocation5XXErrors and ModelLatency. Creating CloudWatch alarms on these metrics provides near real-time monitoring with zero custom code and minimal overhead. Model Monitor (B) and Lambda/S3 solutions (C) introduce unnecessary complexity, and Clarify (D) is not designed for error or latency monitoring."
  },
  {
    "id": "273694fd51c0683a8803b7e05bd27081b3f9f1c1acb6262680bbf23657f034a7",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A financial analytics team deployed a credit-scoring model on SageMaker real-time endpoints. They want to detect univariate feature drift in critical numeric input features (such as \u2018annual_income\u2019 and \u2018debt_ratio\u2019) and alert if the distribution has shifted beyond acceptable thresholds. They already have training data to serve as a baseline. Which SageMaker feature and configuration should the team use to meet this requirement with minimal custom coding?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure SageMaker Clarify ModelBiasMonitor with the training dataset as a baseline and set the 'drift_detection' parameter for numeric features; schedule hourly.",
      "B": "Use SageMaker Model Monitor's DefaultModelMonitor to create a MonitorSchedule with a DataQualityJobDefinition that specifies baseline_statistics and baseline_constraints created from the training dataset; set the monitoring schedule to run hourly.",
      "C": "Deploy a Python-based Lambda function that loads the training dataset baseline, fetches the latest batch of inference requests, computes Kolmogorov\u2013Smirnov tests for each numeric feature, and sends alerts via Amazon SNS.",
      "D": "Use Amazon CloudWatch Metrics Insight to query the 'InputDataMean' metric for each feature and configure alarms when mean deviates by more than a threshold from the training data mean."
    },
    "explanation": "SageMaker Model Monitor\u2019s DefaultModelMonitor supports univariate feature drift detection by automatically generating baseline_statistics and baseline_constraints from training data, and scheduling the monitoring job. This fully managed solution minimizes custom coding. Clarify\u2019s ModelBiasMonitor (A) is for fairness, not general drift, and options C and D require significant custom development or rely on unsupported metric calculations."
  },
  {
    "id": "03ef03d52057a2c84ec3429444d82ed5fff902740a6fd8c53a49b3ca57c50b46",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A company hosts a SageMaker real-time inference endpoint that experiences predictable daily traffic spikes at 9 AM and 6 PM, plus unpredictable fluctuations throughout the day. The ML engineer needs to minimize costs during off-peak hours while ensuring the endpoint maintains a p95 latency below 100 ms. The solution must use built-in AWS services and require minimal custom code. Which configuration should the engineer implement?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure a step scaling policy in Amazon CloudWatch that scales out when CPU Utilization > 70% and scales in when CPU Utilization < 30%; set MinCapacity=1 and MaxCapacity=10.",
      "B": "Deploy an AWS Lambda function triggered by Amazon EventBridge schedule rules at 8:50 AM and 5:50 PM to call UpdateEndpointWeightsAndCapacities, and use a target tracking policy for unpredictable loads.",
      "C": "Define an Application Auto Scaling target tracking policy on the SageMaker endpoint based on p95 latency with MinCapacity=1, MaxCapacity=10, and add a scheduled scaling action to increase capacity to 8 at 8:50 AM and 5:50 PM.",
      "D": "Use Spot Instances by converting the endpoint to asynchronous inference, configure Spot for inference to de-provision overnight, and rely on target tracking for bursts."
    },
    "explanation": "Option C leverages built-in Application Auto Scaling for SageMaker to handle both scheduled and dynamic scaling in a single service with minimal custom code. Step and Lambda solutions are more complex and less precise; asynchronous/Spot inference cannot guarantee real-time p95 latency."
  },
  {
    "id": "2397d5ecc86a79b2db0c6f88036379a3d81c7974f8702f0ab49abd448d566046",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "An ML engineer needs to optimize cost and performance of multiple SageMaker real-time inference endpoints. Each endpoint has different traffic patterns and latency requirements. The engineer wants to identify the most cost-effective instance type for each endpoint, based on sample inference payloads, throughput, and latency SLOs. Which tool or workflow should the engineer use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Compute Optimizer to generate EC2 instance recommendations and manually map those to SageMaker endpoints.",
      "B": "Use SageMaker Inference Recommender to run profiling jobs with sample payloads and choose the instance types with the lowest cost per inference that meet the SLOs.",
      "C": "Use AWS Trusted Advisor to forecast inference endpoint costs and recommend Reserved Instances.",
      "D": "Use AWS Cost Explorer\u2019s RI purchase recommendations to apply savings to SageMaker endpoint instance types."
    },
    "explanation": "SageMaker Inference Recommender is designed to profile real-world inference workloads and recommend the optimal instance types for cost and performance. Compute Optimizer and Trusted Advisor do not profile SageMaker workloads at the application level."
  },
  {
    "id": "6b1f0fcfbd7cec4931c0836d754129b7713fbb8279916e037d0a216341701787",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A global team uses SageMaker across 10 AWS accounts under consolidated billing. They must enforce that every SageMaker training job, endpoint, and pipeline carries the tags Project and CostCenter, automatically deny creations without them, and generate a centralized monthly cost dashboard per project. Which solution meets these requirements with the least operational overhead?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy AWS Config rules in each account to audit SageMaker resource tags, use AWS Service Catalog with TagOptions to solicit the tags, and build QuickSight dashboards from Cost Explorer filtered by tags.",
      "B": "Implement AWS Organizations Tag Policies to require Project and CostCenter tags on SageMaker resources, enable AWS Config for compliance evaluation, and use Cost Explorer/AWS Budgets with tag filters for centralized dashboards.",
      "C": "Create Service Control Policies to forbid creation of untagged SageMaker resources, use CloudTrail + Lambda to remediate missing tags, and aggregate costs via Cost Explorer.",
      "D": "Enforce all SageMaker deployments through AWS CloudFormation StackSets with mandatory tags in the templates, and view costs in the AWS Billing console."
    },
    "explanation": "AWS Organizations Tag Policies provide native, centralized enforcement of required tags across accounts with minimal operational effort. Combined with AWS Config compliance checks and Cost Explorer/AWS Budgets, this delivers automated enforcement and centralized cost reporting. Other options require custom code or more operational overhead."
  },
  {
    "id": "89ff859873f64f751369b9e16d9757e43fcf62b4b7a43bf44e0df0796ce8cb6b",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "An enterprise mandates that all SageMaker training and inference workloads must run within a secured VPC without any public internet access. However, these workloads need to read training data from Amazon S3, pull container images from Amazon ECR, write logs to Amazon CloudWatch Logs, and use a customer-managed AWS KMS key. The security team requires the solution to minimize ongoing maintenance overhead while enforcing least-privilege network connectivity. Which architectural configuration meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable SageMaker network isolation and launch jobs outside any VPC; rely on network isolation to prevent internet access.",
      "B": "Launch SageMaker jobs inside a VPC with no internet gateway, configure interface VPC endpoints for S3, ECR (both API and Docker), CloudWatch Logs, and KMS, and remove any NAT gateways.",
      "C": "Launch SageMaker jobs in the default VPC and attach a security group that blocks 0.0.0.0/0; use public endpoints for AWS services.",
      "D": "Use a network ACL on the private subnet to deny all outbound traffic; allow S3 and ECR access by whitelisting their public IP ranges."
    },
    "explanation": "Enabling interface VPC endpoints for S3, ECR, CloudWatch Logs, and KMS in a VPC without an Internet Gateway ensures workloads have private, least-privilege access to required AWS services and keys, while preventing any public internet access. Network isolation alone (A) doesn\u2019t provide VPC-private access to AWS services. Security groups cannot filter based on AWS service endpoints (C). NACLs cannot reliably allow AWS service traffic by IP (D)."
  },
  {
    "id": "245a94359282ff1b76614c46588f4f29e01cdb612ec3bb81211ef32e0c4b2de4",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A company\u2019s sensitive training data is stored in an S3 bucket encrypted with a customer-managed KMS key (DataKey). They must run SageMaker training jobs that decrypt the data and write model artifacts to a separate S3 bucket encrypted with another KMS key (OutputKey). The SageMaker execution role must follow the principle of least privilege. Which combination of IAM role policy statements and KMS key policy statements satisfies this requirement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "IAM role: Grant s3:GetObject and s3:PutObject on both buckets and kms:Decrypt, kms:Encrypt on both keys. KMS key policies: Trust the entire account principal for all operations.",
      "B": "IAM role: Grant s3:GetObject on the data bucket; s3:PutObject on the artifact bucket; kms:Decrypt and kms:GenerateDataKey on DataKey; kms:Encrypt and kms:GenerateDataKey on OutputKey. KMS key policies: Specify the SageMaker execution role as the only principal allowed to use each key.",
      "C": "IAM role: Grant s3:* on both buckets and kms:* on both keys. KMS key policies: No changes needed (account admins manage keys).",
      "D": "Use S3 bucket policies to grant SageMaker service principal full access, and KMS grants to allow any IAM principal in account to decrypt keys."
    },
    "explanation": "Least-privilege requires the IAM role only have s3:GetObject on the data bucket, s3:PutObject on the output bucket, and only the specific KMS operations. The key policies must explicitly allow the SageMaker execution role to use each key. Over-permissive (A, C) or relying on bucket policies alone (D) violates least-privilege."
  },
  {
    "id": "aca264a7d6cb8e4e254cbfaf2c4d0ed2ff81e459a3edc8531c0760d1bb9067fa",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A security team manages multiple AWS accounts using AWS Organizations. They want to ensure that data scientists in member accounts can only create, update, or delete SageMaker endpoints within their own accounts, and prevent any SageMaker endpoint operations in accounts outside their OU. Which mechanism should be implemented at the organization level to enforce this restriction?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use an AWS Organizations service control policy (SCP) at the OU level to Deny all sagemaker:CreateEndpoint, UpdateEndpoint, and DeleteEndpoint actions when the resource\u2019s account ID does not match the requesting account.",
      "B": "Apply IAM permission boundaries to the data scientists\u2019 roles in each member account that restrict sagemaker:* to specific endpoint ARNs.",
      "C": "Attach a resource-based policy to each SageMaker endpoint specifying allowed IAM principals from the same account.",
      "D": "Use the SageMaker service-linked role policy to restrict endpoint operations to certain accounts."
    },
    "explanation": "An AWS Organizations SCP can centrally deny SageMaker endpoint management actions across accounts outside the OU, enforcing the requirement. IAM permission boundaries (B) must be set per role and cannot prevent actions outside the OU. SageMaker endpoints do not support resource-based policies for management operations (C). Service-linked roles cannot enforce cross-account restrictions (D)."
  },
  {
    "id": "1d1890cccead8f324f72b7f8932769e5cb21c2b33a5168c80165b90689134518",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A fintech company needs to ingest 1 million financial transaction events per second into Amazon SageMaker Feature Store for real-time fraud detection. The solution must support low latency, horizontal scalability, and minimal data loss. Which ingestion architecture meets these requirements?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "Use Amazon Kinesis Data Firehose to deliver events to S3 and batch import into SageMaker Feature Store Offline Store.",
      "B": "Use Amazon Kinesis Data Streams with enhanced fan-out consumers and AWS Lambda functions invoking the PutRecord API to ingest into the SageMaker Feature Store Online Store.",
      "C": "Use Amazon MSK with brokers backed by EBS volumes and use a Kafka consumer to push records into the Feature Store offline bucket.",
      "D": "Use AWS IoT Core to route transaction events to SageMaker Feature Store via a custom AWS Lambda integration."
    },
    "explanation": "Real-time, low-latency ingest into the Feature Store Online Store requires Kinesis Data Streams (for horizontal scaling and enhanced fan-out) combined with Lambda invoking the PutRecord API. Firehose and batch imports do not meet real-time SLAs; MSK and IoT Core approaches add unnecessary complexity and latency."
  },
  {
    "id": "241db487428d0a45e146e611ccbd958bb4a8120bcb2e9e84470e5aca02632424",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An analytics team has 200 GiB of nested JSON log files in Amazon S3. They require interactive profiling and transformation in SageMaker Data Wrangler with minimal cost and latency. Which file format should they convert the data into before ingestion?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CSV files (flattening nested structures into columns).",
      "B": "Apache Parquet with nested column support and predicate pushdown.",
      "C": "Avro files with JSON serialization.",
      "D": "RecordIO binary format."
    },
    "explanation": "Parquet is a columnar storage format with efficient compression, nested column support, and predicate pushdown, which significantly reduces I/O and speeds interactive profiling in Data Wrangler. CSV lacks nested support, Avro is row-oriented, and RecordIO is optimized for deep learning, not interactive ETL."
  },
  {
    "id": "9ed3b54e9acb03161623e311a656887ef55ae931dbd99651b2675f17b27eaf72",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An ML engineer must merge hourly transaction updates from Amazon RDS and real-time clickstream events from DynamoDB Streams into a single dataset for training in SageMaker Data Wrangler, while automatically handling schema evolution in both sources. Which ingestion solution should the engineer choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Schedule an AWS Glue ETL job to export RDS to S3 and join with Lambda-pushed DynamoDB data once per hour.",
      "B": "Use SageMaker Data Wrangler\u2019s built-in connectors for Amazon RDS and for DynamoDB streams to import and join data in a single flow.",
      "C": "Provision an EMR Spark cluster to read from both sources nightly and write merged Parquet to S3.",
      "D": "Use AWS DMS to replicate both RDS and DynamoDB to Redshift and join via Redshift Spectrum in Data Wrangler."
    },
    "explanation": "Data Wrangler connectors natively support incremental read and schema inference for RDS and DynamoDB streams, simplifying merges and automatically handling schema changes. Glue jobs or EMR require more operational overhead, and DMS replication to Redshift adds extra cost and latency."
  },
  {
    "id": "4cd782867d83c672c7bf4c074045a3beed39bb3824fd2df1a6c57bf878301a41",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A genomics research team requires sub-millisecond file I/O and high throughput for a 500 GiB NFS-based dataset during preprocessing before model training. Which AWS storage service should they provision to meet performance and POSIX compatibility requirements?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "Amazon EFS One Zone SSD backed by NFS.",
      "B": "Amazon FSx for Lustre file system with link to S3.",
      "C": "Amazon FSx for NetApp ONTAP with SSD volumes.",
      "D": "Amazon S3 bucket mounted via VPC endpoint."
    },
    "explanation": "FSx for Lustre delivers the lowest POSIX read-latencies (sub-millisecond) and highest throughput for large NFS-based datasets. EFS has higher latencies; FSx ONTAP adds data management features but lower throughput; S3 is object storage, not POSIX."
  },
  {
    "id": "cf6d416533978918f00319e6b4f5b64e8448a20393ca7f58525cd54d48f8f005",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An e-commerce company experiences high startup latency for SageMaker training jobs reading hundreds of small CSV files (~5\u201310 MiB each) from S3. Without modifying the training code, which solution will most reduce startup time?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable Amazon S3 Transfer Acceleration on the bucket.",
      "B": "Use AWS Glue to consolidate small CSV files into larger Parquet files and update the S3 prefix.",
      "C": "Upgrade to a SageMaker instance with higher network throughput.",
      "D": "Change the training input mode to Pipe mode."
    },
    "explanation": "Consolidating small files into fewer, larger Parquet files minimizes S3 list and open calls, dramatically reducing startup latency without code changes. Transfer Acceleration and network upgrades do not address metadata overhead; Pipe mode still requires metadata enumeration."
  },
  {
    "id": "736a4d341d1e6a0f8aaa310d6547c76a786e1ec6c7e6a0b6b3cc91b861f9c078",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A media streaming platform requires exactly-once processing of real-time events into its ML ingestion pipeline for feature computation. Which combination of services and configurations provides this guarantee?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Amazon Kinesis Data Firehose with a retry buffer.",
      "B": "Amazon Kinesis Data Streams with Enhanced Fan-Out (guarantees at-least-once).",
      "C": "Amazon MSK (Kafka) configured with idempotent producers and transactional writes.",
      "D": "DynamoDB Streams with AWS Lambda and conditional writes to downstream store."
    },
    "explanation": "Kafka with idempotent producers and transactional writes supports exactly-once semantics end-to-end. Kinesis Data Streams and Firehose only guarantee at-least-once; DynamoDB Streams plus Lambda may still result in duplicates without complex idempotency logic."
  },
  {
    "id": "37909f44f047549cd20042fdefa76fe5edf2ee468cd9978dfcb79bf99e32870b",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An ML pipeline ingests unvalidated CSV files into Amazon S3, but inconsistent schemas and missing columns cause downstream failures. Which AWS service can enforce schema validation during ingestion and alert on violations?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Glue Schema Registry with schema-validation-enabled data producers.",
      "B": "AWS Config rules for S3 object schemas.",
      "C": "Amazon Macie classification jobs.",
      "D": "AWS Lake Formation schema enforcement."
    },
    "explanation": "AWS Glue Schema Registry can define, register and enforce schemas at ingestion for streaming and batch, and can reject or alert on malformed records. Lake Formation handles data access control, not row-level schema validation."
  },
  {
    "id": "488323a43041a6a1531d148654d117daf583c8ca8bf692256b94f7713b424896",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A large oil company must migrate 100 TiB of POSIX file-system data to Amazon S3 within 48 hours, with incremental file changes tracked and minimal management overhead. Which service meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Snowball Edge device shipment.",
      "B": "AWS DataSync configured for incremental sync to S3.",
      "C": "Amazon S3 Transfer Acceleration over the internet.",
      "D": "AWS Storage Gateway file gateway."
    },
    "explanation": "AWS DataSync optimizes incremental transfer of POSIX files to S3 with built-in scheduling and encryption, suitable for large, continuously changing data. Snowball has shipment delays; Transfer Acceleration and Storage Gateway don\u2019t handle incremental POSIX sync as efficiently."
  },
  {
    "id": "fbafc217d0ad9153ca67cd22f33cf477bde1828a7925b684823f11cf18e9b622",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An ML engineer wants to profile, clean, and transform streaming IoT telemetry data directly in SageMaker Data Wrangler without intermediate storage. Which ingestion method should the engineer use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use the Amazon Kinesis Data Streams connector in Data Wrangler to ingest data in real time.",
      "B": "Upload data to S3 and use the S3 connector for batch profiling.",
      "C": "Route data through AWS IoT Analytics to an S3 channel and ingest.",
      "D": "Crawl raw data with AWS Glue and import the Glue table."
    },
    "explanation": "The Kinesis Data Streams connector in Data Wrangler enables real-time profiling and transformation of streaming data. Other options introduce batch windows or extra services, increasing latency and operational overhead."
  },
  {
    "id": "7e98c36cfe4069d019683edeacc5c3d7af3446ea9d6c286ae59915d3e18c5bf5",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "When mounting an Amazon EFS file system to a SageMaker Studio notebook in a VPC, which configuration step ensures secure, low-latency access?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Mount the EFS file system over the public internet using TLS.",
      "B": "Create EFS mount targets in the same VPC subnets as the SageMaker notebooks and attach security groups allowing NFS traffic (TCP/2049).",
      "C": "Use AWS Direct Connect to mount the EFS file system from on-premises.",
      "D": "Peer the notebook VPC to another VPC containing EFS and mount through a transit gateway."
    },
    "explanation": "To achieve secure, low-latency NFS access from Studio in the same VPC, you must create EFS mount targets in the same subnets and open NFS ports in security groups. Public mounts or cross-VPC peering add latency and complexity."
  },
  {
    "id": "5106f8840237539ad3f01235800c5faa1d99619a55c7f94a81c1ff0dbc4b3336",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A cross-account pipeline must ingest data from an S3 bucket in Account A into SageMaker Studio in Account B. To minimize privileges and management, which approach should be used?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a bucket ACL granting Account B the s3:GetObject permission.",
      "B": "Create an IAM role in Account B with a trust policy for Account A\u2019s principal, and update the bucket policy to allow s3:GetObject for that role.",
      "C": "Use an AWS Organizations SCP to allow cross-account access.",
      "D": "Set up VPC peering between the two accounts and access the bucket over the interface endpoint."
    },
    "explanation": "Creating a cross-account IAM role in Account B with trust from Account A and granting that role permission in the bucket policy provides least-privilege, auditable access. Bucket ACLs are legacy, and Organizations SCP/VPC peering do not directly enable S3 access control."
  },
  {
    "id": "993eb70043291e5c1eb8a436b712c919299df724a733efb7cc62909cb31daf68",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A data ingestion job reading from S3 into SageMaker notebooks is intermittently throttled with HTTP 503 SlowDown errors. Which approach will most effectively reduce these errors while maintaining high throughput?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Implement exponential backoff with jitter in the client\u2019s S3 request retry logic.",
      "B": "Enable S3 Transfer Acceleration to optimize network path.",
      "C": "Configure the bucket as requester-pays.",
      "D": "Increase the IAM request quota for the S3 service."
    },
    "explanation": "503 SlowDown errors indicate S3 throttling; the recommended mitigation is implementing exponential backoff with jitter to retry requests gracefully. Transfer Acceleration and requester-pays do not address throttling, and IAM quotas are unrelated."
  },
  {
    "id": "09795333b1552609614442835b422cc2994b9edd5158ebc8b5aaf8cf5ce387fa",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An Amazon SageMaker training job mounts an Amazon FSx for Lustre file system backed by an 8 TiB S3 data repository. After heavy parallel reads, the engineer observes metadata lookup latencies >100 ms for POSIX operations. How can the engineer optimize metadata performance?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase the Lustre file system\u2019s throughput capacity to provision more metadata IOPS.",
      "B": "Switch the workload to Amazon EFS for metadata caching.",
      "C": "Directly mount the underlying S3 bucket instead of Lustre.",
      "D": "Implement a local cache on the training instance\u2019s EBS volume."
    },
    "explanation": "FSx for Lustre provides scalable metadata throughput proportionate to the configured throughput capacity. Increasing capacity raises available metadata IOPS and reduces latency. EFS and direct S3 mounts do not meet the sub-millisecond metadata requirements."
  },
  {
    "id": "8b01989292377991fed54aa3f3a56c070603825a27c92059e67dbec4e394a9f5",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A data science team must join millions of small S3 objects with a 50 GiB Redshift table for feature engineering before training. The join must complete in the lowest end-to-end latency. Which ingestion and compute architecture should they choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run an AWS Glue ETL job to merge the data into a Parquet file and reload into SageMaker.",
      "B": "Use Redshift Spectrum to query the S3 objects in place and join directly with the local Redshift table.",
      "C": "Provision an EMR Spark cluster to read both sources and write joined output to S3.",
      "D": "Use Athena CTAS to precompute the join and then read from S3 in SageMaker."
    },
    "explanation": "Redshift Spectrum can directly query external S3 data and join with internal tables, minimizing data movement and achieving the lowest latency. Glue, EMR, and Athena introduce additional data shuffles or write phases, adding latency."
  },
  {
    "id": "220c0f4baf9efd3b426bab0801ae89f3099cd76876256d96d5422be6c1a14209",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "When performing offline ingestion into SageMaker Feature Store, which file format and partitioning scheme will maximize batch import throughput and query efficiency?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A single unpartitioned CSV file containing all records.",
      "B": "Many small gzipped JSON files partitioned by feature group name.",
      "C": "Parquet files partitioned by ingestion date (yyyy/mm/dd) and hour.",
      "D": "Avro files with no partitions."
    },
    "explanation": "Partitioning Parquet files by date and hour enables parallel reads by the batch import process and efficient predicate pushdown, maximizing throughput and query efficiency. Single large CSVs or unpartitioned formats become bottlenecks; Avro lacks columnar benefits."
  },
  {
    "id": "2a52f2f685cd417df300982bc567caf5fbf4aeb202c568673e8679668426497d",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You have a numeric feature in your dataset that is strongly right-skewed. You plan to preprocess the data in SageMaker Data Wrangler and then train a linear regression model. Which sequence of transformations in Data Wrangler will best reduce skewness and satisfy the assumptions of linear regression?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Standardize the feature first, then apply a log transform",
      "B": "Apply a log transform first, then standardize the resulting values",
      "C": "Min\u2013max scale the feature first, then apply a log transform",
      "D": "One-hot encode the feature, then normalize the encoded columns"
    },
    "explanation": "A log transform first reduces skew, and subsequent standardization yields zero mean and unit variance, aligning with linear regression assumptions."
  },
  {
    "id": "3635354f7f482fd6f50ddac74e6d37d7ccb4f644a798b46a3f59a094427c4a45",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "Your streaming sensor pipeline ingests data via Kinesis Data Streams. You need to filter out invalid readings and impute missing values in real time with sub-second latency. Which solution provides the lowest operational overhead and meets performance requirements?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Kinesis Data Analytics SQL to filter and invoke Lambda for imputation",
      "B": "Use a Lambda function to filter and then call SageMaker Data Wrangler batch job for imputation",
      "C": "Use Kinesis Data Analytics for Apache Flink to filter and impute within the streaming application",
      "D": "Use AWS Glue streaming ETL job to filter and impute before writing to S3"
    },
    "explanation": "Kinesis Data Analytics for Apache Flink supports stateful, low-latency streaming transforms including filter and impute within the service, minimizing custom infrastructure."
  },
  {
    "id": "924336d4ad4d37d14095452d3f1e572272d1445bf19fea9451cf934481443a02",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A categorical feature has 5,000 unique values. You intend to use a tree-based algorithm and want to avoid excessive dimensionality. Which encoding technique should you apply using SageMaker Data Wrangler?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "One-hot encoding of all categories",
      "B": "Label encoding (assigning integer codes)",
      "C": "Frequency encoding (replace category with its occurrence frequency)",
      "D": "Hashing trick with a large hash space"
    },
    "explanation": "Frequency encoding reduces cardinality to a single numeric value per category, retaining information without high-dimensional expansion."
  },
  {
    "id": "7ee1c5baedbd7451b67261ce76a154c87ec5bf4e23903d3216858cf519f531b9",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You need to remove duplicate records from two large S3 datasets in preparation for feature engineering. Which AWS Glue Spark ETL approach ensures schema enforcement and minimal data loss?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Glue DynamicFrame.drop_duplicates on the combined DynamicFrame",
      "B": "Convert to Spark DataFrame and call DataFrame.distinct()",
      "C": "Use a SageMaker Processing job with custom dedupe code",
      "D": "Use a Glue DataBrew recipe step to drop duplicates"
    },
    "explanation": "Glue DynamicFrame.drop_duplicates preserves schema and metadata, integrates with the Glue catalog, and scales automatically."
  },
  {
    "id": "8d6eead6870ceee39c75aac6db5a3c957efce5b67dbc682e8fdc33605d359129",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You need to detect and remove outliers beyond 3 standard deviations for a numeric column using AWS Glue DataBrew. Which recipe step accomplishes this with minimal effort?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Add a Filter rows step with condition > mean + 3*stddev",
      "B": "Use the built-in Remove outliers step and set the z-score threshold to 3",
      "C": "Add a Cluster rows step and drop the smallest cluster",
      "D": "Use Impute missing step to replace values beyond 3 standard deviations"
    },
    "explanation": "The Remove outliers step in DataBrew natively handles z-score thresholds, detecting and dropping extreme values automatically."
  },
  {
    "id": "c940655260368df5703b2d0b942425e11577f454df86e6c2bb1d910b1ec110ce",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "Your team needs to implement a custom feature transformation in a visual pipeline that includes Python code (e.g., complex binning logic). Which SageMaker tool supports this requirement and collaborative workflows?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Glue DataBrew",
      "B": "SageMaker Data Wrangler",
      "C": "AWS Glue Python shell job",
      "D": "Spark on Amazon EMR notebook"
    },
    "explanation": "Data Wrangler provides a visual flow with built-in support for custom Python steps, versioning, and collaboration in Studio."
  },
  {
    "id": "d86f91d26fec83f60aebd90d097171e9622e12f32b93277f5ace36491a4e721d",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You need to compute a 7-day rolling mean on a time-series feature for millions of records. Which approach in an AWS managed service offers the highest throughput with minimal code?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run a SageMaker Processing job with PySpark window functions",
      "B": "Write a Glue Spark ETL job using Spark SQL window functions",
      "C": "Use SageMaker Data Wrangler\u2019s rolling statistics transform",
      "D": "Use Lambda functions with batched DynamoDB lookups"
    },
    "explanation": "Data Wrangler\u2019s built-in rolling statistic transform executes efficiently in a managed environment without custom code or cluster management."
  },
  {
    "id": "092d96cff9ab9d41f595f51d878a092af8fc695381e3c2ed7ab32d2799c20987",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You have a free-text column that needs tokenization into word tokens and conversion into integer IDs for embedding. Which Data Wrangler transformation sequence should you apply?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use the Tokenize step (word level) and then Encode step (label encoding)",
      "B": "Use TextVectorization step with TF-IDF output",
      "C": "One-hot encode the text column directly",
      "D": "Write a custom PySpark transform in SageMaker Processing"
    },
    "explanation": "The Tokenize step splits text into words, and Label Encoding maps each token to a unique integer ID, preparing for embeddings."
  },
  {
    "id": "b96fa7b6b51cb41ba695172365bf6cabf4c96fa1535fd2cf69aa0e9195354b01",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "Before training, you must enforce schema constraints (data types, value ranges) programmatically. Which AWS feature will you use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Glue Data Quality rules leveraging Deequ",
      "B": "SageMaker Data Wrangler profile job",
      "C": "DataBrew column profiling summary",
      "D": "Glue Data Catalog crawler validation"
    },
    "explanation": "Glue Data Quality uses Deequ to define and evaluate table constraints and thresholds in code, enforcing schema at scale."
  },
  {
    "id": "8b95f04ec2b21205446fc1fc9bc855aa6001599fc73d0c35126faaf98ccfff4e",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You plan to store engineered features for real-time inference in SageMaker Feature Store. Which Data Wrangler export option supports batching data into a Feature Group?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Use a SageMaker Processing job to call PutRecord API",
      "B": "Use a Lambda function to batch and call PutRecordBatch",
      "C": "Export directly from Data Wrangler to SageMaker Feature Store",
      "D": "Upload CSV to S3 and configure offline store only"
    },
    "explanation": "Data Wrangler has a direct export feature to batch-write transformed data into a Feature Store Feature Group."
  },
  {
    "id": "a6370e9392d74cd5fa3a324a44e0c0bc8d1ef77e4d16ecadc01713156b9496e6",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You need to reduce a numeric feature\u2019s cardinality to 100 bins based on equal-frequency intervals using a no-code solution. Which DataBrew step do you choose?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Group by step with custom quantile aggregation",
      "B": "Bin numeric values step with equal-frequency option",
      "C": "Cluster rows step with K-means clustering",
      "D": "Custom recipe with Python UDF"
    },
    "explanation": "The Bin numeric values step with equal-frequency creates quantile-based bins automatically, reducing cardinality."
  },
  {
    "id": "538f043f0f6c11b5dfcd8759e7229b1e4e1264bf649f0b46ac3d404a73f99d11",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "To impute missing age values by country group using DataBrew, which step sequence is correct?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use Partition by country, then Impute missing step with median strategy",
      "B": "Aggregate to compute medians, then perform a join back",
      "C": "Apply global median imputation for the age column",
      "D": "Export data and impute in SageMaker Processing"
    },
    "explanation": "Partitioning by country followed by median imputation uses group-wise statistics directly in DataBrew with minimal extra steps."
  },
  {
    "id": "e2d6ccbe0b247bdd3829f662c21b2953418cde3434fa30f81141bce9b0ce7af7",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "Two 1 TB CSV files in S3 must be inner-joined on a composite key before feature engineering. You want a serverless, managed service that handles scaling and scripting. Which approach is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run an AWS Glue ETL job using DynamicFrame.join",
      "B": "Launch an EMR cluster with custom PySpark code",
      "C": "Use a SageMaker Processing PySpark job",
      "D": "Perform a CTAS join in Amazon Athena"
    },
    "explanation": "AWS Glue ETL with DynamicFrame.join provides serverless scaling, schema enforcement, and integration with the Glue Data Catalog."
  },
  {
    "id": "bdb904b9aa5721edee6d6b90c543c86ee2ef6408434636683673023f536c8efa",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You need to anonymize email addresses in real time within a Kinesis Data Stream, ensuring <200 ms processing per record. Which architecture meets this requirement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Glue streaming ETL job with built-in masking",
      "B": "Kinesis Data Analytics for Apache Flink with a UDF to hash emails",
      "C": "AWS Lambda subscribed to the stream invoking a hashing library",
      "D": "SageMaker Data Wrangler in streaming mode"
    },
    "explanation": "Kinesis Data Analytics for Flink executes in-stream UDFs with low, consistent latency, meeting sub-200 ms requirements without cold starts."
  },
  {
    "id": "2e1fdd6cfdcd8b8836f8cdf20305a7476394168c7122fa8cbd07e8e2c034703e",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "Duplicate events may occur within a 2-minute window in your Kinesis stream. You need to drop duplicates in real time before storing to S3. Which solution is most appropriate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Kinesis Data Analytics for Flink, keyBy event ID and apply a 2-minute deduplication window",
      "B": "Configure an AWS Glue streaming job with dedupe enabled",
      "C": "Trigger a Lambda function per record and check DynamoDB for prior IDs",
      "D": "Run an EMR streaming job with custom dedupe code"
    },
    "explanation": "Kinesis Data Analytics for Flink supports stateful windowed deduplication on event keys with minimal infrastructure overhead."
  },
  {
    "id": "3e3104bd7ef0dceed4bbf787b51c1716829178863a48c20a3db56863f935d9d5",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "An ML engineer needs to set up automated, scheduled data quality checks on an Amazon S3 dataset to validate completeness, uniqueness, and detect numeric outliers over time. Which solution meets these requirements with the LEAST operational overhead?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Use AWS Glue DataBrew to author a recipe with transformations and schedule it as a DataBrew job.",
      "B": "Use AWS Glue Data Quality to define data quality rules and configure a schedule to run them.",
      "C": "Use SageMaker Clarify DataQualityCheckConfig in a SageMaker Processing job triggered by EventBridge.",
      "D": "Use Amazon Athena SQL queries inside a Lambda function scheduled by EventBridge."
    },
    "explanation": "AWS Glue Data Quality is designed for automated, rule-based validation and scheduling of data quality checks with minimal operational overhead."
  },
  {
    "id": "62f85fba101c774f597783609370214df8a61366cf436c2ae0f357aa3f5f7989",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A data scientist must detect pre-training label disparity between male and female groups in a binary classification dataset. Which SageMaker Clarify metric should be used?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "pre_training_bias:CI (class imbalance)",
      "B": "pre_training_bias:DPL (difference in proportions of labels)",
      "C": "post_training_bias:CI",
      "D": "post_training_bias:DPL"
    },
    "explanation": "Difference in proportions of labels (DPL) is the appropriate pre-training metric to measure label distribution disparity between protected groups."
  },
  {
    "id": "d630c09c8e99cd9a2dbd44175c42fb2f8f378767390107583d48e83fceec4037",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A dataset for fraud detection has severe class imbalance (0.5% positives). The engineer wants synthetic minority oversampling before training. Which AWS service/plugin should be used to implement SMOTE?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Specify class_weight in the training script and rely on model loss adjustment.",
      "B": "Use AWS Glue DataBrew sampling transform to oversample the minority class.",
      "C": "Use a SageMaker Processing job that implements imbalanced-learn\u2019s SMOTE algorithm.",
      "D": "Use SageMaker Clarify to automatically generate synthetic samples."
    },
    "explanation": "A SageMaker Processing job allows you to run custom code (e.g., imbalanced-learn) to generate SMOTE synthetic samples before training."
  },
  {
    "id": "fbeae12818f504d085a78c304502c41b39c662c008c31a61bd29888ca154cf41",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A tabular dataset containing PII must be anonymized before model training. The engineer wants to mask names, emails, and SSNs in CSV files with minimal coding. Which solution should be implemented?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use AWS Glue DataBrew recipe steps with the built-in PII Mask transform on the relevant columns.",
      "B": "Write a Lambda function to scan each file in S3, mask PII, and write back to S3.",
      "C": "Use SageMaker Clarify\u2019s bias analysis and misapply it for PII detection.",
      "D": "Use AWS KMS custom encryption to encrypt only the PII columns at rest."
    },
    "explanation": "AWS Glue DataBrew provides built-in PII Mask transforms that can mask specified columns with minimal code."
  },
  {
    "id": "8b4b2a3d856c7b7bf943523211f29119bbd82005a1247f6771269658a21a73f3",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "An ML engineer uses an Amazon EFS file system to store training data. To meet compliance, data must be encrypted at rest and in transit between the SageMaker training instance and EFS. Which configuration meets these requirements?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable only SSE-KMS encryption on the EFS file system.",
      "B": "Configure the EFS mount with NFSv4 and no encryption.",
      "C": "Use Amazon FSx for Lustre with default settings.",
      "D": "Enable EFS encryption at rest (SSE-KMS) and mount using EFS mount options with encryption in transit (TLS)."
    },
    "explanation": "To meet both requirements, enable SSE-KMS on EFS and use the EFS mount option to enforce TLS encryption in transit."
  },
  {
    "id": "85ebf23bd5a7c2caca601502d8ec19e0f9be11cbb5b34cbef0016b1907edc31d",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A dataset contains multiple records per user_id. To prevent data leakage, how should an ML engineer split the dataset into train and test subsets?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Randomly split 80/20 at the record level.",
      "B": "Use stratified split to preserve label proportions.",
      "C": "Use a group-based split (e.g., scikit-learn\u2019s GroupShuffleSplit) with user_id as the group key.",
      "D": "Split based on time, taking the latest 20% of records as test."
    },
    "explanation": "A group-based split ensures that all records for a given user_id are either in train or test, preventing leakage."
  },
  {
    "id": "b0e39c30dda2a6b0c5e8a8224bf9628823b101872cfa46e5cde249f152c9424e",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A text dataset with PII must be anonymized before training. The engineer wants automatic detection of names and SSNs. Which AWS service should be used in a SageMaker Processing job?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Glue DataBrew PII Masking",
      "B": "Amazon Comprehend PII detection API",
      "C": "SageMaker Clarify bias detection",
      "D": "AWS Lake Formation data labeling"
    },
    "explanation": "Amazon Comprehend\u2019s PII detection API can be called from a Processing job to automatically identify and anonymize PII in text."
  },
  {
    "id": "313806f3b9302299cb5a19eb34971da6cb3803c58276634f2d0fcd14bb0a615d",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "To detect statistical parity in a numeric target variable across demographic groups before training, which SageMaker Clarify component should be used?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "DataQualityCheckConfig in SageMaker Clarify",
      "B": "PreTrainingBiasCheckConfig in a SageMaker Clarify Processing job",
      "C": "ModelBiasMonitor",
      "D": "BatchTransform with a custom script"
    },
    "explanation": "PreTrainingBiasCheckConfig in a Clarify Processing job enables computation of pre-training bias metrics on the dataset."
  },
  {
    "id": "98e6a8ccb32f294d6a996932bc6d5fd35d347d2c2331f296defc340997ae662b",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "Before ingesting S3 data into SageMaker, an engineer needs to automatically discover and classify PII fields at scale. Which AWS service should be used?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Config",
      "B": "Amazon Athena",
      "C": "Amazon Macie",
      "D": "AWS CloudTrail"
    },
    "explanation": "Amazon Macie automatically discovers and classifies PII in S3 objects at scale."
  },
  {
    "id": "8637995352c18d8d812aa3b618fe5ec798f15c213d35c307c45fd914d2383927",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A SageMaker training job must encrypt all EBS volumes with a customer-managed KMS key. How should the engineer configure the training job?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Specify the KmsKeyId parameter in the CreateTrainingJob API.",
      "B": "Configure the InputDataConfig to include an EncryptionKeyId.",
      "C": "Enable SSE-KMS on the S3 bucket only.",
      "D": "Use a SageMaker Pipeline with a default KMS key."
    },
    "explanation": "The CreateTrainingJob API\u2019s KmsKeyId parameter applies the specified customer-managed KMS key to encrypt all training EBS volumes."
  },
  {
    "id": "ba1c2c774dee7a5421d7dbad866ece159dcfe0e1968d614ad147d2aef5779d31",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A company must enforce fine-grained access control on PII columns in its Data Catalog tables. Which AWS capability enables this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "S3 bucket policies on the underlying data files.",
      "B": "AWS Lake Formation LF-Tags attached to Data Catalog columns.",
      "C": "IAM identity-based policies only.",
      "D": "AWS KMS key policies on the Data Catalog."
    },
    "explanation": "Lake Formation LF-Tags on Data Catalog columns provide column-level access control for sensitive data."
  },
  {
    "id": "881a47235376e9cfccdc62f64ac5b2edef5c67c72d2be86768011d48c74acde4",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "An engineer needs to ensure that a string column contains only unique values before training. Which AWS service and rule type should be used?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Glue DataBrew with a completeness rule.",
      "B": "AWS Glue Data Quality with a passing ratio rule.",
      "C": "SageMaker Clarify DataQualityCheckConfig uniqueness check.",
      "D": "AWS Glue Data Quality with a uniqueness rule."
    },
    "explanation": "AWS Glue Data Quality supports a rule to check column uniqueness to enforce that values are unique."
  },
  {
    "id": "be4440bb4da008933c1b1a57a1e273c5bd864881613b5f09412ea8fd3e1eaac5",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "To prevent order-based bias in large S3 datasets during training, how can the engineer shuffle data in a SageMaker training job?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use File input mode (default shuffling).",
      "B": "Use RecordIO input mode (shuffles automatically).",
      "C": "Use Pipe input mode with ShuffleConfig set to an appropriate buffer size.",
      "D": "Download and shuffle in the training script only."
    },
    "explanation": "Pipe input mode with ShuffleConfig allows SageMaker to shuffle streaming records before training to avoid order bias."
  },
  {
    "id": "3ecb3defd5948aa47260427faa3a673c5112e25e3b77041cd7bfa4773763d6a3",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A dataset contains missing numeric values that must be imputed with the median before feature engineering. Which tool provides a built-in transform for this?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Glue DataBrew recipe step \u201cFill missing values\u201d with median.",
      "B": "SageMaker Debugger preprocessing hook.",
      "C": "SageMaker Feature Store ingestion transform.",
      "D": "AWS Glue Data Quality imputation rule."
    },
    "explanation": "AWS Glue DataBrew includes a \u201cFill missing values\u201d transform that can impute missing entries with statistics like median."
  },
  {
    "id": "96e8576a97e55d4af5b30a2dee112dba7e29e0e1d8b2fde780ec8d8ef0eee7ad",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "An engineer wants to use a consistent, repeatable set of cleaned and validated features for both offline training and real-time inference. Which AWS capability should be used?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon S3 with versioned CSV files.",
      "B": "Amazon DynamoDB table with precomputed features.",
      "C": "SageMaker Feature Store to store and retrieve features.",
      "D": "Amazon Redshift external table."
    },
    "explanation": "SageMaker Feature Store provides a centralized store for features that ensures consistency between offline training and online inference."
  },
  {
    "id": "b3657b6900888a5d6503e2584b68b6b9ce1acc73f2318e66618d8a7d44fa81c4",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A credit risk team needs a highly interpretable, low-latency regression model for predicting applicant default probability using 50 numerical and categorical features. They require clear feature coefficients for regulatory reporting. Which SageMaker built-in algorithm should they choose?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "XGBoost built-in algorithm with SHAP explanations",
      "B": "Linear Learner built-in algorithm in regression mode",
      "C": "Factorization Machines built-in algorithm",
      "D": "DeepAR forecasting algorithm"
    },
    "explanation": "Linear Learner in regression mode provides direct feature weights for interpretability and low latency. XGBoost can require SHAP post hoc analysis, and other algorithms aren\u2019t designed for regression interpretability."
  },
  {
    "id": "fcd05758a5316621cebd5776bf573ab507cb4fab1bf42b6ba633a5b0b9e97f98",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "An online advertising platform must predict click-through rates with hundreds of categorical features of high cardinality. The team wants to capture pairwise interactions between sparse features efficiently. Which built-in SageMaker algorithm is most appropriate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "XGBoost built-in algorithm",
      "B": "Linear Learner built-in algorithm",
      "C": "Factorization Machines built-in algorithm",
      "D": "K-Means built-in clustering algorithm"
    },
    "explanation": "Factorization Machines are designed to model interactions among high-cardinality categorical features more efficiently than tree-based or linear models."
  },
  {
    "id": "d2487cb0312e603b6b6581ab0208683ea5ff33f016f32c7cc255f25eda279adb",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A retail company needs to forecast hourly product demand for 10,000 SKUs with seasonal patterns and occasional promotions. They require a probabilistic forecast of future demand. Which built-in SageMaker algorithm should they select?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "DeepAR forecasting algorithm",
      "B": "XGBoost regression algorithm",
      "C": "Linear Learner regression algorithm",
      "D": "K-Nearest Neighbors built-in algorithm"
    },
    "explanation": "DeepAR provides probabilistic time series forecasts at scale and handles seasonality and promotions. Other algorithms do not directly support probabilistic forecasting."
  },
  {
    "id": "c9acb4e7db3b7debdc6c68939d278b2bdd9ad69b2974542ec6f6b27f72c07d4f",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "An operations team needs to detect anomalies in streaming server CPU metrics. They require a one-class unsupervised method that adapts to drift. Which SageMaker built-in algorithm meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Isolation Forest built-in algorithm",
      "B": "Random Cut Forest built-in algorithm",
      "C": "One-Class SVM via script mode",
      "D": "K-Means built-in clustering algorithm"
    },
    "explanation": "Random Cut Forest is an unsupervised, streaming-friendly anomaly detection algorithm that adapts to drift; it\u2019s a SageMaker built-in algorithm."
  },
  {
    "id": "dcac06f2f3d075e4be1ffc39008c0b8a471d6c8567a84c25ec501314da4099e2",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A global publisher must classify news articles in 10 languages with minimal training data per language. They need an AWS managed NLP service with built-in support. Which service should they choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Custom BERT model on SageMaker",
      "B": "Custom TF-based text classifier in script mode",
      "C": "Amazon Comprehend DetectDominantLanguage and Custom Classification APIs",
      "D": "Amazon Translate to English plus custom classifier"
    },
    "explanation": "Amazon Comprehend offers multi-language classification with minimal training data and managed infrastructure. Custom models require more data and maintenance."
  },
  {
    "id": "8523370456a5d4d722d940669034d2837c95b70dd803984d0ce4d4572b25a585",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A biotech startup wants to identify cell types in microscopy images with minimal ML expertise. They prefer a no-code AWS solution that adapts to new classes. Which service should they use?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Custom CNN built in TensorFlow on SageMaker script mode",
      "B": "Deploy a PyTorch model on SageMaker endpoint",
      "C": "Use SageMaker AutoML via built-in algorithms",
      "D": "SageMaker Canvas image classification with Amazon Rekognition Custom Labels integration"
    },
    "explanation": "SageMaker Canvas provides a no-code interface, and integration with Rekognition Custom Labels supports labeling and incremental class adaptation without deep ML expertise."
  },
  {
    "id": "4b52dba9fa5631e36aaf2863340caea88b17d9d42f71bbce168da04c27693651",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A call-center analytics team needs real-time transcription and sentiment analysis of customer calls. They require a managed service with low operations overhead. Which combination should they use?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Custom DeepSpeech model on SageMaker",
      "B": "Amazon Transcribe for transcription and Amazon Comprehend for sentiment",
      "C": "Deploy open-source Kaldi on SageMaker",
      "D": "Amazon Lex chatbot for transcription and sentiment"
    },
    "explanation": "Amazon Transcribe and Comprehend are managed services for ASR and sentiment analysis, respectively, minimizing operational overhead compared to custom models."
  },
  {
    "id": "011c44951673a973a7867ad645914132fa7c05909ad52560a66f48d63fbf6564",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A financial modeling team needs to generate synthetic financial reports using a foundation model. They require a generative text service with minimal training. Which AWS service best fits?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Bedrock with a foundation LLM",
      "B": "Custom GPT-2 on SageMaker",
      "C": "Use TensorFlow Sequence-to-Sequence on SageMaker",
      "D": "Amazon Translate with custom glossary"
    },
    "explanation": "Amazon Bedrock provides foundation LLMs for text generation, reducing heavy training and tuning efforts versus custom Seq2Seq models."
  },
  {
    "id": "581e4e587b6004a76d3498ca3fb3b1018293da61b5fc90550adfc930349a8228",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "An edge computing use case requires running an image classifier on IoT cameras with limited compute. They have a trained ResNet model. Which SageMaker feature should they use to optimize the model?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker built-in Image Classification algorithm",
      "B": "SageMaker Script Mode with TensorFlow",
      "C": "SageMaker Neo compilation to target edge architecture",
      "D": "Containerize model for SageMaker endpoint"
    },
    "explanation": "SageMaker Neo compiles and optimizes trained models for edge devices, reducing latency and resource usage on IoT cameras."
  },
  {
    "id": "e76f07a743132cee0b79913a2fe9332bf27df0c94444e40c44b7350a3f9bf564",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A team needs to quickly prototype a multi-label text classification for internal documents with zero code. They want to experiment with pre-trained models and fine-tune them. Which SageMaker capability should they use?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "SageMaker Clarify for data bias analysis",
      "B": "SageMaker Model Monitor for drift alerts",
      "C": "Notebook instance with pre-built scripts",
      "D": "SageMaker JumpStart solution templates for multi-label text classification"
    },
    "explanation": "SageMaker JumpStart provides pre-trained model solutions and templates for multi-label classification that can be fine-tuned with minimal code."
  },
  {
    "id": "639356dcbf3885f3f8cc7f51b14ec304af6fe815f23cbe8c5931d25951ded222",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A startup has a small tabular dataset (5,000 rows) and needs a quick binary classifier with built-in regularization and automated hyperparameter tuning. Which SageMaker built-in algorithm and mode is most appropriate?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Linear Learner in binary classification mode",
      "B": "XGBoost regression mode",
      "C": "K-Means clustering mode",
      "D": "DeepAR forecasting mode"
    },
    "explanation": "Linear Learner supports binary classification with built-in regularization and integrates with SageMaker Automatic Model Tuning for small datasets efficiently."
  },
  {
    "id": "d13638d7ca498cf3338bcd6ce530f8ff60a159fa31c8f229e4218a0a75304f5d",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A fraud detection team requires an algorithm that handles class imbalance and produces probabilistic scores for each transaction. They prefer a tree-based approach. Which built-in algorithm should they select?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Linear Learner with L1 regularization",
      "B": "XGBoost built-in algorithm",
      "C": "Random Cut Forest",
      "D": "K-Nearest Neighbors"
    },
    "explanation": "XGBoost produces probabilistic outputs, handles class imbalance via objective weighting, and is a high-performance tree-based algorithm."
  },
  {
    "id": "5104ec9eb433324aff47302f022a18990d4edffd4d1026a847a619e05b1dde92",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A medical imaging project needs semantic segmentation on X-ray images. The team has no pre-built algorithm in SageMaker. They want minimal development overhead. Which approach should they take?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker built-in Object Detection algorithm",
      "B": "Use Rekognition Custom Labels",
      "C": "Use JumpStart semantic segmentation pre-trained model in script mode",
      "D": "Develop a U-Net from scratch in a notebook"
    },
    "explanation": "JumpStart offers pre-trained semantic segmentation models that can be fine-tuned in script mode, reducing overhead compared to building models from scratch."
  },
  {
    "id": "9e1af6f268027b1f4f147f80eb63111e77942296a0a5bcac52a95b1e24a58536",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A business needs to extract key phrases from customer reviews in Spanish. They want a managed solution that supports custom phrase detection. Which service should they choose?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Custom spaCy model on SageMaker",
      "B": "Custom PyTorch NLP in script mode",
      "C": "Amazon Translate to English plus Comprehend key phrases",
      "D": "Amazon Comprehend with custom entity recognizer for Spanish"
    },
    "explanation": "Amazon Comprehend supports custom entity detection in multiple languages including Spanish, avoiding translation pipelines or heavy custom models."
  },
  {
    "id": "c5eb0941391949da18aec74e3baabbb7e1a3200f1b3e6b4e6049aa2b46c3efaa",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A team must choose between deploying a SageMaker built-in algorithm versus a pre-packaged deep learning framework in script mode. They prioritize faster training on CPU-only instances for a moderate-sized tabular dataset. Which should they choose?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "SageMaker built-in XGBoost algorithm",
      "B": "TensorFlow in script mode",
      "C": "PyTorch in script mode",
      "D": "Bring-Your-Own container with Scikit-learn"
    },
    "explanation": "XGBoost built-in algorithm is optimized for CPU training on tabular data and will train faster with minimal configuration compared to deep learning frameworks."
  },
  {
    "id": "0f11b1213aec191edf316b941cebd60e352cd5c0bd1e00578f5f5149d16c14f5",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A data scientist has an existing SageMaker automatic model tuning (AMT) job that produced optimal hyperparameter values for a regression model. New data arrives and the scientist wants to reuse the previous tuning results as a starting point for a new hyperparameter tuning job to save compute time. Which warm start configuration type should the scientist choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a warm start tuning job with type IDENTICAL_DATA_AND_ALGORITHM.",
      "B": "Use a warm start tuning job with type TRANSFER_LEARNING.",
      "C": "Use a warm start tuning job with type CURRENT_BEST.",
      "D": "Use a regular hyperparameter tuning job without warm start."
    },
    "explanation": "TRANSFER_LEARNING reuses previous tuning job results to inform new search on similar data or algorithm. IDENTICAL_DATA_AND_ALGORITHM reruns the same search space exactly. CURRENT_BEST and non\u2013warm-start jobs don\u2019t utilize prior results to reduce compute."
  },
  {
    "id": "d870924b425ab3d2b4a52653ca1a3c005800d58e2a8d0c67a5139714d8c98b09",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An ML engineer wants to reduce wasted compute and stop unpromising training jobs early during an AMT hyperparameter tuning job. Which configuration change will achieve this with minimal code changes?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set EarlyStoppingType to AUTO in the hyperparameter tuning job configuration.",
      "B": "Set MaxRuntimePerTrainingJob to a lower value.",
      "C": "Add a CloudWatch alarm to terminate long-running training jobs.",
      "D": "Wrap each training job in a custom script that kills the process if validation loss stops improving."
    },
    "explanation": "Setting EarlyStoppingType='Auto' enables SageMaker to automatically terminate unpromising training jobs. Lowering MaxRuntime limits total time but doesn\u2019t target unpromising jobs. CloudWatch alarms and custom scripts add operational overhead."
  },
  {
    "id": "4e1e1016e1bc54367fa9cb4ee2cf40e840681ddf00fcb3bea328d9139dc9e194",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A team trains a large PyTorch model on 8 GPU instances, but training is slow and network communication is a bottleneck. Which SageMaker feature will provide an efficient out-of-the-box distributed training solution to accelerate this job?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure Horovod for distributed training.",
      "B": "Enable smdistributed.dataparallel in Script Mode.",
      "C": "Use the built-in PyTorch Multi-GPU Estimator.",
      "D": "Use a parameter server architecture implemented in the training script."
    },
    "explanation": "smdistributed DataParallel integrates with Script Mode and optimizes gradient synchronization. Horovod requires extra setup, custom parameter server needs code changes, and there is no separate built-in PyTorch multi-GPU Estimator."
  },
  {
    "id": "4e59ab2a8f17cbe66b85bac26315e40eb3034b1a71677d3182e7dbd2dd788a94",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "During training of a convolutional neural network, an ML engineer observes overfitting: training accuracy far exceeds validation accuracy. Which single change to the training job will increase generalization with the least complexity?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase the dropout rate from 0.25 to 0.75.",
      "B": "Reduce the initial learning rate by a factor of ten.",
      "C": "Add L2 weight-decay regularization to the optimizer.",
      "D": "Switch to early stopping and set patience to 3 epochs."
    },
    "explanation": "L2 weight decay penalizes large weights and reduces overfitting without requiring new callbacks. Excessive dropout may underfit; reducing learning rate doesn\u2019t directly address overfitting; early stopping adds extra tuning and monitoring."
  },
  {
    "id": "dfb5cc0882d42a5a2cbfc828a251c551517f553c1e68609160e66d87004e668f",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An ML engineer has trained both a random forest and an XGBoost model for the same classification task and wants to combine them to improve accuracy. Which ensemble approach should the engineer implement to learn an optimal combination of predictions?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a meta-learner on the model outputs (stacking).",
      "B": "Average the predictions from both models (bagging).",
      "C": "Train the XGBoost model to correct errors of the random forest (boosting).",
      "D": "Select the model with the higher validation accuracy for each input dynamically (voting)."
    },
    "explanation": "Stacking uses a meta\u00ad-model trained on base-model outputs to learn optimal weights. Bagging pools multiple instances of one algorithm. Boosting uses sequential training, not two distinct models. Voting is a simple unweighted ensemble."
  },
  {
    "id": "dc9902c433c97100dedbdc20536bcfb745af419a6d8e160b9498dc2ca9e87403",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A financial services firm needs to track, promote, and audit multiple versions of their ML models in SageMaker. Which feature should they use to centrally manage model versions and stages (e.g., Approved, Pending)?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon S3 object versioning on the model artifacts bucket.",
      "B": "SageMaker Model Registry.",
      "C": "SageMaker Experiments.",
      "D": "AWS CodeCommit repository."
    },
    "explanation": "SageMaker Model Registry is designed to store, version, and annotate model artifacts and their approval lifecycle. S3 versioning tracks raw objects but lacks metadata. Experiments track experiments, not production stages. CodeCommit is source control."
  },
  {
    "id": "66837aeed621b63283e9aed9f707999517e2424166a1b9e5ef0a45294ac45223",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "When using smdistributed.dataparallel for PyTorch training, the network bandwidth is still limiting training throughput. Which additional configuration can reduce communication overhead?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch to Horovod with NCCL backend.",
      "B": "Enable gradient compression (fp16) in smdistributed.dataparallel.",
      "C": "Use MPI and ring-allreduce instead of reduce-scatter.",
      "D": "Increase batch size to amortize communication."
    },
    "explanation": "smdistributed DataParallel supports fp16 gradient compression to reduce bandwidth. Horovod setup is heavier, using MPI doesn\u2019t enable compression, and larger batches may help but don\u2019t directly reduce communication volume."
  },
  {
    "id": "6f3c457540cd894dcbda47bc29bd9f0f9d1a43b007ceefaab14704c1dbebd8ba",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An ML engineer is constrained by budget and can run at most 50 training jobs for hyperparameter tuning. Which search strategy in SageMaker AMT will likely find the best solution within the least number of jobs?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Grid search.",
      "B": "Random search.",
      "C": "Bayesian optimization.",
      "D": "Genetic algorithm search."
    },
    "explanation": "Bayesian optimization balances exploration and exploitation, converging faster. Grid search is exhaustive, random search is less efficient, and genetic algorithms aren\u2019t natively supported in SageMaker AMT."
  },
  {
    "id": "435a0d0ac440756ff57e4594535ba51bba2031c6faed7e2ec543bf1b7168a006",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A company must deploy a deep learning model to edge devices that have strict memory limits. Which approach using SageMaker will produce the smallest model artifact with minimal manual optimization?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Apply manual weight pruning in the training script.",
      "B": "Use SageMaker Neo to compile and quantize the model for the target device.",
      "C": "Enable mixed-precision training and save fp16 weights.",
      "D": "Use model distillation to train a smaller student network."
    },
    "explanation": "SageMaker Neo automates compilation and quantization for the target hardware, reducing model size. Manual pruning requires custom code; mixed precision lowers memory at runtime but not artifact size; distillation requires additional training."
  },
  {
    "id": "07abe8cabce53d1ff185bd4593eec97060757495b093de766fbab6413975cdc8",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An ML engineer wants to fine-tune a large pre-trained Hugging Face transformer using SageMaker. Which method requires the least boilerplate code?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Bring your own container with transformers installed.",
      "B": "Use the SageMaker Hugging Face estimator in Script Mode.",
      "C": "Translate the model code to MXNet and use the built-in MXNet Estimator.",
      "D": "Use Amazon SageMaker built-in text classification algorithm."
    },
    "explanation": "The SageMaker Hugging Face estimator simplifies fine-tuning with minimal code. BYOC adds container management; translating to MXNet is error-prone; no built-in algorithm for large transformer fine-tuning exists."
  },
  {
    "id": "29381f20d249930c9c6b1d2a029ccfa5d1bd20a56d390533c4829bb2c6d09c9f",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A data scientist configures an AMT tuning job but forgot to configure early stopping. Which change should they make to the tuning configuration to enable automated early stopping of poor performing jobs?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set EarlyStoppingType='Auto' and choose an appropriate WaitInterval.",
      "B": "Lower the MaxJobs parameter to force quicker completion.",
      "C": "Define a CloudWatch rule to terminate jobs with low metrics.",
      "D": "Wrap the training script in a debugger rule to kill unresponsive jobs."
    },
    "explanation": "EarlyStoppingType='Auto' with WaitInterval allows SageMaker to stop jobs whose objective does not improve. Lowering MaxJobs only limits total count. CloudWatch rules and debugger scripts are more operationally complex."
  },
  {
    "id": "d3eb650d9ceda87d5ca1c9109b5a6eeaf532618bd091ca19d1e7cd151f3ec1d7",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A neural network training job shows high training accuracy but validation accuracy plateaus early. The engineer wants an automated mechanism to detect this during training. Which SageMaker Debugger rule should they enable?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "VanishingGradientDetector.",
      "B": "LossNotDecreasing.",
      "C": "OverfitDetector.",
      "D": "WeightUpdateVerifier."
    },
    "explanation": "OverfitDetector monitors training vs. validation metrics to detect early signs of overfitting. VanishingGradientDetector and LossNotDecreasing focus on gradients and loss. WeightUpdateVerifier checks parameter updates."
  },
  {
    "id": "4278aae79bedfa676e0c232ec4b742b719911ec2022da7f25df39333970457e6",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A team wants to track parameters, metrics, and artifacts across multiple training jobs and compare runs in SageMaker. Which feature should they use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Model Monitor.",
      "B": "SageMaker Experiments.",
      "C": "Amazon CloudWatch metrics.",
      "D": "AWS X-Ray."
    },
    "explanation": "SageMaker Experiments is built to organize, track, and compare training runs. Model Monitor observes deployed models. CloudWatch and X-Ray are for logs and traces, not training experiment management."
  },
  {
    "id": "32abad9b73cfdcbb7e2fc8fe8c522c8f966a1f03a1c73653e5b943a98d53851d",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A neural network trained on tabular data shows many small nonzero weights and generalizes poorly. Which regularization change will encourage sparsity and reduce overfitting?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase the L2 (weight\u2010decay) coefficient.",
      "B": "Increase dropout to 0.8.",
      "C": "Switch to L1 regularization on weights.",
      "D": "Use batch normalization after every layer."
    },
    "explanation": "L1 regularization (Lasso) encourages many weights to become exactly zero, producing sparse models. L2 shrinks but doesn\u2019t enforce sparsity. Dropout and batch normalization address generalization but not weight sparsity."
  },
  {
    "id": "7a7d915ef097c78f633425a11a96dbf9d15a2de55e598d3f0794b6d721d54087",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You have trained a binary classifier on a dataset where the positive class constitutes only 1% of the data. On a held-out test set, the model achieves an AUC-ROC of 0.90, but its precision at low recall is poor. Which evaluation metric should you use to better assess the model\u2019s ability to identify the minority positive class?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Accuracy",
      "B": "AUC-ROC",
      "C": "Area Under the Precision-Recall Curve (AUC-PR)",
      "D": "Log Loss"
    },
    "explanation": "With extreme class imbalance, the precision-recall curve (AUC-PR) focuses on performance for the positive class and is more informative than AUC-ROC or accuracy."
  },
  {
    "id": "84da18b487e74819fbffe6964298b0333803d07bd8fb99abe8e664195feb47d6",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "A regression model predicts house prices. Stakeholders are more concerned about a few very large prediction errors than many small ones. Which metric should you minimize?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Mean Absolute Error (MAE)",
      "B": "Root Mean Square Error (RMSE)",
      "C": "R\u00b2 (Coefficient of Determination)",
      "D": "Mean Absolute Percentage Error (MAPE)"
    },
    "explanation": "RMSE penalizes large errors more heavily than MAE, making it appropriate when large deviations are particularly undesirable."
  },
  {
    "id": "68c19455fd1b272f14ff21eeb58db5e7405ee2020a8c58657777b7bef30e57b2",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "An ML engineer wants to run reproducible training experiments that track hyperparameters, metrics, and artifacts across multiple training jobs in Amazon SageMaker. Which AWS service should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon CloudWatch Logs",
      "B": "AWS CodePipeline",
      "C": "Amazon SageMaker Experiments",
      "D": "AWS Config"
    },
    "explanation": "SageMaker Experiments provides managed tracking of experiments, trials, hyperparameters, metrics, and artifacts to ensure reproducibility."
  },
  {
    "id": "5a890e92fa110ba7dd2dda1ec7a95d085cb620158b68929122974b2f8c6c7db7",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "During training, you notice that your model\u2019s loss on the training set stops decreasing early and plateaus. Which built-in SageMaker Debugger rule should you enable to detect this convergence issue?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "OverfitDetector",
      "B": "LossNotDecreasing",
      "C": "LearningRateFinder",
      "D": "WeightNormMonitor"
    },
    "explanation": "The LossNotDecreasing rule monitors training loss and raises an alert if it does not decrease sufficiently, indicating convergence problems."
  },
  {
    "id": "332f630c243b92dec0b1e7b6ab9fab87d8654382692ec5beb2689e9e83df1be9",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You want to generate local feature\u2010level explanations (SHAP values) for your production model using SageMaker Clarify. Which configuration should you use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "DataBiasConfig in ClarifyProcessingJob",
      "B": "ModelBiasConfig in ClarifyProcessingJob",
      "C": "SHAPConfig in ClarifyProcessingJob",
      "D": "DriftConfig in ClarifyProcessingJob"
    },
    "explanation": "SHAPConfig enables Clarify to compute SHAP feature attributions for local explainability of model predictions."
  },
  {
    "id": "4ace03638f09bfb4ff273faa5b088f738635e4542f716021b7ca9dbd31db04c7",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "To select an optimal classification threshold that maximizes F1 score, what procedure should you perform on your validation data?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Evaluate F1 at various probability thresholds and choose the threshold yielding the highest F1",
      "B": "Select the threshold where ROC curve slope equals 1",
      "C": "Use the threshold where precision equals recall",
      "D": "Choose the threshold that balances true positive and false positive rates"
    },
    "explanation": "Maximizing F1 requires evaluating F1 at multiple thresholds on validation data and selecting the threshold with the highest F1."
  },
  {
    "id": "f12abb565ff9fd97bf4382bafe6c17d8d84f065b73faf2718b890fd58f485511",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your deep neural network exhibits vanishing gradient issues. Which SageMaker Debugger configuration helps you inspect gradient distributions during training?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable the base and gradient_histogram collections",
      "B": "Use the TorchLrFinder rule",
      "C": "Use the OverfitDetector rule",
      "D": "Configure only the losses collection"
    },
    "explanation": "The gradient_histogram collection captures gradient distributions so you can detect vanishing or exploding gradients."
  },
  {
    "id": "9c7fef7d1141f06b8245bc86715dcce620956e870372418a4529686bcc8cbe36",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You need to perform A/B testing between two model variants in production with minimal overhead. Which SageMaker feature supports traffic splitting between variants?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Deploy two separate endpoints and use Route 53 weighted routing",
      "B": "Configure two production variants in a single endpoint and set variant weights",
      "C": "Use an AWS Lambda function to proxy and split traffic",
      "D": "Use AWS CodePipeline for traffic routing"
    },
    "explanation": "SageMaker endpoints support multiple production variants with configurable weights for built-in traffic splitting (A/B testing)."
  },
  {
    "id": "84c1a8c621ecb061a1f65926cb2699ee97e3ee88e50a5d3cacd32bbc83cf74b7",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "For a multi-class classification problem with imbalanced classes, which evaluation metric gives equal importance to each class regardless of its frequency?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Micro-averaged F1 score",
      "B": "Macro-averaged F1 score",
      "C": "Overall accuracy",
      "D": "Weighted precision"
    },
    "explanation": "Macro-averaged F1 computes the F1 score for each class and averages them equally, treating all classes with equal importance."
  },
  {
    "id": "b51637dcc4f000a591f0e369fa83d1136170f15ef68706ee6e42558507dd8c3b",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You compare two regression models: Model A (RMSE = 5.0), Model B (RMSE = 4.8). How can you determine whether the observed difference is statistically significant?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Compare the RMSE values directly",
      "B": "Compute bootstrapped confidence intervals for the RMSE difference",
      "C": "Use one-way ANOVA on prediction residuals",
      "D": "Compare R\u00b2 values"
    },
    "explanation": "Bootstrapping provides confidence intervals on the RMSE difference to determine if the improvement is statistically significant."
  },
  {
    "id": "3c321ae4b8be8e250e9870340383e6be180fec2c7f0032aec7a406019f595d34",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Which SageMaker Clarify metric measures the difference in the proportion of positive outcomes between two demographic groups?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Class imbalance (CI)",
      "B": "Difference in Proportions of Labels (DPL)",
      "C": "Kolmogorov\u2013Smirnov (KS) statistic",
      "D": "Confusion matrix parity"
    },
    "explanation": "DPL quantifies the difference in label rates (positive outcome proportions) between a sensitive group and the baseline group."
  },
  {
    "id": "afc219a4984cc644be909de9712ddca5ea546ad2939c57d716a7ee1a47909b3d",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your model\u2019s training loss decreases steadily while validation loss increases after a point. Which SageMaker Debugger rule can detect this behavior automatically?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "OverfitDetector",
      "B": "LossNotDecreasing",
      "C": "LearningRateFinder",
      "D": "GradientHistogram"
    },
    "explanation": "OverfitDetector monitors divergence between training and validation losses to detect overfitting during training."
  },
  {
    "id": "d6933ee484dca211286e1d062aad6aee21e0e35249668202033fc1a1e421ddf7",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You want to identify a suitable initial learning rate for faster convergence. Which SageMaker Debugger rule should you run?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "LearningRateFinder",
      "B": "LossNotDecreasing",
      "C": "OverfitDetector",
      "D": "EarlyStoppingRule"
    },
    "explanation": "LearningRateFinder systematically varies the learning rate to help identify an optimal learning rate for training convergence."
  },
  {
    "id": "42d26b688faffeef0b6d82717dc4afa8848e7dc1a8535d85f57e6e9f143732a1",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You need to compute evaluation metrics at multiple classification thresholds using SageMaker Clarify. Which configuration option enables this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Specify multiple thresholds in the ClarifyProcessor\u2019s ModelPredictionsConfig",
      "B": "Use SageMaker Model Monitor\u2019s default job",
      "C": "Run a SageMaker batch transform and manually compute metrics",
      "D": "Use SageMaker Automatic Model Tuning"
    },
    "explanation": "You can configure multiple thresholds in ModelPredictionsConfig for Clarify processing jobs to compute metrics (precision, recall, F1) at those thresholds."
  },
  {
    "id": "a24829ad48bc8c34b3c191c9cd54599236614fd8b9cacf63b6276412abfcb68c",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "During training you observe that both training and validation losses are high and nearly identical, and the model fails to improve. What issue does this indicate?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Overfitting",
      "B": "Underfitting",
      "C": "Data leakage",
      "D": "Gradient explosion"
    },
    "explanation": "High and similar training/validation losses indicate underfitting, meaning the model is too simplistic to capture the underlying data patterns."
  },
  {
    "id": "7272d30d500223bfe1673457f52a3ea5ea7c646ac35630dff3d17779f07d357c",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A startup has developed an ML model for an internal analytics tool that is invoked fewer than 100 times per day. Each inference must return within 200 ms, and total monthly cost must be minimized. Which SageMaker deployment option best meets these requirements?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Provision a real-time SageMaker endpoint with minimal instance capacity and auto scaling.",
      "B": "Deploy the model as a SageMaker serverless endpoint.",
      "C": "Use a SageMaker asynchronous endpoint with low concurrency settings.",
      "D": "Schedule a SageMaker batch transform job once per day."
    },
    "explanation": "A serverless endpoint incurs no idle instance cost, supports <200 ms latency for light workloads, and is optimal for very low invocation volume."
  },
  {
    "id": "1878af30d1509c24b416fb2bfb73509761bd3e554210282090e8ce6ffe6e3924",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A machine learning team must host 500 small models (<50 MB each) behind a single API. All models share identical inference logic but have different weights. Which deployment infrastructure should be used to minimize cost and management overhead?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A multi-model SageMaker endpoint.",
      "B": "500 separate real-time SageMaker endpoints.",
      "C": "A SageMaker batch transform job per model.",
      "D": "Amazon ECS on Fargate with mounted S3 weights."
    },
    "explanation": "Multi-model endpoints load models on demand into a shared container, reducing cost and simplifying management compared to separate endpoints."
  },
  {
    "id": "74a70083a71ddf177e4f07038edfa5ed7ccdc2f12c3824ebf17749509bca8f7f",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An automotive OEM wants to run an object-detection model on in-vehicle devices with limited compute and no internet connectivity. The model must start in <50 ms after request. Which deployment approach meets these requirements?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Host the model on a SageMaker real-time endpoint and stream data from the vehicle.",
      "B": "Use a SageMaker asynchronous endpoint with cached results on the vehicle gateway.",
      "C": "Compile the model with SageMaker Neo and deploy it to the edge container on the device.",
      "D": "Package the model in a Lambda@Edge function and invoke it from the vehicle."
    },
    "explanation": "SageMaker Neo compiles and optimizes models for specific hardware and enables sub-50 ms local inference without connectivity."
  },
  {
    "id": "796c135477271c35212fc63a5d68ed9aa27da0f82676834fda62adac4b87686e",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A document-processing model accepts 8 MB JSON payloads and may run up to 10 minutes per request. Customers must receive a response per request. Which SageMaker endpoint type is the most cost-effective?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Asynchronous SageMaker endpoint.",
      "B": "Real-time SageMaker endpoint.",
      "C": "Serverless SageMaker endpoint.",
      "D": "SageMaker batch transform job."
    },
    "explanation": "Asynchronous endpoints support large payloads, long processing durations, and return one response per request, avoiding persistent instance costs of real-time endpoints."
  },
  {
    "id": "bf202883f38950a24eb30ec92095dcb28bfd4b86d7b3f317b36500dc1127c756",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An e-commerce site needs <100 ms inference latency for its image-classification model at peak traffic. Daily traffic patterns vary widely. Which compute environment should the ML engineer choose?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CPU-based serverless SageMaker endpoint.",
      "B": "GPU-based serverless SageMaker endpoint.",
      "C": "CPU-based real-time SageMaker endpoint with fixed instance count.",
      "D": "GPU-based real-time SageMaker endpoint with auto scaling."
    },
    "explanation": "GPU real-time endpoints deliver required low latency, and auto scaling adjusts capacity to traffic variation."
  },
  {
    "id": "b3b94ef6bd40a6f9e1750f477213eb42de8fdcf88493e84446f67b54f5686deb",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A custom TensorFlow model requires specific OS libraries and drivers that are not available in SageMaker built-in containers. What is the least operationally intensive way to deploy the model?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Translate the model code to one compatible with a built-in container.",
      "B": "Run a batch transform job instead of hosting a real-time endpoint.",
      "C": "Build a custom Docker container, push to ECR, and use it in a SageMaker endpoint.",
      "D": "Use a Lambda function with layers containing the required libraries."
    },
    "explanation": "Custom containers in SageMaker allow full control over dependencies and integrate seamlessly with endpoint deployment."
  },
  {
    "id": "fc68159dc90867260805e2be53c7c9967d12fb20980045517c96ac5c64d5059d",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A regulated enterprise requires all inference traffic to traverse a secured, private network with no public internet access. Which configuration satisfies this requirement with minimal changes?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy a public SageMaker endpoint and block internet via security groups.",
      "B": "Deploy a SageMaker real-time endpoint in the customer VPC with private subnets and no NAT gateway.",
      "C": "Use a serverless SageMaker endpoint and disable outbound access.",
      "D": "Host the model in EKS and restrict Internet at cluster level."
    },
    "explanation": "Launching the endpoint in private subnets of the customer VPC ensures no public internet access without additional NAT configuration."
  },
  {
    "id": "b79af35e1f025246716cce5fe3208c78001e23b25b44a62f642ff89dfd70fc8d",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An ML engineer must deploy a new model version and gradually shift 20% of production traffic to it for canary testing, with automatic rollback on errors. Which SageMaker feature accomplishes this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy a new real-time endpoint and use a Route 53 weighted record.",
      "B": "Create a second endpoint and manually switch after validation.",
      "C": "Use a SageMaker batch transform job for the canary.",
      "D": "Use SageMaker endpoint\u2010update with traffic shifting via endpoint configurations."
    },
    "explanation": "SageMaker\u2019s endpoint configuration supports traffic weights for A/B and canary deployments and automatic rollback on alarms."
  },
  {
    "id": "0a1baaf2930422b8cd1173c32651c15509de9a27de2b4df145f9eaf15acefd29",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "Your team uses Amazon Managed Workflows for Apache Airflow (MWAA) but wants tighter integration with SageMaker model deployments and lineage tracking. Which orchestrator should you choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Continue with MWAA and use custom operators.",
      "B": "SageMaker Pipelines.",
      "C": "AWS Step Functions directly.",
      "D": "AWS CodePipeline."
    },
    "explanation": "SageMaker Pipelines provides built-in integration for training, model registry, and deployment with lineage tracking."
  },
  {
    "id": "5d9a98c1e991c6529dd6d4ca2765368b00b05111b92b277a1ff9a5541a860d80",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A model requires a preprocessing container and a separate inference container on each request. How should you deploy this in SageMaker?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Multi-container real-time endpoint with both containers in the same inference pipeline.",
      "B": "Two separate endpoints chained via Lambda.",
      "C": "Batch transform job with a custom script calling both containers.",
      "D": "ECS service with multiple containers per task."
    },
    "explanation": "SageMaker multi-container endpoints allow a preprocessor and model container to run sequentially in the same endpoint."
  },
  {
    "id": "86c31dfedf5831d6631b65cf11c1055b9d5c8e153f64ff406a6b95ba8ca611fd",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A lightweight NLP model (<10 MB) must be invoked by other AWS services via API and handle occasional spikes of up to 50 requests per second. You want to avoid managing servers. Which deployment target is best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Serverless SageMaker endpoint.",
      "B": "Real-time SageMaker endpoint with a single instance.",
      "C": "Amazon Lambda container image with the model packaged and hosted behind an API Gateway.",
      "D": "ECS Fargate service."
    },
    "explanation": "A Lambda container image offers serverless scaling to handle spikes and exposes a simple API gateway endpoint without EC2 management."
  },
  {
    "id": "7e3ae7589dabbd50adb07d07630060d939a306f57443a9115af84e106f6e2dd1",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "Your team needs to optimize inference performance on ARM-based instances in AWS. The model currently runs on x86 CPU. Which deployment approach will yield the greatest performance gain?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch to a serverless endpoint on ARM.",
      "B": "Compile the model with SageMaker Neo for ARM and host on ARM EC2 instances.",
      "C": "Use a GPU-based real-time endpoint on x86.",
      "D": "Deploy in ECS with ARM container images."
    },
    "explanation": "Neo compiles and optimizes for ARM hardware, delivering significant inference speedups compared to generic CPU execution."
  },
  {
    "id": "45ad688e528b55c4951ffafb01b2272c710150e60dc93a0c9c740a6836c9eca7",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A hybrid environment uses on-premises Kubernetes and AWS. You need to deploy an ML model so that it's available to both environments with unified CI/CD. Which deployment infrastructure is most appropriate?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker serverless endpoint with VPC peering.",
      "B": "SageMaker real-time endpoint in a public subnet.",
      "C": "ECS with Fargate in a public VPC.",
      "D": "Amazon EKS with GPU node group and a shared Terraform pipeline."
    },
    "explanation": "EKS provides Kubernetes compatibility on-prem and in AWS and integrates with existing CI/CD pipelines."
  },
  {
    "id": "28531842e05c4b728c91213c2fd7ebeaea40e7e9549b9dfed86d68f919d287b2",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A data-science team must process nightly batches of 10 million records through an ML model. Sub-second latency is not required, but overall runtime must complete within 2 hours. Which deployment strategy is optimal?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High-capacity real-time SageMaker endpoint with multiple GPU instances.",
      "B": "Serverless SageMaker endpoint with high concurrency.",
      "C": "SageMaker batch transform job with GPU instances.",
      "D": "SageMaker asynchronous endpoint with high timeouts."
    },
    "explanation": "Batch transform jobs are optimized for high-throughput offline inference and can leverage large GPU fleets to meet deadlines without request-based endpoint costs."
  },
  {
    "id": "9f3d6a1f8d4af974526c9b666a41ce27136946b1ccf59f2a83b88811e64c5684",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "Your organization uses Kubernetes microservices on Amazon EKS. You must deploy an ML model as a microservice in this architecture with GPU acceleration. Which approach do you choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a SageMaker real-time endpoint and call it from EKS pods.",
      "B": "Deploy the model in a container on EKS with a GPU instance profile.",
      "C": "Package the model in a Lambda function behind an Application Load Balancer.",
      "D": "Use SageMaker multi-model endpoint within the VPC and proxy through EKS."
    },
    "explanation": "Hosting directly on EKS with GPU nodes integrates with existing microservices, avoids network hops, and provides GPU acceleration under Kubernetes control."
  },
  {
    "id": "649c2a11bc4cf2f670c91f40b2b5fcb0fa75ce426094b350d30fb4176afc4a8c",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "You have two CloudFormation stacks: one provisioning a VPC with subnets and security groups, and another provisioning a SageMaker real-time endpoint. The endpoint stack needs to reference the VPC subnets and security group ARNs created by the VPC stack. What is the most maintainable way to share these values?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "In the VPC stack, export the subnet IDs and security group ARNs using Outputs with Export names. In the endpoint stack, import them with Fn::ImportValue.",
      "B": "Store the subnet IDs and security group ARNs in an S3 object in the VPC stack and download them in CloudFormation custom resources in the endpoint stack.",
      "C": "Pass the subnet IDs and security group ARNs as parameters manually each time you deploy the endpoint stack.",
      "D": "Use a Lambda-backed custom resource in the endpoint stack to call DescribeStacks on the VPC stack and parse the JSON output."
    },
    "explanation": "Cross-stack references via Outputs and Fn::ImportValue is the recommended, maintainable approach for sharing values between CloudFormation stacks."
  },
  {
    "id": "387a31099e03dea44cf2c1d54b729fc401d047bc13d328af588ee44440937fdd",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "You need to configure application autoscaling for a SageMaker real-time endpoint variant using CloudFormation. Which CloudFormation resource type must you declare to register the endpoint variant with AWS Application Auto Scaling?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS::SageMaker::Endpoint",
      "B": "AWS::ApplicationAutoScaling::ScalableTarget",
      "C": "AWS::SageMaker::EndpointConfiguration",
      "D": "AWS::ApplicationAutoScaling::ScheduledAction"
    },
    "explanation": "AWS::ApplicationAutoScaling::ScalableTarget is required to register the SageMaker endpoint variant with Application Auto Scaling before you attach scaling policies."
  },
  {
    "id": "bef156c9cd6180842b7369132bb9bfd2b46ae669f9315731d17e64706e663140",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "Your SageMaker endpoint experiences unpredictable spikes in traffic throughout the day. You want to configure auto scaling to trigger when each instance receives more than 100 requests per minute. Which scaling metric should you choose in your scaling policy?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CPUUtilization",
      "B": "ModelLatency",
      "C": "Invocation4xxErrors",
      "D": "InvocationsPerInstance"
    },
    "explanation": "InvocationsPerInstance is the supported Application Auto Scaling metric for SageMaker endpoints to scale based on request volume per instance."
  },
  {
    "id": "ae50ca5e43c1fd783516d57ecc35a7ec4abb14513ebb5d11ea511b74f3faee83",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "You deploy a SageMaker endpoint inside a VPC without a NAT gateway to save cost. The endpoint fails to pull the container from ECR and the model artifacts from S3. Which minimum set of VPC endpoints must you script to restore functionality?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "An interface endpoint for com.amazonaws.<region>.s3 only",
      "B": "Interface endpoints for com.amazonaws.<region>.ecr.api and com.amazonaws.<region>.ecr.dkr only",
      "C": "Interface endpoints for com.amazonaws.<region>.ecr.api, com.amazonaws.<region>.ecr.dkr, and a gateway endpoint for com.amazonaws.<region>.s3",
      "D": "Interface endpoints for com.amazonaws.<region>.sagemaker.api and com.amazonaws.<region>.sagemaker.runtime"
    },
    "explanation": "SageMaker endpoints in a private VPC need ECR API and DKR interface endpoints plus the S3 gateway endpoint to pull container images and model artifacts."
  },
  {
    "id": "2a96f26176c2b9b0e187be4c074c987151122e8f2f3b24208a99d012858686f2",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "You want to deploy a multi-model endpoint using CloudFormation to host multiple model artifacts in a single container. Which property must you use when defining AWS::SageMaker::Model to support this use case?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "PrimaryContainer",
      "B": "Containers",
      "C": "InferenceExecutionConfig",
      "D": "ModelPackageName"
    },
    "explanation": "The Containers property (a list) is required to define a multi-model endpoint in CloudFormation, whereas PrimaryContainer supports only a single container."
  },
  {
    "id": "e701569ad166e5c94ec46b52a1f74c1a7b0edf6e9d35a193814827d144791d3d",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "Your inference container is stored in Amazon ECR and your SageMaker model execution role must pull the image. When scripting the IAM role in CloudFormation, which managed policy should you attach to grant the minimum required permissions?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "AmazonSageMakerFullAccess",
      "B": "AmazonEC2ContainerServiceforEC2Role",
      "C": "AmazonEC2ContainerRegistryReadOnly",
      "D": "AmazonS3ReadOnlyAccess"
    },
    "explanation": "AmazonEC2ContainerRegistryReadOnly grants the least-privilege permissions necessary for SageMaker to pull container images from ECR."
  },
  {
    "id": "e630adb77ec8cf2a102528bc153c86f9d185f9cb260aecc1ce7b0bacdbf02bb4",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "Your organization hit the export limit for CloudFormation cross-stack references. You still need to share subnet IDs and security group IDs across stacks. What alternative approach should you use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use nested stacks instead of separate stacks.",
      "B": "Store the values in SSM Parameter Store and reference them with dynamic references.",
      "C": "Batch all values into a single export by concatenating comma-delimited strings.",
      "D": "Use AWS Organizations to share parameters between accounts."
    },
    "explanation": "Storing shared configuration in SSM Parameter Store avoids export limits and allows multiple stacks to reference values via dynamic references."
  },
  {
    "id": "08ce970543770bc84f4ed2598566fddfdc7b5db52066c568696958c88a0af958",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "You maintain several CloudFormation templates that define similar network resources for different teams. You want to reuse and standardize these resource definitions. Which CloudFormation feature is best suited for this purpose?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Custom resources",
      "B": "StackSets",
      "C": "Macros",
      "D": "Nested stacks"
    },
    "explanation": "Nested stacks allow you to factor out common resource definitions into reusable templates and include them in multiple parent stacks."
  },
  {
    "id": "88f354f0d98891e385d0809b0caeb32f6bf0fd6d28b27a093a6e3c1b6ad27385",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "When writing AWS CDK code for your SageMaker endpoint, you need to import an existing VPC by its tags so your endpoint can deploy into it. Which CDK method is most appropriate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ec2.Vpc.fromVpcAttributes()",
      "B": "ec2.Vpc.import()",
      "C": "ec2.Vpc.fromLookup()",
      "D": "ec2.Vpc.fromEnv()"
    },
    "explanation": "ec2.Vpc.fromLookup() performs a context lookup by tags or name at synthesis time to import existing VPCs into CDK apps."
  },
  {
    "id": "e5f4e7b078740537dcc2599a6b82a57b4bec7b7613b75ef231e49576cd5a5957",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "You are using AWS CDK to build and push a custom Docker container for SageMaker inference. Which CDK construct should you use to define and publish the image asset?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ecr.Repository",
      "B": "DockerImageAsset",
      "C": "ContainerImage.fromAsset",
      "D": "EcrDockerImage"
    },
    "explanation": "DockerImageAsset (from aws-cdk-lib/aws-ecr-assets) builds a Docker image from a local directory and publishes it to ECR automatically."
  },
  {
    "id": "28de755cdfebdab4c4192ea22cdd1552ca716b7a50ceac4865b53ac4d2083204",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "You want to reduce training costs by using managed Spot Instances for a SageMaker training job defined in CloudFormation. Which property must you add to the AWS::SageMaker::TrainingJob resource?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "EnableManagedSpotTraining: true",
      "B": "UseSpotInstances: true",
      "C": "RuntimeSpotMode: Managed",
      "D": "SpotConfiguration: ManagedSpot"
    },
    "explanation": "EnableManagedSpotTraining set to true enables SageMaker managed Spot training for cost savings."
  },
  {
    "id": "a20ba34a29dda91c47645ecabcf273c27e5976c6b4f100d946e3cefc5c027c18",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "You need to deploy a Lambda function in the same VPC as your SageMaker endpoint so it can invoke the endpoint privately. Which CloudFormation property must you include in the AWS::Lambda::Function resource?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "NetworkConfiguration",
      "B": "SecurityGroupIds",
      "C": "SubnetIds",
      "D": "VpcConfig"
    },
    "explanation": "VpcConfig ({ SubnetIds, SecurityGroupIds }) on AWS::Lambda::Function places the Lambda inside the specified VPC."
  },
  {
    "id": "4c93e883e57020eeaeba930e2570d1a2ec2e0b5a10d961174525e87630a7d35e",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "You configured Application Auto Scaling for your SageMaker endpoint to use CPUUtilization, but you observe that scaling never occurs. Which explanation is correct?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "You must enable detailed monitoring on the endpoint to expose CPU metrics.",
      "B": "SageMaker endpoints only support InvocationsPerInstance as a built-in scaling metric.",
      "C": "CPUUtilization is supported only for asynchronous endpoints.",
      "D": "You must configure a CloudWatch alarm for CPUUtilization even when using Application Auto Scaling."
    },
    "explanation": "Application Auto Scaling for SageMaker real-time endpoints supports only the InvocationsPerInstance metric; CPUUtilization is not supported directly."
  },
  {
    "id": "17b50cf55c83d32485049db16cb7f1b2447401190648f498d2ca5b3663462364",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "When you register your SageMaker endpoint variant with AWS Application Auto Scaling in CDK, what is the correct format of the resourceId property?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "endpoint/YourEndpointName/variant/AllTraffic",
      "B": "endpoint/YourEndpointName/variantName/AllTraffic",
      "C": "sagemaker:endpoint:YourEndpointName:variant:AllTraffic",
      "D": "YourEndpointName/AllTraffic"
    },
    "explanation": "The resourceId for a real-time SageMaker endpoint variant must be specified as \"endpoint/<endpointName>/variant/<variantName>\"."
  },
  {
    "id": "f1efcc2295f4440399e42d6b9d49daa6af350a006614ae025cdfb628c5f9d067",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "You are deploying a new version of your VpcConfig for a SageMaker endpoint in CloudFormation. You modify the Subnets list in the template and redeploy, but the change is not applied. Why?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CloudFormation cannot update VpcConfig on an existing EndpointConfiguration; you must modify another property.",
      "B": "You must delete the EndpointConfiguration resource manually before CloudFormation can apply changes.",
      "C": "CloudFormation only creates a new AWS::SageMaker::EndpointConfiguration when the Endpoint resource\u2019s EndpointConfigName property is updated; you haven\u2019t changed it.",
      "D": "VpcConfig changes require an update to AWS::SageMaker::Model, not EndpointConfiguration."
    },
    "explanation": "Modifying VpcConfig in the EndpointConfiguration has no effect until you update the Endpoint resource\u2019s EndpointConfigName to reference the new configuration."
  },
  {
    "id": "26f1516bea8fc91e4ffbe90df63d5cef1c097f8f33c4b79a60d3088f021bc5d2",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "You have an existing CloudFormation template defining your SageMaker model and endpoint. You decide to adopt AWS CDK but want to reuse the existing template without rewriting it. Which CDK construct allows you to embed and extend the existing template?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CfnModel",
      "B": "TemplatePart",
      "C": "IncludeTemplate",
      "D": "CfnInclude"
    },
    "explanation": "CfnInclude (from aws-cdk-lib/cloudformation-include) lets you import an existing CloudFormation template into a CDK app for extension."
  },
  {
    "id": "b685d0ee502af6130bb84bc38ee186cdbd6b104f404e74350edcbe3f39fc4ea7",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A data science team needs to implement a CI/CD pipeline that automates model training, testing, and deployment for a SageMaker ML model. The pipeline should enforce a manual approval before deploying to production and use AWS CodePipeline with minimal custom code. Which pipeline configuration meets these requirements?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a manual approval action between the CodeBuild action that trains the model and the CloudFormation Deploy action that updates the production SageMaker endpoint.",
      "B": "Use a Lambda function as a CodePipeline action to pause for approval between the training and deployment stages.",
      "C": "Configure the SageMaker Model Training action to require manual confirmation before executing the SageMaker Model Deploy action.",
      "D": "Use a CloudWatch Events rule to trigger a manual deployment after the training stage finishes."
    },
    "explanation": "CodePipeline supports manual approval actions natively; inserting an Approval action between the training (CodeBuild) stage and the deployment (CloudFormation Deploy endpoint) stage provides a built-in manual gate with minimal custom code."
  },
  {
    "id": "f2399d32a880e27d4cfd26ced81dc9420e189636e2a07455f37d9dfcac3ecee5",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "An ML engineer must trigger a CI/CD pipeline whenever new training data arrives in an S3 bucket. The pipeline uses CodePipeline and CodeBuild to preprocess data and train a SageMaker model. Which configuration best accomplishes this requirement with event-driven automation?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure the S3 bucket as the Source stage in CodePipeline and enable change detection polling.",
      "B": "Create a CloudWatch Events rule for s3:ObjectCreated:* that targets the CodePipeline API to start a pipeline execution.",
      "C": "Use an S3 Event Notification to invoke a Lambda function that calls StartPipelineExecution for the CodePipeline.",
      "D": "Configure EventBridge to listen for S3 notifications and invoke the CodeBuild project directly."
    },
    "explanation": "Using an S3 event notification to invoke a Lambda function that calls StartPipelineExecution provides event-driven triggering with fine-grained control and low latency."
  },
  {
    "id": "3928ece5bd6b2649b91239411440658181bd7497b26ddbd9b563fb07a173af23",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A team wants to deploy updated SageMaker endpoints in a blue/green deployment fashion as part of their CI/CD pipeline. They need to shift traffic gradually to the new model and enable easy rollback if issues occur. Which approach using AWS native services meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a SageMaker CreateModel action in CodePipeline with the 'Blue/Green' deployment type and specify the traffic shifting percentage.",
      "B": "Use CloudFormation Deploy action in CodePipeline configured with CodeDeploy SafeMode for CloudFormation change sets to perform traffic shifting between endpoints.",
      "C": "Invoke the SageMakerUpdateEndpoint API in CodeBuild and script traffic weights in the buildspec.",
      "D": "Configure a Lambda function in a pipeline action that calls UpdateEndpointWeightsAndCapacities for traffic shifting."
    },
    "explanation": "Using a CloudFormation Deploy action with CodeDeploy-managed change sets enables native blue/green deployments and traffic shifting with rollback capabilities without custom scripts."
  },
  {
    "id": "8e4c25f75b790e4aa90e3f7f9b88b9a200c44ea1286ba1238f63289951f55f60",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A CodeBuild project in a CI/CD pipeline needs to access resources inside a VPC (private RDS and SageMaker endpoint). Builds are failing because the project cannot reach VPC-only endpoints. How should you modify the CodeBuild configuration to allow access while maintaining least privilege?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add the CodeBuild project to the same security group as the RDS instance and SageMaker endpoint.",
      "B": "Configure the CodeBuild project with VPC configuration specifying the subnets and security groups that allow access to the private endpoints.",
      "C": "Create a NAT gateway in the VPC and enable public access for the CodeBuild project.",
      "D": "Enable CodeBuild network isolation and whitelist the VPC CIDR block."
    },
    "explanation": "Configuring the CodeBuild project with the correct VPC, subnets, and security groups allows it to access private resources securely and maintain least privilege."
  },
  {
    "id": "3288744e396624795532f7fd8e46df8a8bf8f7a3b6e4651e5e521a5c6122900a",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "You need to add automated testing stages to your ML CI/CD pipeline: unit tests for preprocessing code, integration tests against a development SageMaker endpoint, and performance validation of the new model. Which pipeline actions should you include to satisfy these requirements?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Three CodeBuild actions: one running pytest for code, one invoking the dev endpoint via AWS CLI, and one running a custom performance test script.",
      "B": "One CodeBuild action with sequential buildspec phases for unit, integration, and performance tests.",
      "C": "Unit tests as a CodeBuild action, integration tests as a Lambda invoke action, and performance tests as a SageMaker Batch Transform action.",
      "D": "Integration tests first, then unit tests, then performance validation, all in a single CodeBuild action."
    },
    "explanation": "Separating testing into distinct CodeBuild actions for unit, integration, and performance ensures isolation and clear visibility of failures, and uses the native build environment to invoke tests."
  },
  {
    "id": "0000e07ce6916883984e41915532e0fbea8c553194f950084da365dcbb4abd67",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "An ML engineer has a SageMaker Pipeline defined for data preprocessing, model training, and evaluation. To include this SageMaker Pipeline in a larger CodePipeline CI/CD workflow, which native CodePipeline action type should they use to start and monitor the SageMaker Pipeline execution?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS CloudFormation action with a custom resource to invoke the SageMaker Pipeline.",
      "B": "AWS Lambda invoke action that calls StartPipelineExecution.",
      "C": "SageMakerPipeline action type provided by AWS CodePipeline.",
      "D": "CodeBuild action using AWS CLI to start the SageMaker Pipeline."
    },
    "explanation": "CodePipeline provides the SageMakerPipeline action to natively integrate SageMaker Pipelines as a stage, handling execution and status monitoring without custom scripts."
  },
  {
    "id": "d7e22c21bc86e0229406fba353cd3f6ec822e3c74204e056721024eb3ed0c157",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your ML CI/CD pipeline uses CodeBuild to package and push Docker images to a private ECR repository encrypted with a customer-managed KMS key. Builds are failing when CodeBuild attempts to push the image. What minimum IAM policy change should you apply to the CodeBuild service role to resolve this issue?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Add kms:Decrypt permission on the KMS key used to encrypt the ECR repository.",
      "B": "Add kms:GenerateDataKey permission on the KMS key.",
      "C": "Add kms:Encrypt and kms:Decrypt permissions on the KMS key.",
      "D": "Add kms:DescribeKey permission on the KMS key."
    },
    "explanation": "CodeBuild needs kms:GenerateDataKey to encrypt the image layers before pushing to ECR. Decrypt is not required for push operations."
  },
  {
    "id": "400aec24693b7b7178236da5e342fbf8189ef48a6f231c6398994b24aafe27b9",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A team adopts GitFlow branching for ML code and infrastructure definitions. How should branches map to CodePipeline stages to implement dev, test, and prod environments with automated promotion?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Map the 'develop' branch as the Source for the dev pipeline, 'release' for test, and 'master' for prod, each with its own CodePipeline using source triggers.",
      "B": "Use 'feature' branches for dev, 'develop' for test, and 'release' for prod, all in a single pipeline with multiple source actions.",
      "C": "Use 'master' branch for all environments and control deployments with manual approval actions.",
      "D": "Use 'hotfix' branches to promote code directly to production pipeline."
    },
    "explanation": "GitFlow maps develop \u2192 dev pipeline, release \u2192 test pipeline, and master \u2192 production pipeline, enabling automated promotions based on branch."
  },
  {
    "id": "f9835fe2169eeb2fe16c3b8138b6b52bd384280e5aed947b4694a7a2516ad867",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "You configured SageMaker Model Monitor to detect data drift for a production endpoint. You want your CI/CD pipeline to automatically retrain the model when drift is detected. Which event pattern and target configuration should you use in EventBridge to integrate drift detection with your pipeline?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Event pattern: SageMaker Model Monitor DataQualityCheckNotification; Target: CodePipeline StartPipelineExecution action.",
      "B": "Event pattern: CloudWatch Alarm for drift metric; Target: Lambda that updates the SageMaker endpoint.",
      "C": "Event pattern: SageMaker TrainingJobStateChange; Target: EventBridge rule that triggers retraining.",
      "D": "Event pattern: S3:ObjectCreated for captured data; Target: CodeBuild to start training."
    },
    "explanation": "Model Monitor emits DataQualityCheckNotification events; capturing these in an EventBridge rule targeting StartPipelineExecution automates retraining when drift is detected."
  },
  {
    "id": "0d0a153fb5b07933c858cc18b6abaefea4f999b85ac2d98f11d5ea4a528fb75e",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "You want to define your entire CI/CD pipeline in AWS CDK, including stages for building, testing, and deploying a SageMaker model. Which CDK construct and patterns should you use to best represent stages and actions?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use pipelines.CodePipeline construct with pipelines.CodeBuildStep and pipelines.ShellStep.",
      "B": "Use aws_codepipeline.Pipeline with aws_codepipeline_actions.CodeBuildAction and aws_codepipeline_actions.CloudFormationCreateUpdateStackAction.",
      "C": "Use aws_sagemaker.CfnPipeline and embed CodePipeline definitions as metadata.",
      "D": "Use a single CodeBuild project in CDK and run all steps in buildspec."
    },
    "explanation": "Using aws_codepipeline.Pipeline with native CodeBuild and CloudFormation actions allows explicit definition of CI/CD stages and is the recommended pattern in CDK for pipelines."
  },
  {
    "id": "1da0ca29d4f6f0f421b597e0d16421ea7e4c62969069f82589d643eeed27af66",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "During a CodePipeline execution, the CodeBuild step that builds the Docker container for a custom SageMaker algorithm fails due to insufficient privileges for Docker. Which setting in the CodeBuild project should you enable to allow Docker builds?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set privileged mode to true in the CodeBuild project settings.",
      "B": "Assign the CodeBuild role to the DockerUsers group in IAM.",
      "C": "Enable inbound and outbound network access in CodeBuild.",
      "D": "Grant CodeBuild service role permissions for sagemaker:CreateAlgorithm."
    },
    "explanation": "Privileged mode allows CodeBuild to run Docker commands needed for building and pushing container images."
  },
  {
    "id": "82a70d463ecb72ce1973a5b44c16c82096084433bf923b7f40672a9aad8b49b1",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "You need to validate model performance metrics generated during training before deploying the model in the CI/CD pipeline. Which CodePipeline action can you use to automatically compare the new metrics against a baseline and fail the pipeline if the model does not meet thresholds?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a Lambda invoke action that reads metrics from S3 and throws an error if thresholds are not met.",
      "B": "Use a CloudWatch Alarm action in CodePipeline to evaluate metrics.",
      "C": "Use the built-in SageMaker Model Quality check action in CodePipeline.",
      "D": "Use a CodeBuild action with a buildspec that runs a custom validation script."
    },
    "explanation": "CodePipeline does not have a native metric gating action; invoking a Lambda to assert metric thresholds provides a serverless, automated gate."
  },
  {
    "id": "ea858ee0f37b3fd7b42f546811790991f42c4b377fea9de325bd2f47fc07c7b9",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "In your pipeline, the output of the model training stage is a model artifact location in S3. You need to pass this dynamic S3 path to the SageMaker CreateModel deployment stage in CodePipeline. How should you configure the deployment action to consume this artifact location?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use CodePipeline artifact variables and parameter overrides in the SageMaker CreateModel action configuration.",
      "B": "Hardcode the S3 path in the CreateModel action ARN.",
      "C": "Write the path to Systems Manager Parameter Store and reference it in the deployment stage.",
      "D": "Use a Lambda function to retrieve the location and call CreateModel."
    },
    "explanation": "Artifact variables allow CodePipeline stages to access outputs from previous stages and dynamically pass them to action parameters without custom code."
  },
  {
    "id": "7be24b1dd9815acec24889b0aa926a956fb6db9965393efa50707eb613e3c860",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your team is evaluating whether to use SageMaker Pipelines or AWS CodePipeline for end-to-end ML workflows. They need automated data preprocessing, hyperparameter tuning, model evaluation, and deployment gates. Which is the most appropriate orchestration tool?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Pipelines for ML steps and embed it as a stage in AWS CodePipeline for approvals and multi-account deployments.",
      "B": "Use AWS CodePipeline exclusively for all ML and deployment tasks.",
      "C": "Use AWS Step Functions to orchestrate both ML and deployment steps.",
      "D": "Use Amazon Managed Workflows for Apache Airflow (MWAA) to orchestrate SageMaker training and deploy."
    },
    "explanation": "SageMaker Pipelines provides first-class support for ML workflows; embedding it in CodePipeline adds enterprise-grade CI/CD features like approvals and cross-account deployments."
  },
  {
    "id": "b4f2ca63a4b1a02f60bc1a1833837b138f35993b24d55e6f658f5e2c46feac2f",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "To implement automated rollback on a SageMaker endpoint if post-deployment smoke tests fail, which deployment pattern should you configure in CodePipeline and CodeDeploy?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "All-at-once deployment with a Lambda test hook in CodeDeploy that triggers rollback on failure.",
      "B": "Canary deployment type in CloudFormation Deploy action with pre-traffic and post-traffic validation Lambda hooks.",
      "C": "Linear deployment with time-based rollout and manual approval for rollback.",
      "D": "In-place deployment with CodeBuild test stage preceding deployment."
    },
    "explanation": "Using a canary deployment with pre- and post-traffic validation hooks in CodeDeploy allows automated testing and rollback if smoke tests fail."
  },
  {
    "id": "ae647a02a96de52795944d136af6d1c204f314f30b0b1e1123a285ce38e1a996",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An ML engineer has deployed a real-time SageMaker inference endpoint and needs to detect both input feature distribution drift and prediction quality degradation in production with minimal custom code. Which combination of SageMaker services and configurations should the engineer use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure DataCaptureConfig on the endpoint, deploy a DefaultModelMonitor job to detect data drift, and schedule a ModelQualityMonitor job with ground truth to detect prediction drift.",
      "B": "Configure DataCaptureConfig on the endpoint and schedule a Clarify DataBiasMonitor job to detect both data and prediction drift.",
      "C": "Stream inference events to Kinesis Data Firehose and use AWS Glue to analyze drift for both features and predictions.",
      "D": "Use SageMaker Clarify ModelExplainabilityMonitor to detect distribution shifts and model accuracy degradation."
    },
    "explanation": "Use DataCaptureConfig + DefaultModelMonitor for input drift and ModelQualityMonitor with ground truth for prediction/concept drift; other options do not cover both aspects or require more custom work."
  },
  {
    "id": "8c54f2e843a01858fd7fdbace0f68fe2272558fc1a03c8cea9a0b332a7116b76",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "Before enabling continuous data drift detection on a production SageMaker endpoint, an ML engineer must establish baseline statistics and constraints. Which approach should the engineer use to generate these baselines automatically?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run a SageMaker Processing job using the DefaultModelMonitor container on a representative historical dataset in S3.",
      "B": "Use SageMaker Clarify DataBiasMonitor to create baseline drift constraints on the training data.",
      "C": "Write custom code to compute statistics and manually author the constraint JSON file.",
      "D": "Use AWS Glue DataBrew to profile the data and export a constraint file to S3."
    },
    "explanation": "The DefaultModelMonitor Processing container provides built-in functionality to compute baseline statistics and constraints; other options either don\u2019t generate constraints in the required format or don\u2019t support SageMaker\u2019s drift detectors."
  },
  {
    "id": "9b5c5d2e2caa3bb23585241a98a30ee7a902bcea9b288f84d45534fe9f48d280",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A financial services company must monitor inference data while excluding PII fields from retention. Which configuration in SageMaker Model Monitor allows the engineer to capture inference payloads but filter out PII attributes?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set DataCaptureConfig sample_percentage to 0 and use encryption to prevent PII capture.",
      "B": "Use Clarify ModelBiasMonitor with an exclude_columns parameter for PII fields.",
      "C": "Configure DataCaptureConfig with a CaptureFilter to exclude PII JSON paths before writing to S3.",
      "D": "Enable a processing container script to mask PII after the monitor job runs."
    },
    "explanation": "DataCaptureConfig\u2019s CaptureFilter lets you specify JSONPath or CSV column filters so only non-PII fields are captured; other methods either capture PII or require post-processing."
  },
  {
    "id": "8c94bdd0ab6f2d75fee6f6f7cf26d5a5114d57a005b4446fe17feee93e008111",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A Model Monitor violation report shows a numeric feature\u2019s distribution exceeding baseline constraints for variance only. What is the most appropriate next step?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Adjust the variance threshold in the baseline constraint to suppress false positives.",
      "B": "Investigate the production data distribution change, update training data or retrain the model if the shift reflects new valid patterns.",
      "C": "Disable monitoring for that feature to reduce alert noise.",
      "D": "Regenerate the baseline constraints on the current production data without retraining."
    },
    "explanation": "A constraint violation indicates real drift; you should investigate and retrain or augment training data if needed. Changing thresholds or disabling the monitor risks missing true drift."
  },
  {
    "id": "d64d1851cf75b3ab2a189976df9ed36a9d5c889e605fcdabe36ef32c63fa8329",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An operations team requires near real-time alerts whenever model input data drift is detected. Which integration provides automated notifications upon Model Monitor constraint violations?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Have a Lambda function poll the S3 violation reports folder every minute.",
      "B": "Create an Amazon EventBridge rule for SageMaker MonitoringExecutionStatus change events and target an SNS topic.",
      "C": "Subscribe an SNS notification directly to the Model Monitor S3 bucket.",
      "D": "Use CloudWatch Logs Insights to search for violations and trigger alarms."
    },
    "explanation": "EventBridge can capture SageMaker monitoring execution status change events and forward them to SNS; polling or log-based solutions introduce delay or complexity."
  },
  {
    "id": "502b52f875abaa646d35d3ca519b092659258c0f0c3dcceb5714b48d3319cddc",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A team wants to detect anomalies in inference latency, error rates, and invoke rates on a SageMaker real-time endpoint. Which approach will meet these requirements with the least operational overhead?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use Amazon CloudWatch built-in metrics (Latency, Invocations, 4xx/5xx error counts) and configure CloudWatch Alarms.",
      "B": "Use SageMaker Model Monitor to detect infrastructure anomalies.",
      "C": "Use SageMaker Clarify to monitor model performance metrics.",
      "D": "Enable AWS CloudTrail on the endpoint and analyze logs in S3."
    },
    "explanation": "CloudWatch automatically captures endpoint performance and error metrics; Model Monitor focuses on data quality, and Clarify focuses on bias/explainability."
  },
  {
    "id": "36673d835b3aaea0e18fac6a411286ba933a536cd4f8ad6399dd23524280ea75",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "Which statement best describes concept drift in a deployed ML model?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A change in the input feature distribution compared to baseline data.",
      "B": "A change in the statistical relationship between input features and target labels over time.",
      "C": "An imbalance in class label frequencies in production data.",
      "D": "A bias introduced during model training that only affects edge cases."
    },
    "explanation": "Concept drift refers to changes in P(Y|X), i.e., the relationship between features and labels; data drift is about P(X) changes."
  },
  {
    "id": "3dd2cafb82d0d896c23637194d7bef6f2eeec9532ab9fbbadda709ce7643fea8",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An ML engineer needs to gradually shift traffic to a canary variant of a SageMaker endpoint and automatically roll back if error rates exceed a threshold. Which deployment mechanism should they use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker multi\u2010variant endpoints with AWS CodeDeploy canary traffic\u2010shifting and configure a CloudWatch Alarm on 5xx error rate.",
      "B": "Deploy two separate endpoints and manually switch DNS when errors are low.",
      "C": "Use SageMaker asynchronous endpoints with weighted routing.",
      "D": "Use SageMaker batch transform jobs scheduled hourly and compare error rates."
    },
    "explanation": "SageMaker multi\u2010variant endpoints integrated with CodeDeploy support canary traffic shifts and automatic rollback via CloudWatch Alarms."
  },
  {
    "id": "5120acb05287fdf37c933f4d567547ba272856ba7370ee6995dbb1350568c0bc",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "To detect shifts in feature attributions of a production model over time, which SageMaker monitoring job should an ML engineer schedule?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A Clarify ModelExplainabilityMonitor job with SHAP baseline and monitoring configuration.",
      "B": "A Clarify DataBiasMonitor job on the inference data.",
      "C": "A DefaultModelMonitor job focusing on data quality constraints.",
      "D": "A ModelQualityMonitor job comparing accuracy against ground truth."
    },
    "explanation": "ModelExplainabilityMonitor (SHAP) jobs track how feature attributions change; data bias or quality monitors don\u2019t measure attribution shifts."
  },
  {
    "id": "216be463f900f3584c5457ffab72b77850c653d451594a197b44025be0ab37ca",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "What is the minimum supported frequency for scheduling a SageMaker MonitoringSchedule to detect production data drift?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Every 5 minutes",
      "B": "Every 1 minute",
      "C": "Every hour",
      "D": "Every 24 hours"
    },
    "explanation": "SageMaker MonitoringSchedule supports a minimum interval of 5 minutes; shorter intervals are not allowed."
  },
  {
    "id": "a134e42c8ca95fb8717607f7e76a37536a703742991e65d499e48a823655a30c",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An engineering team uses batch transform jobs for asynchronous inference and needs to monitor prediction quality drift against offline ground truth. Which solution requires the least operational overhead?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable DataCaptureConfig on the batch transform endpoint and run DefaultModelMonitor.",
      "B": "Implement a custom Lambda to compare S3 outputs to ground truth and publish metrics.",
      "C": "Schedule a ModelQualityMonitor ProcessingJob using the batch transform output folder and S3 ground truth labels.",
      "D": "Use Clarify DataBiasMonitor on the batch transform results."
    },
    "explanation": "Scheduling a ModelQualityMonitor ProcessingJob directly on transform outputs and labels uses built-in monitoring; other options require custom polling or inappropriate monitors."
  },
  {
    "id": "0c348b6a49383715f4b3bd258e3074846958e0615ba6ec65e4c7b5504e0033b8",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A production endpoint\u2019s data quality monitor runs but model accuracy has degraded without any data drift reported. What additional monitoring configuration is required?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a Clarify DataBiasMonitor to detect label shifts.",
      "B": "Increase the sampling percentage in DataCaptureConfig.",
      "C": "Enable multivariant monitoring on the same monitor.",
      "D": "Configure and schedule a ModelQualityMonitor job with ground truth labels to detect accuracy degradation."
    },
    "explanation": "DataQuality monitors P(X) shifts, not P(Y|X); ModelQualityMonitor with ground truth is required to detect concept or accuracy drift."
  },
  {
    "id": "49046e792bc61e6ff640c55c08df94db3f8f66536e804d64fc2f278296d026af",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A company must monitor production inference outputs for fairness drift (e.g., change in demographic parity) over time. Which SageMaker Clarify monitor should they schedule?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Clarify DataBiasMonitor",
      "B": "Clarify ModelBiasMonitor",
      "C": "ModelExplainabilityMonitor",
      "D": "DefaultModelMonitor"
    },
    "explanation": "ModelBiasMonitor detects fairness metrics (demographic parity, equalized odds) on inference data; DataBiasMonitor analyzes training set biases."
  },
  {
    "id": "9b640b26166f50330844db82815745eb66ca769fbdceaf0acd985b5e785adb72",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An ML engineer must filter out sensitive fields from inference data before it reaches the Model Monitor processing container. Which component should they customize?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provide a custom preprocessing script in the MonitoringSchedule\u2019s ProcessingJobConfig.",
      "B": "Write a post-monitoring ETL job to remove sensitive columns from violation reports.",
      "C": "Modify the DefaultModelMonitor container image to drop PII.",
      "D": "Configure DataCaptureConfig to drop features via sample_percentage."
    },
    "explanation": "A custom preprocessing script in MonitoringSchedule\u2019s ProcessingJobConfig lets you transform or filter data before constraint evaluation; other methods occur after capture or require image modification."
  },
  {
    "id": "3c4ff11d981b0e0eb94b32fb17a1e92953bce4f0e1d6c3aea387fb6def63ac41",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An ML engineer wants to integrate drift detection into a SageMaker Pipelines workflow and automatically trigger retraining when drift is detected. Which pipeline step should they include to evaluate drift violations before invoking the retraining branch?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a MonitoringStep with fail_on_violation=True to stop the pipeline on any drift.",
      "B": "Use a ConditionStep to check if the MonitoringStep status equals 'Failed'.",
      "C": "Use a RegisterModel step to register the model only if no drift is detected.",
      "D": "Use a ConditionStep to inspect the MonitoringStep output property 'BaselineViolations' > 0 and branch accordingly."
    },
    "explanation": "A ConditionStep can examine the MonitoringStep output (e.g., violated constraint count) and route to retraining; this avoids hard failures and enables branching logic."
  },
  {
    "id": "6ad8ce54c61ee031cf26ec9074a92edf315ae3d75bd53987a5da3dc50640c7d5",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "An ML team needs to right-size SageMaker inference endpoints to minimize cost while maintaining performance. They want to benchmark actual model inference performance (latency and under various loads) to choose optimal instance types. Which AWS service should they use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Compute Optimizer",
      "B": "AWS SageMaker Inference Recommender",
      "C": "AWS Cost Explorer",
      "D": "AWS Trusted Advisor"
    },
    "explanation": "SageMaker Inference Recommender runs performance benchmarks on your model to recommend optimal instance families and sizes. Compute Optimizer provides general EC2/EBS recommendations, not ML-specific inference benchmarks."
  },
  {
    "id": "400217422e25ecb3cf1259428137994015e5da169d271d3f73de611ebacba2f3",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A financial organization observes unexpected monthly spikes in SageMaker inference costs due to seasonal usage. They need to receive near real-time alerts when cost anomalies occur in their AWS account. Which solution meets this requirement with minimal operational overhead?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create a CloudWatch alarm on the AWS/Billing namespace EstimatedCharges metric",
      "B": "Configure an AWS Budget with email notifications when forecasted spend exceeds a threshold",
      "C": "Enable AWS Cost Anomaly Detection and configure alerts for anomaly events",
      "D": "Schedule daily Cost Explorer reports and parse them with Lambda for thresholds"
    },
    "explanation": "AWS Cost Anomaly Detection uses machine learning to detect unusual cost spikes and can send near real-time alerts without manual report parsing or budget forecasting."
  },
  {
    "id": "bdcff12635bcfa74ac6aa74983519d5f6c3b36a1cdead28508953460d066d8ec",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A deployed SageMaker endpoint is experiencing intermittent increases in tail latency. The ML engineer needs to trace requests through the network stack and service mesh to pinpoint network bottlenecks. Which AWS service or feature should they enable?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run CloudWatch Logs Insights on the endpoint logs",
      "B": "Monitor EC2 CPUUtilization metrics in CloudWatch",
      "C": "Enable AWS X-Ray integration with SageMaker",
      "D": "Enable VPC Flow Logs on the endpoint\u2019s ENIs"
    },
    "explanation": "AWS X-Ray provides distributed tracing across network and service calls, enabling end-to-end latency analysis. VPC Flow Logs show packet metadata but not distributed service traces."
  },
  {
    "id": "99890328d44b1d6ca2c04976489769a4943f88e9a6a24a417760ef2f0f7b6447",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "For compliance, the security team wants to audit all Amazon SageMaker CreateEndpoint API calls over the past 30 days and retain logs for 90 days. Which configuration achieves this with the LEAST administrative overhead and ensures immutability?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Enable CloudTrail management events for SageMaker and send to an S3 bucket with Object Lock enabled",
      "B": "Create a CloudWatch Events rule for CreateEndpoint and log events to CloudWatch Logs",
      "C": "Enable CloudTrail Data events on CreateEndpoint and stream to Kinesis Data Firehose",
      "D": "Enable CloudTrail Insights to capture anomalous CreateEndpoint activity"
    },
    "explanation": "CloudTrail management events capture all CreateEndpoint API calls; delivering them to an S3 bucket with Object Lock provides immutability. Data events and Insights are unnecessary for standard API logging."
  },
  {
    "id": "b56459d5a9435cbbec4cd5b3b370f3e8a3625ed16ec61a373532194dd98adb24",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A director wants a single dashboard that shows per-endpoint invocation count, average 95th percentile latency, and cost per hour for each SageMaker endpoint in the account. Which solution meets this requirement with minimal development effort?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Build a custom web app that calls CloudWatch metrics API and Cost Explorer API",
      "B": "Use Amazon QuickSight to visualize a dataset combining AWS Cost and Usage Reports and CloudWatch metrics",
      "C": "Use CloudWatch Dashboards with metric math combining performance and billing metrics",
      "D": "Use the AWS Pricing API to fetch rates and combine with CloudWatch metrics in dashboards"
    },
    "explanation": "QuickSight can natively ingest both Cost and Usage Report data (cost per resource) and CloudWatch metrics to build a unified dashboard with minimal custom code. CloudWatch Dashboards cannot display cost per resource granularity."
  },
  {
    "id": "531d08de65ab4f3774a631f477f302de66b1b4d3df2e3c270d3e77a8929c0863",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "To reduce off-peak costs, an ML engineer wants to automatically update SageMaker endpoint instance types to smaller ones every night and revert to original sizes each morning. Which solution requires the LEAST operational overhead?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Use SageMaker Pipelines with a scheduled pipeline to call UpdateEndpointConfig",
      "B": "Create EventBridge cron rules that trigger Lambda functions invoking UpdateEndpointConfig",
      "C": "Configure AWS Step Functions with Wait states and Lambda tasks to update endpoints",
      "D": "Use Systems Manager Automation documents scheduled via State Manager"
    },
    "explanation": "Using EventBridge with cron schedules triggering lightweight Lambda functions to call UpdateEndpointConfig is the simplest and lowest-overhead scheduling solution."
  },
  {
    "id": "47ed3f9de590c65ba5bb0f0a914b9e563d105e75cb06a42e130adaa80b711ca1",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A SageMaker real-time endpoint is exhibiting memory pressure, but memory utilization is not visible in CloudWatch by default. To monitor container-level memory for this endpoint, what should the ML engineer do?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable SageMaker Debugger profiling to collect memory metrics",
      "B": "Deploy the endpoint on EC2 instances where memory metrics are published by default",
      "C": "Install and configure the CloudWatch agent via a container lifecycle configuration",
      "D": "Use CloudWatch metric filters on container logs to estimate memory usage"
    },
    "explanation": "The CloudWatch agent must be installed inside the container via a lifecycle configuration script to collect OS-level metrics such as memory usage. Debugger profiles model internals, not OS memory."
  },
  {
    "id": "df815831082c31f100ca6594107f4d1c3e5722c41a144e353b9c308ba5ead178",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "An ML application experiences periodic bursts of inference requests that exceed the 70% CPU utilization threshold. The ML engineer wants to add step scaling to the existing auto scaling policy to provision two additional instances when InvocationsPerInstance exceeds 100 for 2 minutes, and remove one instance when it drops below 50 for 5 minutes. Which configuration meets these requirements?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Define two CloudWatch alarms on the SageMakerVariantInvocationsPerInstance metric with thresholds 100 (2-minute evaluation) and 50 (5-minute evaluation), and attach a step scaling policy with +2 and -1 adjustments",
      "B": "Use CPUUtilization alarms instead of InvocationsPerInstance and a target-tracking policy",
      "C": "Use a single target-tracking policy on InvocationsPerInstance with a target value of 75",
      "D": "Configure two AWS Budgets for usage and link them to auto scaling actions"
    },
    "explanation": "Step scaling requires separate CloudWatch alarms on the SageMakerVariantInvocationsPerInstance metric with specified evaluation periods and corresponding step adjustments (+2, -1). Using CPUUtilization or budgets would not meet the specified invocation-based requirements."
  },
  {
    "id": "08f75bc48dddcfb5906c53b32a56293ef948f913d12c3dfa40a03802c75fc596",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A financial ML endpoint must maintain 99th percentile latency under 200 ms. To automate scaling to achieve this SLO, which Application Auto Scaling policy should be configured?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Target tracking policy on CPUUtilization at 70%",
      "B": "Target tracking policy on SageMakerVariantInvocationLatency at the p99 200 ms target",
      "C": "Step scaling policy on InvocationsPerInstance thresholds",
      "D": "Step scaling policy on ModelLatencyAvg metric"
    },
    "explanation": "A target-tracking policy on the p99 lifecycle metric of SageMakerVariantInvocationLatency targeting a 200 ms threshold will automatically adjust capacity to maintain the latency SLO. CPUUtilization or average latency would be less precise for the p99 requirement."
  },
  {
    "id": "fdf8722b3a3c9c4085befec970e0aaa6cb8a2b30df8645a9b1031dc6181528f6",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "After deploying a model, the team wants to trigger a retraining SageMaker pipeline whenever a new training dataset file is uploaded to S3. They also want to log all such retraining triggers for audit. Which combination of AWS services should they use?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Configure an S3 EventBridge notification on object upload to start the pipeline, and rely on CloudTrail to log the pipeline StartPipelineExecution API call",
      "B": "Use a CloudWatch scheduled rule to scan S3 daily and start the pipeline if new files exist, logging via CloudWatch Logs",
      "C": "Deploy a Lambda function to poll S3 every hour, invoke the pipeline, and log to DynamoDB",
      "D": "Send S3 events to SNS and have the pipeline poll SNS, with audit in CloudWatch Metrics"
    },
    "explanation": "Using an S3 EventBridge notification provides immediate trigger of the pipeline on new data. CloudTrail automatically logs the StartPipelineExecution API call for auditing."
  },
  {
    "id": "f0007b83a452b00e4ec40da6ecfe0b01e03a8371b10ab8ce3a2dcbeb4cfcc957",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "Which AWS service provides generalized compute resource recommendations, including SageMaker endpoints and EC2 instances, based on historical utilization metrics and can be applied across accounts with minimal configuration?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Inference Recommender",
      "B": "AWS Compute Optimizer",
      "C": "AWS Cost Explorer rightsizing recommendations",
      "D": "AWS Trusted Advisor"
    },
    "explanation": "AWS Compute Optimizer analyzes historical utilization across EC2, SageMaker endpoints, and other resources to recommend optimal instance types, whereas Inference Recommender focuses specifically on ML inference benchmarking."
  },
  {
    "id": "46b7d9f9991dfe038a8a3299e3c6c54812cd1876e4b7bfcc517917e70e25d196",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "An engineering team needs to break down monthly SageMaker spend by project. They require cost allocation by tagging endpoints and jobs, and reporting at tag granularity. Which steps should they take?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Tag SageMaker endpoints and training jobs with project identifiers, activate those tags in AWS Billing Cost Allocation Tags, and use Cost Explorer with tag filters",
      "B": "Create separate AWS accounts per project and use consolidated billing",
      "C": "Use CloudWatch metric dimensions to filter cost metrics",
      "D": "Group SageMaker resources into separate CloudFormation stacks and view stack costs"
    },
    "explanation": "Activating resource tags for cost allocation and filtering in Cost Explorer is the standard way to break down costs by project without needing separate accounts."
  },
  {
    "id": "4bfc0a003c1be0d0b53e32e6c7a7b62f7e50539081fcd41803f1a513240e27f2",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "To optimize inference costs for stable production workloads with predictable traffic, which purchasing option should an ML engineer choose?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "On-Demand SageMaker Instances",
      "B": "SageMaker Savings Plans",
      "C": "EC2 Reserved Instances attached to SageMaker endpoints",
      "D": "SageMaker Spot Instances"
    },
    "explanation": "SageMaker Savings Plans offer a commitment discount on SageMaker compute usage for stable workloads. Spot Instances are not supported for real-time endpoints and Reserved Instances apply only to EC2, not SageMaker directly."
  },
  {
    "id": "8e734a17f38396f5bb6fd41d7732af9d10ad7fa9bc14289723207559957b50cd",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A model training workflow uses Amazon FSx for Lustre as input storage. The ML engineer notices frequent I/O throttling. To monitor FSx performance and identify bottlenecks, which CloudWatch metrics should they add to their dashboard?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "BurstCreditBalance and DataReadIOBytes",
      "B": "FreeStorageCapacity and NumberOfConnections",
      "C": "DataWriteIOPS and MetadataOperations",
      "D": "PercentIOPSUtilization and NetworkThroughput"
    },
    "explanation": "BurstCreditBalance shows available throughput credits and DataReadIOBytes shows actual read throughput, key metrics for diagnosing FSx for Lustre I/O throttling issues."
  },
  {
    "id": "de504f8ce82580f704c28033576a19271543d4a8308c572face38d26111e45cf",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "An ML engineer is troubleshooting intermittent 5XX errors from inference endpoints. The engineer has enabled data capture to S3. To quickly identify patterns in the errors, which approach provides the fastest operational insight?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure a CloudWatch Logs subscription filter to Lambda to parse S3 logs",
      "B": "Use CloudWatch Logs Insights with a query on the /aws/sagemaker/Endpoints log group to count HTTPStatus5XX occurrences",
      "C": "Download the S3 logs locally and run custom scripts",
      "D": "Use AWS X-Ray traces to calculate error percentages"
    },
    "explanation": "CloudWatch Logs Insights can rapidly query large volumes of log data in the /aws/sagemaker/Endpoints log group to identify and aggregate 5XX errors without moving data."
  },
  {
    "id": "63bfc6afcb79fcffead26e146c031b41a06ade1c559c8657e0a7dbc878185fad",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "An ML engineer has deployed a SageMaker inference endpoint that writes inference results and debug logs to an S3 bucket encrypted with a customer-managed KMS key. The security team requires that only this endpoint can write to the bucket and decrypt the objects. Which configuration meets these requirements with the least administrative effort?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Attach an IAM policy to the SageMaker execution role allowing s3:PutObject on the bucket and kms:Decrypt on the CMK.",
      "B": "Configure the S3 bucket policy to allow s3:PutObject only from the VPC endpoint used by SageMaker.",
      "C": "Configure the S3 bucket policy with a condition aws:SourceArn equal to the SageMaker endpoint ARN, and update the KMS key policy to grant only the SageMaker execution role decrypt permissions.",
      "D": "Create an S3 ACL that grants the SageMaker service principal write access and rely on the IAM role for decryption."
    },
    "explanation": "A resource-based S3 bucket policy using aws:SourceArn ensures only that SageMaker endpoint can write objects, and the CMK key policy must explicitly grant the SageMaker execution role kms:Decrypt. IAM identity policies alone or ACLs cannot enforce both write and decrypt at the resource level as simply."
  },
  {
    "id": "3135fa8e34d4d607f4c8ae9e6d85623ef1ae13bd952938ebce097180bb4ab081",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A security audit finds that any IAM user in the account can create SageMaker Studio user profiles, but the security team wants only members of the IAM group ProdMLUsers to be allowed. Which approach enforces this requirement at the SageMaker domain level?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add an identity-based IAM policy to the ProdMLUsers group allowing sagemaker:CreateUserProfile.",
      "B": "Attach a Service Control Policy in AWS Organizations denying sagemaker:CreateUserProfile unless the user is in ProdMLUsers.",
      "C": "Attach a resource-based policy to the SageMaker Domain that allows CreateUserProfile only when aws:PrincipalIsInGroup equals ProdMLUsers.",
      "D": "Use an S3 bucket policy to block studio creation from principals not in the group."
    },
    "explanation": "A SageMaker Domain resource policy can restrict CreateUserProfile to principals in a specific IAM group via aws:PrincipalIsInGroup. SCPs or identity policies alone cannot enforce at the domain resource level."
  },
  {
    "id": "53596bbb38c97b048f62f038e764d6071256343608b2246fdb3ec2050a30194b",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "An organization wants to run SageMaker training jobs in a VPC with no internet egress, but the training data resides in S3. Which network configuration meets these requirements while minimizing cost and operational overhead?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provision a NAT Gateway in the private subnet to access S3.",
      "B": "Attach an Internet Gateway to the VPC and allow routing to S3.",
      "C": "Create a VPC Gateway endpoint for S3 and update the private route table to direct S3 traffic through it.",
      "D": "Create an Interface VPC endpoint for SageMaker in the VPC and rely on AWS private networking for S3 access."
    },
    "explanation": "A Gateway VPC endpoint for S3 allows private, cost-effective access to S3 from a VPC with no internet egress. An interface endpoint for SageMaker does not enable S3 data access, and a NAT Gateway adds cost."
  },
  {
    "id": "fdebb6a65ff88896af46b691ed5e77d355e26d8ee067daa5dd75f15d9662d4ad",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A data science team wants to share a SageMaker Model Registry package with a partner AWS account. The model artifacts are encrypted by a customer-managed CMK. The partner must be able to deploy the model from their account. Which combination of steps is required?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "In the CMK console, add the partner account as a key administrator, and update the model package resource policy to grant DescribeModelPackage.",
      "B": "Add the partner AWS account principal to the CMK key policy with decrypt permissions, and attach a resource policy to the model package allowing that account DescribeModelPackage and CreateModel.",
      "C": "Enable cross-account sharing in SageMaker Model Registry settings and share the CMK ARN.",
      "D": "Create a cross-account IAM role in the partner account with sagemaker:CreateModel permission and trust the partner\u2019s account; no changes to CMK."
    },
    "explanation": "Both the CMK and the model package need resource policies. The key policy must allow the partner account to decrypt, and the model package policy must allow DescribeModelPackage and CreateModel so they can register and deploy it."
  },
  {
    "id": "c48baea135c630c5a865041e5e134bbddd122563e9778445860752de5628f93a",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A centralized CodePipeline in Account A builds ML packages and needs to deploy a trained model to a SageMaker endpoint in Account B. The security team requires least privilege. How should the cross-account permissions be configured?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "In Account A, grant the CodePipeline role sagemaker:* across all resources in Account B and trust Account B\u2019s principals.",
      "B": "Use CloudFormation StackSets from Account A to provision SageMaker resources in Account B without IAM role assumption.",
      "C": "In Account B, create an IAM role with sagemaker:CreateEndpoint and related actions, trust policy allowing assumption by the CodePipeline role\u2019s ARN in Account A, and configure the pipeline to assume that role.",
      "D": "In Account B, add the CodePipeline service principal from Account A to an IAM group with full SageMaker privileges."
    },
    "explanation": "The pipeline in Account A should assume a dedicated IAM role in Account B that has only the permissions needed to deploy the SageMaker endpoint. This follows least-privilege and standard cross-account role assumption practices."
  },
  {
    "id": "91a584c6bbffcc62bc049fe4b0bff74faa411babeb86c5521a90d93f272825a3",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "The security team requires that all SageMaker data-plane API calls (for example, CreateModel, InvokeEndpoint) be logged in CloudTrail. By default, only management events are recorded. What must the ML engineer do to capture these data-plane operations?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable CloudTrail management events in all regions.",
      "B": "In the CloudTrail console, enable data events for Amazon SageMaker to record data-plane API calls.",
      "C": "Enable AWS Config recording for SageMaker resource types.",
      "D": "Create CloudWatch Logs metric filters for SageMaker API calls."
    },
    "explanation": "To log data-plane API operations such as CreateModel and InvokeEndpoint, you must enable CloudTrail data events specifically for the SageMaker service. Management events alone do not capture these."
  },
  {
    "id": "eb60edb580ee9da6df807a65bcbff0643872c966b03a7f41ad687039f1e9d491",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "An enterprise wants to expose a private SageMaker inference endpoint to on-premises clients over their VPN without using the internet. Which architecture meets this requirement securely with minimal overhead?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy an Amazon NLB in front of the SageMaker endpoint and configure an Interface VPC endpoint (AWS PrivateLink) for the NLB; route on-prem VPN traffic to it.",
      "B": "Create an Internet-facing Application Load Balancer in front of the endpoint and restrict access via CIDR.",
      "C": "Peer the SageMaker VPC with the on-premises network and access the endpoint via peering.",
      "D": "Use AWS Transit Gateway to route to a public-facing endpoint with strong security groups."
    },
    "explanation": "Using a Network Load Balancer with a PrivateLink interface endpoint allows on-premises VPN clients to connect privately without exposing the endpoint to the internet, and with minimal additional components."
  },
  {
    "id": "3c560b392e27924189b1afd86fe3bf4776ee43e54668453c21d55d66b5a9d57a",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A security policy requires that every SageMaker training job include a CostCenter tag and use the execution role arn:aws:iam::123456789012:role/MLExecRole. Which feature can the ML engineer use to enforce both requirements at job creation?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use an IAM permissions boundary on ML users to restrict tag values.",
      "B": "Deploy an AWS Organizations SCP that denies sagemaker:CreateTrainingJob when tags or role do not match.",
      "C": "Enable an AWS Config rule for SageMaker jobs to require tags and roles.",
      "D": "Attach an IAM policy to users with a condition on sagemaker:RequestTag/CostCenter and sagemaker:ResourceTag/CostCenter and a condition requiring the UseServiceRole parameter equals MLExecRole."
    },
    "explanation": "An IAM identity policy with sagemaker:RequestTag and sagemaker:ResourceTag conditions can prevent job creation if the CostCenter tag is missing or if the specified ServiceRole is not MLExecRole. SCPs cannot inspect request tags."
  },
  {
    "id": "dee40f8336224dd094a2b0d441eadfcc0582072f45942edf346ad7914b9a4aa0",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A processing job running in SageMaker needs to mount an Amazon EFS file system for intermediate data. The security team requires encryption at rest and in transit for EFS, and only the processing job\u2019s subnets should be able to mount it. Which configuration meets these requirements?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create an unencrypted EFS, rely on encryption in transit only, and restrict mount via security group.",
      "B": "Create an encrypted EFS with SSE-KMS, mount it in the processing job with encryption in transit disabled.",
      "C": "Use FSx for Lustre with default encryption, and mount via a VPN.",
      "D": "Create an EFS file system encrypted at rest with a CMK, enable encryption in transit (TLS) on the mount target, and restrict the EFS security group to only allow mount traffic from the processing job\u2019s security group."
    },
    "explanation": "An Amazon EFS file system can be encrypted at rest using a CMK and can enforce TLS encryption in transit. Security groups can restrict mount access so that only SageMaker processing job instances can connect."
  },
  {
    "id": "f3a581256373d29e8ebca50d627e25ff93f352b239bb54059911c6596efe47c6",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "The security team wants only members of the SecurityAudit role to read SageMaker logs in CloudWatch Logs. Other users must be denied. How can this be implemented?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Attach a resource-based policy to the specific CloudWatch Log Group that allows only the SecurityAudit role to GetLogEvents and FilterLogEvents.",
      "B": "Encrypt the log group with a CMK and grant decrypt only to the SecurityAudit role.",
      "C": "Use an IAM identity policy denying CloudWatch Logs actions unless aws:PrincipalArn equals the SecurityAudit role.",
      "D": "Move the log group to a different AWS account used by security."
    },
    "explanation": "CloudWatch Log Groups support resource-based policies that can explicitly allow only a principal to perform log-reading actions. This is the most direct way to restrict access at the log-group level."
  },
  {
    "id": "c3bfca4b1a88683070a9339766db439bbbe075c5741f17d4db290dfdb6364b68",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A CI/CD pipeline in CodePipeline uses an ECR repository for custom container images. The security team requires that only this pipeline can pull images and no other IAM principals. Which configuration enforces this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Attach an IAM policy to the pipeline\u2019s role allowing ecr:BatchGetImage on the repo.",
      "B": "Add an ECR repository resource policy that allows ecr:BatchGetImage for the CodePipeline service role ARN and denies all others.",
      "C": "Use SCP to deny ecr:BatchGetImage globally for all principals except the pipeline.",
      "D": "Rely on IAM identity policies on all users to not allow ECR actions."
    },
    "explanation": "An ECR repository policy can directly specify which principals (the pipeline role) are allowed to pull images and deny everyone else, enforcing container image access at the resource level."
  },
  {
    "id": "5131acebeb26b4bc0a1f5a677b19a60fc5faedce73f4e58c7c2e1549ecd775e5",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "The security team wants to prevent developers from creating any SageMaker notebook instances or Studio domains that are internet-facing. Which control can achieve this across the organization?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use an IAM permissions boundary for all developers disallowing sagemaker:CreateDomain.",
      "B": "Apply an AWS Organizations Service Control Policy that denies sagemaker:CreateDomain or CreateNotebookInstance when NetworkIsolation is false.",
      "C": "Attach an identity policy to each developer denying notebook creation.",
      "D": "Use AWS Config to detect and remediate non-VPC notebooks."
    },
    "explanation": "A Service Control Policy can centrally deny creation of SageMaker notebooks or domains unless the NetworkIsolation parameter is set to true, preventing internet access organization-wide."
  },
  {
    "id": "a3e5d4d4ba2b5205f2cda810f6a51f014c9e0855be1d36c501aef6d29e734557",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "An automated retraining workflow triggers a Lambda function via EventBridge. The function needs only permissions to DescribeEndpoint and CreateTrainingJob for a specific endpoint and training job prefix. How should the IAM role be defined to follow least privilege?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Grant sagemaker:* on all resources in the account.",
      "B": "Grant sagemaker:DescribeEndpoint and sagemaker:CreateTrainingJob on all SageMaker ARNs.",
      "C": "Grant sagemaker:DescribeEndpoint on the specific endpoint ARN and sagemaker:CreateTrainingJob with a resource ARN pattern matching the training-job-prefix*, and no other actions.",
      "D": "Attach AWS managed SageMaker full-access policy."
    },
    "explanation": "Defining resource-level permissions for only the specific endpoint and a wildcard training job prefix ensures the Lambda role has the minimum permissions required for its function."
  },
  {
    "id": "2b2a2fecbd7fca639f1e257375d85083dc80045f9dbbb306e5f7154dbf797e52",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "Security requirements dictate that SageMaker notebook instances access internal services only on port 443 and cannot initiate outbound traffic to the internet. Which combination of controls enforces this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure the notebook instance\u2019s security group to allow outbound 443 to specific subnets and attach an Internet Gateway to the VPC.",
      "B": "Use a NAT Gateway with a route table restricting to internal CIDR.",
      "C": "Configure the SG to allow outbound TCP 443 to internal subnet CIDR, set the route table for the private subnet with no internet gateway, and use a VPC Endpoint for required AWS service calls.",
      "D": "Deploy a firewall appliance in the VPC to filter outbound traffic."
    },
    "explanation": "Removing an internet gateway and not configuring a NAT Gateway prevents internet egress. A security group can allow only TCP 443 to internal subnets. VPC endpoints enable needed AWS service access without the internet."
  },
  {
    "id": "71a26e8ce27f4d8b2912e47a8710ffb8657f3b883d2e27a09c60db5e87939a13",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "An ML engineer needs to capture inference requests and responses in a SageMaker endpoint. The security team requires that the data capture archive in S3 be encrypted at rest with a CMK and that all data in transit use TLS. Which configuration achieves this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable S3 default encryption on the bucket and configure DataCaptureConfig without specifying KMS.",
      "B": "Specify a KMS key in the SageMaker ModelRegistry settings and enable DataCaptureConfig.",
      "C": "Use an S3 bucket policy to require server-side encryption and rely on endpoint defaults.",
      "D": "In the endpoint\u2019s DataCaptureConfig, set EnableCapture true and specify the CMK KmsKeyId, and ensure the endpoint uses HTTPS (TLS) invocations."
    },
    "explanation": "DataCaptureConfig allows specifying a customer-managed KMS key for server-side encryption of captured data. SageMaker endpoints always use HTTPS for traffic, ensuring TLS in transit."
  },
  {
    "id": "ade2bbebd8400f5cc8f5c98964c759c2e83d1f7a85adeb409bb869fb9fb7ec76",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A data engineering team must transfer 100 TB of historical log files from an on-premises data center to Amazon S3 within 48 hours. The network bandwidth is limited to 1 Gbps and is cost-constrained. Which ingestion method meets the requirement with the lowest operational overhead?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS DataSync over the 1 Gbps link with parallel transfers.",
      "B": "Use S3 Transfer Acceleration over the internet connection.",
      "C": "Ship multiple AWS Snowball Edge devices and import data.",
      "D": "Use a custom multi-threaded upload client over the VPN."
    },
    "explanation": "At 1 Gbps, 100 TB over the network exceeds 48 hours; Snowball Edge avoids bandwidth constraints and minimizes operational complexity."
  },
  {
    "id": "bf98e61a123f7f2c309935d1c6aed9adcece4536f99de1aa3eb63df09c7d418d",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An ML workflow requires high-throughput reading of a 200 GB dataset during training in SageMaker. The data is stored in CSV format in S3. Training jobs are currently I/O-bound. Which change yields the greatest reduction in I/O time?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable S3 Transfer Acceleration on the bucket.",
      "B": "Convert the dataset to Parquet with Snappy compression and use Apache Arrow for reading.",
      "C": "Increase the SageMaker instance EBS volume throughput.",
      "D": "Enable S3 request rate optimization on the bucket."
    },
    "explanation": "Parquet is a columnar, compressed format that reduces data transfer size and read time, outperforming acceleration or EBS tweaks."
  },
  {
    "id": "2e4406cdc301c9ff1b2faf485c71b48736e6b126e4a5b0435dfb762a0f4b7e0b",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A streaming ingestion pipeline uses Amazon Kinesis Data Streams with 10 shards. Downstream training jobs require data partitioned by customer ID. The engineer notices uneven data distribution and hot shard errors. Which solution evenly distributes load?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase to 20 shards without changing partition key.",
      "B": "Use a composite primary key combining customer ID and timestamp.",
      "C": "Switch to Amazon MSK and let Kafka partition dynamically.",
      "D": "Hash the customer ID with a random suffix as the partition key."
    },
    "explanation": "Appending a random suffix to the customer ID spreads records across shards evenly, avoiding hot shards while preserving grouping on average."
  },
  {
    "id": "b5c2d9de4b1315053d702a95b2f48c83156a365ec69dd35de13e7e376b3c3d70",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A SageMaker Data Wrangler flow reads multiple small JSON files (~100 MB each) from S3. The job startup and cataloging latency dominates time. How can the engineer minimize this overhead?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Consolidate the small files into larger partitioned Parquet files.",
      "B": "Increase the SageMaker Data Wrangler instance type.",
      "C": "Use S3 Select on each JSON file.",
      "D": "Enable S3 Requester Pays on the bucket."
    },
    "explanation": "Consolidating files into fewer larger Parquet partitions reduces per-object overhead and speeds up scanning and schema inference."
  },
  {
    "id": "0e0ba8a1cc2b619dd1513cc3f8f9e7120191802b0c3009c2c629c82396f479a9",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An application uses Amazon Kinesis Data Firehose to deliver JSON records to S3. The delivery latency spikes under peak load. Which Firehose configuration change will smooth delivery latency while controlling costs?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set buffer size to minimum and buffer interval to default.",
      "B": "Set buffer size to maximum and buffer interval to minimum.",
      "C": "Increase buffer size to max and buffer interval to 300 seconds.",
      "D": "Decrease buffer size to minimum and buffer interval to 300 seconds."
    },
    "explanation": "Larger buffer size with a longer interval results in fewer deliveries, smoothing spikes and reducing egress costs."
  },
  {
    "id": "10b61c34305d70b4142cc57c62a3089832d9660aec2553b96d935a80bd761952",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A team must ingest near-real-time IoT telemetry (5 KB messages, 5,000 msg/sec) into S3 for batch training. Which AWS service combination best meets throughput and minimal operational overhead?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS IoT Core + Lambda writing to S3.",
      "B": "Kinesis Data Streams + Kinesis Data Firehose to S3.",
      "C": "MSK cluster + custom consumer that writes to S3.",
      "D": "Directly upload from devices to S3 using the AWS SDK."
    },
    "explanation": "Kinesis Streams scales to handle throughput and Firehose auto-batches and delivers to S3 without infra management."
  },
  {
    "id": "2453f20c51c08945f1c1ab92aa463a6275be348c972f93133ce38520bbfd52b8",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A data scientist merges customer profile data from Amazon RDS and clickstream logs stored in S3. They need an automated, serverless solution with minimal code. Which approach is optimal?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Deploy an EMR cluster with Spark to join RDS and S3 datasets.",
      "B": "Use AWS Glue ETL jobs on a scheduled basis.",
      "C": "Use SageMaker Processing with custom Pandas scripts.",
      "D": "Use AWS Glue DataBrew recipe to connect to RDS and S3 and join datasets."
    },
    "explanation": "DataBrew provides serverless, no-code recipes to join RDS and S3 datasets, minimizing code and infra."
  },
  {
    "id": "3ef1bd12f29f9429003533136c0fd9e8fc3ecb800b8f02fd9214a05de3e3be81",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A company requires secure ingestion of PII data into Amazon S3. Data must be encrypted in transit and at rest, and decryption only occurs in SageMaker. Which configuration satisfies these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable SSE-S3 on the bucket and HTTPS for uploads.",
      "B": "Configure HTTPS for uploads, enable SSE-KMS with a CMK granting only SageMaker decryption.",
      "C": "Use client-side encryption and store keys in AWS Secrets Manager.",
      "D": "Enable SSE-KMS and grant all users read access to the CMK."
    },
    "explanation": "SSE-KMS with CMK access restricted to SageMaker ensures encrypted transit and at-rest encryption, with decryption only in SageMaker."
  },
  {
    "id": "db52f94bbb9df4b34428f7abe00c4cb81940c8c64635412d3141e70b5223c2c8",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An ML pipeline uses Amazon MSK to stream data into S3 via Kafka Connect. During peak, Kafka Connect tasks lag behind. Which change will improve ingestion throughput?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase Kafka Connect workers and partitions for the connector topic.",
      "B": "Switch connector batch.size to 1.",
      "C": "Use a single large Kafka Connect worker.",
      "D": "Decrease connector retries to zero."
    },
    "explanation": "More workers and partitions allow parallelism in Kafka Connect, increasing ingestion throughput and reducing lag."
  },
  {
    "id": "774b55002992cbe2ce614c297c75b1c19dab196ebf45d42a18f74d826a841c1a",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A training job on SageMaker needs to read a dataset stored on Amazon FSx for Lustre for low-latency I/O. The dataset originates in S3. Which configuration provides the fastest initial data access?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Mount FSx for Lustre and copy data manually from S3.",
      "B": "Use AWS DataSync to mirror S3 to FSx for Lustre.",
      "C": "Use FSx\u2019s native S3 integration by specifying the S3 bucket as the data repository.",
      "D": "Use a SageMaker Processing job to prefetch data into EBS."
    },
    "explanation": "FSx for Lustre native S3 integration transparently caches and streams S3 data into the file system without manual copy."
  },
  {
    "id": "19d8e4182bbdd5f2d2a9879b2b47eb6b4c846268a5c2155fba6c931ac73f2ec7",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A dataset stored in S3 is consumed by multiple SageMaker training instances concurrently. To minimize read latency and S3 request costs, what ingestion pattern should be used?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Each instance downloads the full dataset to its EBS volume.",
      "B": "Use S3 Select API within training scripts.",
      "C": "Use a shared EFS mount backed by S3.",
      "D": "Use S3 Replication with Cross-Region Replication disabled and parallel GETs with HTTP Keep-Alive."
    },
    "explanation": "Parallel GETs with HTTP Keep-Alive reuse connections, reducing latency and request overhead compared to duplicated EBS or EFS."
  },
  {
    "id": "6e394e7f75debc687d43f5470d09efea7bdfcbb82f6dabc303b7e6e578c386cf",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A company needs to ingest 10 TB of data daily from S3 into SageMaker Feature Store with minimal lag. Which approach scales ingestion while controlling costs?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use service API PutRecord sequentially for each feature.",
      "B": "Use the batch ingest offline feature with RecordIO protobuf files.",
      "C": "Use real-time endpoint calls for each record.",
      "D": "Use Glue Streaming to write directly to Feature Store."
    },
    "explanation": "Batch ingest with RecordIO is optimized for large-scale ingestion into Feature Store and is cost-effective compared to per-record API calls."
  },
  {
    "id": "4bf0595addd3961091d5aa2a00d9b79845bd39f6780fe05823a888999eaf5f98",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An engineer must ingest data from multiple AWS accounts into a central S3 data lake. Data producers may intermittently lose network connectivity. Which ingestion pattern ensures reliability and scalability?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use S3 Cross-Account replication with retry policies.",
      "B": "Use a central Firehose delivery stream with producers as PUT clients.",
      "C": "Use AWS DataSync agents in each account.",
      "D": "Use AWS Transfer Family SFTP endpoints in the central account."
    },
    "explanation": "Cross-Account replication handles intermittent producers with built-in retry and scales to multi-account sources without agents."
  },
  {
    "id": "8923157c586eba880b4f56ca83fbda447dc86eca012a7821489786e146d38b58",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A SageMaker processing job fails intermittently reading from an S3 bucket due to 503 throttling errors. Which change reduces the throttling while optimizing cost?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable S3 Transfer Acceleration.",
      "B": "Increase the processing instance count.",
      "C": "Enable S3 Requester Pays and use signed requests with exponential backoff in the SDK.",
      "D": "Switch the bucket to use SSE-C encryption."
    },
    "explanation": "Signed requests with retry/backoff reduce throttling; Requester Pays can shift cost but throttling reduction via SDK backoff is key."
  },
  {
    "id": "146156a048e38c1eca54f4e2dba6eee9dc4cb4a86b605995771d8e386194fdb3",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A compliance team requires that logs ingested into S3 by Kinesis Firehose are immutable and versioned. Which S3 bucket configuration achieves this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable MFA Delete and SSE-S3.",
      "B": "Enable Object Lock in Governance mode with versioning.",
      "C": "Enable default encryption with SSE-KMS.",
      "D": "Enable Lifecycle rules to transition logs to Glacier."
    },
    "explanation": "Object Lock with versioning set to Governance mode enforces immutability on ingested S3 objects."
  },
  {
    "id": "39d706db4876cfc2ef98cf0bb247764caaa1a2f7bfa54d00ca7a1faf27252a5b",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "During a large dataset migration, objects are written to an S3 bucket in parallel. The team observes increased 503 errors. Which bucket-level configuration resolves this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable S3 Transfer Acceleration.",
      "B": "Enable default encryption with SSE-KMS.",
      "C": "Enable MFA Delete on the bucket.",
      "D": "Enable S3 request rate optimization on the bucket."
    },
    "explanation": "Request rate optimization (prefix deprecation) reduces 503 errors under high parallel write rates by distributing keys."
  },
  {
    "id": "734311885bc2c637b6b0048937eff3f81a94d2c084e90acf0993fc3d364f282e",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A pipeline uses AWS Glue to extract data from an Amazon Aurora MySQL database to S3. The Glue job times out during snapshot extraction. Which approach resolves the timeout while minimizing developer effort?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Increase the connection timeout in the Glue job script.",
      "B": "Use AWS Database Migration Service with full load to S3.",
      "C": "Split the Glue job into multiple smaller jobs.",
      "D": "Export Aurora snapshot to S3 using native export."
    },
    "explanation": "Using DMS for full load to S3 handles large data extracts reliably with minimal custom code versus Glue timeouts."
  },
  {
    "id": "c29763df7c5a181234178cc906e41ea71e97e48f4e7eb155fe67ce33f21ee1a6",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An ML engineer must ingest encrypted CSV files from S3 to SageMaker Processing. The files are encrypted with SSE-KMS using a custom CMK. The processing role lacks decrypted access. What adjustment grants the least privilege?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Attach an IAM policy to the processing role allowing kms:Decrypt on the CMK.",
      "B": "Re-encrypt the files with SSE-S3.",
      "C": "Use client-side encryption and share the key.",
      "D": "Grant the role AmazonS3FullAccess."
    },
    "explanation": "Granting kms:Decrypt on the CMK to the processing role grants only needed permissions rather than broad S3 access."
  },
  {
    "id": "41bb9c67d47143dda3a404f7d463f826de7c3ed3135d4b8ba276846cc6a68bd4",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A data lake uses S3 with AWS Lake Formation. Data ingestion from streaming sources must respect table-level LF permissions. Which integration ensures permissions enforcement?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Kinesis Data Firehose direct to S3 outputs.",
      "B": "Use Lambda to write to S3 then catalog with Glue.",
      "C": "Use Lake Formation transactions with AWS Glue streaming jobs.",
      "D": "Use Athena to INSERT INTO Lake Formation tables."
    },
    "explanation": "Lake Formation transactions in Glue streaming enforce table-level LF permissions during ingestion."
  },
  {
    "id": "b041ee4788c84c2bc65c09e38b5f188d58e320b7dc35a2e5c503d1a2e519dad2",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An ML pipeline ingests files via S3 Event Notifications to Lambda. Under high throughput, some events are lost. Which change ensures all records are ingested?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase the Lambda concurrency limit.",
      "B": "Use SQS as the Event Notification target before invoking Lambda.",
      "C": "Switch to SNS Event Notifications.",
      "D": "Enable retry behavior in S3."
    },
    "explanation": "Using SQS between S3 and Lambda buffers events, ensuring delivery and retry rather than direct event loss."
  },
  {
    "id": "31cc1ea391cdc0b4bf820a48c5c7500b40d43002e0fedf64fb9b66a65cadc23e",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A company plans to centralize logs from multiple regions into a single S3 bucket. Logs must remain in native partition folders. How should a cross-region replication rule be configured?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a replication rule with IncludePuts and prefix replication per region.",
      "B": "Use replication time control to replicate objects hourly.",
      "C": "Use batch Athena CTAS to copy logs across regions.",
      "D": "Use DataSync to mirror entire buckets."
    },
    "explanation": "A replication rule with region-specific prefixes replicates objects into same partition paths in central bucket."
  },
  {
    "id": "239cd0757b7c523f4db9b85bf66dac29b92bc01f76fd9b3de52cf2b484f6559c",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A secure pipeline must ingest data from Amazon DynamoDB to S3 for ML training. The transfer must be encrypted, serverless, and cost-efficient. Which service accomplishes this?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Use EMR Spark job to export DynamoDB to S3.",
      "B": "Use AWS DataSync to copy table exports.",
      "C": "Use AWS Glue with DynamoDB connector.",
      "D": "Use DynamoDB Export to S3 with SSE-KMS encryption."
    },
    "explanation": "DynamoDB native Export to S3 is serverless, encrypted, and cost-efficient compared to EMR or Glue."
  },
  {
    "id": "d0118dcc5279e40443b10388f4b77b946ed35243429297664da7a6b2cd9f05f0",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An ML pipeline needs to ingest image files from multiple SFTP servers into S3. The process must be fully AWS managed and scalable. Which solution meets these requirements?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Deploy EC2 instances running custom SFTP-to-S3 scripts.",
      "B": "Use AWS DataSync agents on EC2.",
      "C": "Use AWS Transfer Family SFTP endpoints with custom identity providers and S3 backing.",
      "D": "Use Glue FTP connector."
    },
    "explanation": "AWS Transfer Family provides managed SFTP endpoints that store files directly in S3, scaling without custom servers."
  },
  {
    "id": "77dbd82e8c577fc4999a162d2480658eaa4f6fca002605c72be15ba2772cc1e2",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A pipeline needs to ingest sensor data at 100 MB/s into S3 for real-time analytics. Which combination of services and configurations meets throughput while controlling costs?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Kinesis Data Streams with 100 shards directly writing to S3.",
      "B": "MSK with 5 broker nodes, MirrorMaker to Firehose with gzip compression.",
      "C": "Direct Firehose delivery with default shard count.",
      "D": "Lambda triggered by S3 upload events."
    },
    "explanation": "MSK scales to high throughput; MirrorMaker populates Firehose which batches and compresses data to S3 cost-effectively."
  },
  {
    "id": "fa80e21835caa9c5d2a1983c259c3de8d5d02cbd814644d7a7a5c9078660838a",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "During ingest of nested JSON event data into S3, the team needs to infer schema and partition by event date. Which AWS Glue configuration fulfills this with minimal code?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Use a custom PySpark Glue job to parse JSON and write Parquet.",
      "B": "Use Athena CTAS to convert and partition data.",
      "C": "Use SageMaker Processing to flatten and write to S3.",
      "D": "Use AWS Glue JSON classifier with crawler partitioning on event_date field."
    },
    "explanation": "A Glue crawler with JSON classifier can infer schema and apply partitioning on a specified JSON field automatically."
  },
  {
    "id": "c0616abae522b5c50549b66a00461372c51aaa25e2ac93dde3640882a1b47b96",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A dataset must be ingested daily from an external partner to S3 via the internet. The partner cannot use AWS SDK. How should the company securely and reliably ingest files?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provision an AWS Transfer Family SFTP endpoint backed by the S3 bucket.",
      "B": "Provide S3 PUT URLs with public write ACL.",
      "C": "Use HTTP POST to S3 REST API with credentials.",
      "D": "Ask partner to email files and upload manually."
    },
    "explanation": "Transfer Family SFTP endpoint allows partners to upload over SFTP without SDK, delivering files directly and securely to S3."
  },
  {
    "id": "dba7b88c787c83f6ee0c7d7d297a29e73944443443620a6cb5cc6e034470f466",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "The team needs to ingest 500 GB of data daily from an on-prem Hadoop cluster to S3 under network constraints. Which approach provides incremental ingestion and optimizes bandwidth?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Snowball Edge weekly ingestion.",
      "B": "Deploy AWS DataSync agent with incremental sync enabled.",
      "C": "Use a Glue job with JDBC connector.",
      "D": "Use AWS Glue DataBrew to pull data."
    },
    "explanation": "DataSync incremental sync only transfers changed data daily, optimizing bandwidth and automating ingestion."
  },
  {
    "id": "444301c16debbed171f90ddd87a65fb785d7e5a4a1494b2ef357bb1ab0ff3531",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An ML pipeline reads an S3 directory of Avro files during training. The engineer wants to reduce startup latency when listing thousands of files. Which S3 configuration reduces LIST cost?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable SSE-KMS on the bucket.",
      "B": "Enable bucket versioning.",
      "C": "Use S3 Inventory to generate daily manifest and use the manifest for file listing.",
      "D": "Enable S3 Transfer Acceleration."
    },
    "explanation": "S3 Inventory provides a manifest file listing objects, eliminating costly API ListObjects calls during job startup."
  },
  {
    "id": "917f8f0845ff33fb8fcc7f878febb3f0b8568524556d8fefaaf0b4a3a7c52653",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A team needs to ingest HTTP logs into S3 for historical analysis. Logs are generated at 10 GB/hour. The solution must auto-scale and require no server management. Which ingestion architecture is best?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "EMR cluster with Flink reading logs.",
      "B": "Kinesis Data Firehose with Lambda data transformation to S3.",
      "C": "Custom ECS service tailing logs and writing to S3.",
      "D": "S3 multipart upload from application servers."
    },
    "explanation": "Firehose auto-scales ingestion and can deliver data to S3 with optional Lambda transformation, requiring no servers."
  },
  {
    "id": "edb7492a33375c3ac96983f9b815fa1fa85d5689578fed0cf6499590a9c097d7",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A multi-region application produces data that must land in a centralized S3 bucket in the same AWS Region as processing. Replication must preserve object metadata and ACLs. Which configuration meets this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use CloudTrail replication to central bucket.",
      "B": "Configure Firehose across regions.",
      "C": "Use DataSync cross-region replication.",
      "D": "Use cross-region replication with replicate object metadata and ACL."
    },
    "explanation": "S3 cross-region replication configured to replicate metadata and ACLs preserves all object attributes in central bucket."
  },
  {
    "id": "265e6241835afbb1169836a5a1d98c1e3aae84063ce0cb49ef20c15bbef04bda",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A SageMaker training job must read millions of small JSON files from S3. The engineer notices high GET request costs. Which solution reduces GETs and maintains low latency?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Combine JSON files into larger S3 objects, e.g., Parquet, using a Glue job.",
      "B": "Enable S3 Intelligent-Tiering.",
      "C": "Use S3 Select for each JSON file.",
      "D": "Use Lifecycle rules to consolidate files."
    },
    "explanation": "Combining small files into larger Parquet objects reduces individual GET calls and lowers request costs."
  },
  {
    "id": "060726a89cf252c0462cf5c785110695c6faf0f2fb8464f816136269d431f0ad",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "To ingest clickstream events into S3 with exactly-once semantics, an engineer uses Kinesis Data Streams and Lambda to write to S3. At times, duplicates occur. Which pattern ensures exactly-once writes?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use PutObject with versioning enabled.",
      "B": "Use S3 Multipart upload with unique upload IDs.",
      "C": "Use Kinesis Data Firehose with S3 delivery and deduplication via record ID.",
      "D": "Use DynamoDB streams and batch writes to S3."
    },
    "explanation": "Firehose supports exactly-once delivery semantics to S3 when configured with record IDs for deduplication."
  },
  {
    "id": "2dfacfeed41f14fdc0c6bad06b3474ca7206a36d93a97d488f6bf8e58682e83e",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A data lake's S3 bucket is configured with CloudWatch Events to start Glue crawlers upon new object creation. Under heavy load, some crawlers start late and miss events. How ensure timely ingestion?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase crawler concurrency in Glue.",
      "B": "Introduce a Step Functions state machine triggered by EventBridge to queue and throttle crawler invocations.",
      "C": "Use Lambda to start crawlers directly.",
      "D": "Enable S3 Intelligent-Tiering."
    },
    "explanation": "Using Step Functions to queue and throttle ensures orderly crawler invocations without overload compared to direct triggers."
  },
  {
    "id": "2c6bba681b7877d35e2297c736e78640b945322c824537690021c491832fda98",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An ML engineer is training a PyTorch model in SageMaker script mode on a single GPU instance. The training job often runs out of GPU memory when using a batch size of 256. To continue training with minimal code changes and without incurring additional infrastructure cost, which action should the engineer take?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable mixed precision training (automatic mixed precision) in the training script.",
      "B": "Switch to SageMaker built-in algorithm XGBoost with the same batch size.",
      "C": "Decrease the number of training epochs to reduce memory footprint.",
      "D": "Use a larger EC2 instance type with more GPU memory."
    },
    "explanation": "Mixed precision reduces memory usage with minimal code changes. Switching algorithms or decreasing epochs doesn\u2019t reduce per-batch memory; upgrading instance increases cost."
  },
  {
    "id": "cd1b32b4a54f02486c7ee42143638af4676f84de1b1809a555771669f9804ef7",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A data scientist uses SageMaker Automatic Model Tuning (AMT) with Bayesian optimization to tune a deep neural network. The search is converging too slowly. The search space contains 10 continuous and 5 categorical hyperparameters. What adjustment will most speed up convergence?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch from Bayesian optimization to random search.",
      "B": "Increase the maximum number of training jobs in the tuning job.",
      "C": "Reduce the hyperparameter search space by fixing low-impact parameters to default values.",
      "D": "Use an IDENTICAL_DATA_AND_ALGORITHM warm start tuning job type."
    },
    "explanation": "Reducing search space focuses Bayesian optimization on impactful parameters. Random search may be less efficient; warm start of type IDENTICAL_DATA_AND_ALGORITHM duplicates prior runs, not solving slow convergence."
  },
  {
    "id": "b22bc3a66b26239a672f24db6ece0f00d5234db10fb7eb467afcf61e6ff9177d",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An ML engineer trains a large transformer model using SageMaker script mode with TensorFlow. Training takes 48 hours. The engineer wants to halve training time with minimal hyperparameter changes. Which approach meets this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Double the batch size and halve the number of epochs.",
      "B": "Enable distributed data parallel training across two GPU instances.",
      "C": "Use L2 regularization with higher weight to speed up convergence.",
      "D": "Decrease learning rate and increase number of epochs."
    },
    "explanation": "Distributed training splits workload across GPUs, reducing wall-clock time. Changing regularization or learning rate affects training dynamics, not time directly; doubling batch/halving epochs may hurt convergence."
  },
  {
    "id": "55fbde9b31616c13c3b55acb3e650c56464e4f953f1f7bd2b7514045f13870ba",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A tabular dataset shows early overfitting: validation loss increases after 5 epochs. Which combination of techniques best reduces overfitting while preserving model capacity?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase batch size and remove dropout layers.",
      "B": "Decrease learning rate and increase number of layers.",
      "C": "Add L2 regularization with high weight and increase epochs.",
      "D": "Implement dropout, add L2 weight decay, and enable early stopping after no validation improvement for 3 epochs."
    },
    "explanation": "Dropout+L2 regularization reduce overfitting; early stopping avoids unnecessary epochs. Increasing layers or removing dropout worsens overfitting."
  },
  {
    "id": "babf8d2abdcaa3ae46376627e75ba29b2cc05580b1451f799241059cf61aa6c1",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A company uses a SageMaker built-in XGBoost algorithm to classify credit risk. The model size on disk is 1.5 GB, exceeding edge device storage limit of 500 MB. Which strategy best reduces model size without retraining from scratch?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Decrease max_depth and num_round in a new training job.",
      "B": "Use post-training model pruning and quantization using SageMaker Neo compilation.",
      "C": "Enable L1 regularization in the current model registry version.",
      "D": "Decrease the learning rate and increase the number of trees."
    },
    "explanation": "SageMaker Neo can prune and quantize a compiled model without retraining. Changing training hyperparameters requires retraining."
  },
  {
    "id": "953955311d2ac809eebad3ff5490c58d752dc506746514f44c85c9afbf029839",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An ML engineer fine-tunes a pre-trained BERT model on SageMaker JumpStart. To prevent catastrophic forgetting of pre-trained weights, which configuration should they apply?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use lower learning rate for pre-trained layers and higher rate for newly added classification head.",
      "B": "Initialize all layers randomly before fine-tuning.",
      "C": "Freeze the classification head and only train transformer layers.",
      "D": "Increase batch size by fourfold to stabilize gradients."
    },
    "explanation": "Lower LR on pretrained layers preserves learned features; random init or freezing head loses benefits."
  },
  {
    "id": "42056ed492b03fca229a76cbe088557b053d02ba3f3f2a824ac2e58cd45e0937",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "During hyperparameter tuning with AMT, an engineer observes that many training jobs fail due to out-of-memory errors for large batch sizes. To optimize tuning efficiency, which AMT configuration change is best?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase the maximum parallel training jobs.",
      "B": "Switch from Bayesian to grid search.",
      "C": "Decrease the minimum number of training jobs.",
      "D": "Add a conditional statement in the training script to skip batch sizes causing OOM and return low objective metric."
    },
    "explanation": "Skipping invalid hyperparameter combinations via script invalidation avoids wasted jobs; changing search strategy or job counts doesn\u2019t prevent failures."
  },
  {
    "id": "bd2404b9b96e547f5901b93fdc01c068f9951a4ebba66f4707973341c8fc5959",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A deep CNN trained in SageMaker exhibits slow convergence and high variance among epoch losses. Which combination of techniques addresses both issues?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Remove data augmentation and add more layers.",
      "B": "Increase learning rate and decrease batch size.",
      "C": "Apply batch normalization, use learning rate scheduler, and add dropout layers.",
      "D": "Switch optimizer from Adam to stochastic gradient descent with fixed LR."
    },
    "explanation": "Batch norm and LR scheduling stabilize training; dropout reduces variance. Removing augmentation or switching optimizer without scheduler may worsen convergence."
  },
  {
    "id": "87203f13f459d137651fed6adf192577ffb202465b1ce9e008803f99e561371a",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An ML engineer needs to integrate a scikit-learn RandomForest model trained locally into SageMaker for batch transform inference. Which step is required?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Wrap the model in a SageMaker built-in algorithm container.",
      "B": "Create a custom inference script and container using SageMaker script mode to load the pickle file.",
      "C": "Use SageMaker Clarify to register the model directly from local filesystem.",
      "D": "Upload the model artifact to SageMaker Model Registry via console."
    },
    "explanation": "Local models require a custom container/script in script mode. Built-in algorithms can\u2019t import arbitrary pickles."
  },
  {
    "id": "15a37a4a66ff126df69ad27a068a0c8ebcb6903b2b050f1cbb463a6ecfc51d97",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A large NLP model fine-tuned via Hugging Face in SageMaker shows slow throughput per epoch. Which SageMaker feature reduces training time without code changes?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable early stopping in AMT.",
      "B": "Use SageMaker Model Monitor to detect drift.",
      "C": "Enable managed spot training with distributed training.",
      "D": "Switch to a built-in algorithm for NLP tasks."
    },
    "explanation": "Managed distributed spot training uses multiple instances to speed training; early stopping and monitoring don\u2019t affect raw throughput."
  },
  {
    "id": "9349363120e2cf606ca2a16aa6fc930f27282557f511494ee95882536d008c57",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An engineer uses SageMaker script mode to train a PyTorch model. The training time per epoch increases over time due to GPU memory fragmentation. How to mitigate this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable gradient checkpointing to reduce peak memory usage.",
      "B": "Increase batch size to better pack memory.",
      "C": "Disable data parallelism to reduce fragmentation.",
      "D": "Switch from PyTorch to TensorFlow."
    },
    "explanation": "Gradient checkpointing trades compute for memory, alleviating fragmentation. Increasing batch size worsens it; switching frameworks is heavy-handed."
  },
  {
    "id": "f51e5154626a499b309f512fcc0fdb8102889c36efae4e50de671e439347d1d2",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A SageMaker AMT job tunes hyperparameters including learning_rate and optimizer choice. The team needs to incorporate prior tuning results to guide the new job. Which warm start type is appropriate?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "IDENTICAL_DATA_AND_ALGORITHM",
      "B": "TRANSFER_LEARNING",
      "C": "STANDARD",
      "D": "TRANSFER_LEARNING with previous job as parent"
    },
    "explanation": "TRANSFER_LEARNING warm start uses prior results from parent tuning jobs; IDENTICAL_DATA_AND_ALGORITHM reuses only matching configs."
  },
  {
    "id": "aea5b0de57ebac7777b30e841067de63f266bbd8854e78534f42f9f25a7a34c8",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An ML researcher wants repeated reproducible experiments in SageMaker. Which practice ensures identical results across training runs?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use random search in AMT.",
      "B": "Use spot instances to save costs.",
      "C": "Enable early stopping after fixed epochs.",
      "D": "Set and log explicit random seeds, fix training hyperparameters, use the Model Registry for versioning."
    },
    "explanation": "Explicit seed and consistent config plus registry ensure reproducibility; early stopping and spot instances introduce variability."
  },
  {
    "id": "57e6eaebc75b8b88caf55c6f296408b4e4f0b35291e0023c78062f9254eca486",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A team notices high variance between ensemble members when combining models. To improve ensemble performance, which strategy is best?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use identical architecture and data splits for each member.",
      "B": "Use diverse base learners and bootstrap sampling to increase diversity.",
      "C": "Increase depth of all tree-based models.",
      "D": "Use only the top-performing single model."
    },
    "explanation": "Ensemble benefits from diverse learners; identical architectures reduce benefit."
  },
  {
    "id": "06cd007b12d7410dc744d24922a4ffb4e7ee296577a69e72186f520ca837da91",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An engineer needs to fine-tune a large foundation model from Amazon Bedrock in SageMaker for a specialized downstream task. Which approach integrates seamlessly?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Export the model weights and import into a SageMaker built-in algorithm.",
      "B": "Download the model artifacts and retrain from scratch.",
      "C": "Use SageMaker JumpStart to fine-tune the model with custom dataset.",
      "D": "Convert Bedrock model to TensorFlow SavedModel and deploy."
    },
    "explanation": "JumpStart provides ready fine-tuning pipelines for Bedrock foundation models."
  },
  {
    "id": "bff3e6e1fa803133e0c68415bdb2648ca1cb6b95293cec05b3bf7f91679a155b",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A deep learning model exhibits underfitting on training and validation sets. To improve fit, which hyperparameter adjustment is most effective?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Increase model capacity by adding layers or units.",
      "B": "Increase dropout rate.",
      "C": "Increase L2 regularization weight.",
      "D": "Decrease number of training epochs."
    },
    "explanation": "Underfitting requires more capacity; dropout and L2 worsen underfitting."
  },
  {
    "id": "4b907f8ee698d956feaf8e7b5b9ff3ff27947d076ce698cfdbaf97d456b69e0f",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An automatic model tuning job uses AMT with a goal metric of validation error. The team now needs to optimize two metrics: inference latency and accuracy. Which solution supports this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run two separate AMT jobs and manually choose trade-off.",
      "B": "Use Bayesian optimization with a composite metric manually computed in script.",
      "C": "Switch search strategy to grid search for multi-objective coverage.",
      "D": "Use SageMaker multi-objective hyperparameter tuning-enabled job."
    },
    "explanation": "Multi-objective tuning in SageMaker supports simultaneous optimization of metrics."
  },
  {
    "id": "1011a4e0e1a15bf0dca0a9ecfbd3eea4f0cfd279519535fa11f0b33957ea0eca",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A training job using Hugging Face Transformers in SageMaker uses a JSON Lines dataset on S3. The engineer observes slow data loading. Which training configuration change accelerates data throughput?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch input mode to Pipe mode.",
      "B": "Use File mode with Amazon FSx for Lustre linked to the S3 bucket.",
      "C": "Decrease number of worker processes in DataLoader.",
      "D": "Disable shuffling of the dataset."
    },
    "explanation": "FSx for Lustre caches data for high throughput; pipe mode streams raw S3, slower for many small records."
  },
  {
    "id": "0226f8f8c9f1809c16122face3e8a6f8bf1ebd59ba8517a789dc7e626637bd8e",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An ML engineer wants to prune a TensorFlow model during SageMaker training to reduce inference latency while preserving >98% of accuracy. Which training callback should be used?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "EarlyStopping on validation loss.",
      "B": "LearningRateScheduler.",
      "C": "TensorFlow Model Optimization pruning callback.",
      "D": "ReduceLROnPlateau."
    },
    "explanation": "TF Model Optimization pruning callback prunes weights during training; others adjust learning rate or stop early."
  },
  {
    "id": "4028ea0d42b22fdfd7c7d2ebf76711565572bd8f22ac8d8ec8082a7d5edfadb2",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A model trained on SageMaker exhibits high training throughput but low GPU utilization. Which configuration change most directly improves GPU utilization?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase per-instance batch size.",
      "B": "Decrease the number of GPUs.",
      "C": "Enable early stopping.",
      "D": "Use a smaller instance type."
    },
    "explanation": "Batch size affects GPU utilization; larger batches fill GPU compute better."
  },
  {
    "id": "479586aa77574eef8d2f73d9973fad78ccd4263364c134119092c02a49e0cd9d",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A team uses AMT with max_jobs=50 and max_parallel_jobs=5. They need to reduce total run time but keep exploring same search space. Which change achieves this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase search space cardinality.",
      "B": "Increase max_parallel_jobs to 10.",
      "C": "Decrease max_jobs to 25.",
      "D": "Switch to random search strategy."
    },
    "explanation": "Increasing parallel jobs reduces wall-clock time; random search doesn\u2019t guarantee faster convergence."
  },
  {
    "id": "2ef334d3b8ba63a2f19e644b7680a899ce431bb0c511851d73a9d32570dc4d19",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An existing model version in SageMaker Model Registry is retrained with new hyperparameters. To automate promoting the version when validation accuracy improves by >1%, which approach is best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Manually compare metrics and click Promote in console.",
      "B": "Use SageMaker Experiments to tag and promote.",
      "C": "Define a SageMaker Pipeline with conditional ModelRegistry promotion step.",
      "D": "Write a Lambda triggered by CloudWatch alarm on metrics."
    },
    "explanation": "Pipelines support conditional promotion; Lambda approach requires custom code and integration."
  },
  {
    "id": "f3a0d2b635950282c4f2945eca673064f1fbd836cd8cd9a06cb7e5abb2ef3cd9",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "During distributed training on multiple GPU instances, training noise leads to divergence. What change reduces variance across replicas?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase initial learning rate.",
      "B": "Disable gradient clipping.",
      "C": "Use smaller batch size per replica.",
      "D": "Enable synchronized BatchNorm across replicas."
    },
    "explanation": "Synchronized BatchNorm ensures consistent statistics; other changes may worsen instability."
  },
  {
    "id": "ec1c1afb088a9268a9a7b185c7fba300262ec08d6e1b16e116ec01e10eac3258",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A SageMaker TensorFlow training job time per epoch is dominated by data preprocessing in training script. How to offload preprocessing to speed training?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Processing or Glue to preprocess and save TFRecords before training.",
      "B": "Enable profiler for training jobs.",
      "C": "Switch to script mode with Python SDK.",
      "D": "Use hyperparameter tuning to find optimal preprocessing parameters."
    },
    "explanation": "Preprocessing offline reduces per-epoch overhead; tuning and profiling don\u2019t offload work."
  },
  {
    "id": "40570a47bfd9c3a7246125cd7e429e0ea4d4c1f703d3ad611cfaee53ac47ee57",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An engineer needs to train a model with imbalanced classes. To ensure balanced gradient updates per batch, which DataLoader or training config change is best?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Shuffle dataset each epoch.",
      "B": "Use a weighted random sampler to form balanced batches.",
      "C": "Increase number of epochs.",
      "D": "Add L1 regularization to loss."
    },
    "explanation": "Weighted sampler balances classes per batch; shuffling alone doesn\u2019t ensure balance."
  },
  {
    "id": "207da57a9eaeeaefe7943bf44ad945f66bbbabe729a2877cd1130c92b1ca857f",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A team wants to combine three diverse models into a final prediction in SageMaker. They need an ensemble pipeline that retrains all members and the meta-learner automatically. Which SageMaker feature should they use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Batch Transform.",
      "B": "SageMaker Clarify.",
      "C": "SageMaker Neo.",
      "D": "SageMaker Pipelines with Ensemble step implementations."
    },
    "explanation": "Pipelines can orchestrate multi-step ensemble training; other services unrelated."
  },
  {
    "id": "acdc675bc63c0d7222a1e9bc17d7e8a0311b7929f0946682163c7eb9a6b0d24c",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You have a dataset with highly skewed numeric features and missing values partitioned across multiple S3 prefixes. You need a pipeline that imputes missing values with the median per feature, applies a log transform to reduce skew, and writes the cleaned data to Amazon SageMaker Feature Store. Which approach meets these requirements with the least custom code?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use an AWS Glue Spark ETL job to compute medians via AWS Glue Data Quality, apply a PySpark log transform, and write to Feature Store using the SageMaker SDK.",
      "B": "Use a SageMaker Data Wrangler flow: add a fill-missing recipe with median, add a built-in log transform step, then export directly to Feature Store.",
      "C": "Use AWS Glue DataBrew: create a project, add fill-missing and log transform recipes, and publish to S3 for later ingestion into Feature Store.",
      "D": "Run a SageMaker Processing job with a custom Scikit-Learn script that imputes medians, applies log transforms, and writes to Feature Store."
    },
    "explanation": "Data Wrangler provides built-in median imputation and log transform steps and can export directly to Feature Store, minimizing custom code."
  },
  {
    "id": "d9df0de463f1cc2ab0b14f57afb42314223d46e9f679f177d8304cf64a7eb43d",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A streaming application ingests JSON records into Amazon Kinesis Data Streams. Each record contains nested user attributes and a categorical field with dozens of categories. You must extract the nested field, one-hot encode the categorical feature, and persist the output in S3 in Parquet format. Which solution is most operationally efficient?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Kinesis Data Analytics for Apache Flink with SQL to flatten JSON, then implement a user-defined function for one-hot encoding, and sink to S3.",
      "B": "Trigger an AWS Lambda function on Kinesis shards to parse JSON, use pandas.get_dummies for encoding, and write Parquet to S3.",
      "C": "Use SageMaker Data Wrangler with a streaming data source, apply transforms, and export to S3.",
      "D": "Use AWS Glue Streaming ETL with Apache Spark: apply JSON extract, use Spark ML OneHotEncoderEstimator, and write Parquet to S3."
    },
    "explanation": "AWS Glue Streaming ETL natively supports Spark transformations on streaming data, including JSON flattening and OneHotEncoder, with minimal custom code and built-in S3 sink."
  },
  {
    "id": "662b5ed6c001766f347a4d7ffcf54c922c14f7eb1bfb0b773affe757b58ec2a4",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You have a multi-class categorical feature with 1000 unique values, most of which appear in fewer than 5% of the records. You need to encode it for a tree-based model without creating high dimensionality. Which feature-engineering technique should you apply in SageMaker Data Wrangler?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use one-hot encoding with drop-last to reduce one dimension.",
      "B": "Apply label encoding directly to assign integer codes.",
      "C": "Group infrequent categories into an \"Other\" bucket, then apply one-hot encoding on the top categories.",
      "D": "Apply ordinal encoding based on average target probability per category."
    },
    "explanation": "Combining infrequent categories into \"Other\" reduces cardinality before one-hot encoding, balancing dimensionality and model interpretability."
  },
  {
    "id": "429ca78b33c23ef87e4d0cb7f627d77ca0a0722ca8d623d5122fe75e2b5f4639",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A text column contains customer reviews that must be tokenized and vectorized for an NLP model. You need to integrate this into a SageMaker feature-engineering pipeline with minimal custom code. Which service and sequence accomplishes this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Glue DataBrew: add tokenization recipe, export to S3, and then train with a custom tokenizer.",
      "B": "Use a SageMaker Processing job with the Hugging Face tokenizer, then push token IDs to Feature Store.",
      "C": "Use SageMaker Data Wrangler: add a built-in text tokenizer transform, then export token counts to Feature Store.",
      "D": "Stream reviews through Lambda to call Amazon Comprehend for tokenization, then store results in S3."
    },
    "explanation": "A SageMaker Processing job with Hugging Face tokenizer integrates easily into pipelines and can write token IDs directly to Feature Store."
  },
  {
    "id": "416c8fbaad16a22f815a230eb362ce1128edac9efc9c80f9f24188adf1165c53",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You need to discretize a continuous feature into bins of equal frequency and standardize another feature to zero mean and unit variance, using managed AWS services. Which combination is most appropriate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Data Wrangler recipe: add quantile binning for equal-frequency bins and Z-score normalization for standardization.",
      "B": "Use AWS Glue DataBrew: add equal-width binning recipe and MinMax scale recipe.",
      "C": "Run a SageMaker Processing job with a Scikit-Learn pipeline that uses KBinsDiscretizer and StandardScaler.",
      "D": "Use AWS Glue ETL: write PySpark code to bucketBy frequency and use VectorAssembler with StandardScaler."
    },
    "explanation": "Data Wrangler supports quantile binning (equal frequency) and Z-score normalization out of the box, minimizing custom code."
  },
  {
    "id": "ad13d38b010b415d4f30be9a8f786f92a37978b371252f1c7d188bfe1a09c868",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "Your dataset contains duplicate records due to multiple ingestion systems. You need a low-code AWS solution to deduplicate by a composite key and write the result back to S3. Which approach is best?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Use SageMaker Data Wrangler: add a drop duplicates recipe on the composite key, then export to S3.",
      "B": "Run a Glue Spark ETL job with custom PySpark dropDuplicates call on the composite key.",
      "C": "Use AWS Glue DataBrew: create a project, add a dedup recipe on the composite key, and publish the output to S3.",
      "D": "Implement a SageMaker Processing job with Pandas drop_duplicates and write to S3."
    },
    "explanation": "DataBrew provides a built-in deduplication recipe that can drop duplicates on any key and export to S3 without custom code."
  },
  {
    "id": "60f183bdf22d14eb68b1a508632eb8164a360356173ef5f78f35684658b1b3b4",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You must engineer features that capture hour-of-day and day-of-week from a timestamp column for downstream modeling. The solution must integrate with SageMaker Pipelines and Feature Store. Which components do you use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A SageMaker ProcessingStep using a custom Pandas script to extract features and call PutRecord to Feature Store.",
      "B": "A SageMaker Data Wrangler step in the pipeline: use built-in ExtractTimestamp transforms for hour and weekday, and export to Feature Store.",
      "C": "An AWS Glue job to run Python code, write to S3, then a ProcessingStep to ingest into Feature Store.",
      "D": "Lambda functions behind EventBridge that trigger on S3 updates to compute features and write to Feature Store."
    },
    "explanation": "Data Wrangler can be used as a ProcessingStep in SageMaker Pipelines, with built-in timestamp extract transforms and direct export to Feature Store."
  },
  {
    "id": "5817af385edfe5c56de2ed396430c045196bb29a104224ee987b7288428743a4",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A dataset contains outliers in multiple numeric columns. You need to detect and cap them at the 1st and 99th percentiles before modeling. Which AWS Workflow is most efficient?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a SageMaker Processing job with custom code to compute percentiles and cap outliers.",
      "B": "Use SageMaker Data Wrangler: add an outlier detection transform to compute percentiles and use a value mapping recipe to cap values.",
      "C": "Write a Glue Spark ETL job using approxQuantile to find percentiles and withColumn to cap values.",
      "D": "Use AWS Glue DataBrew: add a percentile filter recipe then a replace recipe to cap extremes."
    },
    "explanation": "Data Wrangler outlier detection can compute percentiles and supports value mapping recipes to cap outliers without custom code."
  },
  {
    "id": "10e02b185c25c8260aedcc5b07a467171067715b46b0160cfc990d140477a669",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You have a categorical feature with high cardinality and need to generate target-guided encoding (mean target value per category) without leaking test data information. Which pipeline design prevents leakage?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a single SageMaker Processing job to compute global means on full dataset, apply to all splits.",
      "B": "Use Data Wrangler to compute target means on full dataset and join back to features.",
      "C": "Use Glue ETL to compute means on training set and apply same mapping to test set in the same job.",
      "D": "Define two ProcessingSteps in a SageMaker Pipeline: first compute category-target means on the training split only, store mapping; second step applies mapping to train and test splits separately."
    },
    "explanation": "Separating computation on training split from application to test split in different steps prevents target leakage."
  },
  {
    "id": "a1bf4dd4611c9cb61cf51c9dddd5d9c075a27d5386a28bec30be0cd26e43672f",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A feature column contains text with inconsistent casing and punctuation. You need to normalize the text (lowercase, remove punctuation), tokenize, and compute TF-IDF vectors, using managed AWS services. Which sequence is correct?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker ProcessingStep: run a Scikit-Learn script for lowercasing, regex to remove punctuation, CountVectorizer with TF-IDF transformer.",
      "B": "Use Data Wrangler: add lowercase and remove-pattern transforms, then export tokens to S3 for external TF-IDF calculation.",
      "C": "Use Glue DataBrew recipes: add text lowercase and remove punctuation recipes, then a custom recipe for TF-IDF.",
      "D": "Use AWS Lambda triggered on S3: call Amazon Comprehend to normalize and tokenize, then compute TF-IDF in Lambda."
    },
    "explanation": "A ProcessingStep gives full control to use Scikit-Learn pipeline for normalization and TF-IDF vectorization directly in SageMaker Pipelines."
  },
  {
    "id": "fdd13328279d4a1bfe442076569419cb70ef34484059a8bff08c78587bb87d6b",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You need to enrich your dataset with rolling window features (7-day sum and average) on time-series data stored in S3. Which AWS service and pattern is most appropriate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Glue Streaming ETL with sliding window in Spark Structed Streaming to compute rolling features.",
      "B": "Use SageMaker Data Wrangler to define window aggregation transforms directly.",
      "C": "Schedule a SageMaker Processing job with PySpark code that reads S3 parquet, applies window functions, and writes enriched data back to S3.",
      "D": "Use AWS Lambda triggered by new data to update rolling aggregates in Amazon DynamoDB."
    },
    "explanation": "SageMaker Processing with PySpark supports Spark SQL window functions for rolling aggregates in a batch fashion, suitable for scheduled enrichment."
  },
  {
    "id": "1ca87bc125e7238d9bf38c251d193d6182e662b0b1cf10c1098e03c6971a36ee",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "Your dataset has a highly imbalanced binary label. You need to generate synthetic minority samples using SMOTE within a managed AWS pipeline. Which service and pattern achieves this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Data Wrangler\u2019s built-in synthetic data generation transforms.",
      "B": "Use a SageMaker Processing job with Imbalanced-Learn\u2019s SMOTE, then export to S3.",
      "C": "Use AWS Glue ETL with custom PySpark code to perform SMOTE.",
      "D": "Use AWS Glue DataBrew with a custom recipe for synthetic minority sampling."
    },
    "explanation": "Only a Processing job supports custom Python libraries like Imbalanced-Learn to run SMOTE within a managed pipeline."
  },
  {
    "id": "67764d82a002cce26e13001e7bcce68a6c231d2f87460738d7411f41695f2c0d",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You have feature interactions that must be generated (pairwise products) for a logistic regression model. You want to do this at scale on a large dataset in S3. What is the most serverless approach?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run a SageMaker Processing job on large ML instances with a custom script to generate interactions.",
      "B": "Use AWS Glue ETL on Spark with PySpark to compute pairwise products and write back to S3.",
      "C": "Use SageMaker Batch Transform with a script that outputs interactions.",
      "D": "Use AWS Glue DataBrew: create a recipe to generate new columns via formula for each interaction and publish to S3."
    },
    "explanation": "DataBrew scales serverlessly with partitions and can generate new columns via formulas for interactions without managing infrastructure."
  },
  {
    "id": "40177f4a6dbe4199bc3fc05dfe4054abd8f256b44553b271ed0de58ede57234e",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You need to normalize a set of numeric features using robust scaling (subtract median and divide by IQR) within SageMaker Pipelines. Which component do you include?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A ProcessingStep that calls AWS Glue Data Quality for robust scaling.",
      "B": "A ProcessingStep that uses a DataBrew project for IQR scaling.",
      "C": "A ProcessingStep with a custom Scikit-Learn RobustScaler script.",
      "D": "A TransformStep using built-in SageMaker XGBoost with robust scaling parameters."
    },
    "explanation": "A ProcessingStep with a custom script is required for RobustScaler since no built-in recipe exists in Data Wrangler or DataBrew."
  },
  {
    "id": "9785c2bb03bb0114f105ba623f8cf9b4dc96349cfe4e9632f7384bce53628152",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You must anonymize PII in free-text user comments by replacing names and emails, then vectorize text. Which AWS-managed solution minimizes custom code?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Processing with a custom regex script for PII removal and TF-IDF vectorization.",
      "B": "Use a SageMaker Processing job combining Amazon Comprehend PII entity detection API for redaction, then Scikit-Learn vectorizer.",
      "C": "Use AWS Glue DataBrew with PII detection recipes and export tokens to S3 for vectorization.",
      "D": "Use AWS Lambda to call Comprehend for redaction, store in S3, then run DataWrangler for vectorization."
    },
    "explanation": "A single Processing job can orchestrate Comprehend API calls and vectorization in one managed step."
  },
  {
    "id": "f3cb4dc69e5286aed2efaa552d41231bba5440e06a9862a892a6f3c6e6499516",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You need to transform multi-value categorical fields (lists of tags per item) into indicator features and store them for low-latency inference. Which pipeline configuration should you choose?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "In SageMaker Data Wrangler, use the split multi-value transform to explode tags into rows, one-hot encode them, pivot back to wide format, and export to online Feature Store.",
      "B": "In Glue ETL, use PySpark explode then OneHotEncoderEstimator and write to S3 for offline Feature Store.",
      "C": "Use Lambda functions to parse tags into booleans and write JSON to DynamoDB.",
      "D": "Use SageMaker Processing with pandas to create indicator columns and write to S3."
    },
    "explanation": "Data Wrangler supports split multi-value transforms and direct export to online Feature Store for low-latency inference."
  },
  {
    "id": "1cf15f5259b0346fd6f7f67a14a928e340cb4ba660e4ef91dd68d2203ce18824",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A dataset contains decimal representations of categorical codes. You need to convert these codes to binary bit-vector features (one bit per significant bit) for modeling. Which approach is most straightforward using Data Wrangler?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Use a ProcessingStep with custom Python bit-operation code.",
      "B": "Use Glue ETL with PySpark bit operations in withColumn.",
      "C": "Use SageMaker Data Wrangler\u2019s custom transform node: write a small Pandas UDF to convert codes to bit vectors.",
      "D": "Use DataBrew with multiple formula recipes to extract bits manually."
    },
    "explanation": "A custom transform node in Data Wrangler allows concise Pandas UDF bit extraction without full ETL management."
  },
  {
    "id": "f36187b92c3347c98604ea7920e00fda702d373f129997c1b76610b00cd4efe9",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You want to join two large datasets in S3 by a composite key, perform feature computations (ratio of two columns), and write the result in parquet. Which solution scales most cost-effectively?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Data Wrangler to join and compute ratio then export.",
      "B": "Use AWS Glue ETL on Apache Spark with pushdown predicates and write to S3.",
      "C": "Use a SageMaker Processing job with PySpark on large instances.",
      "D": "Use AWS Glue DataBrew: import both datasets, join in project, and publish result."
    },
    "explanation": "AWS Glue ETL offers serverless Spark with pushdown and partition pruning, minimizing cost for large joins."
  },
  {
    "id": "13ee11cfdf5047c068915dae81bdfbcee6257bdb8cff8bd53ad885f9210ab0de",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "Your feature store offline table has outdated records. You need to batch ingest new features while preserving historical records for audit. Which pattern do you follow?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Overwrite the offline store using Feature Group import mode FULL Refresh.",
      "B": "Use a Glue job to append to the offline table in the Feature Store\u2019s backing S3 bucket.",
      "C": "Use a ProcessingJob to write new features to the same S3 prefix without Feature Store APIs.",
      "D": "Use SageMaker Feature Store Batch PutRecord API to append new records, which preserves history if record identifier and event time increase."
    },
    "explanation": "Batch PutRecord appends new feature values and keeps older versions for audit based on eventTime."
  },
  {
    "id": "d9380ac80eea68382f9f183e9ab4dbe0a86285beef63310191d36a480120c586",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You need to generate polynomial features (up to degree 3) on a subset of continuous variables for a regression model, using SageMaker Pipelines. Which step do you include?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A ProcessingStep that runs a Scikit-Learn PolynomialFeatures transformer on the selected columns.",
      "B": "A TransformStep using XGBoost\u2019s built-in polynomial feature parameters.",
      "C": "A Data Wrangler step with custom Python recipe for polynomial features.",
      "D": "A Glue ETL job scheduled separately and results stored in S3."
    },
    "explanation": "A ProcessingStep allows direct use of Scikit-Learn PolynomialFeatures within the pipeline."
  },
  {
    "id": "ef6b120c17591995c0bbd424f9d474a6f922a9efb1c2b2c87da53a0597c02169",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "Your data contains timestamp strings in multiple formats. You must standardize to ISO8601 before downstream feature extraction. Which AWS service and method handle this with minimal code?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Processing with a custom regex parser to normalize formats.",
      "B": "Use Glue ETL with custom Python UDF in Spark to parse and format timestamps.",
      "C": "Use SageMaker Data Wrangler: add multiple parse-date recipes with input patterns, unify output to ISO8601, then export.",
      "D": "Use DataBrew: define multiple date parsing recipes and publish standardized data."
    },
    "explanation": "Data Wrangler supports multiple parse-date recipes and an output format selection, reducing custom code."
  },
  {
    "id": "cc00a47e8f01633c9c2f80c476001f8ca37c31591887ed2d1a05989ed944ff33",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You must encode a hierarchical categorical feature (e.g., Country > State > City) to capture both levels for a tree model. Which sequence in Data Wrangler best preserves hierarchy?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "One-hot encode City only.",
      "B": "Create combined category codes (Country_State_City) then one-hot encode top N combined categories and bucket rest as Other.",
      "C": "Label encode each level separately.",
      "D": "Use Hash encoding on City and ignore higher levels."
    },
    "explanation": "Creating combined codes preserves hierarchy and one-hot encoding top combinations captures joint distribution with controlled dimensionality."
  },
  {
    "id": "10c5e65edc493b09e050262757eaaee9af4ca4cb4f7aab44e6a6a329dc7af1a8",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A dataset includes text descriptions with HTML markup. You must remove tags, lowercase, and compute the length of each description. Which managed flow meets this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Data Wrangler: add a custom transform node that runs a small Python function to strip HTML, lowercase, and compute length.",
      "B": "Use a Glue ETL job with BeautifulSoup and Python to process, then write to S3.",
      "C": "Use AWS Lambda triggered on S3 to clean text and store lengths in DynamoDB.",
      "D": "Use Glue DataBrew recipes: remove HTML pattern, lowercase, compute new column via formula."
    },
    "explanation": "DataBrew\u2019s formula for length may be limited; Data Wrangler custom node allows running Python code for HTML stripping and length calculation."
  },
  {
    "id": "6ba827ee34b2b430f357794f119be3ef9de3bae4811df082812711f04b76cde8",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "Your model benefits from interaction terms between a date and a numeric feature. You need to encode weekday \u00d7 metric interactions. Which pipeline component choice is correct?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use DataBrew: create numeric \u00d7 weekday formula recipes.",
      "B": "Use Glue ETL: write PySpark to compute interaction terms.",
      "C": "Use a SageMaker ProcessingStep with a Pandas script to extract weekday and multiply by the metric, then export.",
      "D": "Use Data Wrangler: extract weekday and use the \u201cGenerate feature combinations\u201d transform."
    },
    "explanation": "Data Wrangler does not auto-generate cross features; a ProcessingStep with Pandas gives precise control for interaction terms."
  },
  {
    "id": "472d812db9ebf29cd79273290f4d75a0d2eb1b14b086aee3e7a5012884325356",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You have high-cardinality ordinal data where label encoding might mislead the model. You need ordinal encoding that respects order but maps to a uniform distribution. Which transform do you apply?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use one-hot encoding after sorting ordinal values.",
      "B": "Use Quantile ordinal encoding: map ordinal rank to quantile value between 0 and 1 via Data Wrangler\u2019s custom transform.",
      "C": "Use label encoding directly.",
      "D": "Use binary encoding to reduce dimensionality."
    },
    "explanation": "Quantile ordinal encoding maps each ordinal category to its quantile position, preserving order without magnitude assumptions."
  },
  {
    "id": "7585aa5c262946d185079e9c91d926f5554c619b8940e382f8395f28391d5d53",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You need to integrate SageMaker Ground Truth labeled data into your feature-engineering flow in Data Wrangler, then merge those labels with features. Which pattern works?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Import the Ground Truth output manifest into Data Wrangler via import manifest, join on record IDs, and proceed with transforms.",
      "B": "Use Glue ETL to read manifest JSON and your features, join, then export to S3.",
      "C": "Use a ProcessingJob to read S3 manifest and feature data and merge in code.",
      "D": "Configure DataBrew to read manifest and features and merge via project join recipes."
    },
    "explanation": "Data Wrangler can directly import a Ground Truth manifest and join on record identifiers within the flow."
  },
  {
    "id": "28537e0e5f9beb202ee08a700c0bb3367aa5a4723b5cbc653daef946bb7897a4",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "During feature engineering, you need to normalize numeric fields per group (e.g., by customer segment) to zero mean within each group. Which service supports this grouping and transform without custom code?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "SageMaker Processing job with Pandas groupby and transform.",
      "B": "AWS Glue Streaming ETL or Glue batch ETL? Actually Data Wrangler? But grouping per segment: Data Wrangler supports groupby transforms in custom? It doesn't. So correct: ProcessingStep.",
      "C": "SageMaker Data Wrangler: add an \u201cAggregate\u201d transform with groupby and join back.",
      "D": "AWS Glue DataBrew: add \u201cGroup\u201d recipe with normalization."
    },
    "explanation": "Data Wrangler does not natively support group-based normalization; you need a custom script in a ProcessingStep to group and normalize."
  },
  {
    "id": "8940e5d538de53a72c756618d4965fc925fbba9e93437f9679fb35d6d306f73f",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You want to cleanse geographical coordinate outliers using interquartile range and then project lat/long into distance features from a fixed point. Which pipeline step do you use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A SageMaker ProcessingStep running a Python script that computes IQR, filters outliers, then applies haversine distance formula.",
      "B": "Data Wrangler recipes: outlier removal and geospatial transform.",
      "C": "Glue DataBrew: percentile filter and custom geospatial formula.",
      "D": "Lambda functions for geospatial cleaning and a Data Wrangler step for distance."
    },
    "explanation": "No managed transform for geospatial distance; ProcessingStep with Python gives full control for outlier filtering and distance computation."
  },
  {
    "id": "0b3d9d6dcd37ca420bb5b188ff883511ac908ab3ed292e81804f1338a51266ae",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A categorical feature contains hierarchical JSON structures per row. You need to flatten to multiple indicator features at any depth. Which approach scales best with minimal code?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Processing with a recursive JSON flattener in Python.",
      "B": "Use AWS Glue ETL on Spark with from_json and explode operations to flatten, then OneHotEncoderEstimator.",
      "C": "Use DataBrew JSON flatten recipe then one-hot encode.",
      "D": "Use Data Wrangler custom transform to flatten JSON manually."
    },
    "explanation": "Glue ETL\u2019s built-in from_json and explode support scalable JSON flattening; Spark\u2019s OneHotEncoder completes encoding."
  },
  {
    "id": "e307b606d0a95b3c6e6830bde98b43c575c4d39db01328d3ca189d99e5b41e3c",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You must generate time-lagged features (t-1, t-2) for a time series dataset with irregular intervals. Which solution handles irregular sampling properly?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Use Glue ETL with window functions assuming regular partitions.",
      "B": "Use Data Wrangler\u2019s lag transform with fixed intervals.",
      "C": "Use a ProcessingStep with pandas shift (but assumes uniform index).",
      "D": "Use a SageMaker Processing job with custom script that resamples by timestamp and computes lag based on nearest previous record."
    },
    "explanation": "Irregular intervals require custom resampling and lag logic in code, best done in a ProcessingStep."
  },
  {
    "id": "ce745610a4dfeb3b55291ef408bbef0ddf5144fc4ae8085d9be38316ecc4612e",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "Your model requires polynomial and interaction features, missing-value flags, and one-hot encodings in one cohesive flow. Which AWS-managed solution is most integrated?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Glue ETL job with custom PySpark combining all transformations.",
      "B": "SageMaker Processing job orchestrating multiple scripts.",
      "C": "SageMaker Data Wrangler flow: use built-in polynomial, fill missing flag, encoding, and export to Feature Store.",
      "D": "DataBrew project with custom recipes for each transformation."
    },
    "explanation": "Data Wrangler supports polynomial features, missing-value flagging, and categorical encodings in a single flow with direct exports."
  },
  {
    "id": "371f4cad4f0b16929a1ba3680906175a2bf006edf9a80c4c5add7a1a4a76c24c",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A bank is building a credit approval model. The training dataset contains a binary \u201capproved\u201d label and a protected \u201cgender\u201d feature. Before training, the ML engineer needs to quantify label imbalance (difference in approval rates) between male and female applicants. Which SageMaker Clarify configuration correctly measures this metric?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run a Clarify \u201cdataset_analysis\u201d job and specify the \u201cmean_difference\u201d metric for the \u201cgender\u201d feature.",
      "B": "Run a Clarify pre_training_bias job, set facet_name to \u201cgender\u201d, label_values_or_threshold to ['approved'], and include the \u201cdifference_in_proportions\u201d metric.",
      "C": "Run a Clarify post_training_bias job after model training and view SHAP importances for \u201cgender.\u201d",
      "D": "Use SageMaker Model Monitor baseline for data quality to detect label skew."
    },
    "explanation": "Pre-training bias jobs with facet_name and difference_in_proportions compute DPL for the protected feature before training."
  },
  {
    "id": "ea02cf57c587aa4f85f9c3c2b7854afd2edf26331cb7cd2c8b1656d88774a3ee",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A healthcare provider must de-identify PHI in free-text clinical notes stored on S3 before any ML processing. Which AWS service and approach automatically finds and redacts PHI entities in text files?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon Comprehend Medical de-identification API to detect and redact PHI, then store de-identified text back in S3.",
      "B": "Use AWS Glue DataBrew PII masking transform to remove PHI patterns.",
      "C": "Use Amazon Macie to classify S3 objects and replace PHI entities.",
      "D": "Run a SageMaker Clarify data quality job to identify and filter PHI."
    },
    "explanation": "Comprehend Medical de-identification API is purpose-built for PII/PHI redaction in clinical text."
  },
  {
    "id": "ecca1a4f44307312b4e8c59f3a325a4b9774d564ea9fe5d53b939cdb1373e409",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "An ML engineer uses an Amazon EFS file system to store HIPAA-regulated data for SageMaker training. To meet compliance, they must encrypt data at rest and in transit. Which configuration satisfies both requirements?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable EFS encryption at rest with a customer-managed CMK, create EFS mount targets with TLS, and reference the EFS file system in the training job.",
      "B": "Enable S3 SSE-KMS on the EFS bucket and configure SageMaker network isolation.",
      "C": "Switch to FSx for Lustre with in-transit encryption enabled and mount in training job.",
      "D": "Enable encryption on the training instance root volume and use HTTPS to access EFS."
    },
    "explanation": "EFS supports encryption at rest via CMKs and in-transit TLS mount targets; training jobs can mount encrypted EFS directly."
  },
  {
    "id": "0054ad6cb835ffbe2063ce4a9b779e56f7f92ed38b5620f50d1db342d7ce7f88",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A regression dataset has 20% missing values in the numeric \u201cincome\u201d column. The engineer must impute missing values by median without leaking test information. Using AWS DataBrew, which sequence of steps is correct?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Profile full dataset to compute global median, then impute \u201cincome\u201d on combined data and split into train/test.",
      "B": "First split dataset into train/test in DataBrew, then apply a recipe step on both sets using the median calculated from the training set.",
      "C": "Run a SageMaker Processing job with scikit-learn SimpleImputer on combined data, then split.",
      "D": "Use AWS Glue ETL to aggregate median and fill missing values, then split datasets."
    },
    "explanation": "To avoid leakage you must split first, compute median on training data, and apply same imputation to test."
  },
  {
    "id": "0de395f252ef66e31ae8180ea13c6bf76953cfd809678b1ac170b17e1828f89b",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "Before training, an engineer must verify that the \u201cuser_id\u201d column in CSV files is non-null and unique. Which AWS Glue Data Quality configuration accomplishes this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Define a Data Quality ruleset with NOT_NULL and UNIQUENESS rules on \u201cuser_id\u201d, run a Data Quality job and inspect violations.",
      "B": "Use SageMaker Data Wrangler profile and manually inspect unique count for \u201cuser_id.\u201d",
      "C": "Write a SageMaker Processing pandas script to drop nulls and duplicates.",
      "D": "Use AWS Glue DataBrew with a data quality job and assertions on \u201cuser_id.\u201d"
    },
    "explanation": "AWS Glue Data Quality jobs with built-in rules handle NOT_NULL and UNIQUENESS assertions at scale."
  },
  {
    "id": "3c960df7df958ddc56592a14c6ed5447b5408c0f2ace48de3f599d66cd75dc4e",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A large S3 dataset may contain anomalous rows that degrade model accuracy. Which AWS tool combination efficiently detects and removes outliers across multiple features during preprocessing?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Data Wrangler to profile the dataset, apply the Outlier Detection transform, and add recipe steps to filter anomalies.",
      "B": "Write custom Spark in AWS Glue to compute z-scores and drop outliers then load cleaned data to S3.",
      "C": "Run a SageMaker Clarify data bias job to detect feature outliers and filter.",
      "D": "Use AWS Glue Data Quality to generate anomaly reports and manually remove rows."
    },
    "explanation": "Data Wrangler\u2019s built-in outlier detection recipe automates detection and removal within the ETL workflow."
  },
  {
    "id": "311a28fcf55785225c7321a99db052ff9b2d48401410634bc8325c6d1c17aaee",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "The S3 training data files are partitioned by date, causing temporal ordering bias. To randomize samples across partitions in SageMaker training with minimal overhead, which option is best?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Specify ShuffleConfig with \u201cFullyReplicated\u201d in the S3 input channel of the training job.",
      "B": "Use AWS Glue DataBrew to shuffle rows within each file and write to new S3 files.",
      "C": "Run a SageMaker Processing job using AWS Wrangler to shuffle and rewrite the dataset.",
      "D": "Use Lambda to concatenate and shuffle data before training."
    },
    "explanation": "SageMaker\u2019s native ShuffleConfig on S3 inputs randomizes across objects without extra ETL."
  },
  {
    "id": "e8243e7bddab6f3ed59aa487a26c8d647f9ec2221d0f572fd893f362189d0984",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A dataset contains an \u201cssn\u201d column that must be irreversibly hashed before training. Which AWS service and transform securely performs this operation with minimal code?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use AWS Glue DataBrew \u201cMaskValue\u201d transform on \u201cssn\u201d column with SHA-256 salt and export the recipe.",
      "B": "Write a SageMaker Processing job in Python to hash values and save to S3.",
      "C": "Use AWS Glue Spark ETL with hash functions to overwrite values.",
      "D": "Use SageMaker Clarify anonymization to drop the \u201cssn\u201d field."
    },
    "explanation": "DataBrew\u2019s MaskValue step supports cryptographic hashing with salt in a low-code recipe."
  },
  {
    "id": "1edd96128e796b950584c307bb6106cb0a0d671b688a2f216d48a1c20ef14c73",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "In a fraud dataset, the minority class is only 1% of records. The engineer wants to quantify this imbalance via SageMaker Clarify and then mitigate it. Which metric and mitigation approach should be used?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use the ClassImbalance metric to measure ratio and apply SMOTE to oversample the minority class.",
      "B": "Use DifferenceInProportions metric and apply random undersampling of the majority class.",
      "C": "Use KLDivergence metric to detect drift and add class weights during training.",
      "D": "Use MutualInformation to rank features and drop low-importance ones."
    },
    "explanation": "ClassImbalance gives the minority ratio; SMOTE is a standard synthetic oversampling method to correct it."
  },
  {
    "id": "2bca6874510c4fe3a212cd6d5d914c7f08717a261fc53a3dc4152efb26e9988d",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "An organization mandates that all S3 input data for SageMaker training be encrypted with a customer-managed CMK. How should the training job be configured to honor SSE-KMS on input channels?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "In InputDataConfig for each S3 channel, set S3DataDistributionType to \u201cFullyReplicated\u201d and specify KmsKeyId with the CMK ARN.",
      "B": "Enable EncryptVolume on the training instance and rely on default S3 key.",
      "C": "Set EnableNetworkIsolation to true to force KMS encryption.",
      "D": "Configure OutputDataConfig KmsKeyId \u2013 input will inherit the same key."
    },
    "explanation": "Specifying KmsKeyId on InputDataConfig ensures SageMaker uses SSE-KMS to decrypt input data with the CMK."
  },
  {
    "id": "796f627c92616bbc2a5da76ec46941a9cc5075847395e8a477ac99bf6cfbc0ca",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A deployed ML model processes IoT telemetry. To detect production input drift and alert when numeric features shift beyond a threshold, which AWS service and metric should be used?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Model Monitor with a data quality JobDefinition that uses the KLDivergence metric.",
      "B": "Use SageMaker Clarify explainability to monitor SHAP drift in features.",
      "C": "Use AWS Config to evaluate drift rules on input buckets.",
      "D": "Use AWS CloudWatch anomaly detection on CPU usage of endpoint."
    },
    "explanation": "Model Monitor\u2019s data quality jobs with KLDivergence compare production vs. baseline distributions to detect drift."
  },
  {
    "id": "8ff581a9cc19689322a2e36883469c84e5b84797935921e9e5c9f86d2c245e68",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A dataset contains an ordinal \u201crating\u201d feature encoded as text (\u201cpoor\u201d, \u201caverage\u201d, \u201cgood\u201d, \u201cexcellent\u201d). To prepare for numeric modeling and avoid bias, which AWS DataBrew transform sequence is correct?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "One-hot encode \u201crating\u201d directly in DataBrew recipe.",
      "B": "Apply label encoding mapping \u201cpoor\u201d\u21920,\u2026\u201cexcellent\u201d\u21923 then one-hot encode in training.",
      "C": "Define custom map transform in DataBrew to assign ordinal values (0\u20133) preserving order, and leave as numeric.",
      "D": "Use SageMaker Clarify to auto-encode text categories during training."
    },
    "explanation": "Ordinal features should be mapped to numeric preserving order; one-hot would lose ordinality."
  },
  {
    "id": "c9a6b367daaf54eca31af63c928fc72889b5e1f0877893b2130a01b5f460c229",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A regulated dataset includes random noise in a \u201csalary\u201d column to preserve privacy. Before training, the engineer needs to detect abnormal noise levels that might distort the model. Which tool and check accomplish this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Glue DataBrew to profile mean and std deviation of \u201csalary\u201d across batches.",
      "B": "Use SageMaker Clarify data quality job with \u201cstatistics\u201d check to compare batch distributions to baseline.",
      "C": "Use SageMaker Model Monitor feature drift job with PSI metric on \u201csalary.\u201d",
      "D": "Use AWS Glue Data Quality anomaly detection rule on \u201csalary.\u201d"
    },
    "explanation": "Clarify data quality jobs support statistics checks to compare current vs. baseline distributions for numeric features."
  },
  {
    "id": "3e32f9f20429b6f3707e6b434d8685ee9a024b664c90d9f83bf91d8a40a468fc",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A ML pipeline ingests user-submitted text that may contain offensive words. The engineer must remove any rows containing profanity before training. Which approach is most automated?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Glue DataBrew FilterRows with a profanity regex.",
      "B": "Run a custom SageMaker Processing job calling an external profanity API.",
      "C": "Use AWS Glue DataBrew with built-in \u201cReplaceText\u201d transform referencing a profanity wordlist to flag and drop rows.",
      "D": "Use SageMaker Clarify to identify toxic text and filter."
    },
    "explanation": "DataBrew\u2019s ReplaceText transform and conditional filter can flag profanity and drop offending rows in a recipe."
  },
  {
    "id": "79c5053a69c09bd0a44f34af371704dbfaa6049450a18057445ec55abae58494",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A sensitive column \u201cemail\u201d must be pseudonymized by replacing with reversible tokens, ensuring only the training job can reverse it. Which design below meets this requirement securely?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Use DataBrew MaskValue transform with a static salt stored in S3.",
      "B": "Use SageMaker Processing with Python to hash emails and store hash key on the notebook.",
      "C": "Use AWS Glue ETL to encrypt email using default KMS key without rotation.",
      "D": "Use a SageMaker Processing job that calls AWS KMS GenerateDataKey to encrypt each email, storing ciphertext and encrypted data key columns."
    },
    "explanation": "GenerateDataKey provides a data key for encrypt/decrypt; processing job can encrypt email with the data key and store encrypted data key per row."
  },
  {
    "id": "c889520f5f1ccb8e5489d893d5094566e80e01b65e5d334d472c505bacc44b3a",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A health insurance company needs to predict claim costs using tabular data with dozens of numeric and categorical features. The model must be explainable for compliance and deployed quickly without custom code. Which modeling approach should the ML engineer choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a deep neural network in SageMaker with a custom PyTorch script.",
      "B": "Use the SageMaker built-in XGBoost linear learner in linear-learner mode.",
      "C": "Deploy an Amazon Bedrock foundation model for tabular regression.",
      "D": "Use SageMaker JumpStart to fine-tune a random forest model."
    },
    "explanation": "The SageMaker linear learner in linear mode offers both speed of built-in algorithms and coefficient-based explainability, meeting compliance without custom code."
  },
  {
    "id": "bbe58e51112e54494f1d97015f379c6bf6f888629b52ef4e35128ee1f71f4798",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A startup has millions of short text support tickets. They need to automatically categorize them into about ten categories. They require near real-time inference and minimal infrastructure management. Which approach is most appropriate?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Use SageMaker built-in Latent Dirichlet Allocation (LDA) to extract topics.",
      "B": "Train a K-means clustering model on bag-of-words features.",
      "C": "Use Amazon Comprehend custom classification endpoints.",
      "D": "Fine-tune a SageMaker JumpStart GPT model for classification."
    },
    "explanation": "Amazon Comprehend custom classification provides a managed, low-latency service optimized for text classification without heavy infrastructure or custom training."
  },
  {
    "id": "841123f57216771625e3ef4b38b601bb5bcbb39717b6577e6c087eecf8469afb",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A financial firm must model customer churn using a dataset of 100,000 rows and 200 features, many of which are sparse. They need both high accuracy and moderate interpretability. Which algorithm should they select?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Regularized gradient-boosted trees with SHAP explainability in SageMaker.",
      "B": "Deep neural network with attention layers in SageMaker Training.",
      "C": "K-nearest neighbors classifier on the full feature set.",
      "D": "SageMaker linear learner without feature interactions."
    },
    "explanation": "Gradient-boosted trees provide strong accuracy on structured data and SHAP integration yields moderate interpretability, balancing both requirements."
  },
  {
    "id": "2cc3ea5f02ee001e4de0a2cf4be0eaac3bac12d2ed14044e7b6086d986efbf6b",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A retailer wants to forecast hourly product demand across 500 stores. They need a model that handles seasonality and multiple time series simultaneously with minimal custom development. Which AWS capability should they use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Build a Seq2Seq model in PyTorch with SageMaker script mode.",
      "B": "Use SageMaker built-in DeepAR forecasting.",
      "C": "Fine-tune a Bedrock LLM for time series forecasting.",
      "D": "Use Amazon Forecast with a custom recipe for seasonal demand."
    },
    "explanation": "Amazon Forecast is a managed service specialized in multi-dimensional time series forecasting with built-in seasonality handling and minimal development."
  },
  {
    "id": "03424eb18ec95fcc9f26fc905663e5b7d591b76c52eb2fb7cb8def3c42bb646f",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "An e-commerce company needs to detect anomalous orders in real time. The data has 50 continuous and categorical features. They want full control over algorithm selection and hyperparameters. Which modeling approach is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train an Isolation Forest using SageMaker built-in with batch inference enabled.",
      "B": "Deploy Amazon Lookout for Metrics for anomaly detection.",
      "C": "Use SageMaker KNN anomaly detection with default parameters.",
      "D": "Fine-tune a JumpStart anomaly detection model on Amazon Bedrock."
    },
    "explanation": "SageMaker built-in Isolation Forest gives full control over hyperparameters and supports custom batch inference while being managed."
  },
  {
    "id": "026c68047ac9891c1cdcc15da1c67a01bf9f07d3713ec29bbed4178c6a3e136c",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A logistics company wants to segment customers into behavioral clusters using both shipment frequency and cost. They expect clear cluster centroids for interpretability. Which algorithm should they choose?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "DBSCAN clustering in SageMaker Processing.",
      "B": "Hierarchical clustering in SageMaker Spark.",
      "C": "Gaussian Mixture Model with EM in SageMaker.",
      "D": "K-means clustering with the SageMaker built-in algorithm."
    },
    "explanation": "K-means provides clear centroids and is available as a built-in SageMaker algorithm, balancing interpretability and managed operation."
  },
  {
    "id": "e3f56f91e24b2df12e2350b290079210f0e1d6059ebef5489335867f8d0ac7a4",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A media company needs to recommend movies using collaborative filtering on user-item ratings. They require a scalable solution integrated with SageMaker. Which modeling approach fits?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a matrix factorization model in Scikit-Learn on SageMaker Training.",
      "B": "Use Amazon Personalize batch recommendations.",
      "C": "Use SageMaker built-in factorization machines algorithm.",
      "D": "Fine-tune a JumpStart recommender system foundation model."
    },
    "explanation": "SageMaker factorization machines are optimized for collaborative filtering, scalable, and integrated into SageMaker workflows."
  },
  {
    "id": "56e403792c07aabbfe502109ab80ab055801b767f656f838e423ef5fde216e2b",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A biotech startup must classify protein sequences. Labeled data is scarce, and interpretability is not critical. They want to leverage pretrained models. Which approach is most appropriate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a CNN from scratch in SageMaker script mode.",
      "B": "Fine-tune a JumpStart protein foundation model.",
      "C": "Use SageMaker built-in XGBoost with k-mer features.",
      "D": "Develop a random forest on physicochemical features."
    },
    "explanation": "JumpStart protein foundation models leverage pretrained knowledge, requiring limited data and engineering, ideal for scarce labels without interpretability needs."
  },
  {
    "id": "d67af7dc86b52ab26e268b221982edb53113b8912b807d5a3f28e3fbb67a1bd4",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A compliance-driven enterprise must predict default risk on personal loans. They need to justify each decision to auditors. Which modeling approach balances accuracy and full explainability?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker built-in linear learner and present feature coefficients.",
      "B": "Train an XGBoost model and approximate SHAP explanations.",
      "C": "Deploy an Amazon Bedrock LLM to explain historical decisions.",
      "D": "Use DeepAR to forecast default probabilities."
    },
    "explanation": "Linear learner provides transparent coefficients for direct auditing, ensuring full explainability though with modest accuracy trade-off."
  },
  {
    "id": "3536c7687f7252c09176c1f24027524029543e2d46f217b7a8ed59d08d02c556",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A gaming company needs to classify images of player avatars into categories. They want to minimize GPU cost and only require coarse-grained accuracy. Which SageMaker option should they choose?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Fine-tune a JumpStart ResNet model.",
      "B": "Train a custom CNN in PyTorch on GPU.",
      "C": "Use SageMaker built-in image classification algorithm on CPU.",
      "D": "Deploy Amazon Rekognition with custom labels."
    },
    "explanation": "The built-in image classifier can run on CPU for coarse tasks with lower cost, avoiding GPU and heavy customization."
  },
  {
    "id": "081e523a7ada472e3fb38d1196721a2ad68e91da56de2daa6acd17987de395bb",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "An IoT company collects multivariate sensor time series and needs to detect anomalies streaming. They prefer managed services and minimal model maintenance. Which approach is best?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Train an LSTM autoencoder in SageMaker script mode.",
      "B": "Use the SageMaker built-in random cut forest algorithm in batch.",
      "C": "Fine-tune an LLM for time series anomaly detection.",
      "D": "Use Amazon Lookout for Equipment for streaming anomalies."
    },
    "explanation": "Amazon Lookout for Equipment is managed, supports streaming anomaly detection on sensor data with minimal maintenance."
  },
  {
    "id": "ae5fbbc22b07a65e82b2c48f8cd83adcb8ae81a3c6ddf1d396825433f457d155",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A marketing team wants to segment emails by topic automatically. They need a fully managed NLP solution that adapts over time. Which AWS service should they use?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Train LDA in SageMaker Processing.",
      "B": "Use Amazon Comprehend topic modeling APIs.",
      "C": "Fine-tune a JumpStart GPT-3 model for topic extraction.",
      "D": "Use Amazon SageMaker BlazingText for unsupervised topic clusters."
    },
    "explanation": "Amazon Comprehend provides managed topic modeling that adapts to new data without custom training."
  },
  {
    "id": "cfb3f89b06b548f78d0b4895e263279661e8944e7a7d9d4d9b0196550fd4c7f7",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A telecom wants to predict customer churn with an imbalanced dataset and needs to prioritize recall over precision. They plan to tune thresholds. Which algorithm and metric combination should they choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train XGBoost in SageMaker and optimize for recall using hyperparameter tuning with custom F-beta metric.",
      "B": "Use logistic regression and evaluate accuracy.",
      "C": "Deploy random forest and adjust class weights only at inference.",
      "D": "Use built-in factorization machines with default evaluation."
    },
    "explanation": "XGBoost with custom metric optimization allows tuning hyperparameters for recall, matching business priority on false negatives."
  },
  {
    "id": "ad1e91e9a8a3e998f9f6c5b8096f7bb2bcf013fcf82dcde1720940c23fe18c43",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A media analytics team needs to extract entities from video transcripts. They require deep customization of entity categories and managed model hosting. What should they choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a custom spaCy NER model on SageMaker Processing.",
      "B": "Fine-tune a JumpStart language model and deploy on SageMaker.",
      "C": "Use Amazon Comprehend custom entity recognition endpoints.",
      "D": "Use Amazon Textract to extract text and regex on transcripts."
    },
    "explanation": "Comprehend custom entity recognition offers managed training and hosting with customizable entities, minimizing infra effort."
  },
  {
    "id": "5951872eea36605ec90c37d89c36464e53ecd90121e9c01941ef9551926dcef6",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A fraud detection use case has a growing number of streaming events per second and requires sub-millisecond inference. They need high throughput and low latency with minimal operational overhead. Which approach fits?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "Deploy XGBoost model on SageMaker real-time endpoint on CPU.",
      "B": "Use SageMaker serverless inference with ProvisionedConcurrency.",
      "C": "Deploy model on ECS Fargate behind API Gateway.",
      "D": "Use Lambda to host model via Docker image."
    },
    "explanation": "Serverless inference with provisioned concurrency handles variable volumes and low latency without managing servers."
  },
  {
    "id": "48f66cd7970ca9a5e658eb14c32dd137282ad131a6641d0d78274c619630ca7e",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A retailer wants price elasticity estimates per product category, requiring interpretable coefficients. They have limited compute budget. Which modeling approach should they select?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Train a tree-based ensemble and use SHAP.",
      "B": "Fine-tune a Bedrock regression foundation model.",
      "C": "Use SageMaker XGBoost regressor default.",
      "D": "Use SageMaker linear learner with L1 regularization for sparse features."
    },
    "explanation": "Linear learner yields direct coefficients for price elasticity, with L1 handling sparsity and minimal compute overhead."
  },
  {
    "id": "156960a1b19dc53e92e48fea9c8bbb7df1afd8a42d53d88811694a99379ab6c5",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "An ML engineer must automatically classify customer reviews into positive or negative sentiment. They need near zero code and managed endpoints. Which AWS service should they choose?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Build a logistic regression in SageMaker script mode.",
      "B": "Use Amazon Comprehend sentiment analysis endpoints.",
      "C": "Fine-tune a JumpStart BERT model.",
      "D": "Train SageMaker built-in BlazingText skip-gram model."
    },
    "explanation": "Comprehend provides out-of-the-box sentiment analysis API with no code and managed hosting."
  },
  {
    "id": "a720cb733524d39a42a808e2b95c8fd22d0fa649db9d493bcc609a59a0a65802",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A company has sparse high-dimensional clickstream data and wants a scalable algorithm for binary classification. They prioritize L1 regularization to perform feature selection. Which SageMaker built-in algorithm should they choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker built-in linear learner with L1 mode.",
      "B": "Built-in XGBoost with default settings.",
      "C": "Built-in KNN.",
      "D": "Built-in K-means."
    },
    "explanation": "Linear learner supports L1 regularization for feature selection and scales to high dimensions."
  },
  {
    "id": "e0c8f073b5f6ee9fbc93a85a568b9f9318b58aa0f9dc5cc10c21e7299656ee41",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A healthcare provider needs to detect disease from X-ray images with regulatory constraints on data privacy. They want to use a managed service and avoid moving data off-premises. Which solution is best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker built-in image classification in public endpoints.",
      "B": "Deploy a JumpStart Vision transformer in SageMaker Studio.",
      "C": "Configure SageMaker Inference on AWS Outposts with the built-in image classification algorithm.",
      "D": "Use Amazon Rekognition for custom labels."
    },
    "explanation": "SageMaker on AWS Outposts runs locally, preserving data privacy without sending images off-premises."
  },
  {
    "id": "b202f75a54a1e8d75867faab2dce7112ce94e5080247a7d732e7f1f9ef8d041e",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A startup needs to detect default probability from loan applications with limited GPU budget. They require class probability outputs and fast training. Which algorithm should they pick?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deep AR forecasting model.",
      "B": "Built-in XGBoost classifier.",
      "C": "Built-in K-means clustering.",
      "D": "Built-in PCA."
    },
    "explanation": "XGBoost classifier offers fast training on CPU/GPU, outputs probabilities, and is cost-efficient."
  },
  {
    "id": "6ff270b728a54a4cd4e3d4d4e3905416968a7072b1eb954612dae709b162691d",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A legal firm wants to identify key contract clauses in documents. They need a managed NLP solution they can tailor without hosting infrastructure. Which should they use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Comprehend custom classification for clause categories.",
      "B": "Fine-tune JumpStart GPT-3 and host on SageMaker.",
      "C": "Use SageMaker built-in BlazingText.",
      "D": "Train a custom BERT model in SageMaker script mode."
    },
    "explanation": "Comprehend custom classification lets them define clause categories and uses managed endpoints without infra overhead."
  },
  {
    "id": "17a90fa254c2ae4d2a1a7c64ee02316610268e5617829435adf2fb5c5af9f539",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A search engine needs to rank results by relevance using user click logs. They require pairwise ranking and integration with SageMaker. Which algorithm is suitable?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Built-in random forest classifier.",
      "B": "Built-in KNN algorithm.",
      "C": "DeepAR forecasting.",
      "D": "SageMaker Factorization Machines with ranking objective."
    },
    "explanation": "Factorization Machines support pairwise ranking objectives and integrate seamlessly in SageMaker pipelines."
  },
  {
    "id": "9e4e45500ecd0a9a6f1c546097610b539824af964f9a86da0351cb01c9610f78",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "An agricultural startup wants to detect crop diseases from leaf images. They have limited labeled data and need transfer learning with minimal fine-tuning. Which approach should they choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a CNN from scratch in PyTorch on SageMaker.",
      "B": "Use SageMaker built-in XGBoost on flattened image features.",
      "C": "Fine-tune a JumpStart transfer learning image classification model.",
      "D": "Deploy Amazon Rekognition Custom Labels."
    },
    "explanation": "JumpStart transfer learning models require minimal labeled data and fine-tuning for image tasks, reducing effort."
  },
  {
    "id": "99b7d199121c2888992b939d924f070023b15abe35ccf5cd42c048197781b7a8",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A robotics company collects multi-modal sensor data (images and lidar). They need a model that natively handles heterogeneous inputs. Which modeling approach is best?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "Ensemble separate built-in algorithms for each modality.",
      "B": "Train a multimodal PyTorch model in SageMaker script mode.",
      "C": "Use Amazon SageMaker built-in Linear Learner with concatenated features.",
      "D": "Fine-tune a JumpStart text foundation model."
    },
    "explanation": "A custom PyTorch script enables combining different data types in a single model, which built-ins cannot natively handle."
  },
  {
    "id": "49fa82dd5d1ad7d8ff5057d4c11b9735bc3dda2b9df2ca7b2b25b66cd6e477a2",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A voice assistant service must detect five spoken commands in streaming audio. They need low latency and a managed solution. Which AWS offering should they use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a CNN in SageMaker to classify spectrograms.",
      "B": "Use SageMaker built-in KNN on MFCC vectors.",
      "C": "Fine-tune a JumpStart Transformer.",
      "D": "Use Amazon Transcribe to convert speech to text and Comprehend custom classification."
    },
    "explanation": "Transcribe plus Comprehend custom classification offers managed low-latency command detection without building audio models."
  },
  {
    "id": "b4309d85cf69eaa6a7ba2e945a606572ff6e6c19faa68cebc5b162460fb2c025",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A rideshare company needs to price surge multipliers based on real-time location and demand. They require sub-second inference at scale. Which modeling approach and deployment should they choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a decision tree in SageMaker and deploy on CPU endpoint.",
      "B": "Use SageMaker real-time multi-model endpoints with XGBoost model.",
      "C": "Host a neural network on ECS Fargate.",
      "D": "Use Amazon Forecast predictor endpoint."
    },
    "explanation": "XGBoost on multi-model endpoints provides low-latency at scale with minimal endpoint overhead for multiple regions."
  },
  {
    "id": "a3387a501193e675ae8432d37620a1820838bf98142ba4b6f4686a39965bee1b",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "A financial services company observes that their fraud-detection model has high overall accuracy but low recall on high-value transactions. They suspect the model underfits rare, high-value cases. Which of the following evaluation strategies best identifies this issue?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Plot a single ROC curve on the full dataset and inspect AUC",
      "B": "Compute macro-averaged precision across all transaction values",
      "C": "Segment the test set by transaction value and compute per-segment recall",
      "D": "Evaluate overall F1 score without stratification"
    },
    "explanation": "Segmenting by transaction value and computing recall highlights underperformance on high-value cases. Overall AUC or aggregated metrics mask segment-specific issues."
  },
  {
    "id": "4d10dfbed32b11e6c51240de0166544acdc3e6701e49bf9593f144cc233290e2",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "An ML engineer deploys two XGBoost models: a control and a new variant. After 1M predictions, the variant shows a slightly higher latency but 2% higher recall. How should they evaluate if this trade-off is acceptable?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Compare single-sample inference logs for outliers",
      "B": "Plot precision-recall curves and latency distributions for both variants",
      "C": "Compute overall accuracy for both and pick model with higher accuracy",
      "D": "Use confusion matrix of the slower model only"
    },
    "explanation": "Precision-recall curves reveal how recall gain trades off against precision, while latency distributions quantify performance cost. Overall accuracy and single-sample latency don\u2019t capture distribution details."
  },
  {
    "id": "fefc8d934e14068213a6c9dfbbd782405c089f739c8548c78f589720e6bf2160",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "During hyperparameter tuning of a neural network, SageMaker Debugger reports gradient vanishing in early layers. Which metric or visualization should you use to pinpoint the affected layers?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Plot TensorBoard histograms of weight and gradient distributions per layer",
      "B": "Inspect model\u2019s overall loss curve only",
      "C": "Compute validation accuracy after each epoch",
      "D": "View average prediction confidence on test data"
    },
    "explanation": "Layer-wise weight and gradient histograms directly show vanishing or exploding gradients. Loss curves and accuracy trends are too coarse to locate the problem."
  },
  {
    "id": "6985c96b83e924118c558855fd986c6cc6503d1396ee9c5f962b1d44ac6dd5ea",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "A classification model shows 95% accuracy but customers complain of frequent false positives. Which metric will best capture the operational false-positive rate?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ROC AUC",
      "B": "Accuracy",
      "C": "Macro F1 score",
      "D": "False positive rate (FP/(FP+TN))"
    },
    "explanation": "False positive rate directly measures proportion of non-events incorrectly flagged. Aggregate metrics and AUC do not quantify FP in production terms."
  },
  {
    "id": "e8ce03d06624ec0b33a32b647b04324fcf8dd6d4c16be115077eb3837536be07",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "An ML engineer wants to compare a shadow variant\u2019s performance to production. They have stored logs with predictions and ground truth over a week. What\u2019s the most reproducible approach?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Re-run inference on production endpoint and compute metrics",
      "B": "Compute metrics in real time on live data",
      "C": "Use the stored logs to batch calculate identical evaluation metrics for both models",
      "D": "Use shadow variant accuracy reported in CloudWatch metrics"
    },
    "explanation": "Batch calculating evaluation metrics on the same logged data ensures reproducibility. CloudWatch reports may differ in metric definitions, and re-running inference can produce drift."
  },
  {
    "id": "1a7b356b16ebefc8bb2336ddf957067b1e72264c7ebb31081fae5dff47d21ec5",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "After deploying a regression model, business users report that error increases when feature \"loan_amount\" exceeds $100k. Which diagnostic step is most appropriate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Check overall RMSE",
      "B": "Plot residuals against loan_amount",
      "C": "Compute R\u00b2 on training data",
      "D": "Re-train model without loan_amount"
    },
    "explanation": "Residual-vs-feature plots reveal heteroscedasticity or feature-specific error patterns. Overall RMSE or training R\u00b2 mask inequalities across feature ranges."
  },
  {
    "id": "52e0d9bb272af28e14c4432ff49050066d94fdf19cbbc22f7c042a061381951b",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "SageMaker Clarify indicates a high feature importance contribution from \"zipcode\" in loan approval model. Finance team worries about proxy bias. What evaluation should be done next?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Compute disparity metrics for protected groups defined by zipcode demographics",
      "B": "Remove zipcode and retrain immediately",
      "C": "Ignore since importance doesn\u2019t imply bias",
      "D": "Compute overall feature correlation matrix"
    },
    "explanation": "High importance for zipcode may proxy demographic bias; computing disparity metrics for protected groups reveals actual bias. Simply removing the feature may degrade performance without confirming bias."
  },
  {
    "id": "6b56be7426ff67142286d8648f86e8674665e188b6e4cf6ab4bd179f23f3f18e",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "A model\u2019s AUC on validation is 0.85 but drops to 0.75 in production. Which step best isolates the cause?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase regularization to reduce overfitting",
      "B": "Retrain with more epochs",
      "C": "Change the threshold to match validation TPR",
      "D": "Compare feature distribution between validation and production using Clarify data drift reports"
    },
    "explanation": "Distribution drift analysis identifies covariate changes causing performance drop. Regularization and threshold tuning don\u2019t address data mismatch."
  },
  {
    "id": "f806b6f256b4f5c7af912add786bc6c0640cb9ff328e983a29058b0630ccc47f",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "During batch experiments, an engineer notices that increasing tree depth raises precision but lowers recall. Which plot best illustrates this trade-off?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Precision-recall curve parametrized by depth",
      "B": "ROC curve at each depth",
      "C": "Confusion matrix for the deepest tree only",
      "D": "Histogram of prediction confidences"
    },
    "explanation": "Precision-recall curves for each depth show the precision/recall trade-off directly. ROC curves and single-depth confusion matrices do not illuminate that trade dynamically."
  },
  {
    "id": "b3cf329671ff9890f77f560da98fb72e3f21accbb82b75e8fba5a89238564891",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "After hyperparameter tuning, two models have identical validation loss but different generalization gaps. Which metric indicates stronger overfitting?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Higher training loss",
      "B": "Larger difference between training and validation accuracy",
      "C": "Higher validation AUC",
      "D": "Lower inference latency"
    },
    "explanation": "A large gap between training and validation accuracy indicates overfitting. Validation AUC and latency don\u2019t measure overfitting directly."
  },
  {
    "id": "743a7dd585268866d3a103c6f5a7f67fee22c4cf9f1b4464d96698714b87b925",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "An NLP classification model shows significantly higher F1 for short texts versus long texts. How do you quantify performance disparity?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Overall F1 score",
      "B": "Word count distribution histogram",
      "C": "Group test samples by text length bins and compute per-bin F1",
      "D": "Pearson correlation between length and predicted label"
    },
    "explanation": "Bin by text length and compute F1 per bin to quantify disparity. Overall metrics and correlation don\u2019t isolate the problem."
  },
  {
    "id": "fb848d53d308b41fd1b46343f3b4dd2161c4b807b238f820d4a97f5a4189c2b9",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You need to determine if a drop in model accuracy after a code change is due to random seed variance or genuine performance loss. Which practice ensures reproducibility?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Fix random seed across training, log model parameters, and compare artifacts",
      "B": "Re-run training with different seeds and average",
      "C": "Use early stopping to stabilize performance",
      "D": "Only compare inference results on live data"
    },
    "explanation": "Fixing seeds and logging artifacts ensures reproducible runs. Averaging across seeds helps smooth noise but doesn\u2019t isolate change impact."
  },
  {
    "id": "a22845786cb27a4154408c1dcd874d57f0f2c3b1a897da16338ff3d18c1deadb",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "An engineer suspects label leakage because a high validation AUC is too good to be true. Which diagnostic will reveal leakage?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase regularization",
      "B": "Shuffle labels and retrain to observe AUC",
      "C": "Add more layers",
      "D": "Compute feature correlations only"
    },
    "explanation": "Shuffling labels destroys true signal; if AUC remains high, leakage exists. Feature correlations alone may miss leakage patterns."
  },
  {
    "id": "0fede1b1d3ff80e98d31f555b98281e43effcf1571696312f2850dfc3c19884f",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "A deployed model exhibits high memory consumption during inference. Which SageMaker Debugger profiler metric helps pinpoint bottlenecks?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Overall endpoint invocation count",
      "B": "CloudWatch memory usage graphs",
      "C": "Inference latencies per request",
      "D": "Model container\u2019s PeakHostMemoryUsage metric"
    },
    "explanation": "PeakHostMemoryUsage from Debugger profiling identifies model container memory peaks. Invocation counts and latency don\u2019t measure memory usage.\n"
  },
  {
    "id": "fd556de0517e601bbf4b92f3e85e2375afc20f0ad82d8d78962fd94d3c543a94",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "When comparing two regression models, you find one has lower MAE but higher RMSE. What does this indicate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "The second model has larger outlier errors",
      "B": "The second model performs better on average",
      "C": "The first model is overfitting",
      "D": "The first model has higher variance"
    },
    "explanation": "Higher RMSE than MAE implies presence of larger outlier errors, since RMSE penalizes them more. Average performance may be similar.\n"
  },
  {
    "id": "79c377f070ad590a96571d92e5cc5dbe874944a2abbfcec4bb6bc8eecce849ed",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You deploy an image classifier and monitor per-class recall. Class A recall drops below SLA overnight. Logs show no data drift. What is your next diagnostic?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase batch size",
      "B": "Retrain with more epochs",
      "C": "Analyze inference input distribution for class A by sampling inputs",
      "D": "Reduce learning rate"
    },
    "explanation": "Sampling inputs checks if unexpected inputs are arriving for Class A. No drift in overall features doesn\u2019t guarantee class-specific input change.\n"
  },
  {
    "id": "6e1dbf1fd236dfbcb9f818db8f52dffe625a1c37007656359f56ef8b3e5708ad",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your model\u2019s production log shows many low-confidence predictions. Which threshold-based method helps maintain production SLA?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use maximum probability per sample as a calibration correction",
      "B": "Implement a confidence threshold to send low-confidence cases to manual review",
      "C": "Retrain until all predictions exceed a fixed threshold",
      "D": "Always return top-2 classes to clients"
    },
    "explanation": "A confidence threshold with fallback to manual review maintains SLA. Calibration alone won\u2019t meet SLA guarantees.\n"
  },
  {
    "id": "b19f2d10f5d444be1db8d0eb18e621f442088402cd570ef39eb8cfbfde7edd2f",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "An ML pipeline reruns hyperparameter tuning daily. You want to compare results over time. Which practice aids trend analysis?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Log only best tuning job\u2019s metrics",
      "B": "Store only final model artifacts",
      "C": "Compare only today's and yesterday's tuning graphs",
      "D": "Archive tuning job metrics and hyperparameter configurations in SageMaker Experiments"
    },
    "explanation": "SageMaker Experiments archives metrics and configs over time, enabling trend and retrospective analysis. Comparing partial data isn\u2019t sufficient.\n"
  },
  {
    "id": "5cb1c006d63ff83c5ee9fbe99ee4234ae4549fe2dd8918fb08d8ec4387c59e87",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "After retraining a model with more data, confusion matrix shows increased FN but decreased FP. Business impact of FNs is higher. How do you adjust the model?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Shift decision threshold to favor recall over precision",
      "B": "Increase regularization",
      "C": "Reduce training data size",
      "D": "Switch to a different algorithm"
    },
    "explanation": "Threshold adjustment trades precision and recall bias to reduce FNs, aligning with higher cost of FNs. Algorithm shift or data size changes are heavier-handed.\n"
  },
  {
    "id": "b1276a5801db89ca3de986495271469c0ae86deec98cd938e067102a43178a2e",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "SageMaker Clarify reports a DPL (difference in proportions of labels) of 0.12 for a protected group. Which conclusion is correct?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model is fair, since DPL < 0.2",
      "B": "There is moderate demographic bias requiring mitigation",
      "C": "DPL only applies to continuous features",
      "D": "Bias can be ignored if overall accuracy is high"
    },
    "explanation": "DPL of 0.12 indicates non-trivial bias that should be addressed. Fairness thresholds vary, but non-zero DPL warrants mitigation. Accuracy doesn\u2019t nullify bias.\n"
  },
  {
    "id": "c790ec93d2357f361d0487875dedf1de6368bb0d846a378b9f6be4ba8adb1426",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You need to diagnose why training loss plateaus early. Which combined SageMaker Debugger rule is most helpful?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "LossNotDecreasing and LargeGradient",
      "B": "VanishingGradient and OverTraining",
      "C": "LossNotDecreasing and VanishingGradient",
      "D": "OverTraining and HighThroughput"
    },
    "explanation": "LossNotDecreasing flags plateau, VanishingGradient identifies small gradients. Together they pinpoint stalled training due to gradient issues.\n"
  },
  {
    "id": "4b73984e34e0b2066f2a0a2941f3eb9caaf71fc9be57e0e6561807fb26d24827",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "After enabling endpoint data capture, you observe data skew for feature X. Which remediation reduces skew immediately?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Retrain online with captured data",
      "B": "Increase endpoint instance count",
      "C": "Switch to a different instance type",
      "D": "Implement input feature transformation to normalize X at inference"
    },
    "explanation": "Applying transformation at inference normalizes skewed feature values immediately. Retraining is longer-term and won\u2019t reduce skew on streaming data.\n"
  },
  {
    "id": "0c06c9f934f918454bec0dafc9b3a0b29fd05809ee037dd818ac4d0f9cfded28",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your binary classifier has high variance between folds in cross-validation. What change will most reduce variance?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use ensembles like random forests",
      "B": "Increase learning rate",
      "C": "Use fewer features",
      "D": "Reduce training set size"
    },
    "explanation": "Ensembling reduces model variance by averaging multiple learners. Lower LR or fewer features may increase bias. Reducing data increases variance.\n"
  },
  {
    "id": "ca4b801a2d70ceec4583b13ea2dbd753ff5a93566c91ddae3c7d1043d8cc924f",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "A class-imbalanced dataset yields high recall but poor precision. Which metric-guided tuning step helps restore balance?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase tree depth",
      "B": "Use mean squared error loss",
      "C": "Optimize model using precision-recall AUC as objective",
      "D": "Switch to accuracy objective"
    },
    "explanation": "Using PR AUC aligns hyperparameter tuning to both precision and recall on imbalanced data. MSE or accuracy objectives aren\u2019t appropriate for classification imbalance.\n"
  },
  {
    "id": "bc109c223e8c78d0120073a2180f76b082c4f47a054bc9efceffc79ba9b461a4",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "An LSTM model overfits after 10 epochs. You add dropout but performance still degrades. Which debugging step helps find root cause?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase L2 regularization",
      "B": "Use Debugger\u2019s tensor shape rule to confirm no unintended dimension mismatch causing overfitting",
      "C": "Increase learning rate",
      "D": "Reduce batch size"
    },
    "explanation": "Tensor shape mismatches can cause incorrect weight updates that appear as overfitting. Debugger shape rules detect such bugs. Regularization alone may mask deeper issues.\n"
  },
  {
    "id": "011e80d79ff48b264a6e5659e55b835d7b1fe1e7fdceaeb2d340f3bd90c42c20",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your model produces inconsistent outputs when served on GPU versus CPU instances. How do you isolate source?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Retrain separately on GPU and CPU",
      "B": "Enable inference latency metrics",
      "C": "Use SageMaker Clarify to detect drift",
      "D": "Run inference deterministically on both environments with same input batch and compare element-wise outputs"
    },
    "explanation": "Deterministic inference comparison on CPU vs GPU pinpoints numerical instability or library inconsistency. Drift detection and latency metrics don\u2019t isolate environment differences.\n"
  },
  {
    "id": "d0ab58e04ab2b907f41bad523021bf85368567f4ba6d2d98c4f33e4d36c05e77",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "A regression model\u2019s residuals exhibit non-constant variance. Which transformation corrects this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Standardize all features",
      "B": "Apply log transform to the target variable",
      "C": "Increase polynomial degree of predictors",
      "D": "Use one-hot encoding for categorical features"
    },
    "explanation": "Log-transforming the dependent variable often stabilizes variance (heteroscedasticity). Feature standardization doesn\u2019t address residual variance issues.\n"
  },
  {
    "id": "bb425e7edaa3c3b495d7ecb4ed7af2b3a096a1ed03952f3cd6bdcaa0e3355485",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "After changing preprocessing, model performance unexpectedly improved in test but dropped in validation. Which practice uncovers this data leakage?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Inspect preprocessing pipelines for operations applied before train-test split",
      "B": "Increase test set size",
      "C": "Add more regularization",
      "D": "Use k-fold CV on original split"
    },
    "explanation": "Applying preprocessing (e.g., normalization) before splitting leaks test information. Reviewing pipeline order reveals leakage.\n"
  },
  {
    "id": "22d91288dff54fcdce8247456454dfa530917e98805d393a37a5a5b6f09291ce",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your model\u2019s precision increases when you add a new feature but recall drops. You need to quantify the net business impact. What analysis should you perform?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Compute new overall F1 score",
      "B": "Plot ROC curve",
      "C": "Estimate expected cost change using per-error cost matrix",
      "D": "Compare AUC-PR before/after"
    },
    "explanation": "Cost matrix analysis translates precision/recall changes into business impact. F1 and curves are abstract performance measures without cost context."
  },
  {
    "id": "73082a5e5845f182f044b78a6eb9b7c7b1edae2905db13b6bd32263c0f211357",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A fintech startup has an image-based document verification model that requires sub-100ms inference latency at the edge. They must deploy the optimized model to ARM-based devices with intermittent connectivity. Which deployment infrastructure best meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Host the model on a SageMaker real-time endpoint in a VPC and use IoT Greengrass to pull predictions.",
      "B": "Compile the model with SageMaker Neo for ARM, deploy to IoT Greengrass devices for local real-time inference.",
      "C": "Package the model in a custom Docker container, deploy to AWS Lambda on ARM architecture.",
      "D": "Use a SageMaker serverless endpoint and cache responses on edge devices."
    },
    "explanation": "SageMaker Neo produces highly optimized ARM binaries for local (<100ms) inference. IoT Greengrass ensures offline capability."
  },
  {
    "id": "b5d39d7928f10d2229d6ddb80b173d9ef35c94732095d481d0dc05accb9868e8",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An e-commerce application has a recommendation model that sees large traffic spikes during flash sales. It runs in a private VPC, uses GPU instances, and needs multi-model hosting with auto-scaling. Which architecture is optimal?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Deploy separate single-model SageMaker real-time endpoints per model with target-tracking scaling.",
      "B": "Use a SageMaker serverless endpoint with model registry versioning.",
      "C": "Containerize models and host on ECS Fargate behind an ALB with custom scaling scripts.",
      "D": "Use a multi-model SageMaker real-time endpoint in the VPC with provisioned concurrency and auto-scaling policies."
    },
    "explanation": "Multi-model endpoint reduces overhead, shares GPU, and auto-scaling in VPC meets spikes."
  },
  {
    "id": "ec8520bae2679b43dd5177d1c3d40a13daef061c3985c25d6493c7d14d145acd",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A research team needs to batch-process 10 TB of genomic data weekly. They want to minimize cost, use the same container as real-time inference, and avoid idle pods. Which infra is best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create a SageMaker asynchronous endpoint with batch transform inside it.",
      "B": "Deploy models on EKS pods with KNative autoscaling.",
      "C": "Use SageMaker batch transform jobs with the real-time container and spot instances.",
      "D": "Host on a long-running ECS Fargate service and schedule container tasks."
    },
    "explanation": "Batch transform jobs spin up only for jobs, reuse container, support spot to reduce cost."
  },
  {
    "id": "86fc5736421007cf5b6e08d1e6534f64175da23ef401fc8a52039408004621a7",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A global SaaS company requires GDPR compliance. Their NLP model must serve EU-only traffic in dedicated subnets and scale on demand. Which deployment solution is most appropriate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy regional SageMaker real-time endpoints in EU-based subnets with endpoint auto scaling.",
      "B": "Deploy a single SageMaker serverless endpoint and enforce subnet restrictions.",
      "C": "Host on ECS Fargate in one EU cluster and route all traffic through it.",
      "D": "Use Lambda functions behind API Gateway with VPC peering to subnets."
    },
    "explanation": "Regional real-time endpoints in EU subnets ensure data residency and scalable performance."
  },
  {
    "id": "05f6f86d0471a63060beded06443572376ee32a8b40d566f6b5592b853046c32",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A startup needs to test a new fraud-detection model in production without risking customer impact. They want to route 5% of traffic to the new model, track performance, and easily roll back. Which deployment strategy and infra should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy two SageMaker asynchronous endpoints and split payloads manually.",
      "B": "Use a SageMaker serverless endpoint for the new model and shift DNS entries.",
      "C": "Deploy the new model to a SageMaker real-time endpoint variant in the same endpoint, configure a canary traffic shift.",
      "D": "Host the new model in ECS Fargate and use Application Load Balancer weighted routing."
    },
    "explanation": "Real-time endpoint variants support canary deployments and easy rollback within the same endpoint."
  },
  {
    "id": "d60f586d1b1d7351eef4c8199ec9de854c388bb3ab1a13724b4eea01b2cd63c3",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An advertising platform has multiple small models (<100MB) that share similar preprocessing code. They need to deploy 50 models, optimize resource utilization, and minimize cost. What is the best approach?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy 50 separate real-time SageMaker endpoints.",
      "B": "Use ECS Fargate and spin up a container per model.",
      "C": "Bundle all models into one Docker image and host on a single serverless endpoint.",
      "D": "Use a SageMaker multi-model endpoint with shared container in a single VPC endpoint."
    },
    "explanation": "Multi-model endpoints share the container and infra, reducing cost and overhead for many small models."
  },
  {
    "id": "50aab3b132e5f3f709a94957350a2110987d988ab5ef69d810a093b6cfe57a5c",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A biotech firm must run long-running inference jobs (up to 30 minutes each) that process large data packages. They want predictable scaling and pay-per-use cost. Which SageMaker deployment fits best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure a SageMaker asynchronous inference endpoint with appropriate max payload size.",
      "B": "Use a SageMaker real-time endpoint and extend Lambda timeouts.",
      "C": "Submit each job as a batch transform.",
      "D": "Host it on ECS Fargate with long-running tasks."
    },
    "explanation": "Asynchronous endpoints handle long payloads, non-blocking, scale predictably and charge per call."
  },
  {
    "id": "2820da3e63347264d05c154a30b19bc3eb8d466996442171bfcf9e31501f8bc8",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A media company needs GPU acceleration for video-related inference but wants to minimize idle GPU costs during off-hours. They can tolerate 1\u20132 second cold start. Which infra is optimal?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Keep a provisioned GPU SageMaker real-time endpoint running with auto-scaling.",
      "B": "Use SageMaker serverless endpoints configured with GPU support.",
      "C": "Deploy on ECS Fargate GPU instances with manual scaling schedules.",
      "D": "Host on Lambda GPU functions behind an ALB."
    },
    "explanation": "Serverless endpoints with GPU avoid idle costs and manage cold starts within tolerance."
  },
  {
    "id": "f548c62185e3393174e68fa7575aaeae74e80450ca5c51468385dcf466f790e6",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An auto manufacturer needs to deploy multiple model versions concurrently for A/B tests, with equal traffic share and easy rollback. They must run in the same VPC. Which infra supports this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy separate ECS services per version behind an ALB.",
      "B": "Use Lambda versions and aliases in VPC.",
      "C": "Deploy separate SageMaker serverless endpoints per version.",
      "D": "Use a SageMaker real-time endpoint with two production variants and traffic weights."
    },
    "explanation": "Production variants in one real-time endpoint allow weighted traffic splitting and quick rollback."
  },
  {
    "id": "bf0934796900a0d3b8b6592ed45c65212f887ba26b77158054fbd2c2418f2818",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A retailer needs to deploy a recommendation model to handle unpredictable holiday season traffic. They need <50ms latency, GPU inference, and minimal ops overhead. Which solution is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker serverless endpoints with GPU acceleration and auto-scaling.",
      "B": "Deploy a spot-based ECS GPU cluster with custom autoscaler.",
      "C": "Maintain a provisioned GPU real-time endpoint with scheduled scaling.",
      "D": "Run inference in Lambda with EFS-mounted model files."
    },
    "explanation": "Serverless GPU endpoints auto-scale without manual capacity management and meet latency."
  },
  {
    "id": "5efd6349b8c2bf5f300fc332a050fff742f59e1c7f4c4a070c5769e800e0c716",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A healthcare application requires batch scoring of PHI data with encryption at rest and in transit. They want to reuse existing VPC resources and avoid cross-account data movement. Best infra?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker serverless endpoint in VPC with data capture.",
      "B": "Real-time endpoint in VPC with KMS encryption.",
      "C": "Batch transform jobs configured in the same VPC and encrypted S3.",
      "D": "ECS Fargate with EFS encryption."
    },
    "explanation": "Batch transform in VPC uses S3 encrypted data, no constant endpoint and reuse VPC security."
  },
  {
    "id": "6bd9e98fd165809b65254602ab17267e1bde93a5bb6238b7934c9702e1fd2bc8",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A gaming company needs to deploy a scoring model to hundreds of edge kiosks with no always-on internet and limited compute. They need occasional connectivity to sync metrics. Which infra fits?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a SageMaker real-time endpoint through AWS IoT Core.",
      "B": "Compile via SageMaker Neo, deploy to Lambda@Edge on Greengrass devices.",
      "C": "Host containerized model on small EC2 instances at each kiosk.",
      "D": "Deploy inference code as AWS Lambda functions invoked by MQTT."
    },
    "explanation": "Neo-compiled model on Greengrass devices runs locally, syncs periodically over IoT Core."
  },
  {
    "id": "8a5c5d8f2380aa5506aeccd0db5b93c281e005d60b8c1115fb6b6211da79a6da",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A logistics company has a micro-batch inference requirement: group deliveries into sets of 100 and process every 5 minutes. They want minimal latency overhead and no idle endpoints. What infrastructure should they choose?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy a real-time endpoint and buffer requests into batches.",
      "B": "Use serverless endpoint with batched invocation.",
      "C": "Submit jobs to ECS Fargate tasks scheduled every 5 minutes.",
      "D": "Configure a SageMaker asynchronous inference endpoint with batching config."
    },
    "explanation": "Asynchronous endpoint supports batching and scheduled triggers without idle infra."
  },
  {
    "id": "2e07a995aceb5bc74a1811bed26d8bc1520497746b2464532c1344f0a8994c4a",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A startup wants to experiment with GPU- and CPU-based inference to compare cost-performance trade-offs. They need a common orchestration tool and seamless model promotion. Which approach is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Pipelines to deploy the same model to GPU and CPU real-time endpoints.",
      "B": "Script deployments in Terraform to provision ECS GPU and CPU clusters.",
      "C": "Use AWS Batch for GPU and Lambda for CPU inference.",
      "D": "Manually launch EC2 GPU and CPU instances and copy containers."
    },
    "explanation": "SageMaker Pipelines automates deployment steps, tracks versions, and supports multiple compute targets."
  },
  {
    "id": "1025afd9618885f1d6b3c5648a35d4df975aa44b6ff3597a1477fd5c5de244ec",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A financial modeling team needs to deploy Python-based models that require custom system libraries. They want minimal management and autoscaling. Which deployment target is most appropriate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy as a SageMaker serverless endpoint with layers.",
      "B": "Use Lambda with custom runtime layer.",
      "C": "Build a custom Docker container and host on SageMaker real-time endpoint in VPC.",
      "D": "Use ECS Fargate with code-build pipelines."
    },
    "explanation": "Custom container on SageMaker real-time endpoint supports all dependencies, auto-scales, VPC support."
  },
  {
    "id": "e8669e1663af8bd86ceac6115d625ee256ef638210ec8eb39db32b0ba014450b",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A data science team wants to automate their ML workflow so that any code commit on the \u2018develop\u2019 branch runs unit tests, data validation, and triggers a SageMaker training pipeline. After successful training, the model should be registered and automatically deployed to a staging endpoint for integration tests, before manual approval pushes to production. Which CI/CD design best meets these requirements?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use CodePipeline with a Source action on GitHub \u2018develop\u2019, CodeBuild to run unit tests and data validation scripts, a CodeBuild action to invoke a SageMaker Pipeline for training, a stage to register the model in the Model Registry, a CloudFormation action to deploy to staging, an integration-test CodeBuild action, followed by a manual approval and a Lambda action to swap the staging endpoint to production.",
      "B": "Use SageMaker Pipelines with Git integration triggering on \u2018develop\u2019 for training and registration, then use a separate CodePipeline triggered by new Model Registry entries to deploy to staging and run integration tests, with manual approval for production.",
      "C": "Use CodePipeline with a GitHub source on \u2018develop\u2019, a CodeBuild stage to run unit tests and data quality checks, a SageMaker Pipeline action to train and register the model, a CloudFormation stage to deploy to a staging endpoint, a CodeBuild integration-test stage, then a manual approval stage and a CloudFormation canary-deployment stage for production.",
      "D": "Use CodeDeploy directly to push code changes to SageMaker endpoints, adding a CodeDeploy pre-deployment hook to run tests and trigger training before deployment."
    },
    "explanation": "Option C uses CodePipeline native actions for each stage, integrates SageMaker Pipelines, staging, tests, manual approval, and a canary production deployment with minimal glue code."
  },
  {
    "id": "e76af5e3ad981af4873a13d190cba5ebbf209038522a990fa485bb5fb32500af",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A compliance policy requires monthly retraining of an ML model using new data uploaded to s3://company/data/monthly/ at 1 AM on the first of each month. The workflow must run data quality checks, trigger SageMaker model training, register the model, and send an email if any stage fails. How should you configure this with minimal overhead?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create an EventBridge scheduled rule that invokes a Lambda function which runs data quality checks, starts a SageMaker training job, registers the model, and publishes to SNS on failure.",
      "B": "Create an EventBridge scheduled rule to start a CodePipeline at 1 AM monthly. In CodePipeline, add: a Source stage (S3 polling), a CodeBuild stage for data quality checks, a SageMaker Pipeline invoke stage for training and registration, and a CloudWatch Events action to notify via SNS on failure.",
      "C": "Use SageMaker Pipelines with a Schedule trigger for monthly runs, include DataQualityCheck, Train, Register steps, and configure an SNS Alarm for failed steps.",
      "D": "Use AWS Batch scheduled on EventBridge to run a container that executes the entire workflow and sends SNS notifications."
    },
    "explanation": "Option B leverages EventBridge\u2192CodePipeline schedule, built-in actions (S3 source, CodeBuild, SageMaker Pipeline, SNS) with minimal custom code."
  },
  {
    "id": "e9ba5893651cc88f9359baf5056a1883159c74263f941be0f1d7f73e876b090a",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "An organization maintains separate dev and prod AWS accounts. They want a centralized CI/CD in dev that, upon merging to master in CodeCommit, triggers a deployment pipeline in prod account to update the SageMaker endpoint. How should they configure this cross-account pipeline?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "In dev account create a CodePipeline with a CloudFormation action targeting prod account using StackSets.",
      "B": "Push artifacts from dev S3 bucket to prod S3 bucket, then have a prod pipeline polling that bucket to start deployment.",
      "C": "Use AWS CodeDeploy cross-account deployments from dev to prod.",
      "D": "In dev CodePipeline, add an IAM role action with a role ARN in prod account. Grant the dev pipeline role permission in prod to assume that role. In prod, that role\u2019s trust policy allows the dev pipeline service role to assume it and run deployment actions."
    },
    "explanation": "Option D correctly uses IAM role assumption to allow a pipeline in one account to execute actions in another account without custom polling."
  },
  {
    "id": "14db6e71240d0c5436718d731f0fa0a206fcf6505e61cb725fc2e5505b7fcb9a",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A team follows Gitflow: feature/* branches, develop, release/*, and master. They want two pipelines: one for develop that runs tests and staging deployments on merges to develop, and one for production on merges to master. How should they filter source triggers in CodePipeline?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use two CodePipelines with GitHub source actions configured: one with branch filter pattern '^refs/heads/develop$' and webhooks enabled, the other with '^refs/heads/master$'.",
      "B": "Use a single pipeline triggered on all branches; in the first stage inspect the branch name and abort if not develop or master.",
      "C": "Use CloudWatch Events on push events with wildcard branch patterns to start pipelines via Lambda.",
      "D": "Configure CodePipeline to poll S3 artifacts tagged with branch names to distinguish branches."
    },
    "explanation": "Option A uses CodePipeline\u2019s built-in branch filters in source actions to isolate triggers per pipeline."
  },
  {
    "id": "50c4c77ab9bf8d4a025fad72a3d65be4e382a0293f8b2fdd6d802fcf8e0d87a9",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A data engineering pipeline produces a delta file in S3 upon completion. You must ensure that your ML CI/CD pipeline ingests this file, validates schema against a baseline, and fails if mismatched, before triggering training. How do you integrate this into CodePipeline?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Have CodePipeline poll the S3 prefix for the delta file and pass it to a Lambda that does schema validation and returns success/fail.",
      "B": "Trigger CodePipeline with an S3 event notification directly to the pipeline\u2019s StartPipelineExecution API.",
      "C": "Use an EventBridge rule matching the S3 PutObject event to call StartPipelineExecution, then in CodePipeline add a CodeBuild stage that runs a schema-validation script against the new file and fails on mismatch.",
      "D": "Use Lambda to copy the file to a Kinesis stream, use a CodePipeline source action on the stream, and a CodeBuild validation stage."
    },
    "explanation": "Option C uses EventBridge\u2192CodePipeline to start on S3 writes and uses CodeBuild to validate schema, failing if mismatched."
  },
  {
    "id": "2dd69d1de0b3d1c0f8674b5e648d7ddaf10545b908f8c646958f7c4e80cfcc62",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your ML pipeline requires a secret API key available to training jobs at build time. You must not expose the secret in plain text or logs. Which CodePipeline stage configuration meets this requirement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Store the API key in S3 encrypted with KMS. In CodeBuild, download and decrypt it inside the build script.",
      "B": "Store the API key in AWS Secrets Manager. In the CodeBuild project definition, set an environment variable that references the Secrets Manager ARN and enable 'Reveal secrets in build logs' to false.",
      "C": "Hard-code the secret in an encrypted parameter in Systems Manager Parameter Store and pull it during the build.",
      "D": "Pass the secret as a plaintext environment variable in the CodePipeline action configuration, relying on IAM to protect it."
    },
    "explanation": "Option B uses Secrets Manager with environment variables in CodeBuild, ensuring secrets aren\u2019t logged."
  },
  {
    "id": "2f9feff958140fd2d94a5244ad11983117fe57c39a40285f588873dd251cfb95",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A reloadable data pipeline updates feature data daily. You want to trigger model retraining automatically whenever a new featureset lands in the SageMaker Feature Store. What\u2019s the least\u2010maintenance way to integrate this into your CI/CD pipeline?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Modify your daily ETL job to call the CodePipeline StartPipelineExecution API at its end.",
      "B": "Use S3 event notifications on the Feature Store underlying S3 bucket to trigger a Lambda that starts the pipeline.",
      "C": "Add a CloudWatch alarm on Feature Store PutRecord metrics and have it trigger pipeline via SNS.",
      "D": "Create an EventBridge rule matching the SageMaker Feature Store PutRecord API via CloudTrail, target StartPipelineExecution for your training pipeline."
    },
    "explanation": "Option D leverages CloudTrail events in EventBridge to detect new records and start the pipeline with zero custom code in the ETL job."
  },
  {
    "id": "c246531f6389950b37bb592e442797da2c457b95c98e2b88769175e20b74e5b8",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "You need to implement a canary deployment for a real-time SageMaker endpoint in your pipeline. It must shift 5% traffic to the new model initially, then 50%, then 100%, with automatic rollback on errors. Which combination of actions do you use in CodePipeline?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Use CloudFormation deploy action with TYPE=BLUE_GREEN and trafficRoutingConfig CanaryInterval (5%) and CanaryInterval (50%), and a CloudWatch alarm action to auto-rollback.",
      "B": "Use CodeDeploy with application type 'AWS Lambda' against the SageMaker endpoint and set canaryDeploymentConfig.",
      "C": "Invoke a Lambda function in each stage to call UpdateEndpointWeightsAndCapacities on the endpoint.",
      "D": "Use a custom CodeBuild action to call the SageMaker API to shift traffic and monitor latency; if latency rises, call rollback API."
    },
    "explanation": "Option A uses CloudFormation blue/green with canary traffic routing and automatic rollback via alarms, requiring no custom code."
  },
  {
    "id": "d66e0f8d79caa4424af1ec169926afccf7d98cff954700e6453f5b04657701fe",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A governance requirement mandates that any pipeline change must be peer-reviewed before execution. You want to enforce code reviews for the buildspec.yaml and pipeline definition in CDK. What CI/CD pattern accomplishes this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Allow developers to commit directly to main; have a pre-build hook that runs only if PR label 'reviewed' exists.",
      "B": "Store CDK pipeline code and buildspec in a feature branch, require PR approval in CodeCommit/GitHub, merge only after two approvers, then trigger pipeline on merge to main.",
      "C": "Use CodeBuild PreBuild commands to validate a 'reviewed-by' tag in Git metadata before proceeding.",
      "D": "Configure a manual approval action at the start of every pipeline run to verify that PRs were reviewed externally."
    },
    "explanation": "Option B leverages standard Git-based code-review gating before code merges and pipeline triggers."
  },
  {
    "id": "845197e1f00ca54137c1735479c5b63c09e3ec8c9e2f61cce4fe626adfc30a32",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your pipeline runs SageMaker training jobs in parallel for hyperparameter tuning. Sometimes runs collide and exceed resource quotas, causing failures. How do you prevent concurrent training executions?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Limit the maximum concurrent CodeBuild builds in the project settings to 1.",
      "B": "Use a Lambda in your pipeline to list current SageMaker jobs and fail fast if any RUNNING jobs exist.",
      "C": "Configure the CodePipeline stage for training to use an ActionConfiguration including a custom concurrency token in your state machine so that only one execution at a time proceeds.",
      "D": "Set the SageMaker training action\u2019s maximum concurrency to 1 in the pipeline definition."
    },
    "explanation": "Option C uses a concurrency token to serialize that action\u2019s invocations; CodePipeline supports concurrencyToken to prevent parallel runs."
  },
  {
    "id": "b541d5489d1e458af44f3cb052607e65258fb392e6785d8ee26ae2e2f092a509",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "You want to include a data-skew check using SageMaker Clarify in your CI/CD before training. Which pipeline stage sequence and actions accomplish this without custom containers?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "After unit tests, add a CodeBuild stage that invokes SageMaker Clarify ProcessingStep via AWS CLI, passing baseline and new dataset; fail build on detected skew.",
      "B": "Add a custom Docker build to run a Python script that calls Clarify SDK inside CodeBuild.",
      "C": "Trigger SageMaker Pipelines containing a ClarifyBaselineStep; ingest results via SNS into CodePipeline.",
      "D": "Use Lambda action in CodePipeline to call Clarify\u2019s API and post results to S3, then have pipeline poll for results."
    },
    "explanation": "Option A uses CodeBuild with the AWS CLI to call SageMaker Clarify ProcessingJob, avoiding custom Docker images."
  },
  {
    "id": "53ee1f661c8e27115695f3f9d26cbc4cf6b16e534a15e07cf581d30301c707ca",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your pipeline must run unit tests, then integration tests against a temporary SageMaker endpoint after deployment, and delete the endpoint automatically when tests complete or fail. What do you add?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A CodeDeploy preTraffic hook to run tests and a postRollback hook to delete the endpoint.",
      "B": "A Lambda in CodePipeline to run tests and then call DeleteEndpoint.",
      "C": "A CodeBuild test stage that spins up the endpoint, tests, then calls DeleteEndpoint in the same build.",
      "D": "Two CodeBuild actions: one for integration tests against the staging endpoint, then one that deletes the endpoint by calling AWS CLI delete-endpoint; add them sequentially and configure failureAction=Abort."
    },
    "explanation": "Option D cleanly separates testing and teardown in CodeBuild stages, ensuring teardown always runs after tests if preceding stages succeed."
  },
  {
    "id": "24608ba3dfacdfce85c334389f4054c6ca50b372cbf39e93661868b2bf1700fe",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A developer accidentally pushed sensitive data to a feature branch. You need to prevent that branch from ever triggering your CI/CD pipeline. How do you modify your pipeline\u2019s source action?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a whitelist of allowed branch patterns in the buildspec to abort on unallowed patterns.",
      "B": "Configure the GitHub source action to include an exclude filter 'refs/heads/feature/sensitive*'.",
      "C": "Use a Lambda in the source stage to inspect commits and abort if the file pattern matches.",
      "D": "Add a pre-build CodeBuild action that checks commit history for sensitive files."
    },
    "explanation": "Option B uses the source action\u2019s exclude filter to prevent specific branch patterns from triggering the pipeline."
  },
  {
    "id": "ed64c6969367769dc315900c05a5f14280d504d96de1203ab0dbe168b15501d9",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your ML pipeline needs to use Spot Instances for cost-saving on training. How do you enable this in your CodePipeline/SageMaker training integration?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "In the SageMaker Pipeline definition YAML, set ResourceConfig.AuxiliarySpotCapacity to the desired percentage and invoke via CodePipeline\u2019s SageMakerPipelineAction.",
      "B": "Add a CodeBuild stage that runs 'aws sagemaker create-training-job' CLI with --use-spot-instances flag.",
      "C": "Wrap training in a Lambda function configured with capacityType=SPOT and call it from CodePipeline.",
      "D": "Switch the CodePipeline action type to AWS Batch and configure Spot in the ComputeEnvironment."
    },
    "explanation": "Option A leverages SageMaker Pipelines spot training support via ResourceConfig and CodePipeline\u2019s SageMakerPipelineAction without custom code."
  },
  {
    "id": "4496588a31bfd25b55a668722fc04c2054fe6d35df81c1f764c8eaab5a86e860",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "You need to trigger your CI/CD pipeline whenever a pull request is opened or updated, and only for PRs targeting develop. How is this configured?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure CodePipeline Source with GitHub webhook on all PR events and in the build stage inspect the target branch.",
      "B": "Use EventBridge rule for GitHub PullRequest events filtering on repository name.",
      "C": "Enable GitHub Pull Request webhooks in the source action and set the targetReference filter to 'refs/heads/develop'.",
      "D": "Use a Lambda subscriber to GitHub webhooks to call StartPipelineExecution when PR target is develop."
    },
    "explanation": "Option C uses the source action\u2019s webhook pull request event support with targetReference filters to limit triggers to PRs against develop."
  },
  {
    "id": "a939f25bd4b64cb606b66b05ff78e1f1283f8f950b057b425c4549de3d1f5e66",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your staging endpoint runs expensive benchmarks as part of integration tests. To avoid excessive cost, you only want to run tests when changes affect inference code. How do you optimize your pipeline?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Cache build artifacts between runs to speed up deployment.",
      "B": "Use a manual approval before integration tests.",
      "C": "Use a Lambda pre-check in CodePipeline to compare diff of inference directory and skip tests stage if no changes.",
      "D": "Split your repository: place inference code in its own folder and configure two pipelines\u2014one that triggers full workflow on inference folder changes (using source filters) and one for other changes."
    },
    "explanation": "Option D uses separate pipelines with source filtering to avoid running expensive tests when unrelated code changes."
  },
  {
    "id": "1f00993d2989e8cb431a51208d8f3fae216c1c8f9fcef7367c9840f12436cb4a",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your pipeline fails intermittently at the SageMaker Pipeline invocation stage due to transient throttling errors. How do you make the pipeline more resilient?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Wrap the SageMakerPipelineAction in a CodeBuild action with a retry loop using exponential backoff on throttling errors.",
      "B": "Increase the service quota for StartPipelineExecution calls.",
      "C": "Add a manual retry approval stage after the failure.",
      "D": "Use a Lambda-based custom action that catches throttles and auto-retries once."
    },
    "explanation": "Option A uses a CodeBuild wrapper to implement retries with backoff, improving resilience without manual intervention."
  },
  {
    "id": "e0253559827159bfd8ccad03d7afa8f0ca4bf1ce627e6e34ca25fe7832610398",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "You want your CI/CD pipeline to automatically rollback to the last known good model if performance metrics degrade after deployment. How can you implement this in CodePipeline?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Include a manual approval stage that runs a performance test, and if it fails, manually invoke rollback.",
      "B": "Use a Lambda in the post-deploy stage to run a test; if metrics below threshold, call UpdateEndpoint to the previous variant.",
      "C": "Use a CloudWatch alarm on performance metrics to trigger a CodePipeline retrigger of the deploy stage with the previous model via CodeDeploy automatic rollback.",
      "D": "Publish the previous model as an alias in Model Registry and switch alias in a Lambda action on failure."
    },
    "explanation": "Option C ties CloudWatch alarms to CodePipeline\u2019s deploy stage with a configured rollback in CodeDeploy, automating fallback."
  },
  {
    "id": "2abea8b9af9a6ca77daf7c88ec19d1c03a60ce519e39bdb04dac83ec08dbd083",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your team uses GitHub Flow, pushing directly to main and using feature branches only for experiments. They want every merge to main to trigger a full pipeline. But they also use semantic version tags (e.g., v1.2.0) to denote releases. How do you ensure only tag pushes trigger production deployments?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure CodePipeline source to only trigger on main branch updates.",
      "B": "Use two pipelines with GitHub source actions: one listening to refs/tags/v* (for production) and another to refs/heads/main (for tests), and in the production pipeline restrict to tag pattern '^refs/tags/v[0-9]+.*$'.",
      "C": "Add logic in the build stage to parse GIT_REF and only continue if it\u2019s a tag.",
      "D": "Use EventBridge pattern matching on GitHub Tag events to start the pipeline."
    },
    "explanation": "Option B cleanly separates pipelines by source filters: one triggers on tag pushes for production deployments."
  },
  {
    "id": "829030693cc314f850d892f15dca4a45e7c38a817da0c53280ce413e85294c9a",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A pipeline source stage uses AWS CodeCommit. Developers occasionally force-push and rewrite history, causing source metadata mismatches. How do you make the pipeline robust against force pushes?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable polling instead of webhooks in the CodeCommit source action.",
      "B": "Add a pre-build stage to Git fetch --prune and reset --hard origin/branch.",
      "C": "Configure the CodePipeline source action to use 'FullClone' fetch method to always get the current tip regardless of history rewrite.",
      "D": "Disable code commit history rewrite in the repository settings."
    },
    "explanation": "Option C uses the FullClone fetch mode so the pipeline always pulls the latest commit independent of history changes."
  },
  {
    "id": "81ddd315d470a6d55ac1ce23829ec8d53b1e57d1dad29b9c20b26231b8b8c4b7",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "You need to integrate model explainability tests into your pipeline: after training, fail the pipeline if any feature attribution for protected classes exceeds a threshold. Which stage should you add?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a CodeBuild stage that runs a SageMaker Clarify ModelExplainabilityProcessingJob via AWS CLI with the trained model, parses the results JSON, and exits non-zero if attribution drift exceeds the threshold.",
      "B": "Invoke a Lambda function in a pipeline action that calls Clarify SDK and uses CloudWatch metrics to decide pass/fail.",
      "C": "Embed the Clarify step inside the SageMaker Pipeline and then have CodePipeline poll for model registry tags to decide.",
      "D": "Use a CloudFormation custom resource to run the Clarify job and roll back the stack on failure."
    },
    "explanation": "Option A uses CodeBuild to orchestrate Clarify explainability checks, parse results, and enforce pipeline success/failure cleanly."
  },
  {
    "id": "8ab0a3449378588cf8bbf7439688f27d46c5c7f22a7c6c38f767dbcff166c34a",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A pipeline stage builds and pushes a Docker image to ECR, then a SageMaker training action uses that image. Occasionally, the training job starts before the image is available, causing errors. How do you prevent this race condition?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a fixed sleep delay in the training action\u2019s custom script.",
      "B": "Use an EventBridge rule on ECR image-push to trigger the pipeline second stage.",
      "C": "Configure CodeBuild to push to a different tag and have training wait on tag propagation.",
      "D": "Split the build-and-push into one CodeBuild stage that outputs the image URI as an artifact, and configure the next SageMakerPipelineAction to consume that artifact, which enforces sequencing."
    },
    "explanation": "Option D uses CodePipeline artifact dependency to guarantee that the training stage does not start until the image has been successfully pushed and the URI passed along."
  },
  {
    "id": "1ba3213386e9e7c256054bd5ff2b52de47b2b06315b78985b2f44e674b1337b7",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your pipeline\u2019s CodeBuild unit-test stage frequently downloads the entire repository, slowing builds. How do you optimize clone behavior?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure the GitHub source action to use 'ShallowClone' with a depth of 1, and enable CodeBuild\u2019s source cache for the repo.",
      "B": "Use Git LFS to store large binaries outside the repo.",
      "C": "Split the repo into micro-repos each with its own pipeline.",
      "D": "Switch the source action to S3 and upload only changed files manually."
    },
    "explanation": "Option A combines shallow cloning at the source action with CodeBuild caching to speed up fetch time without repo restructuring."
  },
  {
    "id": "64419fa043cc650b496c0ddcf7e4a0eff91dda97fa7263035cc94caa8b4932ba",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A CI/CD pipeline should only deploy models when both unit tests and integration tests pass. However, you need integration tests to run against a live endpoint, which itself requires deployment. How do you model this in CodePipeline?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run both unit and integration tests in the same CodeBuild action after deployment.",
      "B": "Chain two separate pipelines: one for tests, one for deployment, and only call the deploy pipeline if tests pass.",
      "C": "In a single pipeline: Stage1 run unit tests via CodeBuild, Stage2 deploy to staging via CloudFormation, Stage3 run integration tests via CodeBuild, Stage4 manual approval and production deploy.",
      "D": "Use a SageMaker Pipeline that supports validation steps natively and call it from CodePipeline."
    },
    "explanation": "Option C lays out a linear pipeline with distinct stages for unit tests, deployment, integration tests, and production approval."
  },
  {
    "id": "b449c918133a1c95bfc2077112493a1e45a2c0810ecfe29139ce764e2f8ae0fd",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "You want to notify your team in Slack whenever any pipeline stage fails, including the stage name and error message. How do you configure this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Attach an SNS topic to pipeline notifications and configure an AWS Chatbot integration for Slack.",
      "B": "Create a CloudWatch EventBridge rule matching CodePipeline 'Stage Execution Failed' events, target a Lambda that formats the message and posts to Slack via webhook.",
      "C": "In each CodePipeline stage\u2019s onFailure hook, invoke a Lambda to post to Slack.",
      "D": "Enable CloudTrail logs for CodePipeline and stream failures to Slack using Log Insights."
    },
    "explanation": "Option B uses a single EventBridge rule and a Lambda to capture failure events and send formatted Slack messages, minimizing per-stage configuration."
  },
  {
    "id": "ef42bb960dcd197205aaaefb06bf1bbf17e8c075eb9ffe9e2628d771e92d7046",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "An executive insists that production deployments require two separate approvers. How can you enforce this in CodePipeline?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add two successive ManualApproval actions in the production deployment stage, each assigned to a different user group.",
      "B": "Configure the ManualApproval action with 'approvalThreshold' set to 2.",
      "C": "Use IAM policies to require MFA for invoking the production stage.",
      "D": "Chain two identical manual approval actions but restrict both to the same group."
    },
    "explanation": "Option A ensures two distinct approvals by having two manual approval actions in series, each can be assigned to different user sets."
  },
  {
    "id": "d1002c73abad118b8ee6ef5fcb54fdd8a5cd86cc79a75d77758fbfca7f035cd8",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your CodePipeline deploys a SageMaker model via CloudFormation. When the deployment fails, the pipeline remains stuck in a FAILED state and you must manually clean up. How do you configure automatic rollback of the CloudFormation stack?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a CloudFormation DeleteStack action after the deploy action conditioned on failure.",
      "B": "Use a Lambda in the onFailure hook to delete the stack.",
      "C": "Set the CloudFormation action\u2019s 'ActionMode' to CHANGE_SET_REPLACE and 'RollbackOnFailure' parameter to true.",
      "D": "Wrap the CloudFormation deploy in an AWS Step Functions state machine with a Catch to delete the stack."
    },
    "explanation": "Option C leverages CloudFormation action parameters to rollback automatically, avoiding manual cleanup."
  },
  {
    "id": "3a171374604ed06630690ae4254b48a1e7aa6e9d33f1605f0617b540763f79c5",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "You need to run parallel hyperparameter tuning and data quality steps in your pipeline to reduce total runtime. How do you implement parallel actions in CodePipeline?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create multiple pipelines and start them simultaneously from a Lambda.",
      "B": "Use SageMaker Pipelines which natively parallelizes steps.",
      "C": "Run one CodeBuild action that multiplexes both tasks in background processes.",
      "D": "Define two actions within the same pipeline stage; CodePipeline will execute them in parallel, then proceed only when both succeed."
    },
    "explanation": "Option D uses CodePipeline\u2019s support for parallel actions in a single stage to run tasks concurrently."
  },
  {
    "id": "5af206a1fc951787b2fd7f9c9ac855d534b5276e48117c5883104fd332d1aecf",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An online retailer deploys a real-time inference endpoint for product recommendations. Shortly after launch, they notice that the distribution of a key categorical feature \"user_region\" has shifted, although model accuracy remains within SLA. Which automated monitoring solution should they implement to detect and alert on this feature distribution change with minimal custom coding?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure a SageMaker Model Quality Monitor job to check for data drift on \"user_region\" using ground truth labels.",
      "B": "Use SageMaker Model Monitor DataQuality baselining to establish constraints on \"user_region\" and schedule a drift detection job.",
      "C": "Deploy SageMaker Clarify to compute SHAP values for \"user_region\" on each inference and alert when feature importance changes.",
      "D": "Write an AWS Lambda that polls CloudWatch logs, computes a KS test on \"user_region\", and sends SNS alerts."
    },
    "explanation": "Model Monitor DataQuality jobs can baseline feature distributions and automatically detect drift with minimal custom code. ModelQualityMonitor requires labels; Clarify focuses on bias/importance rather than raw distribution; a Lambda solution requires more custom work."
  },
  {
    "id": "0837c2356b966402307fe38f2531a82da7ae43f8672749626b0ff7560f246d3b",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A financial institution uses a SageMaker real-time endpoint for credit scoring. They need to monitor inference latency degradation and data schema violations. Which combination of SageMaker features meets both requirements natively?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure SageMaker Model Debugger to capture latency and data quality metrics.",
      "B": "Use CloudWatch Contributor Insights for the endpoint to detect schema violations and latency issues.",
      "C": "Enable Data Capture on the endpoint and run SageMaker Model Monitor with both DataQuality and ModelQuality checks.",
      "D": "Deploy AWS X-Ray for latency tracing and AWS Glue Data Quality for schema validation."
    },
    "explanation": "Data Capture plus Model Monitor can track payload schema violations via DataQuality checks and latency via custom metrics. Model Debugger focuses on training/debug; Contributor Insights and Glue don\u2019t natively integrate inference monitoring as well."
  },
  {
    "id": "53414fa813b97f9617e70f97c1ab4aa3666d90dba960a3c1c716abb025414cef",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A company wants to detect concept drift in predicted probabilities of a binary classifier in production in near real-time. They also want to minimize cost. Which monitoring configuration should they choose?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run a SageMaker Clarify ModelExplainabilityMonitor on each batch of incoming requests.",
      "B": "Configure an asynchronous SageMaker endpoint with built-in data drift detection.",
      "C": "Use SageMaker Model Quality Monitor with ground truth labels and a 5-minute schedule.",
      "D": "Enable Data Capture on the endpoint and schedule a SageMaker Model Monitor DataQuality drift check on predicted score distribution."
    },
    "explanation": "Without ground truth, ModelQualityMonitor isn\u2019t applicable. Clarify ExplainabilityMonitor focuses on SHAP. Asynchronous endpoints have no built-in drift. DataQuality drift checks on predictions detect concept drift cost effectively."
  },
  {
    "id": "a70bd604a757007e492138f5e1ddad672514f6885109d8ba3356298301729a96",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An ML engineer deployed two production variants (A and B) for A/B testing. They need to monitor emerging data skew between the variants for a continuous categorical feature \"device_type\". Which approach best surfaces variant-specific data skew?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure two separate Model Monitor DataQuality jobs, each capturing and analyzing \"device_type\" for its variant.",
      "B": "Use a single Model Monitor DataQuality job that collects data from both variants and uses filters on \"variant_name\".",
      "C": "Use SageMaker Clarify to detect bias between variants on \"device_type\".",
      "D": "Aggregate CloudWatch logs across variants and run a post-processing job to compare distributions."
    },
    "explanation": "Separate DataQuality jobs allow per-variant baselines and alerts. A single job with filters isn't supported; Clarify is for bias interpretation, not raw skew; log aggregation is more custom work."
  },
  {
    "id": "5241bbe3f9cbd89ff205c7f8d8ecfbbfc0d4713360653859945b0fde900feb3e",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "During inference, a regression model returns NaN for target predictions intermittently. Which Model Monitor configuration will automatically detect this anomaly and notify the team?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable SageMaker Model Debugger to capture NaNs during inference.",
      "B": "Use Model Monitor DataQuality baseline constraints to flag NaN values in the prediction column.",
      "C": "Set up Amazon CloudWatch anomaly detection on inference response logs to catch NaNs.",
      "D": "Configure SageMaker Clarify to test for invalid prediction values."
    },
    "explanation": "DataQuality monitors can enforce no-NaN constraints on any column. Debugger focuses on training tensors; CloudWatch anomaly detection requires custom log parsing; Clarify is for bias/explainability."
  },
  {
    "id": "30438c7c852f361b50f88dd92ceddc7cecb4d6d612d3a88170746a49ae01637a",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A language translation model hosted on SageMaker Edge Manager runs on devices with intermittent connectivity. The team needs to detect and report when local input data statistics drift significantly. Which architecture is most appropriate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Model Monitor DataQuality jobs in the AWS cloud to poll edge logs hourly.",
      "B": "Install SageMaker Debugger on devices to log input metrics to CloudWatch.",
      "C": "Embed the SageMaker Edge Manager monitoring SDK on device to emit metrics to AWS IoT, then trigger a Model Monitor drift check in the cloud.",
      "D": "Deploy AWS Greengrass Lambda functions to run Clarify drift checks locally."
    },
    "explanation": "Edge Manager SDK plus IoT ingestion allows capture of local stats; cloud Model Monitor then runs drift. Debugger isn\u2019t for inference; Greengrass + Clarify unsupported; cloud polling logs is inefficient."
  },
  {
    "id": "93dc8fdbffdb6a71b7c905ed3b185faa9e0aa333aca8b8ef0588b510f6febc27",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An ML engineer must monitor prediction skew between two ensembles serving the same traffic for anomaly detection. They require automatic, per-minute alerts if the divergence of prediction distributions exceeds a threshold. How should they implement this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable Data Capture for both ensemble endpoints, schedule two Model Monitor drift checks on predictions with a 1-minute interval, and configure CloudWatch alarms on the job results.",
      "B": "Run SageMaker Clarify on both ensembles every minute to compute PSI on prediction distributions.",
      "C": "Write a custom Lambda that pulls logs from both endpoints, computes KS test, and pushes CloudWatch metrics.",
      "D": "Use CloudWatch Contributor Insights to track prediction distribution per minute."
    },
    "explanation": "Two Data Capture + Model Monitor drift checks with CloudWatch alarms automates per-minute divergence alerts. Clarify isn\u2019t for raw distribution drift; custom Lambda is higher overhead; Contributor Insights not suited."
  },
  {
    "id": "c6075cc72cf3bb1b49517f87a259986b54ab4128c3f3aae787dfec69ceb0fa8e",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "After deploying a multi-model SageMaker endpoint, a sudden spike in malformed JSON requests is observed. The engineer wants to monitor schema violations per model. Which solution provides the most granular alerts?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure a single Model Monitor DataQuality job on the endpoint to catch all schema violations.",
      "B": "Enable Data Capture and configure a separate DataQuality job per model container with JSON schema constraints.",
      "C": "Use AWS WAF JSON body inspection rules to log violations to CloudWatch.",
      "D": "Parse inference logs with CloudWatch Logs Insights and trigger SNS alerts."
    },
    "explanation": "Separate DataQuality jobs per model in the multi-model endpoint allow per-container schema checks. A single job cannot differentiate models; WAF and custom log parsing add external complexity."
  },
  {
    "id": "166b50312ec40909945e1e7d56f8f073faddfb1b62c5d9d65e3bb0286b591ff9",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A streaming inference pipeline uses an asynchronous endpoint. They want to detect if more than 1% of requests take longer than 10 seconds. Which AWS-native feature combination best achieves this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Model Monitor latency metrics with custom Python script to parse logs.",
      "B": "Configure AWS X-Ray on the endpoint and set up CloudWatch alarms for trace duration.",
      "C": "Enable Data Capture and schedule Model Quality Monitor to evaluate latency distribution.",
      "D": "Enable Data Capture on the endpoint, configure CloudWatch metric filters for invocation duration, and set alarms for >1% above 10s."
    },
    "explanation": "Data Capture plus CloudWatch metric filters allow latency distribution monitoring and alarms without custom training jobs. Model Quality Monitor isn\u2019t for latency; X-Ray traces require code hooks."
  },
  {
    "id": "3de07ce62f0f3a172a78db37cc337235a37094f98abe25f9472c25c53e3dde1d",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A healthcare inference service must monitor PII leakage: requests and responses must contain no PHI tokens. The model returns free-form text. Which monitoring approach will detect policy violations?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Model Monitor DataQuality to enforce a regex constraint rejecting PHI patterns in responses.",
      "B": "Configure AWS WAF with a custom rule to block PHI regex in JSON payload.",
      "C": "Enable Data Capture, run a scheduled SageMaker Clarify bias check with custom detectors for PHI tokens.",
      "D": "Write a Lambda for each inference to scan for PHI and generate CloudWatch events."
    },
    "explanation": "Clarify allows custom text analyzers via pre-built detectors to detect sensitive tokens. DataQuality regex can catch patterns but is limited; WAF doesn\u2019t inspect model responses; Lambda is higher overhead."
  },
  {
    "id": "3199a048da5944ecff4f7c75dc4ed53babef16769fd32a095337918c00bc6a06",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A model predicts customer churn and uses 50 numeric features. They need to detect multivariate drift (covariate shift) across those features. Which monitoring setup is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Model Monitor DataQuality with a multilook at a multivariate drift check (using dataset_format=\u201cJSON\u201d with drift_check_categories).",
      "B": "Configure SageMaker Clarify to compute pairwise SHAP drift metrics for all feature pairs.",
      "C": "Schedule a batch transform job daily and compare covariance matrices via AWS Glue.",
      "D": "Write custom Spark on EMR to compute Mahalanobis distance across features."
    },
    "explanation": "Model Monitor supports multivariate drift checks during DataQuality monitoring. Clarify focuses on feature importance; batch transform and custom Spark are inefficient and higher overhead."
  },
  {
    "id": "c0b96370403f369a420fea85f1b93544e63cf327fc5d8e37e275118a408a2569",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A retailer serves an image-classification model via SageMaker. They need to detect when the distribution of image size (in bytes) drifts and when inference latency spikes. Which out-of-the-box features should they enable?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Clarify DataBias monitors on image_size and SageMaker Model Debugger for latency.",
      "B": "Enable Data Capture and schedule Model Monitor DataQuality jobs to track image_size distribution and custom CloudWatch latency metrics.",
      "C": "Configure a Lambda pre-processor to log image_size and usage of SageMaker Profiler for latency.",
      "D": "Deploy a Step Functions workflow around the endpoint to log metrics and analyze separately."
    },
    "explanation": "Data Capture plus Model Monitor DataQuality tracks input size drift; CloudWatch latency metrics catch spikes. Clarify and Debugger don\u2019t cover both needs; lambda and Step Functions add complexity."
  },
  {
    "id": "b8400eae0d6f30647991e1073e65c6bb56baaa3201292122c5c4b7e316a4eac6",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An enterprise requires that no inference job runs outside their VPC and that monitoring data remains within their private subnet. How can they deploy Model Monitor under these constraints?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable network isolation on Model Monitor jobs.",
      "B": "Run Model Monitor on a public subnet with NAT gateway to pull data.",
      "C": "Use VPC endpoints for S3 and CloudWatch and run jobs on public instances.",
      "D": "Configure Model Monitor Processing Job with VPC configuration in private subnets and use VPC S3 endpoints."
    },
    "explanation": "Model Monitor jobs run as SageMaker Processing Jobs and support VPC config. Private subnets plus S3 VPC endpoints ensure data stays in VPC. Network isolation is for training/inference only."
  },
  {
    "id": "2f57df90c1c61833bd081cd990e6a56d7cc8d798b99eed1f34d3fb2750adaa72",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A financial model returns probability scores. The team needs to detect if the median score changes by more than 5% compared to baseline daily. Which feature and configuration accomplish this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Model Monitor DataQuality drift check on the prediction field with a median tolerance constraint of \u00b15%.",
      "B": "Schedule a Clarify ModelBiasMonitor job to compute median changes.",
      "C": "Extract daily predictions to Athena and run SQL analytics to compare medians.",
      "D": "Enable CloudWatch percentile-based alarms on the endpoint\u2019s inference metric."
    },
    "explanation": "DataQuality drift constraints support median checks on numeric fields. Clarify bias monitors aren\u2019t for central tendency; Athena SQL is custom; CloudWatch latency alarms only handle infrastructure metrics."
  },
  {
    "id": "a753702f001e5e77118c5ad26408b7014f01d06ddb5a3c40695b3ed99e502598",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "To comply with regulations, a bank must audit all inference inputs and outputs and ensure they cannot be modified post-hoc. Which combination of services and features meets this requirement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable Data Capture to write logs to an encrypted S3 bucket and use ACLs for write-once.",
      "B": "Use Data Capture to store inputs/outputs in an S3 bucket with S3 Object Lock in Compliance mode and encryption by KMS.",
      "C": "Configure CloudTrail to log inference API calls and point logs to S3 with versioning.",
      "D": "Use Athena for logging inputs/outputs and store results in a write-only DynamoDB table."
    },
    "explanation": "Data Capture plus S3 Object Lock (Compliance) and KMS encryption provides immutable audit trail. CloudTrail logs API calls but not payloads; ACLs alone don\u2019t enforce write-once."
  },
  {
    "id": "b305bf4585dfadc2427e71c3d8bd90e76716a116c077982879788bb308d41b86",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A data scientist wants to spot-time batch inference data quality issues such as missing values and invalid formats before consuming results. Which approach should they apply?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Clarify to compute bias metrics on batch responses.",
      "B": "Schedule a CloudWatch Logs Insights query on batch transform logs.",
      "C": "Run a SageMaker Processing job with Model Monitor DataQuality checks on batch output.",
      "D": "Validate outputs manually in Jupyter notebooks after batch jobs complete."
    },
    "explanation": "Model Monitor Processing jobs can validate batch outputs via DataQuality checks. Clarify is for bias; Logs Insights doesn\u2019t parse output payload; manual validation lacks automation."
  },
  {
    "id": "d6cfaed5302fa752103c4c730cf5bf1c6c93a3685e5efa5b4b3c76a260ad052f",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An autonomous driving model processes high-frequence sensor data. They need to detect when any input channel has degraded signal quality (e.g., constant zeros) in real time. Which solution best addresses this?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Use Model Debugger to capture input tensor anomalies.",
      "B": "Set up CloudWatch metric filters on inference logs to detect zero-value readings.",
      "C": "Implement an AWS IoT rule to check sensor message contents for zeros.",
      "D": "Enable Data Capture on the endpoint, schedule a Model Monitor DataQuality job with custom tolerances for each channel."
    },
    "explanation": "Data Capture and DataQuality allow custom numeric constraints per feature. Debugger isn\u2019t for inference; CloudWatch requires custom parsing; IoT rules don\u2019t integrate inference model context."
  },
  {
    "id": "26b5dd41cebd3bbec40709082e86e0b15435dc9f27deb37a63a28a66904a9077",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A model returns a confidence score as a float between 0 and 1. The team must ensure no inference returns a score outside this range. Which monitoring configuration should they use?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Write a custom CloudWatch metric filter on response logs to detect invalid scores.",
      "B": "Use Model Monitor DataQuality constraints to flag scores <0 or >1.",
      "C": "Apply SageMaker Clarify to validate output distributions within range.",
      "D": "Include assertion logic in the inference container to throw errors on invalid values."
    },
    "explanation": "DataQuality constraints can enforce numeric ranges on columns. Clarify doesn\u2019t enforce hard constraints; custom filter or container logic increases maintenance."
  },
  {
    "id": "954e9966a66e9afc8f5a6fe571ff9456389cd512fcdc4dbee7a55e2da8420168",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A marketing model predicts click-through probability and requires monitoring of high skew in the \"campaign_id\" feature usage. Which Service Lens should they use in Model Monitor to inspect this categorical feature?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use the Categorical Lens in SageMaker Model Monitor DataQuality to monitor \"campaign_id\".",
      "B": "Use the Label Lens in SageMaker Clarify to monitor campaign labeling.",
      "C": "Use the Numeric Lens in Model Monitor to inspect category counts.",
      "D": "Use the Bias Lens in Clarify to track category fairness."
    },
    "explanation": "The Categorical Lens in Model Monitor DataQuality jobs specializes in tracking categorical feature distribution. Numeric Lens monitors only numeric features; Clarify lenses focus on bias/explainability."
  },
  {
    "id": "35f31f719092e4eee4378e2fddd586fbb2a7e0afb051f9ed8d5f5b16d4e09739",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An engineering team notices that model inference failures often coincide with sudden changes in request payload size. They want to correlate error rate with payload size anomalies. Which approach is simplest?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Upload logs to CloudWatch Logs and manually correlate with payload sizes stored in S3.",
      "B": "Instrument the container to push custom CloudWatch metrics for payload size and errors.",
      "C": "Enable Data Capture with custom JSONPath for payload_size and error_code, then run Model Monitor DataQuality job with both fields.",
      "D": "Use AWS X-Ray annotations to trace payload size and exceptions."
    },
    "explanation": "Data Capture with custom JSONPath can extract both payload_size and error_code; DataQuality jobs can detect anomalies and correlations. Container instrumentation or X-Ray require more custom code."
  },
  {
    "id": "4b367b9b2c0daebe7a09e20256481d0b90f14cfec4f169dfca5274d114f1f7c7",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A translation service uses a multi-model endpoint. They need to monitor latency at the container level for each model separately. What\u2019s the most direct AWS-native way?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure Model Monitor on the endpoint to track per-model latency.",
      "B": "Enable per-model invocation logging with Data Capture and use CloudWatch metric filters to calculate latency.",
      "C": "Deploy a CloudWatch Agent on the endpoint hosts to capture container metrics.",
      "D": "Use SageMaker Debugger profiling on inference containers."
    },
    "explanation": "Data Capture logs invocation metadata per model; CloudWatch logs filters can compute latency per container. Model Monitor and Debugger don\u2019t provide per-container latency; CloudWatch Agent isn\u2019t supported on managed endpoints."
  },
  {
    "id": "82b3ff207c109dd2cc554e1b99955fc5edb44e7acfeaf8f24068caac5a7e89f0",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A company must detect malicious adversarial inputs that cause anomalous model outputs. They define an acceptable range for each output. Which monitoring solution enforces this at scale?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model Monitor DataQuality constraints on the output columns with defined min/max thresholds.",
      "B": "Deploy a WAF rule inspecting response bodies against known attack signatures.",
      "C": "Use Clarify Explainability to detect adversarial perturbations.",
      "D": "Stream inference logs to Lambda for custom anomaly detection."
    },
    "explanation": "DataQuality constraint checks on output enforce numeric ranges and can scale. WAF and Clarify aren\u2019t designed for content-based validation; Lambda adds overhead."
  },
  {
    "id": "365e3e2ff63666d2cb38b2cfeadfad412889053b834421c56223b6f8f467e87c",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An endpoint serves an NLP model that occasionally returns excessively long generated text (>1000 tokens). They want real-time alerts when this happens. Which setup is most efficient?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Model Monitor to check response length length via DataQuality.",
      "B": "Run Clarify to measure token counts and alert.",
      "C": "Enable CloudWatch RUM to capture response sizes.",
      "D": "Configure Data Capture to extract response_text, set a DataQuality constraint for max token count, and schedule frequent jobs."
    },
    "explanation": "Data Capture plus DataQuality constraint handles response length checks. Clarify and RUM unsuited; CloudWatch RUM is for front-end metrics."
  },
  {
    "id": "5f67689a69797305adee5c4690483a63b3c739f78759ec355887d8edc1c73b11",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A pharmaceutical company must catch silent inference failures where the model returns a default value of 0. They want automated detection. Which AWS feature configuration accomplishes this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure SageMaker Debugger to flag constant zero outputs.",
      "B": "Use Model Monitor DataQuality constraints to detect when output equals 0 more than a threshold.",
      "C": "Enable CloudWatch metric filters on logs for zero values and alert.",
      "D": "Instrument the model container to throw errors on zeros."
    },
    "explanation": "DataQuality constraints detect when a column value matches an anomaly pattern. Debugger is for training; logs filtering is custom; container instrumentation has higher overhead."
  },
  {
    "id": "569ef7f287726b5cd09435296cd06a8c144bc8192105f5707c85d6b6a2a3b773",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "To monitor inference drift for streaming predictions without ground truth, which monitor type and category should they choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ModelQualityMonitor with concept drift category.",
      "B": "Clarify ModelBiasMonitor on predictions.",
      "C": "Model Monitor DataQuality with drift_check_categories set to \"data\" on the prediction field.",
      "D": "Clarify ModelExplainabilityMonitor for prediction distribution."
    },
    "explanation": "DataQuality monitors support drift_check_categories on arbitrary fields. ModelQualityMonitor requires labels. Clarify focuses on bias/explainability."
  },
  {
    "id": "4a60b5687d899984d67b56253dc48238600863b12bc17317daad67445a90d2f4",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A company needs to ensure that feature engineering code doesn\u2019t introduce new features unseen during training. They want to monitor for any new column names in inference payloads. What\u2019s the best strategy?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Model Monitor DataQuality with input schema constraints listing allowed column names.",
      "B": "Implement a Lambda authorizer on API Gateway to validate JSON keys.",
      "C": "Enable SageMaker Clarify DatasetDriftMonitor on input keys.",
      "D": "Use AWS Config rules on the SageMaker endpoint inference setting."
    },
    "explanation": "DataQuality constraints can enforce allowed input schema. Lambda authorizer works but is outside SageMaker; Clarify and Config aren\u2019t for schema enforcement."
  },
  {
    "id": "1a833c7d8bc6aa4ce86ff5aa5e6db65b459d084645299d47c56529a25379a6f4",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An ML engineer observes that the distribution of a numerical feature is bimodal in training but has become unimodal in production. They must detect this change automatically. Which Model Monitor configuration handles multimodal distribution drift?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Clarify ModelBiasMonitor to detect distribution shape changes.",
      "B": "Schedule a batch transform and post-process histograms in Athena.",
      "C": "Use CloudWatch anomaly detection on the feature metric.",
      "D": "Configure Model Monitor DataQuality with DistributionLens on the feature and enable drift checks."
    },
    "explanation": "DistributionLens in DataQuality monitors supports multiple bins for drift detection. Clarify bias monitors don\u2019t assess raw distribution shape; Athena/CloudWatch require custom coding."
  },
  {
    "id": "320597bacf7702374a5915cfd2cd40a97ef0b963d4b884a9a98b28c6ea5f5d9c",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A subscription service uses asynchronous batch endpoints. They need to monitor end-to-end inference job failures and durations. How can they instrument this without modifying model code?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Use Model Monitor DataQuality to track batch job status and duration.",
      "B": "Configure Clarify to track job failures.",
      "C": "Configure CloudWatch Events on SageMaker BatchTransform job state change and subscribe to SNS.",
      "D": "Deploy GuardDuty to detect anomalies in batch failures."
    },
    "explanation": "CloudWatch Events for job state and duration require no code changes. Model Monitor doesn\u2019t monitor batch job lifecycle; Clarify doesn\u2019t cover jobs; GuardDuty is for security threats."
  },
  {
    "id": "ed9089351d7f3b0abe10295f9d649bb4cb72e63d25731ffea8e96d127aeb9375",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An experiment uses shadow deployments: incoming traffic is mirrored to a new model. They want to compare output distributions in near real time and alert if divergence >2%. Which design is simplest?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy two DataQuality jobs separately and post-process alerts via Lambda.",
      "B": "Enable Data Capture on both endpoints, schedule a single DriftsCheck Job that references both datasets as baseline and target.",
      "C": "Use Clarify to compute PSI between baseline and candidate in near real time.",
      "D": "Implement a Step Functions workflow to fetch logs and compare distributions."
    },
    "explanation": "A single DriftCheck job can accept two datasets as baseline and target. DataQuality jobs per endpoint or Step Functions add complexity; Clarify mismatched purpose."
  },
  {
    "id": "f93b4da12b183f4d2a137007771b394ce2f9f0786da462a72b46a1531def08f7",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A biotech startup requires monitoring of model feature importance drift in production. They want to detect if SHAP importance of any feature changes by >10%. Which AWS feature combination achieves this?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Use Model Monitor DataQuality drift_check on SHAP values.",
      "B": "Schedule a Model Quality Monitor job with SHAP-based metrics.",
      "C": "Configure Clarify ModelBiasMonitor with feature_importance thresholds.",
      "D": "Enable SageMaker Clarify ModelExplainabilityMonitor on the endpoint with max_absolute_shap_change constraint."
    },
    "explanation": "Clarify\u2019s ModelExplainabilityMonitor captures SHAP importance drift and supports constraints. DataQuality and ModelQualityMonitoring don\u2019t handle SHAP; ModelBiasMonitor focuses on bias not drift."
  },
  {
    "id": "4252515252ffb58de29f2363a03b5c0424abb43f1875cc7169100541b829c3cf",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A team must implement an alert if any inference job experiences input payload with more than 50 unique categorical feature values. Which approach scales best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Config custom rules to inspect payloads.",
      "B": "Deploy WAF rate-limit rules on JSON body parser.",
      "C": "Enable Data Capture and configure a Model Monitor DataQuality custom constraint on cardinality >50.",
      "D": "Run Athena queries on S3 logs and send alerts via Lambda."
    },
    "explanation": "DataQuality custom cardinality constraints enforce unique value limits. AWS Config and WAF don\u2019t inspect inference payloads; Athena+Lambda is higher latency."
  },
  {
    "id": "051354db87607e8c9ff996166b7aad65b045ba692c5dc5db9041c8f383f41069",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A model is deprecated but still receives stale requests. The team wants to detect any inference calls to the old endpoint and retire it promptly. Which CloudWatch configuration helps?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Set up a CloudWatch metric filter on SageMakerAPI logs for InvokeEndpointOldModel API and alert.",
      "B": "Use Model Monitor to detect zero-inference values.",
      "C": "Configure a Custom Resource in CloudFormation to log invocations.",
      "D": "Enable SageMaker Clarify to flag calls to old endpoint."
    },
    "explanation": "CloudWatch metric filters on API logs for specific endpoint names detect calls. Model Monitor and Clarify aren\u2019t for endpoint invocation detection; custom resource is unnecessary."
  },
  {
    "id": "416093dcfa226c3fb5ba514db14dd6e74cfad7195615014241404a54a0361780",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A gaming app uses an ML endpoint that must maintain 95th percentile latency under 200ms. They need to alert if the SLA is broken. Which AWS native setup is recommended?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run Model Monitor with a latency constraint on the endpoint logs.",
      "B": "Use CloudWatch percentile-based alarms on the AWS/SageMaker metric InferenceLatency.",
      "C": "Deploy SageMaker Debugger to capture latency tensors.",
      "D": "Implement a Lambda to sample requests and measure SLA."
    },
    "explanation": "CloudWatch percentile-based alarms directly monitor InferenceLatency p95. Model Monitor isn\u2019t designed for latency; Debugger and Lambda require extra work."
  },
  {
    "id": "33879d3fc0b9a977f908fa850cb356218a27b9b420dc2327ecf0420f864e25f5",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "An e-commerce company uses a SageMaker real-time endpoint for TensorFlow models with a strict p95 latency SLA. During peak traffic hours, end-to-end latency spikes intermittently even though CPU and memory utilization remain under thresholds. Which monitoring approach and remediation provides the deepest insight into latency sources and optimizes cost?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Enable AWS X-Ray tracing on the SageMaker endpoint, analyze service map latencies, identify cold-start segments, then provision minimal concurrency and configure auto scaling.",
      "B": "Increase the instance type to a larger CPU-optimized instance and create a CloudWatch Alarm on CPU utilization.",
      "C": "Enable SageMaker endpoint invocation metrics in CloudWatch and auto scale based on p95 latency.",
      "D": "Convert the endpoint to a multi-model endpoint to share a single container across models and reduce overhead."
    },
    "explanation": "X-Ray reveals where time is spent (initialization vs. inference). Provisioned concurrency mitigates cold starts. CPU scaling alone misses framework overhead."
  },
  {
    "id": "d3b40e564435e13877e1da6f674d40bb82146dd71fe4d45c1cfecb9df74754d8",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A financial services company runs a batch transform job on EC2 instances via SageMaker. Transform jobs run once per day, but 80% of capacity is idle for most of the job. The team wants to reduce cost without increasing job duration. Which solution meets this requirement?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch to CPU-optimized On-Demand instances sized for peak utilization and scale cluster size dynamically.",
      "B": "Use larger GPU instances to process in parallel and decrease overall run time.",
      "C": "Run the batch transform job on Lambda functions to parallelize tasks.",
      "D": "Configure the batch transform job to use Spot Instances with a fallback to On-Demand and set instance count for peak load."
    },
    "explanation": "Spot with fallback saves cost during idle periods and matches peak capacity, while On-Demand-only wastes money."
  },
  {
    "id": "d7f9541f2e50f2379a57b2ed0d5779f519038e481e445f1e8e4cb82776a19533",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "Your team deployed multiple SageMaker endpoints in three regions. They must tag endpoints for cost allocation and alert if untagged resources appear. Which configuration achieves this with the least operational overhead?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Write a Lambda polling script to list endpoints daily, check tags, and send notifications.",
      "B": "Use CloudWatch Logs subscription to trigger a Lambda on CreateEndpoint API events to validate tags.",
      "C": "Create an AWS Config rule (required-tags) scoped to SageMaker::Endpoint resources and set SNS notifications.",
      "D": "Implement CloudTrail log analysis in CloudWatch Logs Insights to detect untagged CreateEndpoint events and alert."
    },
    "explanation": "AWS Config rule automatically checks tags on creation with built-in functionality and SNS notifications, minimal custom code."
  },
  {
    "id": "871ed36ae8c1dedc6bc6528c5b73feb659daae45adebce16df6f3a98151ef3fc",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A gaming company uses SageMaker Processing to prepare game telemetry data. They notice sporadic high I/O wait times on EBS volumes attached to processing jobs. Which combination of monitoring metrics and actions will identify and mitigate the issue?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Monitor ProcessingJobIOBytes metric in Amazon CloudWatch and increase instance count.",
      "B": "Enable CloudWatch EBS Volume metrics (VolumeQueueLength, VolumeThroughputPercentage), use higher IOPS gp3 volumes and adjust volume throughput settings.",
      "C": "Monitor SageMaker Processing CPUUtilization and switch to GPU instance.",
      "D": "Enable CloudTrail logging on processing containers and analyze volume errors."
    },
    "explanation": "VolumeQueueLength and throughput % diagnose EBS bottleneck; gp3 allows tuning of IOPS separately and resolves I/O wait."
  },
  {
    "id": "175cc5cdde65925352dcaf641ae5722f8b74e3307432c6cc13adee646c1358ec",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A retail ML pipeline uses SageMaker endpoints and ECS-based preprocessing. The finance team needs a single dashboard showing endpoint cost, CPU utilization, ECS Fargate memory utilization, and Lambda durations. Which solution meets this requirement with minimal latency?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Build separate CloudWatch dashboards per service and share links.",
      "B": "Export all metrics to an external monitoring tool via Fluentd.",
      "C": "Configure CloudTrail Lake queries to join logs and visualize in QuickSight.",
      "D": "Use CloudWatch unified metric namespace with metric math and create a combined CloudWatch dashboard with all resources."
    },
    "explanation": "CloudWatch dashboards support service metrics and metric math in one place without external tools or manual stitching."
  },
  {
    "id": "28faca1a2e8fa6ef1ec519df37deb1cdd19c7a79c0b82103c2608587f29ece0e",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A biotech startup has a GPU-based SageMaker endpoint used intermittently. The CFO demands a cost-optimized solution that does not sacrifice startup latency. Which approach balances both?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use provisioned concurrency with a small number of GPU instances, enable endpoint auto scaling to scale down to zero using asynchronous endpoints.",
      "B": "Switch to smaller CPU instances and batch requests to save cost.",
      "C": "Use Spot Instances for the real-time endpoint to lower cost.",
      "D": "Migrate to multi-model CPU endpoint and load one model at a time."
    },
    "explanation": "Provisioned concurrency preserves low latency, auto scaling to zero reduces idle cost; spot not supported for real-time."
  },
  {
    "id": "1061053da74660ac242945f70d6bd01e0f27d558c93847d32d246d3ac63d702d",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A team observes their inference pipeline fails under burst traffic due to hitting service quotas. They want proactive alerts when approaching SageMaker endpoint invocation quota. How should they implement this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure a CloudWatch alarm on SageMaker API ThrottledRequests metric.",
      "B": "Create a CloudWatch alarm on the ThrottledRequests metric in the AWS/SageMaker namespace with threshold at 80% of quota.",
      "C": "Use EventBridge to detect service quota event and call SNS.",
      "D": "Poll DescribeEndpoint API hourly via Lambda and compare against known quotas."
    },
    "explanation": "ThrottledRequests metric directly signals when quota is approached; threshold at 80% gives early warning."
  },
  {
    "id": "fc8088bccd26cdca7712b400bfc0e3877270bd3a32712eec443b12baa36d72e5",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "Your model training jobs on GPU EC2 instances often underutilize GPU memory, leaving 40% unused for most jobs. You want better rightsizing. Which tool or service gives an automated recommendation?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Trusted Advisor\u2019s cost optimization checks.",
      "B": "CloudWatch anomaly detection on GPU utilization.",
      "C": "AWS Compute Optimizer for EC2 instances.",
      "D": "SageMaker Model Monitor for resource utilization analytics."
    },
    "explanation": "Compute Optimizer analyzes EC2/GPU instance metrics and recommends right-sized instance types and sizes."
  },
  {
    "id": "fee57dd39526a9bbbbac65918329351db4a2c926c7d2b77e8b76147ef1eb3ad0",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A media company streams video analysis via SageMaker batch transform jobs. They want to optimize data transfer costs between S3 and training instances. What combination of configurations reduces cost?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use cross-region replication to move data closer to training instances.",
      "B": "Enable S3 Transfer Acceleration to speed transfer and reduce egress cost.",
      "C": "Configure batch transform to download from a public S3 bucket.",
      "D": "Place instances and S3 in same AZ and use VPC endpoints for S3 traffic."
    },
    "explanation": "Same AZ VPC endpoints keeps traffic on AWS network, avoids cross-AZ data transfer charges."
  },
  {
    "id": "0e03a4f8f68695d97b32860785acfbebb6571ae85edbb718d330a7c572912863",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A regulated healthcare application must log all changes to SageMaker model endpoints for audit. They also want to trigger cost alerts when monthly spend exceeds budget. Which combined setup achieves both?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable AWS CloudTrail data events for SageMaker API and create an AWS Budget with SNS notifications.",
      "B": "Use CloudWatch Logs to collect API calls and set a Lambda to parse logs and send alerts.",
      "C": "Configure CloudWatch Events for CreateEndpoint and UpdateEndpoint events and notify SNS.",
      "D": "Integrate AWS Config for resource changes and rely on CloudWatch billing alarms."
    },
    "explanation": "CloudTrail data events capture every API call for audit; AWS Budgets integrates natively with SNS for cost alerts."
  },
  {
    "id": "4a085cfa828ae1877945656ff3eb7e406ace54ee7210e82c2c05b46d19036e8f",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "Your SageMaker endpoint sees memory out-of-memory (OOM) errors for large images. You need to diagnose the frame where OOM occurs across many hosts. Which logs and queries should you use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Download Docker container logs from CloudWatch Logs and grep for \"OOM\".",
      "B": "Use CloudWatch Logs Insights on /aws/sagemaker/Endpoints/* to query ERROR messages with \"OutOfMemoryError\" and filter by hostID.",
      "C": "Enable SageMaker Debugger hook for memory profiling.",
      "D": "Switch to X-Ray and trace memory allocations."
    },
    "explanation": "CloudWatch Logs Insights can aggregate and filter OOM errors across endpoint hosts quickly; Debugger is for training."
  },
  {
    "id": "01512ac4b2cabf4d64a4b5e64866aed7a6e52f2f56cb510106b0dfa960862754",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A data pipeline uses EMR for preprocessing and then writes to S3 for SageMaker. During heavy loads, EMR tasks throttle due to hitting S3 request quotas. How do you monitor and remediate this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable EMRFS consistent view and increase retries.",
      "B": "Use CloudTrail to monitor S3 503 errors and back off EMR jobs.",
      "C": "Enable CloudWatch anomaly detection on EMRTaskDuration metric.",
      "D": "Monitor S3 4XX/5XX request metrics in CloudWatch and add S3 request rate limiting or use S3 prefix sharding."
    },
    "explanation": "S3 request metrics reveal throttling; prefix sharding or client-side rate limiting reduces 503 errors."
  },
  {
    "id": "663391f42dbd147313fee59e900da11e7e95cbabc3173cabf07b9db859bf22f1",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "Your team deployed a multi-model endpoint behind an Application Load Balancer. They need to track per-model invocation counts for billing and auto scaling. Which approach is most effective?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable ALB access logs and parse them in Athena.",
      "B": "Add custom CloudWatch metrics inside the inference container to emit per-model Invoke counts.",
      "C": "Use CloudTrail logs for InvokeEndpoint calls with model name filters.",
      "D": "Tag each model and use SageMaker built-in metrics."
    },
    "explanation": "Containers can push custom metrics (model=inference) to CloudWatch for precise counts; ALB logs parsing is more complex."
  },
  {
    "id": "9dcc42b0d80d1c230d20aea0b2e3fb3ce930fd83942f7de91406d9086dabcb3f",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A startup runs GPU-based SageMaker training nightly. They want to reduce EC2 instance costs by 30% without increasing job time. Which purchasing option should they choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Reserved Instances for a one-year term.",
      "B": "EC2 Savings Plans covering all compute usage.",
      "C": "Spot Instances with checkpointing in S3 and automatic retry.",
      "D": "Dedicated Hosts to benefit from volume discounts."
    },
    "explanation": "Spot Instances offer largest discounts (up to 90%) and with checkpointing maintain job duration; RIs and SPs only 50%."
  },
  {
    "id": "7646c97f188fe7922a7be8bdb4bafa9b6b3d6b3b0a7f184938beaf50d6783f49",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A large dataset processing job on SageMaker Processing has unpredictable CPU loads. They want to automatically adjust compute within a run. Which strategy works?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Processing with auto scaling instance type list.",
      "B": "Orchestrate multiple processing jobs in Step Functions with dynamic instance count based on CloudWatch metrics.",
      "C": "Embed CPU usage watchers in the processing script and call the AWS API to change instance count.",
      "D": "Switch to EMR on EKS with dynamic allocation."
    },
    "explanation": "Step Functions can branch logic mid-workflow and launch new Processing jobs sized by metrics; Processing itself doesn\u2019t auto scale."
  },
  {
    "id": "b857dde6c006671e70b5eb0d3e3af06af81242adffe36030187dd70475230d04",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "Your SageMaker endpoint is deployed in a VPC. Data scientists observe increased inference latency. Which VPC configuration issue should you monitor and how?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Monitor NAT Gateway CloudWatch NetworkPacketsOut to detect throttling and add NAT Gateways for scale.",
      "B": "Monitor VPC Flow Logs for high packet loss and increase MTU.",
      "C": "Monitor Security Group rejected packet metrics and open more ports.",
      "D": "Monitor Elastic IP usage and allocate more addresses."
    },
    "explanation": "VPC endpoints without dedicated NAT may bottleneck at NAT Gateway; NetworkPacketsOut metric shows if throughput is hitting limits."
  },
  {
    "id": "6a11dddeb446a9fc7e8e8bbc89dc7786c721b54c1a89c2302da5ed090899bd19",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A global SaaS product needs to track inference costs per customer across multiple SageMaker endpoints. What tagging and billing configuration should you apply?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add customer ID as an environment variable in the container and parse logs.",
      "B": "Use separate AWS accounts per customer and consolidate billing.",
      "C": "Instrument the SDK client to send customer metadata in X-ray trace.",
      "D": "Tag each endpoint with a customer cost center tag, activate cost allocation tags in Billing console, and use Cost Explorer grouped by tag."
    },
    "explanation": "Cost allocation tags are the AWS best practice for per-resource cost tracking in Cost Explorer."
  },
  {
    "id": "1020fec8c524dbed70939757ccb5d9504c1d1ae67187e2732b7197adc25a691b",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A team uses Lambda functions for preprocessing before SageMaker. They receive occasional OutOfMemory errors in Lambda, causing increased retries and cost. How should they monitor and prevent this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase Lambda memory and rely on CloudWatch alarms on Duration.",
      "B": "Enable CloudWatch Logs Insights on Lambda error logs to filter OOM, set a CloudWatch alarm on Errors metric, and adjust memory allocation.",
      "C": "Switch to a Step Functions workflow to control memory.",
      "D": "Use X-Ray to trace memory usage of the function."
    },
    "explanation": "Errors metric combined with Logs Insights identifies OOM events; alarms notify and right-size memory to eliminate retries."
  },
  {
    "id": "6188ebbb90e1604d13167c0cbc8c9dbdadb8a05afd68d53d6c27a792c1c7e4c9",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "Your preprocessing uses an ECS service on Fargate that sporadically fails CPU credit balance. Which CloudWatch metrics and configuration should you monitor and adjust?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Monitor Fargate MemoryReservation metric and increase CPU share.",
      "B": "Enable Container Insights and watch host CPUUtilization, then switch to EC2 launch type.",
      "C": "Monitor Fargate CPUCreditBalance and CPUUtilization in CloudWatch; adjust the CPU configuration (e.g., move from 0.25 vCPU to 0.5 vCPU) to maintain credits.",
      "D": "Use CloudTrail to log throttled tasks and set a budget alert."
    },
    "explanation": "Fargate tasks use CPU credits; monitoring CPUCreditBalance prevents throttling, and adjusting vCPU allocation maintains balance."
  },
  {
    "id": "115b15743cddb4bcd8323990555291a8e31cdeae02ec4136d2cbff6bc61fb2d8",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "An ML endpoint in production has highly variable traffic. The team configured auto scaling on CPU utilization. They see scale-in immediately after scale-out, causing thrashing and increased cost. How should they fix it?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add scale-in and scale-out cooldown periods and use target tracking on average invocation per instance.",
      "B": "Switch to step scaling based on p90 latency.",
      "C": "Increase the CPU utilization threshold for scale-in.",
      "D": "Use a multi-model endpoint instead to reduce container startup time."
    },
    "explanation": "Cooldown prevents rapid scale-in/out oscillations; target tracking on per-instance invocations stabilizes scaling."
  },
  {
    "id": "661b3c045109d2c8da351aec59b9345b6029d9e395f4688ebfd678ac5af1e752",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "Your SageMaker Model Monitor baseline job generates drift violation alerts, but you also want to correlate them with infrastructure metrics to determine if CPU saturation causes data delays. How do you integrate these?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Ingest Model Monitor CloudWatch logs into ElasticSearch and join with CPU metrics.",
      "B": "Use CloudWatch metric math to overlay ModelMonitorDataQualityViolationCount and SageMakerEndpoint CPUUtilization in a unified dashboard.",
      "C": "Export Model Monitor S3 reports to Athena and join with CPU metrics exported by CloudWatch.",
      "D": "Implement a Lambda to capture both metrics on violation events and store in DynamoDB."
    },
    "explanation": "Metric math in CloudWatch provides immediate combining of application and infra metrics in one graph without custom code."
  },
  {
    "id": "93082f04b5f721bc48fdb8f3d7dc3a953a406f74d66fd6541ac9b43d325953e1",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A company is charged cross-account for S3 data egress when SageMaker in Account A reads from Account B buckets. They want to avoid egress charges. Which networking option solves this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable inter-account VPC peering and route traffic through private IP.",
      "B": "Use S3 Transfer Acceleration.",
      "C": "Create VPC Interface Endpoints (AWS PrivateLink) for S3 in Account A\u2019s VPC.",
      "D": "Copy data periodically to Account A\u2019s S3 bucket."
    },
    "explanation": "VPC Interface endpoints for S3 keep traffic within AWS network and avoid cross-account egress fees; bucket copy still incurs egress."
  },
  {
    "id": "4a40bdb89c433482b1c6f6d890db7669c59d3faa6823c02bcb6b0b3e10ab587d",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "Your nightly SageMaker hyperparameter tuning jobs sporadically hit EBS throughput limits, causing tunings to fail. Which combination of monitoring and remediation addresses this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable CloudTrail on CreateTrainingJob and throttle job submission.",
      "B": "Monitor EC2 Instance CPUUtilization and switch to instance with higher compute.",
      "C": "Use CloudWatch anomaly detection on TrainingJobRuntime metric.",
      "D": "Monitor EBS VolumeThroughputPercentage metrics and migrate to gp3 volumes with provisioned throughput."
    },
    "explanation": "VolumeThroughputPercentage shows EBS bottleneck; gp3 lets you tune throughput independently of volume size."
  },
  {
    "id": "870f6bfcb4b0b4a9856136cc20371dc414aceb3ada40df33543d809035dbd6db",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A data science team regularly creates and deletes dozens of notebooks in SageMaker Studio. The finance team wants to detect orphaned EBS volumes to stop incurring storage costs. What is the simplest solution?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Enable AWS Config managed rule for unattached EBS volumes and set automatic remediation to delete.",
      "B": "Write a Lambda to query DescribeVolumes for unattached volumes and delete.",
      "C": "Set CloudWatch Events on DeleteNotebookInstance to trigger volume clean up.",
      "D": "Use AWS Trusted Advisor\u2019s underutilized EBS check."
    },
    "explanation": "AWS Config rule for unattached volumes and auto-remediation offers built-in, no-code solution; Trusted Advisor only alerts."
  },
  {
    "id": "8ec428edbc9863d2dc1f1533cfab73367328f9667db1a443d03d7b4624cdf347",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A model training team wants to track costs per experiment across multiple SageMaker jobs, notebooks, and endpoints. Which tagging strategy and AWS service combination provides accurate reporting?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Tag resources at creation with ExperimentID and use CloudTrail logs to calculate costs.",
      "B": "Use SageMaker Experiments API to tag runs and rely on EMR cost reports.",
      "C": "Enable cost allocation tags for ExperimentID, tag all SageMaker resources, and view groupings in AWS Cost Explorer.",
      "D": "Import cost data into Quicksight and join on ExperimentID from logs."
    },
    "explanation": "Cost allocation tags in Cost Explorer directly associate resource costs with tags; Experiments API doesn\u2019t feed billing data automatically."
  },
  {
    "id": "81690ac111000e0ca2262c6ce9e4f2709f96b3c6e5d377c86f27fb44ffcb9a8d",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "Your preprocessing job uses AWS Glue and feeds data into SageMaker. You need to monitor Glue job CPU, Glue Data Catalog calls, and track Glue errors alongside SageMaker job failures in a single pane. Which tool do you choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CloudWatch dashboards per service linked manually.",
      "B": "CloudWatch Unified Dashboards with metrics from AWS/Glue and AWS/SageMaker plus logs insights widgets.",
      "C": "Athena querying Glue and SageMaker logs in S3.",
      "D": "QuickSight pulling billing data."
    },
    "explanation": "CloudWatch dashboards can incorporate multiple metrics and logs insights widgets for near real-time observability without custom ETL."
  },
  {
    "id": "cf739abf27d3d933ad577038936a2e8ca269f4bd0bc9495888e27b2abb94fe83",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A SageMaker endpoint in a private subnet loses internet connectivity after a VPC route table change and fails to download model artifacts. How do you monitor and alert on such failures?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable CloudWatch Logs for the endpoint container and search for network exceptions.",
      "B": "Monitor CloudTrail events for GetObject failures on S3.",
      "C": "Set a CloudWatch alarm on EndpointInvocationErrors metric.",
      "D": "Create a CloudWatch anomaly detection alarm on SageMakerEndpointInvocation5XXErrors to catch connectivity failures."
    },
    "explanation": "5XX errors include network and container issues. Anomaly detection spots sudden error spikes without log parsing."
  },
  {
    "id": "3241ae233d9bddac98fc164e1f01332c679da20aa46454f84042f0a277e234b3",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "Your inference pipeline uses Lambda behind API Gateway calling SageMaker endpoints. You notice high tail latencies occasionally. Which combined metrics should you monitor and how to differentiate between GP and HTTP latencies?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Monitor API Gateway Latency and IntegrationLatency in CloudWatch to separate client-facing vs. backend invocation delays.",
      "B": "Monitor SageMaker Endpoint ModelLatency and InvocationsPerInstance metrics only.",
      "C": "Enable X-Ray for Lambda and use cold start latency metrics.",
      "D": "Use CloudWatch Synthetics to measure end-to-end latency."
    },
    "explanation": "API Gateway Latency measures total, IntegrationLatency isolates backend; ModelLatency doesn\u2019t include network overhead."
  },
  {
    "id": "a2a24a38568b852de299c1ca59e8d015cdb02482fea3b0251691b57a3cc80d66",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "Your team\u2019s SageMaker batch transform jobs intermittently fail due to insufficient vCPU quotas in a new AWS Region. What is the recommended monitoring step and long-term mitigation?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use CloudWatch to monitor ThrottledRequests and request a service quota increase manually when alarms fire.",
      "B": "Implement retries in the job script.",
      "C": "Create a CloudWatch alarm on ServiceQuotaExceeded metric via Service Quotas namespace and automate quota requests via AWS Service Quotas APIs.",
      "D": "Switch to GPU instances which have separate quotas."
    },
    "explanation": "ServiceQuotaExceeded metric alerts on hitting quotas. Automating via Service Quotas APIs ensures timely increases."
  },
  {
    "id": "f6a803fe72e8d3185b991e1a098e00595ad828c1a871fc0167a6efe98ecd5416",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A global inference workload runs on SageMaker endpoints in multiple regions. They need to compare cost per inference across regions. What\u2019s the best way to collect and visualize this data?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Aggregate SageMaker billing metrics into CloudWatch using PutMetricData and graph them.",
      "B": "Enable cost allocation tags for Region on endpoints and use Cost Explorer filtering by tag.",
      "C": "Export Cost & Usage report to S3 and query with Athena manually.",
      "D": "Use QuickSight to connect to Billing API."
    },
    "explanation": "Cost Explorer with allocation tags gives near real-time region-based cost per resource without manual report queries."
  },
  {
    "id": "b71ad1daf6f75f57ccca3cc7dbde6be7e431461c19e521f5f5786990e98507cc",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "An ML endpoint behind a Network Load Balancer exhibits higher packet drop during bursts. Which VPC metric should you monitor, and what corrective action should you take?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Monitor NetworkBytesIn and move to larger instance.",
      "B": "Monitor HostNetworkRxErrors and increase MTU.",
      "C": "Monitor Flow Logs for REJECT entries and open more ports.",
      "D": "Monitor Elastic Network Interface (ENI) ReceivePacketDrops and use enhanced networking (ENA) instances or increase ENI count."
    },
    "explanation": "ReceivePacketDrops signals NIC saturation; ENA or more ENIs improves network performance for burst traffic."
  },
  {
    "id": "0661b8befa7b30f8faae55be327cf78b43f91e7e1255a636c816d1cb648bf60a",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A team using SageMaker Debugger for training also wants to monitor GPU utilization and EC2 metrics in one place. Which monitoring solution should they implement?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use CloudWatch Container Insights for the training job\u2019s underlying EC2 instances alongside Debugger tensor metrics.",
      "B": "Send EC2 metrics to CloudTrail for correlation.",
      "C": "Parse Debugger logs with CloudWatch Logs Insights and join with EC2 metrics in Athena.",
      "D": "Load all metrics into QuickSight and build a dashboard."
    },
    "explanation": "Container Insights collects EC2 host metrics and container-level metrics in CloudWatch seamlessly, enabling side-by-side visualization with Debugger metrics."
  },
  {
    "id": "74321c5f5df1996ad18fd647d2588ca4d94345ce5668322797002821b239d68f",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A team must restrict a SageMaker notebook instance so that it can read and write data only in a specific S3 bucket prefix. The notebook uses its execution role to access S3. Which configuration enforces least privilege?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Attach an IAM policy to the notebook execution role allowing s3:GetObject and s3:PutObject only on arn:aws:s3:::marketing-data/train/*",
      "B": "Add an S3 bucket policy granting the notebook role full access to the bucket and deny other principals",
      "C": "Create a VPC gateway endpoint for S3 with a policy restricting access to that prefix",
      "D": "Modify the KMS key policy to allow decryption only for that bucket prefix"
    },
    "explanation": "Least privilege is enforced by restricting the notebook\u2019s execution role with an inline IAM policy on the specific prefix. S3 endpoint policies or bucket policies can work but are more complex and less direct; KMS policy controls only encryption."
  },
  {
    "id": "af804a32819da4ec17f9753d71002dc93f4ce4204098163205d79d347a138a6a",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A security requirement mandates that SageMaker training jobs must not have internet access, yet must pull training images from Amazon ECR. How should you configure the training job?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Launch the training job in public subnets with a restrictive security group",
      "B": "Provide a customer-managed NAT gateway in the VPC used by the job",
      "C": "Specify VpcConfig with private subnets that have an ECR VPC endpoint and no NAT",
      "D": "Use a service-linked role that disables internet access by default"
    },
    "explanation": "By placing the training job in private subnets with an ECR VPC endpoint and no NAT gateway, you prevent internet access but allow ECR pulls. Public subnets or NAT would allow internet egress."
  },
  {
    "id": "2adea8a948187647d719a07afe8b1d2dcd87d1c0d9f0fc9bab6f0eda67ca7818",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A SageMaker endpoint must be private within a VPC and only allow traffic from instances in another VPC over AWS PrivateLink. Which steps meet these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Launch endpoint without VPCConfig and restrict with security groups referencing peer VPC",
      "B": "Create interface VPC endpoint for SageMaker runtime in the consumer VPC, deploy the endpoint in a private subnet, and apply SG allowing traffic only from peer VPC CIDR",
      "C": "Use NAT gateway and public endpoint but restrict source IPs in SG",
      "D": "Enable endpoint public access and add route53 private hosted zone entries"
    },
    "explanation": "Deploying the endpoint in a VPC with no public access plus creating an interface endpoint in the consumer VPC over PrivateLink and restricting with SG enforces private-only access."
  },
  {
    "id": "a89ae09a5f126d3e638bbab468593e1ef69b42b3a7d84798673dc455a08010bd",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A DevOps engineer needs to grant AWS CodePipeline permission to deploy SageMaker models and endpoints but must follow least privilege. Which IAM policy attachment is correct?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Attach AmazonSageMakerFullAccess to the CodePipeline role",
      "B": "Allow sagemaker:* on resource '*' for the pipeline role",
      "C": "Create an SCP denying all actions except sagemaker:CreateEndpoint",
      "D": "Attach a scoped IAM policy allowing only sagemaker:CreateModel, sagemaker:CreateEndpointConfig, sagemaker:CreateEndpoint on identified resource ARNs"
    },
    "explanation": "Granting only the specific deploy actions on the exact model and endpoint ARNs enforces least privilege. FullAccess or wildcard would be overly permissive; SCP is for accounts."
  },
  {
    "id": "fe0ef927371464c1ef209ef6c7f4f7bd5dd9b2959d1d343f0f60ac067d09ede9",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "An organization requires CloudTrail logs of all SageMaker API calls to be encrypted using a KMS key managed by SecurityOperations. Which configuration satisfies this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable CloudTrail for SageMaker and leave default encryption",
      "B": "Configure CloudTrail with the SecurityOperations KMS key ARN for SSE-KMS encryption of logs",
      "C": "Apply an S3 bucket policy to encrypt objects using SSE-S3",
      "D": "Modify Trail event selectors to include SSE-KMS"
    },
    "explanation": "Specifying the KMS key ARN in the CloudTrail configuration ensures that logs are encrypted with that key. SSE-S3 or bucket policy alone cannot enforce use of a customer-managed key."
  },
  {
    "id": "d9f1ad4ed77d6e76593ca42127c7077153b46b253c0a8e79815007f0b2ccc4b4",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A notebook lifecycle configuration retrieves secrets from AWS Secrets Manager. You must ensure the lifecycle lambda can decrypt the secret but no other SageMaker service can. How do you tighten permissions?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Attach AWSSecretsManagerReadWrite to the notebook role",
      "B": "Add a resource policy on the secret allowing the notebook role",
      "C": "Add a KMS key policy granting decrypt only to the notebook lifecycle role principal",
      "D": "Enable automatic rotation on the secret"
    },
    "explanation": "Tightening access at the KMS key policy level restricts decryption to only the specified lifecycle role. Resource policies on the secret do not control the KMS decryption step."
  },
  {
    "id": "1a1f6effcc65b18ea1a47cde0f121e85a3cd1d24aee4b83f35a6f00473b8ec43",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A SOC team requires that any SageMaker endpoint creation must be approved by a central admin. How can you enforce this in CodePipeline?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a Manual Approval action before the CreateEndpoint stage",
      "B": "Use a CloudWatch rule to detect CreateEndpoint and block it",
      "C": "Implement an SCP to deny sagemaker:CreateEndpoint",
      "D": "Require MFA in the pipeline role"
    },
    "explanation": "A Manual Approval action in CodePipeline forces human approval. SCP would prevent creation globally; CloudWatch can only alert after the fact."
  },
  {
    "id": "d873953064fbcf35804dedfd460b2021539062076982c462b86e7d90eef08374",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A model registry in SageMaker stores sensitive models. You need to prevent direct API deletes of models, but allow deletion through a daily cleanup Lambda. Which IAM policy accomplishes this?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Deny sagemaker:DeleteModel to everyone",
      "B": "Remove sagemaker:DeleteModel from all roles except Lambda by attaching a policy to the Lambda role only",
      "C": "Create an SCP denying DeleteModel for all principals",
      "D": "Attach a deny statement in a permission boundary that applies to principals unless aws:PrincipalArn equals the Lambda function execution role"
    },
    "explanation": "A permission boundary with a conditional deny unless the principal is the cleanup Lambda enforces that only that role may delete models, without affecting Service roles."
  },
  {
    "id": "890eeb1341b83674dd9afcae27c6f2659fda3269e5f91300301ceab96d4e0529",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A compliance rule requires that only approved subnets are used in SageMaker jobs. How can you enforce this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use an SCP to deny SageMaker calls in unapproved subnets",
      "B": "Write an IAM permissions policy with Deny on sagemaker:CreateTrainingJob when sagemaker:VpcConfig.Subnets \u2260 approved IDs",
      "C": "Enforce via CloudWatch alarms and manual remediation",
      "D": "Use AWS Config managed rules without enforcement"
    },
    "explanation": "Embedding a Deny in the IAM policy conditioned on the VpcConfig.Subnets attribute blocks jobs launched in unapproved subnets. SCP cannot inspect parameters."
  },
  {
    "id": "8dd8d10f632462bc87aaa26660840468913aaeab0e7852f1ab3f87585d9e6c6b",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A training job needs to write to a DynamoDB table, but only to specific items. How do you restrict the SageMaker execution role accordingly?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Grant dynamodb:* on the table ARN",
      "B": "Attach AmazonDynamoDBFullAccess",
      "C": "Grant dynamodb:PutItem only on arn:aws:dynamodb:\u2026:table/MyTable with a Condition on dynamodb:LeadingKeys for the key prefix",
      "D": "Use an SCP to restrict all DynamoDB writes"
    },
    "explanation": "Using a resource-level IAM policy with a condition on LeadingKeys ensures writes only to items matching the key prefix. Full table or wildcard actions are too broad."
  },
  {
    "id": "5eff7df1f3b9cac652aecc94b3d72dfc6ad51cc642d04bb69b918a97848afdb1",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A DevSecOps requirement: all SageMaker model artifacts in S3 must be encrypted with a specific KMS key. Which is the simplest enforcement?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add SSE-KMS in the CreateModel call parameters",
      "B": "Use a bucket policy requiring x-amz-server-side-encryption header",
      "C": "Implement a CloudWatch rule that checks artifact uploads",
      "D": "Add an S3 bucket policy Deny PutObject unless x-amz-server-side-encryption-aws-kms-key-id matches the key"
    },
    "explanation": "An S3 bucket policy that denies PutObject requests unless the correct KMS key ID header is present enforces encryption at write time."
  },
  {
    "id": "4093edb07ffe80ae0fb1e213d2e782cd901018a855afe51a0821286b08a18d73",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A SageMaker pipeline stores intermediate data in S3. The security team wants to prevent access to these artifacts except by the pipeline service. How do you enforce this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Grant pipeline roles wide access to the bucket",
      "B": "Use an S3 bucket policy allowing only the SageMaker Pipelines service principal and the pipeline execution role ARN",
      "C": "Encrypt with a KMS key and rely on default key policy",
      "D": "Use a resource-based IAM policy on the pipeline role"
    },
    "explanation": "An S3 bucket policy that permits only the SageMaker Pipelines service principal and the specific execution role to access the bucket restricts all other access."
  },
  {
    "id": "a32270cbb2553b1e0a7298da0c632be6f4f6f4de41315b0112e964dafe3a3bdd",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A model deployed in SageMaker must only be invoked by authenticated users in an Amazon Cognito identity pool. How can you secure the endpoint?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use an endpoint policy with Allow for principal '*' and check Token in code",
      "B": "Attach a resource policy to Cognito to allow SageMakerInvoke",
      "C": "Create an endpoint policy requiring aws:PrincipalOrgID and aws:RequestContext.authorizer.claims.sub from Cognito identity",
      "D": "Use a VPC endpoint and IP-based SG only"
    },
    "explanation": "An endpoint policy can restrict invocation to principals from the specific Cognito identity pool by checking the principal\u2019s token claims in aws:RequestContext."
  },
  {
    "id": "e81da2e1316799c28123e0dc319399a9335c20fa035c16b294a2afeae18fe835",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A notebook lifecycle script clones code from CodeCommit. The IAM role has ListRepos and GitPull permissions but the clone fails. On inspecting the trust policy, you find no conditions. What\u2019s the missing configuration?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add an SCP allowing codecommit:GitPull",
      "B": "Add a Condition in the IAM trust policy restricting the service principal to sagemaker.amazonaws.com",
      "C": "Attach AWSCodeCommitPowerUser to the role",
      "D": "Enable cross-account access in CodeCommit"
    },
    "explanation": "Notebook lifecycle functions are invoked by the SageMaker service, so the role\u2019s trust policy must allow sagemaker.amazonaws.com as a principal to assume the role. Without it, the role cannot be assumed and clone fails."
  },
  {
    "id": "3ed2e2e3da8c276ee275401938991c744be4ae0326764b2c98c6755e9ecd1467",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A security audit reveals CloudWatch Logs for SageMaker were publicly accessible. How do you block public access while preserving individual user access?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Delete the log group ACLs",
      "B": "Enable KMS encryption on the log group",
      "C": "Apply an SCP to prevent DescribeLogGroups",
      "D": "Attach a resource policy to the log group denying Principal '*' for log:Describe and log:Get"
    },
    "explanation": "A CloudWatch Logs resource policy can explicitly deny calls from anonymous principals (*) while allowing authenticated IAM principals to continue accessing the logs."
  },
  {
    "id": "3d61b4a5f6798f5b643d456e5b16f3494b8db74cf59a185fffa41ed4f446517b",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A data scientist left a SageMaker endpoint open to the internet. You need to remediate: lock down to a VPC in us-east-1 and allow only IPs from 10.0.0.0/16. What is the fastest remediation?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Delete the endpoint and recreate within a VPC",
      "B": "Update the endpoint configuration with VpcConfig and apply a security group that allows only 10.0.0.0/16",
      "C": "Attach an endpoint policy restricting CIDR",
      "D": "Apply a bucket policy on model artifacts"
    },
    "explanation": "Updating the existing endpoint\u2019s VpcConfig and security group is fastest. Deletion and recreation is slower; endpoint policies do not control network access."
  },
  {
    "id": "82aee66fdfb3af220b5c38214942dadeb959d3e0a258f0e6118fd4377d79ab31",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "Your CodeBuild project for ML uses Docker containers on shared hardware. You must ensure build logs are encrypted with a custom KMS key. Which change meets this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable CloudWatch Logs for the project",
      "B": "Attach AWSKeyManagementServicePowerUser to CodeBuild role",
      "C": "Specify project\u2019s logsConfig.cloudWatchLogs.encryptionDisabled=false and kmsKey in CloudFormation",
      "D": "Use SSE-S3 for the CodeBuild logs bucket"
    },
    "explanation": "In the CodeBuild logsConfig, setting encryptionDisabled to false and specifying the KMS key ensures logs are encrypted with the customer-managed key."
  },
  {
    "id": "649dec4e103e457b223eee000e065f85e53de5d9f2475c108b8081d3e9ab1123",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A compliance mandate: all secrets accessed by SageMaker (e.g., DB credentials) must require KMS grant usage via key grants rather than policy. How to implement?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Use KMS grants to give the SageMaker execution role temporary decrypt privileges for the secret\u2019s CMK",
      "B": "Modify the key policy to allow the SageMaker role Decrypt",
      "C": "Add the SageMaker role to a KMS key alias",
      "D": "Enable automatic rotation on the secret"
    },
    "explanation": "KMS grants provide temporary, auditable decrypt permissions. Modifying key policies is static and not per-request as required."
  },
  {
    "id": "c5f8601b29739b7fa39810a18efecbe35c05f5dd452480f4a766ddaa47c9f5b9",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A model registry must only allow promotion of approved models. You need to restrict who can call CreateModelPackageVersion. What\u2019s the best practice?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use an SCP denying sagemaker:CreateModelPackageVersion",
      "B": "Enable AWS Config rule for non-approved models",
      "C": "Create a SageMaker model registry tag rule",
      "D": "Attach an IAM policy requiring a specific request tag (e.g., ApprovedBy) for CreateModelPackageVersion"
    },
    "explanation": "Requiring a request tag via IAM policy ensures that only requests including the ApprovedBy tag can create new versions, enforcing the approval workflow."
  },
  {
    "id": "172efc1ef4d6d7ba191e59cf66a0b6017894f2b084693e4ebb636710aad2f115",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A Dev team\u2019s pipeline stores its CloudFormation templates in S3. You must ensure only specific pipeline roles can read and write these templates. Which is correct?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Grant public read and limit write via signed URLs",
      "B": "Apply S3 bucket policy allowing List, Get, Put only to the IAM roles of the pipeline stages",
      "C": "Use KMS encryption only",
      "D": "Restrict via VPC endpoint policy"
    },
    "explanation": "An S3 bucket policy scoped to only the pipeline\u2019s IAM role ARNs for List, Get, and Put enforces that no other principals can read or write."
  },
  {
    "id": "e652e83aa1e57de783e952121884f2ec6c9d2d8da7041e9c39d5adfb1461a144",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A post-deployment check must validate that SageMaker Notebook Instances are encrypted at rest. Which automated check fulfills this?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "CloudWatch alarm on unencrypted volumes",
      "B": "Enable AWS Config managed rule EC2_ENCRYPTED_VOLUMES",
      "C": "Enable AWS Config managed rule SAGEMAKER_NOTEBOOK_INSTANCE_ENCRYPTED_VOLUME_CHECK",
      "D": "Use GuardDuty on notebooks"
    },
    "explanation": "AWS Config\u2019s SageMaker-specific managed rule checks encryption of notebook volumes directly. EC2 rules do not cover SageMaker notebook volumes."
  },
  {
    "id": "f3d76aa4f9e1b9958bc9c01de0eb4428c2a378f0b0faa48d6e8c09c6b3f375b7",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A SageMaker Processing job uses a role that also has S3ListAllMyBuckets permission. Security wants to remove ListAll permission but still allow list on two buckets. How to implement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Replace wildcard list permission with explicit s3:ListBucket on the two bucket ARNs",
      "B": "Add an SCP denying s3:ListAllMyBuckets",
      "C": "Use bucket policies on S3 buckets",
      "D": "Rotate IAM role credentials daily"
    },
    "explanation": "Restricting the role\u2019s IAM policy to only s3:ListBucket on the two specified buckets removes global list permission while allowing needed buckets."
  },
  {
    "id": "817332e393eedf02793409f2bdda81954514c4856e75f590008dccc5a4b55357",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A pipeline deploys a SageMaker model using AWS CDK. The synth step fails due to lack of KMS decrypt for the secret used in a CDK context. How can you fix this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Grant the CDK role kms:GenerateDataKey*",
      "B": "Attach SecretsManagerReadWrite to CDK role",
      "C": "Use plaintext secret in code",
      "D": "Grant the CDK execution role kms:Decrypt on the secret\u2019s CMK in the key policy"
    },
    "explanation": "The CDK synth needs kms:Decrypt rights on the CMK to read the secret. Adding that to the key policy resolves the failure."
  },
  {
    "id": "553ba9df505d79a8abc2dabc80dace5653f6e22fc671a053658a060869f365b6",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A regulatory requirement: all IAM role assumptions by SageMaker must be logged and alerts sent on unusual assume-role calls. How do you implement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable CloudWatch Logs on SageMaker",
      "B": "Create CloudTrail trail capturing IAM:AssumeRole, create CloudWatch metric filter for the SageMaker service principal, and alarm",
      "C": "Use AWS Config rule to check unusual assume-role calls",
      "D": "Rotate the SageMaker service role daily"
    },
    "explanation": "A CloudTrail trail logs all AssumeRole calls; a metric filter on the CloudWatch Logs for the SageMaker principal can trigger alarms for any assumption events."
  },
  {
    "id": "62ef2b27cf9158b3a5a22c38c44b57950ce6b1a6f352a019ef1e148af29cfef8",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A data scientist requests temporary credentials for S3 to use inside a notebook, without exposing long-lived keys. What is the most secure pattern?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use AWS STS GetSessionToken in the notebook to generate time-limited credentials",
      "B": "Embed an IAM user access key in the notebook",
      "C": "Use an S3 pre-signed URL for each object access",
      "D": "Use root account credentials stored in Secrets Manager"
    },
    "explanation": "Using STS GetSessionToken provides temporary credentials with limited lifetime. Pre-signed URLs are object-specific and not convenient; root or static keys are insecure."
  },
  {
    "id": "fb996551a864ef51b2e40a97905fea1461fc67606742d397a9d4f0e950c29746",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A SageMaker Endpoint in VPC must only communicate with ECR and CloudWatch Logs, not the public internet. How do you enforce this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Attach a bucket policy on ECR",
      "B": "Use NAT Gateway with restrictive route tables",
      "C": "Deploy endpoint in private subnets with interface endpoints for ECR and CloudWatch Logs and no NAT",
      "D": "Use Security Group to block 0.0.0.0/0"
    },
    "explanation": "Interface VPC endpoints allow access to AWS services without internet. Deploying in private subnets without NAT ensures no egress to public internet."
  },
  {
    "id": "473618a6efd1610ecbe4205be5e1e8ab282a54b6904aa9d2f07c6306f3b9d7f3",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A build in CodeBuild requires access to a VPC-only RDS instance. The CodeBuild service role has all necessary DB permissions, but builds fail to connect. What must you configure?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Open the DB\u2019s security group to 0.0.0.0/0",
      "B": "Configure the CodeBuild project\u2019s VpcConfig with the DB\u2019s subnets and security group",
      "C": "Use a NAT gateway in the project\u2019s VPC",
      "D": "Attach AWSLambdaVPCAccessExecutionRole to the CodeBuild role"
    },
    "explanation": "CodeBuild must be configured with VpcConfig to attach ENIs in the same subnets/security groups as the RDS. Without this, it runs in default network and cannot reach the DB."
  },
  {
    "id": "8c2fdffc6e8c4b9ac4f3da0efb8edc81629d1a8380be2e7441ab012a7fec2cf2",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "An audit finds that some SageMaker endpoint access logs contain PII. You must restrict retrieval of those logs to a security analysis role. How?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Attach a resource policy on the CloudWatch Logs group allowing only the security role principal to get logs",
      "B": "Encrypt logs with SSE-S3",
      "C": "Use an SCP to deny DescribeLogStreams",
      "D": "Disable logging on the endpoint"
    },
    "explanation": "A CloudWatch Logs resource policy scoped to the security analysis role ensures only that role can retrieve the logs; others will be denied."
  },
  {
    "id": "1e2042f7bd5ed8994f539b02f8b70ef613fd8b84ac4c18f7f4418dccc709eb5f",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A new security control: SageMaker projects must only use predefined IAM permission boundaries. How to ensure new roles created by SageMaker pipelines comply?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use an SCP to deny iam:CreateRole",
      "B": "Tag all new roles and enforce with AWS Config",
      "C": "Use CodePipeline approval stage",
      "D": "Attach the managed permission boundary policy in the SageMaker pipeline role so that any new role inherits the boundary"
    },
    "explanation": "Permission boundaries attached to the role that creates new roles propagate the boundary to child roles, ensuring they cannot be more permissive."
  },
  {
    "id": "aebec2f3fa462f5d0a4bd31d81f2dbcd6212eebf750a4cafc4293a78780c7ca7",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A SageMaker Model Monitor job must write drift reports to S3 but only to a single bucket. The monitor role has wild-card S3 permissions. How do you tighten permissions without affecting other workflows?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a bucket policy to deny PutObject from that role",
      "B": "Replace the role\u2019s S3 wildcard with a policy granting PutObject only on the monitor reports bucket",
      "C": "Implement an SCP",
      "D": "Encrypt the bucket with default encryption"
    },
    "explanation": "Updating the monitor role\u2019s IAM policy to allow only the specific bucket reduces blast radius without impacting other S3 workflows."
  },
  {
    "id": "755ed64b2bbb16739d8032bdd890ffb4752261c3234f529908cd1663d6b13a26",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A security guideline: all SageMaker processing jobs must use a VPC endpoint to S3. One job failed to access data. Which misconfiguration is most likely?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "The role lacks s3:GetObject",
      "B": "The bucket policy denies all traffic",
      "C": "VpcConfig lacked the S3 gateway endpoint in the subnet route tables",
      "D": "The job ran in a public subnet"
    },
    "explanation": "If the S3 gateway endpoint is not added to the subnet route tables in VpcConfig, the job cannot reach S3. A public subnet or role error would yield different symptoms."
  },
  {
    "id": "61d535d68dce9496b9d54001c5518fe59bbc54ea2b6c9a730786ffa6341ce723",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "A data science team needs to deploy a SageMaker real-time inference endpoint in a private VPC. They want to script the entire environment using AWS CloudFormation. Which resources and configuration should they include to ensure proper VPC connectivity and auto scaling of the endpoint?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Define an AWS::SageMaker::Endpoint with VpcConfig, and rely on SageMaker\u2019s built-in auto scaling by setting EnableAutoScaling: true.",
      "B": "Create AWS::SageMaker::EndpointConfig with VpcConfig.SecurityGroupIds and Subnets, an AWS::SageMaker::Endpoint that references that config, an AWS::ApplicationAutoScaling::ScalableTarget for sagemaker:variant:DesiredInstanceCount, and an AWS::ApplicationAutoScaling::ScalingPolicy.",
      "C": "Use AWS::EC2::VPCEndpoint for SageMaker API, then AWS::AutoScaling::AutoScalingGroup to scale instances behind the endpoint.",
      "D": "Use AWS::Lambda-backed custom resource to modify the SageMaker endpoint configuration after deploy for VPC and scaling settings."
    },
    "explanation": "CloudFormation requires separate AppAutoScaling resources; SageMaker endpoint resource does not directly support scaling."
  },
  {
    "id": "c69fd275ff313e86aef5922ece86e348cd2a54fa351b6297cc14a79d7e024bfd",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "An ML engineer wants to provision a multi-container SageMaker endpoint via AWS CDK in Python. They need one container for model A, one for model B, each with different IAM roles and different environment variables. Which CDK constructs and pattern should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use sagemaker.CfnModel for each container with respective environment and roles, then sagemaker.CfnEndpointConfig listing both models, and sagemaker.CfnEndpoint.",
      "B": "Use sagemaker.Model for each container, then sagemaker.MultiModelEndpoint to combine them automatically.",
      "C": "Define two sagemaker.CfnModel constructs with separate ExecutionRoleArn and Environment properties, create a single sagemaker.CfnEndpointConfig with both ModelName entries under ProductionVariants, then a sagemaker.CfnEndpoint.",
      "D": "Use sagemaker.EndpointBatchTransform with multiple TransformJobDefinitions for each container."
    },
    "explanation": "Multi-container real-time endpoints require separate CfnModel and listing in a single EndpointConfig."
  },
  {
    "id": "d04ffb8a7660d31a8a72cdbddd7605d76aeae6ac6c9fb6e36491917d48bb459a",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "A team uses AWS CDK to deploy their SageMaker endpoint, but CloudFormation reports drift in the EndpointConfig when they manually update the environment variables via console. How can they prevent drift and enforce configuration as code?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Remove manual updates; enforce changes only through CDK by maintaining environment variables in code and deploying via CDK pipelines.",
      "B": "Set EnableDriftCorrection: true on the CfnEndpointConfig resource.",
      "C": "Use AWS Config to ignore drift on SageMaker endpoint resources.",
      "D": "Use a CloudFormation macro to override the console changes at deployment time."
    },
    "explanation": "Prevent drift by managing all changes in code; CloudFormation cannot auto-correct SageMaker endpoints."
  },
  {
    "id": "ff25586e1b9bec35e61c74fc8309c07e7dea82c8dc9eeaaa15c161323bf18311",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "An organization requires that all SageMaker endpoint logs be sent to CloudWatch Logs in a separate AWS account. They want to script this via AWS CloudFormation. Which approach satisfies least-privilege and cross-account delivery?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Grant the SageMaker execution role cross-account CloudWatchPutLogEvents permission.",
      "B": "Attach AWS::Logs::SubscriptionFilter on the endpoint\u2019s log group to the destination account.",
      "C": "Use AWS::CloudWatch::Destination in the SageMaker stack to forward logs directly.",
      "D": "Deploy a CloudWatch Log Group export task via AWS::Logs::Destination in the central account and use resource policy on the SageMaker log group to allow PutSubscriptionFilter to the destination."
    },
    "explanation": "Cross-account subscription requires a CloudWatch Logs destination with resource policy granting permissions."
  },
  {
    "id": "4a00c47fdc281cde63779a5078da5b683201d3ba9e5a0ae87aaef8a35fcd781e",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "A dev team needs to automate creation of multiple SageMaker endpoints with different instance types and scaling policies using AWS CDK. They want to avoid duplication of code. Which CDK pattern is most appropriate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Write separate stacks for each endpoint and instantiate them individually.",
      "B": "Define a high-level construct class that takes instance type and scaling parameters as properties, and reuse it for each endpoint.",
      "C": "Use a single CfnInclude to import an existing CloudFormation template with parameters.",
      "D": "Leverage AWS::CloudFormation::Stack resource within the same template for each endpoint."
    },
    "explanation": "A custom CDK construct promotes reuse and parameterization across multiple endpoints."
  },
  {
    "id": "54ee69c84a7d717084b361fdb64e0e96d8883e9b5aea160e137401da8782fff2",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "A security audit requires that SageMaker endpoints be deployed only within private subnets and routed through NAT for internet access. How should this be scripted in AWS CDK to satisfy both requirements?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Specify subnetSelection with subnetType.PUBLIC in VpcConfig so endpoints use NAT automatically.",
      "B": "Set EndpointConfig.KmsKeyId to use private subnets.",
      "C": "Pass private subnet IDs and security group IDs in VpcConfig when defining CfnEndpointConfig in CDK, ensuring they are Private isolated subnets with NAT configured on the VPC.",
      "D": "Use VpcLink to attach the endpoint to a private Network Load Balancer."
    },
    "explanation": "VpcConfig must reference private subnets and SGs; NAT is configured at VPC level, not endpoint."
  },
  {
    "id": "c3bba306e21667f9a38b87834ed29aafa4b5998927bb51f730a38728b4199db1",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "An ML team wants to integrate spot instances into their inference fleet behind an Application Load Balancer with automatic scaling. They decide to use ECS on Fargate Spot. Which CloudFormation resource definitions must they include to ensure the desired capacity and scaling behavior?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS::ECS::CapacityProvider with AutoScalingGroupProvider using Spot Instances, AWS::ECS::Cluster defaultCapacityProviderStrategy including that capacity provider, and AWS::ECS::Service with desiredCount parameter.",
      "B": "AWS::AutoScaling::AutoScalingGroup with MixedInstancesPolicy using ON_DEMAND_ONLY, and AWS::ECS::Service.",
      "C": "AWS::ECS::Service with LaunchType FARGATE only, and AWS::ApplicationAutoScaling::ScalableTarget for ECS service.",
      "D": "AWS::Batch::ComputeEnvironment configured for Spot, and AWS::Batch::JobDefinition for inference jobs."
    },
    "explanation": "ECS Fargate Spot requires a capacity provider defined, then attach to the service for scaling on Spot."
  },
  {
    "id": "0a762632a81b2a9ca0f2ed95d7daa62a7d71275c5e5ad46fe307d448329a999c",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "Your organization mandates that all ML model container images be stored in Amazon ECR with image scanning enabled at push time. You must script this using AWS CDK. Which steps are required?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Define an ECR repository in CDK with imageScanningConfiguration enabled, grant SageMaker execution role pull permissions, then use that repository URI in CfnModel.",
      "B": "Use ecr.Repository with imageScanOnPush: true in CDK, add repository.grantPull(sagemakerRole), then pass repository.repositoryUri to SageMaker Model property in CfnModel or CfnModelPackage.",
      "C": "Create AWS::ECR::LifecyclePolicy in CloudFormation and set scanOnPush: ENABLED.",
      "D": "Use AWS CLI in a custom resource to enable image scan on existing repo."
    },
    "explanation": "CDK\u2019s ecr.Repository supports imageScanOnPush; then grant pull to the role used by SageMaker."
  },
  {
    "id": "009ba744d0000a4e54295bc03d189630d5dbd8d8c82dfceb8d546b83191bb990",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "A SageMaker endpoint fails health checks intermittently due to VPC ENI provisioning delays, causing AWS CloudFormation deployment to rollback. How can you modify your CloudFormation template to handle this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set EndpointConfig.DeleteAfterUse: false.",
      "B": "Add CreationPolicy with ResourceSignal on AWS::SageMaker::Endpoint.",
      "C": "Wrap the CfnEndpoint resource in a Custom::SageMakerEndpoint that retries.",
      "D": "Use DependsOn with a custom wait condition Lambda that polls DescribeEndpointStatus until InService before signaling success."
    },
    "explanation": "CloudFormation offers no native retry for SageMaker endpoints; implement a wait condition via Lambda or custom resource."
  },
  {
    "id": "6f4b778178b010f8dadf81012eb48fe924f52eeb9d838b3cb4a22107da9c3516",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "Your infra-as-code pipeline uses AWS CDK to deploy a SageMaker inference endpoint. During a deployment, you need to update the endpoint's variant weight to shift 20% traffic to a new model variant. How do you script this with least downtime?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Update the existing EndpointConfig in-place via CfnUpdatePolicy in CloudFormation.",
      "B": "Delete and recreate the endpoint with the new variant weight settings.",
      "C": "Create a new AWS::SageMaker::EndpointConfig with both variants and desired weights, then update the AWS::SageMaker::Endpoint to point to the new config.",
      "D": "Use AWS CLI in a CDK CustomResource to call UpdateEndpointWeight directly."
    },
    "explanation": "Endpoint updates must reference a new EndpointConfig; creating a new config and pointing the endpoint to it avoids downtime."
  },
  {
    "id": "e39aead91992e702f8f227ac5ae0422666a4f70e86055c0dc6a157a5b6d46d4c",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "A DevOps engineer must ensure that SageMaker endpoint deployments are idempotent and environment-specific. They decide to use CloudFormation parameter overrides. Which of these strategies will achieve both requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Store instance type and count in template metadata and reference via Fn::GetAtt.",
      "B": "Define Parameters for InstanceType, InitialInstanceCount, VPC Subnet IDs, and SecurityGroup IDs; reference them in CfnEndpointConfig, ensuring different values per environment.",
      "C": "Use Mappings section keyed by environment name to look up all values.",
      "D": "Use a separate template per environment to hardcode values."
    },
    "explanation": "Parameters allow idempotent, repeatable deployments with all settings externalized per environment."
  },
  {
    "id": "22c71e64f79bbb6f6de1b0f095be35a435e939676d1e5a28b7b6bc8cfeabaf46",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "You need to deploy a SageMaker multi-model endpoint for cost-effective hosting of hundreds of small models. You want to automate provisioning with CloudFormation and enforce container image immutability. What is the correct approach?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create one AWS::SageMaker::ModelPackage for each container with ModelApprovalStatus set to Approved and InferenceSpecification referencing ECR image digests, then one AWS::SageMaker::EndpointConfig with MultiModelConfig.",
      "B": "Use AWS::SageMaker::Model with LocalCode option and set ImageConfig.AutoUpdate: false.",
      "C": "Deploy a custom multi-model server on an EC2 Auto Scaling group instead.",
      "D": "Use a single AWS::SageMaker::Endpoint with MultiModelEndpoint property."
    },
    "explanation": "ModelPackage with approved inference spec and image digest ensures immutability; EndpointConfig multi-model supports dynamic loading."
  },
  {
    "id": "313db14f03fe1dc218e8a5d2a236700b036d4748dd72763439b11c9a2f5217a8",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "A regulated environment requires all SageMaker endpoints to be deployed via AWS CloudFormation and reviewed via change sets. When a change to endpoint instance type is approved, CloudFormation update fails because of immutable properties. How should you script your template to allow instance-type changes without replacing resources?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set UpdateReplacePolicy: Retain on the Endpoint resource.",
      "B": "Use AWS::SageMaker::EndpointConfig with EndpointConfigName generated by a GUID to force replacement.",
      "C": "Use a nested stack for the endpoint and update only nested stacks.",
      "D": "Decouple endpoint from endpoint config: script EndpointConfig as separate resource and update the EndpointConfigName in the Endpoint resource, so only the config is replaced, not the endpoint."
    },
    "explanation": "EndpointConfig can be replaced independently; endpoint resource simply points to new config name."
  },
  {
    "id": "cde502b3e903abaa55f005615a172e10a96f876f541dcb0c3d5fabbb08f46359",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "Your team wants to use AWS CDK in TypeScript to deploy SageMaker endpoints and test them automatically post-deployment. They need to add an automated post-deploy test hook in the same pipeline. Which CDK pattern should they adopt?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use CDK aspect to inject Lambda-backed custom resources.",
      "B": "Utilize CDK Pipelines\u2019 testing framework: add CodeBuild Step after deployment stage invoking a smoke-test Lambda or script.",
      "C": "Add AWS::CodeDeploy::DeploymentGroup in the CDK template.",
      "D": "Write a CDK Custom Resource that runs tests as part of deployment."
    },
    "explanation": "CDK Pipelines support post-deploy tests via CodeBuild steps integrated into the pipeline."
  },
  {
    "id": "8e1d1ddb1b10b8a8b6096b4be82863ceb05dc1b5f044770042d4a1b1d1a1d5c4",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "An engineer must ensure that SageMaker endpoint container images are built, pushed to ECR, and deployed in one IaC deployment. Using AWS CDK, which sequence of constructs accomplishes this atomically?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Define a CodeBuild Project in CDK that builds the image, then a separate CDK app that references the ECR repository.",
      "B": "Use a custom resource to call AWS CLI to build and push, then define the CfnModel.",
      "C": "Use CodePipeline CDK constructs: define a pipeline with Source stage, Build stage (CodeBuild building and pushing to ECR), then a Deploy stage with CDK deploy of the SageMaker endpoint stack referencing image URI as pipeline output.",
      "D": "Use CDK Bundling API on the sagemaker.Model construct."
    },
    "explanation": "CDK Pipelines can orchestrate build and deploy stages, ensuring atomic transition from image build to endpoint deployment."
  },
  {
    "id": "e52de6d44c98e018b50f6adf423c394da7cebc99a65f1cc8760e530b6e8682bb",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "Your infra team must grant least-privilege permissions for SageMaker to pull container images from a private ECR in another AWS account. Which IAM policy should you script in CloudFormation on the SageMaker execution role?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Allow ecr:BatchGetImage, ecr:GetDownloadUrlForLayer on the repository Arn directly in the role policy.",
      "B": "Policy allowing sts:AssumeRole on a role in the ECR account that has ecr:GetAuthorizationToken, BatchGetImage, GetDownloadUrlForLayer, and in the ECR account\u2019s repository policy grant the SageMaker role\u2019s AWS principal those actions.",
      "C": "Grant AmazonEC2ContainerRegistryFullAccess to the SageMaker execution role.",
      "D": "Add AWS-managed policy AmazonSageMakerFullAccess which includes ECR pull permissions."
    },
    "explanation": "Cross-account ECR pull requires AssumeRole into the ECR account and corresponding repository policy; broad managed policies violate least privilege."
  },
  {
    "id": "580c9009985f4852394077639f3f458e45112b3f5891e82b03d04c2c9de81063",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "A team uses AWS::CloudFormation::Stack resource to encapsulate SageMaker endpoint provisioning in a nested stack. They now need to expose the endpoint name to the parent stack for AutoScaling configuration. Which method achieves this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "In the nested stack template, declare an Output for EndpointName; in the parent stack, reference it via Fn::GetAtt on the AWS::CloudFormation::Stack resource.",
      "B": "Use ImportValue to pull it from the nested stack without declaring Output.",
      "C": "Write a Lambda-backed custom resource in the parent to call DescribeStacks.",
      "D": "Use AWS::SSM::Parameter to store the endpoint name and read it in the parent."
    },
    "explanation": "Outputs and Fn::GetAtt on nested stacks pass values to parent stacks."
  },
  {
    "id": "69485678cadcf7b48f8f889ab52287695afee6d5e3cd68c18382f77a80cf620d",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "Your company requires that all SageMaker endpoint configurations be version controlled and reviewed. Which AWS CDK and CloudFormation features should you use to allow change visibility and prevent unreviewed drift?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable termination protection on CfnEndpoint resources.",
      "B": "Use sagemaker.Endpoint.fromEndpointName to reference existing endpoint without managing it.",
      "C": "Use CDK to synthesize CloudFormation change sets and require manual approval before execution in pipelines.",
      "D": "Enable rolling updates in CloudFormation on the SageMaker stack."
    },
    "explanation": "Change sets provide visibility into what will change before applying; CDK pipelines can enforce manual approvals."
  },
  {
    "id": "22b303e0fe098d100ca86e2a0439cb6f29b0831ea32b1d75de602f6a4d265a09",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "An ML engineer needs to script blue/green deployments of a SageMaker endpoint using AWS CloudFormation and minimize invocation errors during traffic shift. Which pattern should they implement?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS::SageMaker::EndpointVariants property to specify both blue and green variants and weights.",
      "B": "Define two separate Endpoint resources and swap DNS records in Route53.",
      "C": "Use CodeDeploy with blue/green strategy for SageMaker service.",
      "D": "Use AWS::SageMaker::EndpointConfig to define two production variants with weights, attach AppAutoScaling policy to adjust weights gradually as part of CloudFormation update."
    },
    "explanation": "Defining both variants and adjusting weights via scaling policies enables gradual traffic shift without downtime."
  },
  {
    "id": "da674e56278b74d38d1777c2397afd1a04b47153cc18e38f246295389deabf95",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "A DevOps pipeline must deploy a SageMaker batch transform job definition and a serving endpoint in one CloudFormation template. They encounter circular dependency between the IAM role and the endpoint config. How can they resolve this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Duplicate an IAM role resource for each service to break the cycle.",
      "B": "Use separate CloudFormation nested stacks: one for IAM role definition, one for endpoint and transform referencing an exported role ARN.",
      "C": "Use DependsOn between the role and endpoint config to force order.",
      "D": "Use AWS::IAM::ManagedPolicy to attach permissions instead of inline policies."
    },
    "explanation": "Splitting into nested stacks allows exporting the role ARN to avoid circular resource references."
  },
  {
    "id": "e1b0bf9a567964775dda962c6871dc14e565fa5b31d0915c73a8404b9f095c44",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "Your infrastructure uses AWS CDK to deploy SageMaker endpoints. A CloudFormation rollback occurs because the endpoint creation takes longer than the default timeout. How can you adjust the timeout in CDK?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set cfnOptions.creationPolicy with a Timeout value on the CfnEndpoint CDK construct.",
      "B": "Set the AWS SDK client timeout in CDK context.",
      "C": "Use AWS::CloudFormation::WaitCondition within the same stack.",
      "D": "Wrap the deployment in a longer-running AWS Step Functions state machine."
    },
    "explanation": "CDK\u2019s cfnOptions.creationPolicy allows specifying timeout for resource creation; other methods don\u2019t apply to SageMaker endpoints."
  },
  {
    "id": "72ef63aaba48ec4bfa285d2ca12e6c8a7e895bb68d52d606ef80b903429e2c50",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "An ML platform team must deploy hundreds of endpoints with varying instance counts. They choose AWS CDK to loop constructs. How should they write their code to maintain performance and minimize synth time?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a for loop in CDK to create each endpoint construct directly in the stack.",
      "B": "Use AWS::CloudFormation::Stack for each endpoint to parallelize.",
      "C": "Generate endpoint configurations via a CDK Construct that iterates over a configuration array at synthesis time, rather than at runtime, and emits only necessary CloudFormation resources.",
      "D": "Use AWS Step Functions to call CDK synth for each endpoint separately."
    },
    "explanation": "Constructs iterate at synth time to produce a single CF template; runtime loops in Lambda or Step Functions are inappropriate."
  },
  {
    "id": "06bc3cadd74eb2f20fce6714453ddebe92d367077eb2538b7ffb3f26184b7037",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "A compliance requirement mandates that all SageMaker endpoints be deployed in dedicated subnets per environment. You already have a CDK VPC with multiple isolated subnet groups. How do you guarantee subnet selection per environment?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Hardcode subnet IDs in the CDK code per environment.",
      "B": "Define CDK Stack context values for subnetGroupNames per environment and use vpc.selectSubnets({subnetGroupName: ...}).",
      "C": "Use AWS::TaggedResource tagging on the VPC and filter subnets via CloudFormation Intrinsics.",
      "D": "Use defaultSubnets selection and rely on AWS to pick isolated ones."
    },
    "explanation": "CDK context allows environment-specific parameters; vpc.selectSubnets filters by groupName tag."
  },
  {
    "id": "79cace4b85c46734b4d650843d11ef9b050cd21763efaaf94c1eef43272c51db",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "Your team wants to use AWS CDK to produce YAML CloudFormation templates for all resources. They need to validate their template for SageMaker endpoint VPC settings before deployment. Which CDK command and plugin should they use?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "cdk synth --no-staging and the cfn-lint plugin to lint the synthesized template.",
      "B": "cdk diff and AWS::SageMaker::ValidateEndpoint API.",
      "C": "cdk deploy --dry-run.",
      "D": "cdk docs to generate CloudFormation schema and manually inspect."
    },
    "explanation": "cdk synth outputs the template for cfn-lint validation; diff shows changes but not schema errors."
  },
  {
    "id": "2ff807426adef90a2fa1a0a52e8beb78aa31514c1765f134cfc7834517aabe76",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A healthcare provider has a highly imbalanced dataset (1% positive cases) stored in Amazon S3. They need to generate synthetic minority samples to train a disease detection model, ensure PHI compliance (no real patient data leakage), and prevent inversion attacks on synthetic data. Which approach best meets these requirements?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "Use AWS Glue DataBrew to oversample the minority class and enable record-level masking to anonymize PHI.",
      "B": "Implement a CTGAN model in a SageMaker notebook with differential privacy (DP-SGD), generate synthetic samples within a VPC-backed FSx for Lustre, and use a KMS key for encryption.",
      "C": "Use SageMaker Clarify\u2019s synthetic data feature to generate samples and store results in an S3 bucket with default encryption.",
      "D": "Subscribe to a third-party synthetic health dataset via Amazon Data Exchange and merge it with your S3 data."
    },
    "explanation": "CTGAN with DP-SGD ensures differential privacy (protects against inversion), runs in SageMaker within the VPC, uses FSx for high-performance storage, and KMS encryption prevents PHI leakage, satisfying all requirements."
  },
  {
    "id": "66217aac830eb0f36668c52a8773d92eee3cf8f8606aba3785b1aad46380043b",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A bank wants to detect selection bias in its loan application dataset (applicants by ZIP code). They must quantify the difference in proportions of approval rates between urban and rural ZIP codes before training. Which SageMaker Clarify pre-training bias configuration is most appropriate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use bias_config with 'label'='approval', 'facet_name'='ZIP_code_type', and 'metrics'=['dipl','ci'].",
      "B": "Use ModelMonitor with DataQualityMonitor to compute missing values in the ZIP_code_type feature.",
      "C": "Use bias_config with 'facet_name'='approval', 'label_values_or_threshold'=['urban','rural'], and 'metrics'=['feature_attribution'].",
      "D": "Use Clarify post-training explainability job to get SHAP values for ZIP_code_type."
    },
    "explanation": "Pre-training bias_config must specify label and facet_name (ZIP_code_type) to compute DPL (difference in proportions of labels) and CI (class imbalance) before training. Other options misuse metrics or post-training tools."
  },
  {
    "id": "d5bf0dff089069a407ec31782e6864f3b4d8f38095e494f9d38d8afc7386682f",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "An online retailer stores customer data in DynamoDB with encrypted PII (customer_id, email). They export it to S3 for cleaning. They need to mask PII during data cleansing using AWS Glue DataBrew. Which recipe action and configuration should they use?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Use 'maskValue' on columns [customer_id,email] with a static masked value.",
      "B": "Use 'hash' on columns [customer_id] and 'tokenize' on [email] with default salts.",
      "C": "Use 'encrypt' on columns [customer_id,email] using the DynamoDB table key.",
      "D": "Use 'maskValue' with 'Mask Type'='Random Character', 'Character Set'='Alphanumeric', on [customer_id,email]."
    },
    "explanation": "DataBrew\u2019s maskValue recipe action with Random Character on specified PII columns properly masks values without exposing patterns. Hash and encryption aren\u2019t available as DataBrew recipe actions, and static mask would reveal structure."
  },
  {
    "id": "1342e037ed37f2e24ad518c2fbf2798fd6b0320b9504a190df64d3f500c4ee44",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A credit-scoring dataset has 10% missing values in the 'employment_length' feature. You need to impute missing values to reduce measurement bias and avoid skew in training. The distribution is right-skewed. What is the best imputation strategy using AWS Glue Data Quality rules?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Impute missing 'employment_length' with the mean value computed over all records.",
      "B": "Use median imputation per gender group via a DataBrew 'Fill with median' action partitioned by 'gender'.",
      "C": "Use a Data Quality job to compute the median per 'employment_type' and then apply DataBrew 'Fill with value from column aggregation' partitioned by 'employment_type'.",
      "D": "Apply mode imputation globally for 'employment_length' using the most frequent category."
    },
    "explanation": "Median per employment_type addresses right skew within similar segments and reduces bias. A Data Quality job can compute segment medians and DataBrew can apply partitioned imputation accordingly."
  },
  {
    "id": "9cfd983c958c39f363fd335d6d15143f20d214e7f64093048b26bb77e7a9c824",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "You must ensure all S3 data at rest and in transit is encrypted for an ML training job. Data is loaded from S3 into a SageMaker Training job and stored on an FSx for Lustre file system. Which configuration satisfies both requirements?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable SSE-KMS on the S3 bucket, attach an IAM role with KMS decrypt, launch FSx with transit encryption enabled and a customer-managed KMS key.",
      "B": "Use SSE-S3 on the S3 bucket and launch FSx with default AWS-managed KMS key.",
      "C": "Configure SageMaker training input to use SSL and store data in unencrypted FSx.",
      "D": "Enable client-side encryption for S3 uploads and use FSx encryption at rest without transit encryption."
    },
    "explanation": "SSE-KMS on S3 and FSx with transit encryption via customer-managed KMS ensure encryption at rest and in transit. SSE-S3 lacks customer KMS control, and client-side or missing transit encryption fails one requirement."
  },
  {
    "id": "934146711177085e0fb2df7b37be9fb0c7d347dea80147ab13c0e5248154e679",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A telco data pipeline streams CDR logs into S3, with PII fields (phone_number) encrypted via KMS. A data scientist needs to preprocess and anonymize phone numbers before modeling using SageMaker Processing. Which approach ensures the data scientist cannot decrypt original values?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provide the processing job with KMS decrypt permissions and apply one-way hash.",
      "B": "Use a KMS grant to decrypt phone_number, then pseudonymize with reversible mapping.",
      "C": "Use Lambda triggered by S3 events to replace phone_number with SHA-256 digest before encrypting with a separate KMS key that processing job lacks decrypt permissions for.",
      "D": "Configure SageMaker Processing to use a private KMS key different from S3\u2019s and decrypt in job."
    },
    "explanation": "Lambda replaces PII with irreversible SHA-256 digest, re-encrypts with a key the job can use, ensuring the processing job cannot decrypt the original KMS-encrypted phone numbers."
  },
  {
    "id": "a99c6268d5f9d2897b9b4782e473851a090d23cb0d282a5e0103144243419472",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "An image dataset stored in S3 contains location metadata in EXIF that constitutes PII. You must strip this metadata in a SageMaker Processing job, standardize image sizes, and then encrypt outputs in transit to FSx. Which pipeline achieves this with minimal steps?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Lambda to strip EXIF and resize, store intermediate in S3, then run a Processing job to copy to FSx with SSL.",
      "B": "Run a SageMaker Processing job with a custom container that uses exiftool to strip metadata, Pillow to resize, and output directly to FSx for Lustre with HTTPS mount.",
      "C": "Download images to EFS, run an EC2 batch job to preprocess and re-upload to encrypted S3, then mount S3 to FSx.",
      "D": "Use AWS Glue Spark job to strip EXIF and resize, write to FSx using AWS SDK with SSL."
    },
    "explanation": "A SageMaker Processing job with a custom container can handle EXIF stripping and resizing in one step and write directly over HTTPS to FSx for Lustre, minimizing components."
  },
  {
    "id": "db3cb70e1ed6318143783a5d4359ff5c6e1c32570bb4e73b54ce1d5bf6b36e52",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "You need to detect data drift and bias before training on new monthly data. The monthly datasets arrive in S3. Which combination of SageMaker Clarify configuration and workflow ensures automated pre-training bias and data drift detection?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Clarify post-training job, schedule via EventBridge to run monthly.",
      "B": "Use Clarify ModelBiasMonitor as part of ModelMonitor, schedule daily.",
      "C": "Use DataQualityMonitor to detect drift, then manually analyze with Clarify.",
      "D": "Configure Clarify pre-training bias job with baseline from last month, schedule with SageMaker Pipelines monthly to compute DPL, CI, and feature distribution drift."
    },
    "explanation": "Pre-training Clarify job in SageMaker Pipelines scheduled monthly compares new data to a historic baseline, computes DPL and CI for bias and distribution drift, and automates detection."
  },
  {
    "id": "3595646ebb8da28e62272c591937c80bfb23032ac6407946b1ce35c2de79f867",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A startup must comply with GDPR: remove or mask 'email' and 'user_ip' fields before any ML training. They need an auditable automated solution using AWS services. Which design meets GDPR requirements?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create a SageMaker Processing pipeline that uses a DataBrew job for masking email and user_ip, store masked data in a GDPR-compliant S3 bucket with audit logs via CloudTrail.",
      "B": "Have developers write custom Python in a notebook to drop fields, then manually approve and upload to S3.",
      "C": "Use Amazon Macie to discover PII and instruct DataSync to exclude those columns.",
      "D": "Encrypt email and user_ip with SSE-KMS and train model on encrypted features."
    },
    "explanation": "DataBrew recipe for masking with audit logs in CloudTrail ensures fields are irreversibly masked and provides an auditable, automated workflow. Encryption doesn\u2019t remove PII for GDPR."
  },
  {
    "id": "0e3ce819acae1d5868f4504c2cdfff51dc7d0600a0c3028f65bd240b0809770a",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "You must validate that numeric features shipped to training satisfy min/max thresholds (e.g., age between 18 and 100). Violations should block training. How can you implement this in SageMaker?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a Lambda pre-processing check triggered by S3 upload events to reject invalid datasets.",
      "B": "Write custom validation code in the training script and raise errors if thresholds are breached.",
      "C": "Configure an AWS Glue Data Quality job to run pre-training via SageMaker Processing, fail the Pipeline step if thresholds outside range.",
      "D": "Use SageMaker Model Monitor DataQualityMonitor in real time to monitor feature thresholds."
    },
    "explanation": "Glue Data Quality job integrated into SageMaker Pipeline allows rule-based validation (min/max), and the pipeline can be configured to stop if validation fails, preventing invalid data from training."
  },
  {
    "id": "993caa53dca4b247a8d05ebb8059680f9b615c6693dcca93d9754a1c4d53494f",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A telecom dataset has missing values in time-series features. You need to impute missing timestamps using forward fill within each subscriber partition, ensure no cross-subscriber leakage, and provide a clean CSV to FSx. Which solution accomplishes this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Clarify to detect missing timestamps and fill globally with median intervals.",
      "B": "Use AWS Glue ETL with a window function to forward fill across all records.",
      "C": "Use SageMaker Processing with a Spark container, group by subscriber and apply forward fill, write to S3.",
      "D": "Deploy a SageMaker Processing job with a custom pandas script that partitions by subscriber_id, forward fills timestamps, and writes output to FSx for Lustre."
    },
    "explanation": "A SageMaker Processing job with pandas partitioned by subscriber_id isolates subscribers and forward fills, writing the clean CSV to FSx. Clarify doesn\u2019t impute and Glue window could leak between partitions if misconfigured."
  },
  {
    "id": "80a1e0338521258f90a1b337d1bab73ec6bfd960b22759313134243474c0f8c5",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "Your new dataset contains categorical features with high cardinality (10,000 unique values). You need to reduce dimensionality before modeling to prevent overfitting. Which AWS tool and method should you use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use DataBrew 'One-hot encode' action on the categorical column.",
      "B": "Use SageMaker Feature Store offline store and apply a frequency encoding transform in a Processing job.",
      "C": "Use Clarify bias detection to filter out rare categories.",
      "D": "Use Glue Data Quality to drop categories with low frequency."
    },
    "explanation": "Frequency encoding in a SageMaker Processing job reduces cardinality by mapping to numeric frequencies, avoids one-hot explosion. DataBrew one-hot creates huge feature space. Clarify and Data Quality don\u2019t transform features for modeling."
  },
  {
    "id": "6efdeaecf353eac35761f3730cfc5a6823382dc117cc52ac84dea7558c6d279c",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A sensitive dataset requires K-anonymity (k=10) on PII columns before ML. Which AWS service or combination can enforce k-anonymization in an automated pipeline?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Use AWS Glue DataBrew with the 'Anonymize' recipe action for k-anonymity.",
      "B": "Use Amazon Macie to detect PII and then use Lambda to drop records.",
      "C": "Build a SageMaker Processing job using ARX (or Python library) to enforce 10-anonymity and write to encrypted FSx.",
      "D": "Use SageMaker Clarify to generate anonymized synthetic data with k-anonymity."
    },
    "explanation": "No native AWS service enforces k-anonymity; a custom SageMaker Processing job using an open-source library like ARX or a Python package can implement k-anonymity and integrate with FSx for encrypted storage."
  },
  {
    "id": "4844a00f000ddcba704722120a4578eb8d02e88eaf1e9a543f1557de266a4212",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "During training, a job fails due to data corruption in Amazon FSx. You need to verify data integrity before each training run and fail fast if corruption occurs. What is the best way to implement this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Integrate a SageMaker Processing step using a checksum validation script to compare file checksums against known-good values stored in AWS Secrets Manager before training.",
      "B": "Enable FSx data compression to detect corrupted blocks.",
      "C": "Use ModelMonitor DataQualityMonitor to detect corrupted data during training.",
      "D": "Rely on SageMaker Training job logs to find errors post-start."
    },
    "explanation": "A pre-training Processing step that computes checksums and validates against stored values (in Secrets Manager) will detect corruption and allow the pipeline to fail fast. FSx compression and post-training monitors are insufficient."
  },
  {
    "id": "33b9b9764ba354b64a04be687d2a5977bc4aeae0133e146e346fc1719985fbf0",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A dataset is subject to data residency regulations requiring that no raw data leaves the EU. You have a multi-region SageMaker Pipeline. How do you ensure compliance when copying data from an S3 bucket in us-east-1 to an FSx file system in eu-west-1?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set S3 replication to eu-west-1 for raw data and run training in us-east-1.",
      "B": "Use S3 Cross-Region Replication to an eu-west-1 bucket, then use FSx for Lustre in eu-west-1 and run the pipeline entirely in eu-west-1.",
      "C": "Use AWS DataSync to transfer data directly from us-east-1 S3 to FSx in eu-west-1.",
      "D": "Use SageMaker Training with input_mode='Pipe' to stream data across regions securely."
    },
    "explanation": "Cross-Region Replication ensures raw data is copied to an EU bucket. Training and FSx in eu-west-1 ensure raw data doesn\u2019t leave the EU region. DataSync or streaming across regions violates residency rules."
  },
  {
    "id": "1f33515642506af7feda7d0289a979e9689ffef4981f569c3f5d0d8ce5783d12",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "Your text dataset uses masked PII (<NAME>, <EMAIL>) but downstream tokenization treats these tokens as real vocabulary and biases the model. How do you preprocess these masks to avoid model bias?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use DataBrew to replace <NAME> and <EMAIL> with random real names and emails.",
      "B": "Use SageMaker Clarify to drop samples containing masked tokens.",
      "C": "Use a custom tokenizer to treat <NAME> and <EMAIL> as unique IDs.",
      "D": "In a SageMaker Processing job, replace all masked tokens with null or a uniform [MASK] token recognized by the model\u2019s vocabulary."
    },
    "explanation": "Replacing with a uniform [MASK] token recognized by the model avoids bias from inconsistent placeholders. Random real names reintroduce PII, dropping samples reduces data, unique IDs still bias."
  },
  {
    "id": "2646474e3c3f5739f6a13f3acad5f9b2a6fb3df5733e586c67f2bb9c287abc0a",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "An ML project requires 50 million tabular records. You must ensure that when shuffling and splitting into train/val/test, no data skew occurs across Amazon FSx shards. Which strategy ensures uniform distribution?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Use a SageMaker Processing job with Spark: read data, apply 'repartition(nShards)' and 'randomSplit' for train/val/test, then write to FSx.",
      "B": "Use FSx distributed copy and rely on random file ordering in S3.",
      "C": "Use SM Channel input with ShuffleConfig in a Training job for splitting.",
      "D": "Use SageMaker Clarify pre-processing to shuffle and split data."
    },
    "explanation": "A Spark repartition(nShards) before randomSplit ensures uniform distribution across shards; FSx copy or ShuffleConfig doesn\u2019t guarantee uniform splits; Clarify is not for splitting."
  },
  {
    "id": "7c04877510c7cf46e010325cb9de5b0c5aef61282d385055e65866c1a9bea90e",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "Your numeric feature 'income' has extreme outliers. You need to detect and treat them in an automated pipeline using AWS services before training. Which multi-step process is best?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Use a SageMaker Clarify Explainability job to identify outliers and then manually remove them.",
      "B": "Use DataBrew to compute z-scores and drop records with |z|>3 in a recipe, then feed to training.",
      "C": "In a SageMaker Processing job, run AWS Glue Data Quality rule to detect numeric outliers, replace values beyond 1st and 99th percentiles with percentile caps, and write to FSx.",
      "D": "Use ModelMonitor DataQualityMonitor to capture outliers at inference time and then feed back corrections."
    },
    "explanation": "Glue Data Quality rule can detect percentile-based outliers, and a Processing job can cap outliers automatically in a pipeline. Clarify and ModelMonitor don\u2019t enforce pre-training data corrections."
  },
  {
    "id": "9534d824a3c52dd0ae2e68961cc3e8ea496682c5c36ab17893d81a51216f4f9d",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A legal requirement mandates that social security numbers (SSNs) be tokenized with a one-way hash before leaving a secure environment. Which architecture satisfies this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Trigger a Lambda in the secure subnet to hash SSNs with SHA-256 and write tokens to an S3 VPC endpoint bucket.",
      "B": "Encrypt SSNs in S3 with SSE-KMS before training.",
      "C": "Use SageMaker Processing to call DynamoDB to fetch hashed SSNs.",
      "D": "Use Glue Crawlers to detect SSNs and mask them."
    },
    "explanation": "A Lambda in the secure subnet with VPC endpoint for S3 can perform one-way SHA-256 hashing before data leaves, ensuring SSNs are never exposed. Encryption alone is reversible."
  },
  {
    "id": "597ac931d249725f3952cfd2fa0edaaefcebe06498ff3303b393bde9d8bc112b",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "You want to measure dataset imbalance (CI) and difference in proportions (DPL) for a multiclass image dataset stored in S3. You need to generate these metrics at scale before training. Which solution is most efficient?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a SageMaker Processing job with a custom Python script listing S3 image prefixes, counting labels, and computing CI and DPL.",
      "B": "Use SageMaker Clarify pre-training bias job pointed at the manifest file in S3, specifying 'label' and facet, for multiclass metrics.",
      "C": "Use Glue ETL to load metadata into Redshift and run SQL to compute counts.",
      "D": "Use Athena queries against S3 to compute label distributions and calculate DPL post-hoc."
    },
    "explanation": "Clarify pre-training bias job can compute CI and DPL at scale across a manifest without custom coding. Athena or custom scripts require manual metric computation; Clarify automates it."
  },
  {
    "id": "a814a6851716daeb77c4b4a4b58942640484c9d646cd795a2ce2ceb2678945f0",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "Your dataset includes an IP address feature that you must anonymize with consistent tokenization (same IP always same token) but irreversibly. Which technique in a SageMaker Processing job should you implement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Apply SHA-256 hashing with a secret salt stored in Secrets Manager.",
      "B": "Use Python\u2019s md5 without salt to hash the IP addresses.",
      "C": "Use DataBrew\u2019s encrypt action with KMS.",
      "D": "Use SageMaker Clarify to mask feature and record mapping."
    },
    "explanation": "SHA-256 with salt ensures consistent, one-way mapping and prevents rainbow table attacks; md5 is weak without salt; DataBrew encrypt is reversible with KMS; Clarify doesn\u2019t mask."
  },
  {
    "id": "36caecae6bbb1a4b95a3753259a207d383e89bdef4e3a8eb5bfdfb1bb30268ae",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A data scientist suspects measurement bias due to inconsistent units in a numeric feature (some records in cm, some in inches). They need to detect and correct this automatically before training. How can this be done in AWS?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Clarify data bias to detect inconsistent unit distributions.",
      "B": "Use Glue Data Quality to enforce unit ranges and manual corrections.",
      "C": "Use Athena to query and tag inconsistent units for manual review.",
      "D": "Write a custom SageMaker Processing step that reads the 'unit' column, applies conversions to a standard unit, and writes back to FSx."
    },
    "explanation": "Measurement bias due to units requires a transformation step. A custom Processing job can read the unit metadata column and convert values programmatically. Other AWS services don\u2019t perform unit conversion."
  },
  {
    "id": "c8bc04b9b6f27d60c901c6102fd118441c0ec8a4430ec078d1d4b40b7834c464",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A global dataset contains phone numbers in various formats. You need to standardize formatting and mask all but last four digits, while preserving relational joins via hashed salted values. Which pre-training pipeline components accomplish this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use DataBrew recipe to script regex formatting and static masking of phone numbers.",
      "B": "In a SageMaker Processing job: parse with libphonenumber, format E.164, generate SHA-256 hash with salt from Secrets Manager for join key, and mask all but last four digits in output.",
      "C": "Use AWS Glue Crawlers to detect phone patterns and then use Lambda to transform.",
      "D": "Use SageMaker Clarify to identify PII and then use DataBrew to mask."
    },
    "explanation": "A custom Processing job can use libphonenumber for E.164 formatting, apply salted SHA-256 for join preservation, and mask digits, integrating with Secrets Manager for salt. DataBrew and Clarify can\u2019t fully meet these steps."
  },
  {
    "id": "06f255540be0b48491eefa3a5207dd47e34198d783f7fc783f5fd38a0256f709",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "You need to guarantee that no training dataset has more than 5% of records from any single customer_id to prevent overfitting. How do you enforce this in an automated AWS pipeline?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a SageMaker Processing step with Spark: group by customer_id, sample up to 5% of that group into the final dataset, then write to FSx.",
      "B": "Use ModelMonitor to detect customer_id skew post-training and retrain.",
      "C": "Use Glue Data Quality to drop records exceeding threshold.",
      "D": "Use Clarify pre-training to detect over-representation and fail training."
    },
    "explanation": "A Spark-based Processing job can enforce maximum sampling per customer_id before training. Data Quality and Clarify detect but don\u2019t enforce sampling limits; ModelMonitor is post-training."
  },
  {
    "id": "06e9b5e6ee94f91632f4a404bfd9f61296cf44f5ed5165941c2c61404053235e",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "Your model requires a balanced training set across 4 classes. The raw dataset in S3 is stored with per-class prefixes. You want SageMaker Training to automatically balance classes at job launch. What configuration achieves this?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Enable input data channel 'ShuffleConfig' and rely on equal distribution.",
      "B": "Use Clarify to rebalance classes pre-training.",
      "C": "Create a Processing job that downsamples over-represented prefix folders to match counts of the smallest class and writes balanced data to FSx for Training input.",
      "D": "Use Training hyperparameters to set 'class_weight' parameter during training."
    },
    "explanation": "SageMaker training doesn\u2019t automatically rebalance files by prefix. A custom Processing job must downsample to equal class counts. class_weight adjusts loss but doesn\u2019t rebalance data."
  },
  {
    "id": "221a60d2fedb212abd88986982a7f38ebb7523206ea6a1ff286e96030bbf8032",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A dataset with both categorical and numeric features needs to be split into train/validation/test, ensuring each split reflects the same categorical distribution. Which AWS-based method guarantees stratified sampling at scale?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Glue ETL to randomSplit on the full dataset.",
      "B": "Run a SageMaker Processing Spark job: stratified sampling with DataFrame.stat.sampleBy on the category column, then write splits to FSx.",
      "C": "Use Athena CTAS queries with WHERE RAND() thresholds.",
      "D": "Use SageMaker Clarify preprocessing to stratify."
    },
    "explanation": "Spark\u2019s sampleBy in a Processing job enables stratified sampling by category. Athena or randomSplit don\u2019t guarantee identical category proportions; Clarify doesn\u2019t provide sampling utilities."
  },
  {
    "id": "c58efb18c16f83bbb752d07026a833b69f72ce721c22f81c0ccc8231ae5c507b",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "An ML pipeline triggers monthly. You must automatically generate a data quality report that includes the number of nulls, duplicates, and schema drift before training. Which integration sequence in SageMaker Pipelines accomplishes this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Invoke an AWS Glue Data Quality Check job as a Pipeline step (via Lambda), retrieve results, fail Pipeline on violations, then proceed to training.",
      "B": "Use DataBrew Profile job in Pipeline to generate report, manually check.",
      "C": "Use SageMaker Clarify DataQualityMonitor for pre-training schema drift detection.",
      "D": "Use ModelMonitor after training to analyze data quality."
    },
    "explanation": "AWS Glue Data Quality Check can profile nulls, duplicates, and detect schema drift. Integrating it in Pipelines (via Lambda or a custom step) automates reports and halts on violations. Clarify and ModelMonitor are not designed for pre-training data quality."
  },
  {
    "id": "d07403e866bfdc0bef9c1445e3c579d6b6e6fc80a3aca636a12fc18d9a16d7a6",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A retail company needs to deploy a deep learning model for product image classification to an endpoint. They expect highly variable traffic: peak of 2000 requests per minute during sales and near zero at night. Latency must stay under 200ms. Cost minimization is critical. Which deployment infrastructure meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy the model on a real-time SageMaker endpoint with two ml.p3.2xlarge instances and configure target tracking auto scaling based on CPU utilization.",
      "B": "Use a SageMaker serverless inference endpoint with configurable concurrency limits to scale automatically and only incur cost when the model processes requests.",
      "C": "Deploy a multi-model SageMaker endpoint on ml.m5.xlarge instances behind an Application Load Balancer and scale manually.",
      "D": "Containerize the model on ECS Fargate with fixed CPU and memory settings and use CloudWatch alarms to trigger CloudFormation stack updates at scale events."
    },
    "explanation": "Serverless endpoints automatically scale down to zero between peaks, minimizing cost, and handle variable load with low latency under SageMaker serverless SLA."
  },
  {
    "id": "1a535201be4c4d5d69ef74de77426a8d3f0b181531010f1eb8259c94963d1184",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An IoT startup must deploy a computer vision model at the edge device that has limited compute (ARM CPU) and must process frames at 15 FPS. The model is currently a standard PyTorch model requiring a GPU. They need minimal code changes. Which infrastructure choice satisfies requirements?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy the PyTorch model to an EC2 g4dn.xlarge instance and have the edge device stream images to it for inference.",
      "B": "Use SageMaker real-time GPU endpoint with a Lambda function forwarding requests from edge device.",
      "C": "Convert the PyTorch model to TensorFlow and use SageMaker Neo to compile for ARM, deploy on AWS IoT Greengrass core.",
      "D": "Use SageMaker Neo to compile the PyTorch model for ARM architecture and deploy the optimized model on AWS IoT Greengrass core."
    },
    "explanation": "SageMaker Neo supports PyTorch, compiles to ARM, and Greengrass core runs inference on device, meeting FPS and minimal code change."
  },
  {
    "id": "be56314e00c22271c2ec0320d4e36091f583d02a88d30f2baaa4dc3ffd57287c",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A financial analytics platform requires batch inference on 10 TB of historical data weekly. The job takes too long on EC2 instances using a custom endpoint. They want to reduce run time and maintenance. Which infrastructure change is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch to SageMaker Batch Transform with optimized ML instances and spot instances for cost and performance.",
      "B": "Convert the batch job into a real-time endpoint and stream data through it.",
      "C": "Deploy the model as a Lambda function and invoke in parallel using Step Functions Express Workflows.",
      "D": "Use ECS Fargate tasks to run the custom container in parallel across multiple tasks."
    },
    "explanation": "SageMaker Batch Transform is optimized for large-scale offline inference, supports spot instances, and manages infrastructure automatically."
  },
  {
    "id": "6622596442ed1495da9d9f28d25e24a3199632506a2c71d9fafda8ce080a6074",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A healthcare application must deploy a fraud-detection model requiring compliance isolation. The model must serve 500 TPS with latency <50ms. Deployment must reside in a VPC with no internet access. Which infrastructure choice is most appropriate?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Deploy a serverless SageMaker endpoint and attach a VPC endpoint for network isolation.",
      "B": "Use ECS Fargate on a private subnet with an Application Load Balancer in the VPC.",
      "C": "Deploy a real-time SageMaker endpoint in the customer VPC with VPC-only mode and provision ml.c5.2xlarge instances behind an internal ALB.",
      "D": "Deploy a multi-model endpoint on SageMaker using ml.m5.large instances with VPC mode enabled."
    },
    "explanation": "Real-time endpoints in VPC-only mode on ml.c5.2xlarge satisfy high TPS, low latency, and strict isolation; multi-model ml.m5.large cannot handle 500 TPS."
  },
  {
    "id": "78c64e84be59177f01f055e30b96626cbfe4184fb351997341b4fe94271b19ef",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An e-commerce site uses a text classification model and expects constant moderate load (100 requests/sec). They want lowest latency and minimal infrastructure management. Cost sensitivity is medium. Which deployment type is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy a multi-model SageMaker real-time endpoint on an ml.c5.large instance and preload the model for minimal inference latency.",
      "B": "Use a SageMaker serverless endpoint to scale automatically with moderate management.",
      "C": "Deploy as a Lambda function behind API Gateway to avoid managing servers.",
      "D": "Containerize the model on ECS with Fargate and auto scale."
    },
    "explanation": "Multi-model endpoints reduce cold-start latency by preloading, minimize cost by serving multiple models on same instances, and provide lowest latency."
  },
  {
    "id": "ffed557b4584cfea4089bad42f58f38f7a704b241cf691de23b8bb8bf84e66d4",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A media analytics company has deployed an object-detection model on SageMaker real-time endpoints. They notice high idle time cost overnight. Available traffic is zero after business hours. They need to reduce cost without affecting daytime performance. Which solution achieves this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch to a larger instance and use downscaling to one instance overnight.",
      "B": "Use a multi-container endpoint and move the model to a cheaper container.",
      "C": "Configure a scheduled CloudWatch event to delete and redeploy the endpoint daily.",
      "D": "Migrate to a serverless inference endpoint that scales to zero at night and autos-scales during the day."
    },
    "explanation": "Serverless inference endpoints scale to zero when idle and up during demand automatically, minimizing idle costs."
  },
  {
    "id": "031be56c55e1d3d707815f47df15cc3c225508fba1d4254651a63749c9785a7e",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A biotech firm needs to deploy a variant of their genomic model for two customer segments. Both share 80% of code, but differ in last layer. They want cost-effective multi-tenancy with strict data isolation. Which deployment strategy should they choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy two separate real-time endpoints on dedicated instances for each segment.",
      "B": "Use a multi-model endpoint with separate containers, one for base model and two variants, and IAM policies for isolation.",
      "C": "Deploy a single container with conditional logic to switch layers at runtime.",
      "D": "Use SageMaker serverless inference and deploy two endpoints sharing the same container."
    },
    "explanation": "Multi-model endpoints host multiple containers on same instances, reducing cost, while IAM and container separation ensure data isolation."
  },
  {
    "id": "577b7c8b914ca7f59aadc5699ee0b996e2f12c47c317a019005386bf42f7fb94",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An autonomous vehicle company needs to deploy their sensor fusion model on GPU clusters with InfiniBand to meet <20ms latency. Which SageMaker infrastructure meets these requirements?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Use SageMaker CPU instances in a cluster with Enhanced Networking.",
      "B": "Deploy on ECS GPU tasks with ENA-enabled instances.",
      "C": "Deploy a SageMaker real-time endpoint on ml.p4d.24xlarge instances with Elastic Fabric Adapter (EFA) for high-performance networking.",
      "D": "Use EC2 p2.xlarge instances and manage GPUs manually."
    },
    "explanation": "ml.p4d.24xlarge with EFA supports InfiniBand-like throughput and low-latency networking required for sensor fusion."
  },
  {
    "id": "9612f00e2b054ff49dc8bb03d6de8808ad02bf40cf184894b530ed9ecca1108f",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A startup needs to deploy a model that uses proprietary libraries not supported by SageMaker built-in containers. They require both batch and real-time inference with minimal custom ops. Which deployment approach is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Build a custom Docker container with the libraries, store in ECR, and deploy to SageMaker real-time and Batch Transform endpoints.",
      "B": "Use a Lambda function with layers containing the custom libraries.",
      "C": "Deploy on ECS Fargate with scheduled tasks for batch and ALB for real-time.",
      "D": "Migrate model to supported framework to use built-in SageMaker containers."
    },
    "explanation": "Custom container in ECR lets you include proprietary libraries and deploy to both real-time and batch endpoints with minimal overhead."
  },
  {
    "id": "6fffd064d440e07a581c50fa278715449f450ae8fb957a555e005847a753ebe0",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A financial trading firm requires synchronous inference latency under 5ms. They want maximum CPU throughput and cannot risk cold starts. They also want to optimize for cost. Which combination should they choose?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "SageMaker serverless endpoint with provisioned concurrency of 100.",
      "B": "Provision a real-time multi-model endpoint on bare-metal ml.inf1.24xlarge Inf1 instances with model pinned in memory.",
      "C": "Deploy as Lambda functions with provisioned concurrency of 200.",
      "D": "Use ECS Fargate GPU tasks with pre-warmed containers behind an ALB."
    },
    "explanation": "Inf1 bare-metal reduces virtualization overhead, multi-model pins model, real-time endpoint avoids cold starts, maximizing throughput and low latency."
  },
  {
    "id": "55eef3dbcd36b6b4ec2365c761c65552b2bd56147cc64198d897255e297ff2d6",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A gaming company wants to deploy multiple language translation models for different regions. Total model memory footprint exceeds instance GPU memory. They need low-latency translation. Which infra is most appropriate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy all models on one GPU EC2 instance using a custom orchestration layer.",
      "B": "Use SageMaker serverless endpoints sequentially loading each model.",
      "C": "Deploy each model on separate real-time GPU endpoints using ml.g4dn.xlarge instances and route traffic via Application Load Balancer.",
      "D": "Consolidate models into a single large multi-container endpoint on ml.p3.2xlarge."
    },
    "explanation": "Separate endpoints avoid GPU memory oversubscription, ALB routes based on region, simple scaling per model."
  },
  {
    "id": "72a54de30a17965c338606319821a8e912f2621ed1a72d631f5f234e4e101a7b",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A social media platform requires asynchronous inference for user sentiment analysis. They need to process spikes of 100k requests hourly without provisioning servers. Which deployment infrastructure is best?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy a real-time SageMaker endpoint and enqueue requests in SQS.",
      "B": "Use a Lambda function triggered by Step Functions that calls the real-time endpoint.",
      "C": "Deploy a multi-model endpoint and use Kinesis Data Streams.",
      "D": "Use SageMaker asynchronous inference endpoints that scale storage and compute and store results in S3."
    },
    "explanation": "Asynchronous inference endpoints scale automatically for batch requests, decouple compute via S3, and handle spikes without server provisioning."
  },
  {
    "id": "a6a7c104a4d9570a6c0448df555e6bd724ba0fb79f0d9c51d3e4fee82499bdf8",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A media streaming company experiments with video frame classification. They need to deploy a model where inference takes >5s per image. They want to avoid request timeouts and handle 100 concurrent jobs. What deployment pattern should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a real-time endpoint with high timeout settings.",
      "B": "Deploy in ECS Fargate with long-running tasks.",
      "C": "Use SageMaker asynchronous inference with callback duration set, storing results in S3, polled by the application.",
      "D": "Break the model into microservices and chain Lambda functions."
    },
    "explanation": "Asynchronous endpoints handle long per-request durations without timing out and support concurrency by queueing requests."
  },
  {
    "id": "b3de49f8bc6c7ec6a8de2c299cc3ff35ed0d29e9cd4e5b1c3cd3f4ea94e3b859",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An enterprise mandates use of automated model rollback on failure. They deploy via SageMaker endpoints. Which infrastructure configuration supports blue/green deployments with rollback?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker endpoint variants with traffic shifting in a blue/green deployment orchestrated by CodeDeploy integration.",
      "B": "Deploy two separate endpoints and use Route53 weighted routing to shift traffic manually.",
      "C": "Use ECS Fargate with CodePipeline and manual rollback stage.",
      "D": "Use Lambda functions with version aliases and weight-based alias shifts."
    },
    "explanation": "SageMaker with CodeDeploy supports blue/green endpoint deployments and automatic rollback on failure."
  },
  {
    "id": "918fa53d010e29f538c10ac920cc827cf1ed885ec44b769413099b5056f6d389",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A company must serve ML inferences to internal clients only. They want cost-effective scaling and restrict internet access. They cannot use serverless endpoints. What should they deploy?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Public SageMaker real-time endpoint with IP whitelist.",
      "B": "ECS Fargate tasks in public subnets with security groups.",
      "C": "EC2 GPU instances behind a public ALB with restricted security group.",
      "D": "Private SageMaker real-time endpoint in VPC-only mode with endpoint accessed via VPC interface endpoint."
    },
    "explanation": "Private VPC-only SageMaker endpoints and interface VPC endpoints allow internal-only access and autoscaling without serverless usage."
  },
  {
    "id": "faf216363af8d048c72391d5b7c2f72f1bd0ccd9534b44ed021ee2b16893e1b3",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A research team needs to test multiple model versions in parallel under identical traffic to compare performance in production. Which deployment technique supports this with minimal infrastructure overhead?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy each model version on separate real-time endpoints and use ALB.",
      "B": "Use SageMaker multi-variant endpoint with two production variants and traffic weight splitting.",
      "C": "Deploy models as Lambda aliases and shift weights via AWS CLI.",
      "D": "Use ECS Fargate tasks and update task definitions to switch traffic."
    },
    "explanation": "SageMaker variant endpoint supports two production variants for A/B testing with traffic weight control and shared infrastructure."
  },
  {
    "id": "1bc3dbb73e67c1705197b1f51cbde6c4a330cba357b8375cf1e8523ef71acc0c",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An NLP startup uses foundation models from Amazon Bedrock that require fine-tuning at deploy time. They need to serve personalized chat sessions with low latency. Which SageMaker infrastructure allows direct integration with Bedrock is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker JumpStart endpoints with Bedrock models fine-tuned via SageMaker Script Mode and deploy real-time endpoint.",
      "B": "Deploy Bedrock model directly in a Lambda behind API Gateway.",
      "C": "Use ECS Fargate with Hugging Face containers mounting a Bedrock endpoint.",
      "D": "Use EC2 instances calling Bedrock via SDK."
    },
    "explanation": "SageMaker JumpStart integrates with Bedrock models, supports fine-tuning script mode, and deploys as real-time endpoints with low latency."
  },
  {
    "id": "c6c749b8f4a09213869cbf368e7e58e99df69b7d72d3e5e49b6610ffbc3cbb0f",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A logistics firm needs to host an ensemble of three models for package routing. They want minimal end-to-end latency combining inferences. How should they deploy?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy three separate endpoints and aggregate results in Lambda.",
      "B": "Bundle all models into one container with a microservice orchestration.",
      "C": "Use a SageMaker multi-container endpoint with each model in its container and an aggregator container.",
      "D": "Deploy Ensemble as Batch Transform jobs chained by Step Functions."
    },
    "explanation": "Multi-container endpoints allow co-located containers; a dedicated aggregator container can combine outputs with low latency."
  },
  {
    "id": "0ec4ccdf00f0226ecc967cc36996cee203646d47f8b260729e544daafce62270",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A company has sensitive images requiring inference on-premises behind a firewall. They want to use SageMaker-managed infrastructure. Which deployment architecture meets requirements?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Use SageMaker local mode on EC2 inside their VPC.",
      "B": "Use SageMaker real-time endpoint with VPC peering to on-premises network.",
      "C": "Deploy on AWS Outposts with SageMaker studio.",
      "D": "Use SageMaker Inference on AWS Outposts rack inside their data center sandbox."
    },
    "explanation": "SageMaker Inference on Outposts lets you run endpoints on Outposts hardware on-premises with SageMaker management."
  },
  {
    "id": "097e0d025e87e12d86331892f28f880e6cef7f9e076e43e8a5fedbca81986419",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A public API uses a multi-language model that loads slowly and exceeds response time if loaded on each invoke. Cold starts hurt UX. They need <50ms per request. Which deployment pattern addresses this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable provisioned concurrency on Lambda with large memory to preload model.",
      "B": "Deploy as SageMaker real-time endpoint on ml.c5.xlarge and maintain warm instances.",
      "C": "Use ECS Fargate tasks with pre-warmed containers behind ALB.",
      "D": "Use serverless inference with reserved concurrency."
    },
    "explanation": "Real-time endpoints keep models loaded in memory, ensuring consistent sub-50ms latency without cold starts."
  },
  {
    "id": "9f9c2b92060ff0ee76d2d4510408d1ec7e4bbb453b28d7954d71c5ec8ada4245",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A company needs to deploy a time-series forecasting model that runs monthly and takes hours to run. They want minimal infra management and integration with existing SageMaker pipelines. Which deployment infra is appropriate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy a real-time endpoint and trigger monthly inferences.",
      "B": "Use ECS Fargate tasks triggered by EventBridge.",
      "C": "Use SageMaker Batch Transform job within SageMaker Pipeline step.",
      "D": "Build a Lambda to load model and process data stored in S3."
    },
    "explanation": "Batch Transform jobs integrate directly in SageMaker Pipelines, manage infra, and suit long-running monthly jobs."
  },
  {
    "id": "581ee008832d90e582eb51b7491d67f00001855c6d8a802769520d37be614667",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A SaaS vendor must deploy a model to multiple customer VPCs automatically during onboarding. They need IaC to manage endpoints. Which approach is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS CDK to define a construct for SageMaker endpoint and instantiate stacks per customer VPC.",
      "B": "Manually use CloudFormation templates to create endpoints per account.",
      "C": "Use Terraform modules outside AWS CDK for endpoints.",
      "D": "Use CLI scripts to provision endpoints in each account."
    },
    "explanation": "AWS CDK enables programmatic multi-account, multi-VPC deployments of SageMaker endpoints with constructs and automatic context."
  },
  {
    "id": "b3f8ceb80c48634d572af4882ef5748dd663a4377fa66d3b10f7696d8c507634",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A fintech app uses Spark processing to call an inference endpoint for real-time risk scoring at scale. They want to reduce network overhead per Spark executor. Which deployment option reduces egress latency?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy endpoint in public subnets close to EMR cluster.",
      "B": "Use ECS Fargate tasks as inference cluster next to EMR.",
      "C": "Call endpoint via API Gateway to reduce hops.",
      "D": "Deploy SageMaker real-time endpoint in same VPC and subnet as EMR and use direct VPC endpoint integration."
    },
    "explanation": "Co-locating endpoint in same subnet and using VPC endpoint reduces network hops and egress latency for Spark executors."
  },
  {
    "id": "76ee68a1fc029f1916938eab5228fa44ecea5fdbc1383cef298ea7ce55d7beaf",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An advertiser needs to run real-time bidding inference under unpredictable peak loads up to 100k TPS with <5ms latency. GPU acceleration is needed. Which infrastructure is most suitable?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "Use SageMaker real-time endpoint on GPU ml.g5 instances with auto scaling.",
      "B": "Use SageMaker multi-model real-time endpoint on Inf1 instances with high concurrency and onboard GPU models into single endpoint.",
      "C": "Deploy ECS GPU Fargate tasks behind ALB with auto scaling ECS.",
      "D": "Use Spot Instances in Batch Transform with Lambda to emulate real-time."
    },
    "explanation": "Inf1 instances are designed for high-concurrency real-time inference at low latency and can host multiple models concurrently."
  },
  {
    "id": "6dac56220d2c3811d93b013eac1b788fd153b9604af2ef876089aaff42685f76",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A biotech startup wants to test a prototype model without incurring provisioned instance cost. They need quick feedback with minimal latency (<100ms) at low volume (<10 TPS). Which deployment option is ideal?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Deploy to an ml.t3.medium real-time endpoint.",
      "B": "Use ECS Fargate with minimal CPU.",
      "C": "Use SageMaker serverless inference endpoint with low concurrency and pay-per-request.",
      "D": "Use Lambda with provisioned concurrency of 1."
    },
    "explanation": "Serverless inference lets you pay per request for low-volume prototypes with acceptable latency and no instance provisioning."
  },
  {
    "id": "1dacf216dc3ba01d8c277a56d68a76f7182a598551293dd78ec11177c51ee095",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A customer has strict data residency; inference must occur in EU-West-1. They have global traffic. To reduce latency, they want multi-region endpoints. How should they implement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy SageMaker real-time endpoints in EU-West-1 and US-East-1, and route via Route 53 latency-based routing.",
      "B": "Deploy endpoints in EU-West-1 only and use CloudFront for global clients.",
      "C": "Use a global Application Load Balancer spanning regions.",
      "D": "Deploy serverless endpoints in EU-West-1 and Asia-Pacific, relying on client-side region selection."
    },
    "explanation": "Latency-based routing with Route 53 directs clients to nearest regional real-time endpoints while respecting data residency."
  },
  {
    "id": "44ea6ceee687269c6b3ca6f62399356747ec164bc0f9d7cfd957d3acadf09f48",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A gaming platform requires under-50ms inference on user actions. They host game servers in Kubernetes on EKS. They prefer colocation of inference. What deployment infra integrates best?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker real-time endpoints and call from EKS pods.",
      "B": "Use Lambda functions within VPC for inference.",
      "C": "Deploy ECS Fargate tasks in same VPC.",
      "D": "Use SageMaker Inference using EKS turret by deploying a SageMaker Inference endpoint onto the existing EKS cluster via the SageMaker Operators for Kubernetes."
    },
    "explanation": "SageMaker Operators lets you host real-time inference pods inside EKS cluster, co-located with game servers for low latency."
  },
  {
    "id": "2a8e41d193fd47f59fbebefe822d12c6a026bbef31c55800404f74a8555717cf",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An ML team deployed a model on a shared SageMaker endpoint used by multiple teams. One model version affects another\u2019s latency. They need isolation. What is the best approach?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase instance count to isolate workloads.",
      "B": "Deploy each model as a separate SageMaker endpoint with dedicated instances.",
      "C": "Use multi-model endpoint with separate container per team.",
      "D": "Use ECS Fargate to isolate container resources per team."
    },
    "explanation": "Separate endpoints provide strict compute isolation, preventing one model\u2019s load from impacting another."
  },
  {
    "id": "0fbe7b1f3b7703150890bdcfd13bbd2b93042f38f19b933a27f22b744bbf3b75",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A data scientist is fine-tuning a large Transformer model on SageMaker JumpStart with limited GPU memory. They observe training failure due to out-of-memory errors. Which combination of techniques will most effectively reduce memory usage while preserving model accuracy?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Decrease batch size and disable gradient checkpointing to save memory.",
      "B": "Increase instance type to GPU with more memory and remove dropout regularization.",
      "C": "Enable mixed precision training and gradient checkpointing to reduce memory footprint.",
      "D": "Switch to a smaller pre-trained model and increase learning rate to converge faster."
    },
    "explanation": "Mixed precision reduces memory per tensor, and gradient checkpointing trades compute for memory by storing fewer activations, preserving accuracy while fitting in memory."
  },
  {
    "id": "5b2934e6dacc3255cd337a544a62327e77c3d3219af744f2e284e9df15536d4e",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A SageMaker automatic model tuning (AMT) job using Bayesian optimization is not converging to better results after 20 trials. The objective is to maximize validation AUC for a binary classifier. Which action is most likely to improve the tuning efficiency?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch the tuner from Bayesian optimization to random search to explore hyperparameter space more broadly.",
      "B": "Narrow the hyperparameter search ranges around the best performing values and continue tuning.",
      "C": "Increase the maximum number of training jobs without adjusting the search ranges.",
      "D": "Change the objective metric to validation accuracy instead of AUC."
    },
    "explanation": "Refining search ranges focuses Bayesian optimization around promising regions, improving convergence efficiency without diluting search."
  },
  {
    "id": "5099a7f16dcbd44af6cdba9cef920048f17dca4fc6190594fa04e79d348a2094",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "During distributed training on SageMaker with Horovod, the training job stalls indefinitely at initialization. Logs indicate parameter server connection timeouts. Which configuration change should resolve this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable TCP keepalive and increase ssh_timeouts in the Horovod configuration.",
      "B": "Switch from Horovod to SageMaker\u2019s built-in data parallel library without changing network settings.",
      "C": "Reduce number of workers to one node to eliminate networking issues.",
      "D": "Increase the training batch size to reduce communication frequency."
    },
    "explanation": "Enabling TCP keepalive and adjusting timeouts prevents idle connection drops during Horovod rendezvous, resolving stalls."
  },
  {
    "id": "6491377b03f3ea9eb1c651e9d9e709945270e668e957fa8b8747473c43f2d6e2",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A convolutional neural network trained via SageMaker script mode exhibits training instability when increasing batch size from 32 to 256. The learning rate was left at 0.001. Which adjustment best maintains convergence behaviour?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Decrease the number of epochs proportionally to batch size.",
      "B": "Add weight decay to the optimizer.",
      "C": "Switch optimizer from Adam to SGD.",
      "D": "Scale the learning rate linearly to 0.008 following the batch size increase rule."
    },
    "explanation": "Linear scaling rule suggests increasing learning rate proportional to batch size to maintain gradient variance and stable convergence."
  },
  {
    "id": "5d2e1f3b338dd2d1e2a484342b86e907ac31228ec06858b707984771e5817429",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A tree-based model in SageMaker automatic model tuning shows minimal improvement beyond 50 training jobs. The tuner used random search over max_depth\u2208[3,15] and min_child_weight\u2208[1,10]. What is the most effective next step?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Change the random search strategy to Bayesian optimization for the same ranges.",
      "B": "Narrow max_depth to [3,7] and extend min_child_weight to [0,20] based on observed patterns.",
      "C": "Increase training jobs from 50 to 500 without changing ranges.",
      "D": "Switch to a neural network algorithm to explore different model families."
    },
    "explanation": "Observing diminishing returns suggests narrowing depth to shallower trees and expanding child weight to regulate complexity, improving tuner focus."
  },
  {
    "id": "e3278f12c971a061585e4f2cc241df3721e84747c70fec74152ae2f9b42b3d35",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A data scientist uses SageMaker hyperparameter tuning for a deep learning model and sets EarlyStoppingType to \"Auto\". Training jobs still run to full resource allocation before stopping. Why?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Auto early stopping only applies to built-in algorithms, not custom script mode.",
      "B": "The objective metric isn\u2019t logged frequently enough for the tuner to evaluate early stopping.",
      "C": "The tuning job\u2019s EarlyStoppingPolicy wasn\u2019t enabled; Auto setting requires explicit enablement in the tuner.",
      "D": "Auto early stopping only stops the entire tuning job, not individual training jobs."
    },
    "explanation": "In AMT, early stopping must be enabled in the tuner configuration; setting EarlyStoppingType alone doesn\u2019t apply the policy."
  },
  {
    "id": "eef40b093bfd851604daebad8549788ee6e54004159a0e50a07179fe26368246",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A model exhibits underfitting: both training and validation losses are high. The data scientist suspects insufficient model capacity. Which approach should they take?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase model size by adding layers or units and tune learning rate.",
      "B": "Reduce dropout rate to zero and add L1 regularization.",
      "C": "Decrease number of features via PCA to simplify data.",
      "D": "Increase batch size to stabilize training and reduce noise."
    },
    "explanation": "Underfitting often requires increasing model capacity; adding layers or units addresses capacity issues, then tuning learning rate ensures convergence."
  },
  {
    "id": "170d24044c14f6034f1c241ec55a60a1516e839b9fc3123cc68ffa6c25f91eab",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "While training a PyTorch model in SageMaker script mode, gradient norms explode after epoch 2. Which change will most directly mitigate this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch optimizer from Adam to RMSprop to stabilise gradients.",
      "B": "Apply gradient clipping by setting max_grad_norm in the training script.",
      "C": "Increase batch size to average out noisy gradients.",
      "D": "Add dropout layers to the architecture."
    },
    "explanation": "Gradient clipping directly limits gradient norms, preventing explosion without changing model dynamics or data batching."
  },
  {
    "id": "97f55b47bf492591bbc7d336b8618b417ea41b4bf6bd90ed0a7ac69e22252856",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A pre-trained image classification model deployed via SageMaker JumpStart needs customization for a new 10-class dataset. Which process correctly fine-tunes this model with minimal training time?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Retrain the entire model on new dataset with original learning rate.",
      "B": "Replace final layer and train only that layer with a high learning rate.",
      "C": "Freeze all convolutional layers and train all fully connected layers with a low learning rate.",
      "D": "Freeze base model parameters and fine-tune the final layers with a reduced learning rate."
    },
    "explanation": "Freezing base parameters preserves learned features and reduces training time, while a reduced learning rate avoids large parameter updates."
  },
  {
    "id": "da1b5903d184e98385a9824c2e4e121418c60cd1c7a653e06d49b28b44ecd719",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "During automatic model tuning, a candidate hyperparameter combination crashes training jobs intermittently on one instance type. How should the data scientist handle this to continue tuning?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Remove the crashing hyperparameter combination from the search space manually.",
      "B": "Configure the tuning job\u2019s RetryPolicies to retry failed jobs and exclude fatal errors from stopping the tuner.",
      "C": "Switch to a larger instance type for all tuning jobs.",
      "D": "Reduce the overall max jobs count so the job completes quickly before errors accumulate."
    },
    "explanation": "Using RetryPolicies allows retries for transient failures and prevents rare fatal errors from halting the tuning job."
  },
  {
    "id": "8c8a415184eba336937915dea7c3435eb3df216d2a752e2287fcb96600a0d637",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A model\u2019s training time per epoch is excessive due to data preprocessing in the training loop. Which strategy will speed up training without altering model architecture?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Move preprocessing to a SageMaker Processing Job and store transformed data in S3 for training.",
      "B": "Use larger batch sizes to reduce number of loops.",
      "C": "Switch from Python data loader to a pure-Python implementation.",
      "D": "Increase number of epochs to amortize preprocessing overhead."
    },
    "explanation": "Offloading preprocessing to a separate job avoids repeated computation per epoch, allowing training on preprocessed data."
  },
  {
    "id": "701c4478834c857bcb82834203782acb102d7b2b4dbaa86793ac260f4febee36",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A neural network in SageMaker Model Registry has multiple versions. The team wants reproducible experiments including exact training environments. Which Registry feature enables this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model approval status tags.",
      "B": "Inference container logs.",
      "C": "Model package groups with associated Docker image URIs and training artifacts.",
      "D": "Endpoint deployment timestamps."
    },
    "explanation": "Model package groups record container URIs and artifacts, ensuring reproducible environments for each version."
  },
  {
    "id": "3db44a16bc2b0652a6a06aed552891066fdbfb4e8aef034729f6a56f888b43bd",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A data scientist uses SageMaker automatic model tuning with Bayesian optimization. The prior distributions are too narrow, causing search to get stuck in local minima. What change addresses this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase number of initial random trials before Bayesian sampling.",
      "B": "Widen the prior distribution ranges to cover more hyperparameter space.",
      "C": "Switch to random search strategy entirely.",
      "D": "Decrease objective metric evaluation frequency."
    },
    "explanation": "Widening priors allows the optimizer to explore a broader search space and escape local minima."
  },
  {
    "id": "ced5ee55e9676dc36f3ac92da8fdba6df69b357aae6c147ae6ec59982232aa29",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An LSTM model trained via SageMaker script mode shows overfitting after 10 epochs. The data scientist wants to regularize without reducing model capacity. Which approach is most appropriate?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Decrease number of timesteps to shorten sequences.",
      "B": "Increase batch size to reduce gradient noise.",
      "C": "Add L1 regularization to input weights only.",
      "D": "Add recurrent dropout to LSTM layers for temporal regularization."
    },
    "explanation": "Recurrent dropout applies dropout to recurrent connections, regularizing LSTM without reducing capacity."
  },
  {
    "id": "aadc8424ddaa45a6198fb80e67c043de67d9f68987cbb14d7683069150cebdb2",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A classification model in SageMaker automatically stops improving validation accuracy after 5 epochs. Which tuner configuration ensures the rest of the hyperparameter combinations execute?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable warm start with type IDENTICAL_DATA_AND_ALGORITHM and set max jobs.",
      "B": "Disable early stopping entirely to run all combinations to completion.",
      "C": "Set strategy to GRID_SEARCH to systematically cover all combinations.",
      "D": "Use early stopping with conservatism parameter set to high."
    },
    "explanation": "Warm start IDENTICAL allows reusing completed trials but still runs other configurations; disabling early stopping risks wasted compute."
  },
  {
    "id": "f9b0eab10e0487d0b846ffb40cc93cc06b9fdd0d1a59c6742e09d8a38d2550c6",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A distributed TensorFlow training job on SageMaker shows poor scaling beyond 4 GPU nodes; network I/O saturates. Which modification best improves throughput?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase batch size further to reduce communication frequency.",
      "B": "Switch to Horovod\u2019s ring-allreduce algorithm.",
      "C": "Enable SageMaker\u2019s built-in NCCL backend with GPU-enabled instances.",
      "D": "Use Python-based parameter server implementation."
    },
    "explanation": "NCCL provides high-performance multi-GPU communication optimized for GPU clusters, alleviating I/O bottlenecks."
  },
  {
    "id": "e87fe44d1af43fe52b66936cc2f13cb7a933a3718b9715bba43ebb0549570a6a",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A PyTorch model fine-tuned on SageMaker exhibits catastrophic forgetting on original classes. Which technique best prevents forgetting while learning new classes?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a higher learning rate for new classes to quickly adapt.",
      "B": "Apply Elastic Weight Consolidation (EWC) to penalize deviation from original weights.",
      "C": "Freeze all layers except the last and train only output layer.",
      "D": "Increase dropout to 0.9 to regularize adaptation."
    },
    "explanation": "EWC constrains updates for important weights, preserving original task performance while fine-tuning on new tasks."
  },
  {
    "id": "7626cddccd2048207b1dfe72ea1805eda13e8c7105336752ded8c87f6492c664",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A SageMaker hyperparameter tuning job uses RANDOM strategy. The search space contains complex interdependent hyperparameters. Which strategy provides more efficient search?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Switch to GRID_SEARCH to cover all combinations exhaustively.",
      "B": "Manually sample correlated hyperparameters outside the tuner.",
      "C": "Reduce hyperparameter count by fixing least impactful ones.",
      "D": "Use BAYESIAN optimization to model dependencies and guide search."
    },
    "explanation": "Bayesian optimization learns relationships between hyperparameters, efficiently handling interdependencies compared to random search."
  },
  {
    "id": "ecc11a8aeb228573b0ec85b8777501d77e7802644590ac5643c7dccc3cf963ca",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A model trained with script mode on SageMaker takes too long to converge. Which two changes combined will most effectively reduce training time and maintain accuracy?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase batch size and disable checkpointing.",
      "B": "Enable mixed precision training and apply early stopping based on validation loss.",
      "C": "Switch optimizer to SGD and decrease learning rate.",
      "D": "Use smaller instance type and reduce number of epochs."
    },
    "explanation": "Mixed precision speeds up training; early stopping prevents wasteful epochs after convergence, preserving accuracy."
  },
  {
    "id": "c5a2cc94c1fb939614e5ab8cf79d24f5ac8eb3b9247d185e5bdfef56c132f94e",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A data scientist wants to use SageMaker Automatic Model Tuning to optimize both learning_rate and batch_size. They observe that large batch sizes degrade generalization. Which approach addresses this trade-off during tuning?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Fix batch_size and tune only learning_rate.",
      "B": "Increase search range of both parameters.",
      "C": "Tune batch_size, then in a second tuner fix optimal batch_size and tune learning_rate.",
      "D": "Use a single tuner to optimize sum of learning_rate and batch_size."
    },
    "explanation": "Sequential tuning isolates the batch size effect, then optimizes learning rate for the chosen batch size, handling trade-offs effectively."
  },
  {
    "id": "b017ec9ba9ba9bd2c5ae95acdee47a88600806c75aeac70587dd500a3aa82491",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A model\u2019s F1 score plateaus despite varying hyperparameters. The data scientist suspects feature scaling issues. Which pipeline change is most likely to enable further improvement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Incorporate standardization of numeric features before training in script mode.",
      "B": "Add more dropout layers to reduce variance.",
      "C": "Increase depth of decision trees in ensemble.",
      "D": "Switch to a different activation function in final layer."
    },
    "explanation": "Unscaled numeric features can hamper optimization; standardization often leads to better convergence and metric improvements."
  },
  {
    "id": "b0e652f1334ee7ed2bf7025132b52e3d56b7d98416d6a08b5bbf82d45126ed42",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A SageMaker training job fails due to size of input data exceeding EBS volume. Which solution allows training with minimal code changes?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Switch to processing job and write custom code for streaming input.",
      "B": "Split dataset into smaller files and retrain multiple times.",
      "C": "Modify training code to stream data from S3 in chunks.",
      "D": "Mount Amazon FSx for Lustre to the training instance for high-throughput, large dataset support."
    },
    "explanation": "FSx for Lustre integrates transparently as a filesystem, handling large datasets with minimal code changes."
  },
  {
    "id": "a63d9c6c381751d21ac42407485781f9b077d3cda6f00148b1b3ec24c998b1ba",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "When tuning a neural network\u2019s hyperparameters, the data scientist sets EarlyStoppingType to \"Off\" but still sees early termination of some trials. What explains this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Some built-in algorithms override tuner settings to always early stop.",
      "B": "The SageMaker training script configured its own early stopping callback independent of tuner.",
      "C": "Early stopping only applies when strategy is BAYESIAN.",
      "D": "The objective metric threshold was reached causing automatic stop."
    },
    "explanation": "Custom scripts can implement callbacks (e.g., Keras EarlyStopping) causing training jobs to stop independently of tuner configuration."
  },
  {
    "id": "d43ccf96543f2427f6d307410b85eacf1ec748676365f450583d027d8d7992cc",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A data scientist wants to reduce a model\u2019s memory footprint from 2 GB to under 500 MB for edge deployment. Which two techniques achieve this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Apply post-training static quantization and weight pruning.",
      "B": "Increase dropout to 0.9 and retrain the model.",
      "C": "Use larger batch normalization momentum and lower precision.",
      "D": "Deploy a smaller model architecture without adjustments."
    },
    "explanation": "Static quantization reduces weight bit widths; pruning removes redundant weights, both significantly shrinking model size."
  },
  {
    "id": "1c1210a13b694169299b3393b5ef2e7bbc83618f57c26fa8557c294f36984154",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A SageMaker hyperparameter tuning job\u2019s best candidate uses a combination of hyperparameters untested in grid search due to non-grid ranges. Which tuning strategy should the data scientist have chosen?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use RANDOM search over grid values.",
      "B": "Use GRID search with finer discretization.",
      "C": "Manually evaluate the unseen combination.",
      "D": "Leverage BAYESIAN search to explore continuous hyperparameter space."
    },
    "explanation": "Bayesian search explores continuous spaces beyond specified grid points, identifying combinations not on predefined grid."
  },
  {
    "id": "36d2376eb980fce2e74f6cf986ce0f30fe54e3fc60f32a9ea51509aab456fa4f",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "During multi-model ensembling, a SageMaker pipeline aggregates predictions from three models but sees inferior performance. What ensemble method change is most likely to improve results?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Switch from weighted average to simple average of predictions.",
      "B": "Increase weight of the poorest performing model to diversify errors.",
      "C": "Use stacking with a meta-learner to learn optimal ensemble weights.",
      "D": "Reduce number of models to only the top performer."
    },
    "explanation": "Stacking trains a meta-learner to optimally combine base model outputs, often outperforming fixed averaging."
  },
  {
    "id": "8fff4c936742f575f17a1331f6f2df1822ec03e984998d9c97ccc2d0de83065a",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A deep learning training job on SageMaker script mode uses custom Docker image. It runs slower than built-in containers. Which modification speeds up training in the same image?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch to a smaller base image reducing container size.",
      "B": "Install and configure NVIDIA CUDA and cuDNN optimally inside the container.",
      "C": "Disable container logging to reduce I/O overhead.",
      "D": "Switch optimizer from Adam to Adagrad."
    },
    "explanation": "Optimized CUDA/cuDNN libraries inside the container unlock GPU performance equivalent to built-in images."
  },
  {
    "id": "2140c97de98b6fea3fec55abc02475bb92f62ea60e3bae9213c3a3fc05bd42d4",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A model trained on imbalanced classes shows poor minority recall. The data scientist wants to tune both class_weight and learning_rate. Which SageMaker tuning configuration is most appropriate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Tune only class_weight and manually adjust learning_rate after tuning.",
      "B": "Use GRID search for both parameters with coarse resolution.",
      "C": "Use BAYESIAN optimizer jointly over continuous learning_rate and discrete class_weight.",
      "D": "Perform random search for class_weight and grid search for learning_rate."
    },
    "explanation": "Bayesian optimization can efficiently search mixed continuous and discrete spaces together, optimizing both simultaneously."
  },
  {
    "id": "7486b0ec6f5d90cac8c70d94e576034a492ac8523d9b77e2fee9efb7ef76eaa8",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A CNN model deployed frequently suffers from sudden spikes in validation loss at random epochs. The training script uses Keras with default settings. How should they address this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable shuffling of training data each epoch and add adaptive learning rate scheduler.",
      "B": "Disable dropout layers to stabilize activations.",
      "C": "Use batch normalization only in the first layer.",
      "D": "Increase training tolerance in Keras fit function."
    },
    "explanation": "Shuffling prevents learning order bias; adaptive LR schedulers adjust LR downward when loss spikes, stabilizing training."
  },
  {
    "id": "5fd0c1e668b4a8b05479fd778e9b5080fc48d7c2b54b380b20ddb344217fccf8",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A SageMaker hyperparameter tuning job is set to use warm start of type TRANSFER_LEARNING. It reuses data but is failing to reuse prior checkpoints. What is missing?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "The tuning job must run on the same instance type as prior job.",
      "B": "The HyperparameterTuningJob must reference the previous job\u2019s name in the WarmStartConfig.",
      "C": "Previous job artifacts must be manually copied into new S3 bucket.",
      "D": "Transfer learning warm starts only apply to built-in algorithms."
    },
    "explanation": "Specifying the previous job name in WarmStartConfig lets the tuner locate and reuse prior tuning results."
  },
  {
    "id": "0a8427754a36135fd4899b2d6d6849d17b810e8693257312cc6c284ca48a040a",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A data scientist wants to integrate an external scikit-learn pipeline into SageMaker script mode training. Which step ensures compatibility?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Convert the pipeline to ONNX and use SageMaker\u2019s ONNX container.",
      "B": "Refactor code to remove scikit-learn dependencies.",
      "C": "Wrap the pipeline in a Lambda function called during training.",
      "D": "Include scikit-learn in the training environment and invoke pipeline within the training script.\""
    },
    "explanation": "Bundling scikit-learn in the image and calling the pipeline in the script ensures seamless integration with SageMaker script mode."
  },
  {
    "id": "40f598f45812d60a3f5a7f44fca5dd6f97dea9db74ba62cf992a48c7b9fee49a",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A fintech company must predict customer churn using a dataset of 10 million rows and 50 mixed categorical and numeric features. The model must be highly interpretable to satisfy regulatory audits and be cost-efficient to train. Which modeling approach best meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker XGBoost binary classification with default tree depth and no explainability integration.",
      "B": "Use SageMaker Linear Learner (binary classification) with L1 regularization to produce sparse, interpretable weights.",
      "C": "Build a deep neural network in TensorFlow script mode to capture complex feature interactions.",
      "D": "Use SageMaker Random Cut Forest to detect outliers as churn proxies."
    },
    "explanation": "Linear Learner with L1 yields a sparse, linear model that is interpretable, trainable quickly on large tabular data, and cost-efficient."
  },
  {
    "id": "a5b0d6465c5efb59b6afe1bd6235f76f656baee977580556c5df77391a69d3d6",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "An e-commerce company wants to segment customers into five groups based on browsing behavior without labels. They require a scalable, unsupervised approach that integrates easily with SageMaker Pipelines. Which modeling approach should they select?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Use SageMaker K-means clustering built-in algorithm with k=5.",
      "B": "Use a custom DBSCAN implementation in script mode.",
      "C": "Use SageMaker Random Cut Forest for density-based segmentation.",
      "D": "Use SageMaker XGBoost with k-means objective."
    },
    "explanation": "K-means is the standard scalable built-in algorithm for clustering into a predefined number of segments; integrates natively with SageMaker Pipelines."
  },
  {
    "id": "57f104dd15f31ec87e9299070d89ab0482ba5c3b6a948fd16bf22811891e4310",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A startup needs to classify short customer reviews (under 200 words) into positive, neutral, and negative. They want minimal operational overhead, fast time to market, and are willing to sacrifice some accuracy for ease of use. Which modeling approach best fits?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Train a custom transformer model in PyTorch script mode.",
      "B": "Use SageMaker BlazingText for supervised text classification.",
      "C": "Fine-tune a BERT model in SageMaker JumpStart.",
      "D": "Use Amazon Comprehend sentiment analysis API."
    },
    "explanation": "Amazon Comprehend is fully managed, requires no model training, minimal overhead and rapid deployment at moderate accuracy."
  },
  {
    "id": "b7253ca86090101944fdd63aed549fbc749950a42e4063871894ce770d1bf9d5",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A manufacturing plant monitors sensor streams and wants to detect anomalies in real time. Data volume is high (hundreds of thousands of points per second), and false positives are costly. Which AWS service or algorithm should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a one-class SVM in SageMaker script mode.",
      "B": "Use SageMaker K-means clustering on sliding windows.",
      "C": "Use Amazon Lookout for Equipment anomaly detection service.",
      "D": "Implement a custom LSTM autoencoder in TensorFlow script mode."
    },
    "explanation": "Lookout for Equipment is purpose-built for high-throughput sensor anomaly detection, managed, and tuned to reduce false positives."
  },
  {
    "id": "a4f228685c8e506a928bef08c59fda0f91a94238d2dc492f77f01843021286a0",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A healthcare provider has structured tabular patient data (300K records) to predict disease onset. They require high recall (>90%) and can accept lower interpretability. They want to minimize development time. What is the best modeling approach?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker XGBoost with tree depth tuned via automatic model tuning.",
      "B": "Use SageMaker Linear Learner with L2 regularization.",
      "C": "Build a deep neural network in PyTorch script mode.",
      "D": "Use Amazon Autopilot to generate a pipeline automatically."
    },
    "explanation": "XGBoost balances recall and complexity, trains quickly on tabular data with minimal code; easier than full script mode and more controllable than AutoPilot."
  },
  {
    "id": "7d67de328efa88ff5e40f18ff847d0fefc8b7f13128ef86826de6e136e2181f8",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A business needs real-time translation of customer chat messages from English to Spanish. Latency must be under 100ms per request. Which approach is most appropriate?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Train a custom seq2seq model in SageMaker script mode.",
      "B": "Use Amazon Translate real-time API.",
      "C": "Fine-tune a transformer model in SageMaker JumpStart.",
      "D": "Use AWS Lambda to call SageMaker endpoint hosting a translation model."
    },
    "explanation": "Amazon Translate real-time API provides low-latency translation managed by AWS, meeting the latency SLA."
  },
  {
    "id": "f8662d850f844107bc96d8eb5ce5e5e4cd6a9e4934812246d9781fb0b340a816",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A retail chain wants product recommendations on its website using purchase history. They have sparse user\u2013item interactions. They want a fully managed solution integrated with SageMaker. Which approach should they choose?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Implement an alternating least squares algorithm in Spark on EMR.",
      "B": "Train a collaborative filtering model in TensorFlow script mode.",
      "C": "Use SageMaker BlazingText for embeddings and nearest-neighbor search.",
      "D": "Use Amazon Personalize recommendation service."
    },
    "explanation": "Amazon Personalize is a managed recommendation system that handles sparse interactions and integrates easily, reducing operational overhead."
  },
  {
    "id": "a0a42496364327c7fc2396b67c5d819f4020ff217043d205ce576cb301ab4900",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "An online news service needs to tag articles with topics using a multi-label classification approach. They prefer an AWS-managed solution with custom training only if necessary. Which approach should they adopt?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Fine-tune a SageMaker JumpStart multi-label zero-shot classification foundation model.",
      "B": "Train a multi-label Keras model in script mode.",
      "C": "Use Amazon Comprehend for single-label topic detection.",
      "D": "Use SageMaker BlazingText supervised classification."
    },
    "explanation": "JumpStart foundation models support multi-label zero-shot classification with minimal training, managed by AWS."
  },
  {
    "id": "da0f72460a14e00feb268ff06f94ec3dd4bab0aa8b75e8aea045f81c725ff077",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A logistics company must forecast daily demand at 1,000 locations for the next 30 days. They have three years of daily demand history. They require prediction intervals and seamless SageMaker integration. Which modeling approach should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Implement ARIMA per location in script mode.",
      "B": "Use a custom LSTM in TensorFlow.",
      "C": "Use SageMaker DeepAR forecasting built-in algorithm.",
      "D": "Use SageMaker XGBoost with date features."
    },
    "explanation": "DeepAR provides probabilistic forecasts with intervals, scales across series, and is a built-in SageMaker algorithm suited to many time-series."
  },
  {
    "id": "aaed2f226470a1320e28499410837699b469eab343c66a857d0f8c1436deec0d",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A marketing team wants to group social media posts into topics for monitoring. They have no labeled data and limited compute budget. Which modeling approach is most appropriate?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Train a supervised LDA model in script mode.",
      "B": "Use SageMaker Latent Dirichlet Allocation (LDA) built-in algorithm.",
      "C": "Fine-tune a large transformer model with clustering head.",
      "D": "Use Amazon Comprehend for custom entity recognition."
    },
    "explanation": "SageMaker's built-in LDA efficiently clusters text into topics unsupervised, requiring no labels and minimal compute."
  },
  {
    "id": "4525dfd80664ddcfa8daf5490a0ea0792c7dff391790a1bd81b445b451241f51",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A cybersecurity team wants to detect malicious logon patterns using unsupervised anomaly detection. They need to process terabytes of log data and integrate into SageMaker Pipelines. Which approach should they select?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Random Cut Forest built-in algorithm.",
      "B": "Use custom PyTorch autoencoder in script mode.",
      "C": "Deploy Amazon GuardDuty for logs anomaly detection.",
      "D": "Use SageMaker K-means clustering to detect outliers."
    },
    "explanation": "Random Cut Forest is optimized for large, high-dimensional anomaly detection and integrates natively with SageMaker Pipelines."
  },
  {
    "id": "e2647a9e4d087971aab7861f1f13fabe3420045634e0fae751a8db6d69082f9e",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A life sciences company needs to classify gene expression profiles into disease subtypes. Dataset is small (5,000 samples, 20,000 features). They must avoid overfitting and need interpretability. Which modeling approach is best?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a deep neural network with feature selection.",
      "B": "Use SageMaker Linear Learner with L1/L2 and elastic net for feature selection and interpretability.",
      "C": "Use SageMaker XGBoost with maximum tree depth=10.",
      "D": "Use SageMaker K-nearest neighbors."
    },
    "explanation": "Linear Learner with elastic net reduces overfitting on high-dimensional data and provides interpretable coefficients and embedded feature selection."
  },
  {
    "id": "53343cfe514da4fd60e2232654fbfb91d6bc76d13c0e05f9cb245723c4a4a2b7",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A media company needs to index and tag objects (e.g., people, vehicles) in video footage. They want a fully managed solution requiring no custom training. Which AWS AI service should they select?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Train a custom object detection model in SageMaker script mode.",
      "B": "Use SageMaker built-in Object Detection algorithm.",
      "C": "Use Amazon Rekognition Video for object and activity detection.",
      "D": "Use Amazon Kinesis Video Streams with ML integration."
    },
    "explanation": "Rekognition Video provides managed object and activity detection in video, eliminating need for custom training."
  },
  {
    "id": "f61c98ea216fcc889bf13da8abfa30abf45c8358dab242b9a8abd3800f3ffa2a",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A bank wants to detect fraudulent transactions in real time. They have labeled data but the fraud patterns evolve over time. They need an AWS service with built-in model updating. Which approach should they choose?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker XGBoost and retrain daily in a custom pipeline.",
      "B": "Use Amazon Fraud Detector service.",
      "C": "Train a custom LSTM in script mode.",
      "D": "Use Amazon Lookout for Metrics."
    },
    "explanation": "Amazon Fraud Detector is tailored for fraud, supports continuous learning and real-time inference, simplifying maintenance."
  },
  {
    "id": "66346bce9ce7b83cc9f1d32fbf0bfedcf9273005db3e5f133b92186f2ea6c6cf",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A retailer wants to predict end-of-day inventory needs using external weather and holiday data. They need probabilistic forecasts for safety stock planning. Which modeling approach on SageMaker should they use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker DeepAR forecasting built-in algorithm.",
      "B": "Use SageMaker XGBoost with quantile regression.",
      "C": "Use a custom Prophet model in script mode.",
      "D": "Use SageMaker Random Cut Forest."
    },
    "explanation": "DeepAR natively supports probabilistic forecasting with external covariates and scales to many series."
  },
  {
    "id": "80d350f587295eac060fff910fe55aadcb9d9f3259c38bb0662c7f0d0cb9cd61",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A gaming company needs to recommend in-game content to players based on session behavior. They have tens of millions of sessions per day and need sub-100ms latency. Which approach is most viable?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Train a collaborative filtering model in TensorFlow script mode.",
      "B": "Use Amazon Personalize campaign for real-time recommendations.",
      "C": "Use SageMaker XGBoost with custom nearest-neighbor indexing.",
      "D": "Use Amazon Kinesis Data Analytics for on-the-fly clustering."
    },
    "explanation": "Amazon Personalize offers managed real-time campaigns optimized for low latency at scale."
  },
  {
    "id": "22197ce659fb8706d56dda0caeb2fe5b624a9e3f6cf32a6ca10050f965cb4dd6",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A legal firm wants to extract key phrases from contract documents. They have no labeled data and need high accuracy. Which AWS solution should they employ?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a custom NER model in Hugging Face script mode.",
      "B": "Use SageMaker BlazingText to learn word embeddings.",
      "C": "Use Amazon Comprehend custom entity recognition with auto-labeling.",
      "D": "Use Amazon Textract forms extraction."
    },
    "explanation": "Comprehend custom entity recognition supports auto-labeling for text, yielding high accuracy without full custom model training."
  },
  {
    "id": "13a8bd3f93444a27bd4599c985c654c615d0785bbadf9287b23b414d10fb7c81",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A telecom operator wants to predict network drop calls per cell tower hourly. They have 2 years of data across 10,000 towers. They require forecasting with external features and a managed solution. Which built-in algorithm should they use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker DeepAR forecasting built-in algorithm.",
      "B": "Use SageMaker XGBoost regression with time lags.",
      "C": "Use a custom ARIMA model in Spark on EMR.",
      "D": "Use SageMaker K-means for time-series clustering."
    },
    "explanation": "DeepAR handles large numbers of time series, accepts external covariates, and produces probabilistic forecasts."
  },
  {
    "id": "cc5b56bcc6ef2c10d32ad99252dd99803eed904b4fc06965a1ae8a2d466efcfd",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A startup wants to deploy a chat interface that responds to user queries with company policy answers. They need conversational AI with minimal custom training. Which AWS service should they select?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Train a seq2seq model in SageMaker script mode.",
      "B": "Use Amazon Comprehend to detect intents and build responses.",
      "C": "Use Amazon Lex custom bot with full slot and intent definitions.",
      "D": "Use Amazon Q to build an ML-powered conversational interface."
    },
    "explanation": "Amazon Q is a managed conversational QA service that ingests documents and answers queries with minimal configuration."
  },
  {
    "id": "6fa53d7bc76721d5c48ca34ed1846da7547cd2fccf1a8e5dd0123d9da6582ec3",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A biotech firm must classify microscope images of cells into healthy vs cancerous. They have only 1,000 labeled images and need high accuracy. They also want a managed training pipeline. Which approach should they choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a custom CNN from scratch in PyTorch script mode.",
      "B": "Use SageMaker JumpStart pre-trained Image Classification model and fine-tune on their data.",
      "C": "Use SageMaker built-in Image Classification with default parameters.",
      "D": "Use Amazon Rekognition Custom Labels service."
    },
    "explanation": "JumpStart pre-trained models accelerate training on small datasets; managed pipeline support simplifies fine-tuning."
  },
  {
    "id": "cf1a63769cc13d67d85e47c8ebbdfbcf23da40f177e0b2af2f0da4fdd48c3cad",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A media analytics company must detect brand logos in user-generated videos. They need high accuracy and low latency inference. Which modeling approach on AWS should they use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a YOLO model in SageMaker script mode and deploy on GPU endpoints.",
      "B": "Use SageMaker built-in Object Detection algorithm with custom data.",
      "C": "Use Amazon Rekognition for image detection frame-by-frame.",
      "D": "Use Amazon Rekognition Custom Labels trained on their logo dataset."
    },
    "explanation": "Rekognition Custom Labels offers managed training for object detection with low-latency inference optimized by AWS."
  },
  {
    "id": "584c813b9cf7c564d4e3367351e1fd2157b29b1faa60a093a0154f76a7cf6478",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A logistics startup needs to cluster delivery routes into 100 groups based on GPS coordinates over time for fleet optimization. They want an unsupervised, scalable solution in SageMaker. Which algorithm should they use?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use SageMaker K-means clustering with k=100 and geospatial features.",
      "B": "Use SageMaker Linear Learner in unsupervised mode.",
      "C": "Use a custom DBSCAN in script mode.",
      "D": "Use SageMaker Random Cut Forest for clustering."
    },
    "explanation": "K-means is the standard scalable clustering for a predetermined number of clusters and integrates directly in SageMaker."
  },
  {
    "id": "c475a7b0e24739374efa22b24447121bdb3f7c752a037e3d906765711477a578",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A financial institution wants to build a credit scoring model with a focus on minimizing false negatives over false positives. They need a model that supports cost-sensitive learning. Which SageMaker built-in algorithm should they choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Linear Learner with default settings.",
      "B": "Use SageMaker XGBoost with customized objective and scale_pos_weight.",
      "C": "Train a feed-forward neural network in TensorFlow script mode.",
      "D": "Use SageMaker K-nearest neighbors with class weights."
    },
    "explanation": "XGBoost allows customizing objective and pos/neg weights to handle cost sensitivity and optimize for false negatives."
  },
  {
    "id": "f7e330981ca2257b2e144e53681e89300513aa3b87650683fa6925dd898f33ce",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A news aggregator wants to detect emerging topics from streaming text feeds in real time. They need unsupervised topic modeling with low latency. Which AWS approach should they select?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker LDA in batch mode triggered hourly.",
      "B": "Use SageMaker K-means on streaming windows.",
      "C": "Use Amazon Kinesis Data Analytics with built-in ML to apply LDA in real time.",
      "D": "Use AWS Lambda to call a SageMaker endpoint running topic modeling."
    },
    "explanation": "Kinesis Data Analytics integrates LDA in streaming pipelines for low-latency unsupervised topic detection without batch orchestration."
  },
  {
    "id": "fd83aa4678e5320b04129373ba9bef09b7123d769cc2151990cbcded1c261cd6",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A rideshare company wants to cluster drivers into behavior profiles based on speed, route deviation, and idle time. They need an algorithm that automatically detects the number of clusters. Which approach should they use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker K-means with silhouette analysis to pick k.",
      "B": "Use SageMaker Gaussian Mixture built-in algorithm for EM-based clustering.",
      "C": "Train a self-organizing map in script mode.",
      "D": "Use SageMaker Random Cut Forest to discover clusters."
    },
    "explanation": "Gaussian Mixture automatically estimates cluster probabilities and can infer the number of components via Bayesian Information Criterion."
  },
  {
    "id": "8385a5a153c41bd87db13c4760684a156bb45ea5e914f341a5d7a9ec5bbadd9e",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A broadband ISP wants to forecast hourly network usage patterns per customer segment. They require a model that can incorporate seasonal effects and external events. Which SageMaker algorithm fits best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker DeepAR forecasting with seasonal frequency and event indicators.",
      "B": "Use SageMaker XGBoost regression with dummy variables.",
      "C": "Use a custom Prophet model in SageMaker script mode.",
      "D": "Use SageMaker K-means to cluster time windows."
    },
    "explanation": "DeepAR natively handles seasonality and event covariates at scale with built-in SageMaker support."
  },
  {
    "id": "59b614084579cdad6fd3e529b8186e2052bdc601ea2c628cda326ce8cbae579d",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A telecom analytics team needs to classify customer support calls as billing, technical, or general inquiry. They have 20,000 transcribed calls. They want a model that can be retrained weekly with minimal code. Which approach should they choose?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Train a custom RNN model in TensorFlow script mode.",
      "B": "Use Amazon Transcribe followed by BlazingText supervised training.",
      "C": "Use SageMaker JumpStart text classification solution template.",
      "D": "Use Amazon Comprehend custom classification."
    },
    "explanation": "JumpStart solution templates provide end-to-end pipelines that automate retraining with minimal code changes."
  },
  {
    "id": "692776452eacc73b432e7a0a7980b83cfcb320302320e028bf418989125f3d9b",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A biotech company wants to identify cell types from single-cell RNA sequencing data (20,000 genes per cell). They need unsupervised clustering that scales and can handle high dimensionality. Which SageMaker built-in algorithm should they select?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "Use SageMaker K-means with PCA preprocessing.",
      "B": "Use SageMaker Gaussian Mixture model for soft clustering in high dimensions.",
      "C": "Use a custom t-SNE embedding in script mode.",
      "D": "Use SageMaker Random Cut Forest for clustering."
    },
    "explanation": "Gaussian Mixture can handle high-dimensional soft clustering; combining with EM makes it more flexible than hard k-means."
  },
  {
    "id": "6c24b9b6c6a89caae893bb62889d8c6aeb3fd9d2148ced6506f97629620c0a54",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A publisher wants to rank articles by predicted click-through rate (CTR). They have historical CTR logs and dozens of categorical features with high cardinality. Which algorithm should they use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Linear Learner with one-hot encoding.",
      "B": "Use SageMaker XGBoost with target encoding for high-cardinality features.",
      "C": "Train a deep neural network in PyTorch script mode.",
      "D": "Use SageMaker K-nearest neighbors for CTR prediction."
    },
    "explanation": "XGBoost handles target encoding and interactions efficiently on tabular data with high-cardinality categorical features."
  },
  {
    "id": "c8fb31ab5f8b9228ff3e5ce78158f58a74db7ab3ebda69176a2bee431212477a",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A healthcare analytics startup wants to extract structured data fields (e.g., dosages) from scanned prescription forms. They need a fully managed approach requiring minimal ML expertise. Which AWS service should they choose?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Train a custom form-recognition model in SageMaker script mode.",
      "B": "Use Amazon Textract Analyze ID API.",
      "C": "Use Amazon Textract Forms with pre-built key-value extraction.",
      "D": "Use Amazon Comprehend Medical."
    },
    "explanation": "Textract Forms extracts key-value pairs from documents without custom model training, ideal for structured field extraction."
  },
  {
    "id": "73f2db0fd31fe56d74f7c92ba80842b2169a4eac84f1b1698de986d7efb47fd3",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A data science team needs to automate retraining of a SageMaker model whenever new data arrives in S3. They want to enforce a manual approval step after data validation and before model training. Which combination of AWS services and pipeline stages meets these requirements with least custom code?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Lambda triggered by S3 events to run a Step Functions state machine that includes a manual approval (via SNS) and then starts a SageMaker training job.",
      "B": "Use SageMaker Pipelines: define a DataQualityCheck step, add a ModelTraining step, and insert a ManualApprovalStep between validation and training.",
      "C": "Use CodePipeline with Source stage on S3, a CodeBuild project to validate data, an AWS Lambda for approval, and a CodeBuild to start the training.",
      "D": "Use Step Functions with S3 event trigger, implement data validation in a task, pause with Wait state for manual input via API Gateway, then start training."
    },
    "explanation": "SageMaker Pipelines natively supports data validation, model steps, and ManualApprovalStep with minimal custom code, fulfilling requirements efficiently."
  },
  {
    "id": "123fe3f801319ff23a8119b62fee224bf4e9c29c4c0d28e0733b7796dfacc550",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A pipeline is defined in AWS CodePipeline that builds Docker containers for inference, pushes them to ECR, and deploys to SageMaker endpoints. The team notices that if the build fails, manual rollbacks are needed. Which CodePipeline feature should be added to automate rollback on deployment failures?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable cross-region replication in ECR.",
      "B": "Use CodeBuild 'batch' build mode for transactional builds.",
      "C": "Configure a DeploymentConfig with Canary10Percent10Minutes in the CodeDeploy stage for automatic rollback on failure.",
      "D": "Add a Lambda invoke action after deployment to delete failed endpoints."
    },
    "explanation": "Using a CodeDeploy DeploymentConfig with canary deployments supports automatic rollback on failure without custom Lambdas."
  },
  {
    "id": "2dc2148cf1b72dcc924568a21f080015ec011a5212cf139021895feeb0967473",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "An ML pipeline in CodePipeline runs training, evaluation, and deployment. The evaluation step uses CodeBuild to compute model metrics. The team needs the pipeline to stop and notify if model accuracy drops below 85%. How can they implement this with minimal changes?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Modify the training CodeBuild to throw an exception when accuracy <85% and let the pipeline fail.",
      "B": "Use CloudWatch Alarms on CodeBuild logs to alert and pause the pipeline.",
      "C": "Write a Lambda in the evaluation stage to call CodePipeline StopPipelineExecution if accuracy <85%.",
      "D": "In evaluation CodeBuild, use buildspec checks to fail the build when accuracy <85%, causing the pipeline to stop and send failure notifications."
    },
    "explanation": "Failing the evaluation CodeBuild via buildspec when accuracy <threshold automatically stops the pipeline and triggers failure notifications."
  },
  {
    "id": "b022c1f8ed6758492d2d0beec8694f3e62835ab51843c9dcb54dbd31d3ed1125",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A pipeline must deploy a new ML model version to a real-time endpoint with zero downtime and allow easy rollback. Which deployment strategy and pipeline configuration should be used?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Lambda to deploy new model to same endpoint container, overwriting existing one.",
      "B": "Configure a SageMaker multi-model endpoint in CodePipeline with blue/green deployment via CodeDeploy.",
      "C": "Use SageMaker real-time endpoint with WarmPool and update endpoint configuration via CodeBuild.",
      "D": "Launch a new endpoint, test it, then delete the old one via a final CodePipeline stage."
    },
    "explanation": "SageMaker multi-model endpoints with CodeDeploy blue/green strategy enable zero-downtime updates and easy rollback."
  },
  {
    "id": "d8347e55357ecabd806bbab689aefdd7f66c61b6025bc05403956ae82903bf96",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "The team needs to version control SageMaker Pipeline definitions, enforce PR reviews, and automatically trigger pipeline execution when changes are merged. How should they architect this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Store pipeline definitions in a Git repo. Use CodePipeline with Source from CodeCommit, add CodeBuild for linting, and deploy to SageMaker Pipelines using CloudFormation in a later stage.",
      "B": "Use SageMaker Studio notebooks directly and rely on manual execution upon merge.",
      "C": "Embed pipeline definitions in CodeBuild scripts, trigger on GitHub webhook, and start executions via SDK.",
      "D": "Use AWS CDK in local environment, require manual CDK deploy after merge."
    },
    "explanation": "Using CodePipeline Source from CodeCommit with automated linting and CloudFormation deploy ensures version control, PR reviews, and auto-deploy on merge."
  },
  {
    "id": "c0e65721ad5871d3cc5d255309de4b0319d75d78e7d557951a5312756cec8178",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A CodePipeline pipeline must orchestrate cross-account deployment of model inference containers. Account A builds and tests the image; Account B deploys to SageMaker. How should roles and permissions be set?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "In Account A, create an IAM role that trusts Account B\u2019s CodePipeline service principal to push to ECR.",
      "B": "In Account B, create a pipeline that assumes the CodeBuild role from Account A to pull images.",
      "C": "In Account A\u2019s ECR repository policy, allow pull by a deployment role in Account B. In Account B, create a pipeline with a service role that has pull permissions and assume role rights.",
      "D": "Replicate ECR images to Account B automatically via cross-account replication."
    },
    "explanation": "Granting Account B\u2019s pipeline role pull permissions in Account A\u2019s ECR via repository policy allows secure cross-account access for deployment."
  },
  {
    "id": "c9a1e50fc35d6c2932b7611529c1db11d7d7025b89b9123b84f5dd7d52eb9417",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A pipeline using AWS CodePipeline must package ML code and dependencies into a container, push to ECR, then deploy to ECS for batch inference. They also need to run unit tests on the packaged image. Which steps should the pipeline include?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Source -> Build (Docker build & push) -> Deploy -> Test",
      "B": "Source -> Build (Docker build) -> Test -> Deploy",
      "C": "Source -> Test -> Build (Docker build & push) -> Deploy",
      "D": "Source -> Build (Docker build) -> Test (CodeBuild uses image via local registry) -> Build (Docker push) -> Deploy"
    },
    "explanation": "Building the image, running tests locally in CodeBuild, then pushing and deploying ensures integrity before pushing."
  },
  {
    "id": "e8ca4f12bdcaa0b7ee62957dc83b5c45bfe0d658a69a9988f1880aba9b6e010c",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "An ML team wants to integrate Terraform-managed infrastructure into a CI/CD pipeline for SageMaker endpoint updates. They need a single pipeline. Which service combination achieves this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS CloudFormation with macros in CodePipeline.",
      "B": "Use CodePipeline with Terraform CodeBuild actions: one to plan, manual approval, one to apply changes, then a SageMaker deployment stage.",
      "C": "Use AWS CDK in CodeBuild to translate Terraform into CDK and deploy.",
      "D": "Trigger Terraform from a Lambda invoked by EventBridge in between CodePipeline stages."
    },
    "explanation": "Running Terraform plan/apply in CodeBuild with approval inside CodePipeline integrates IaC and SageMaker deployments in one pipeline."
  },
  {
    "id": "bea324ec1350843fba410252aaf6f350066e78ab715c49f0461056181bdec398",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Which pattern ensures that data scientists can trigger hyperparameter tuning experiments via pull request merge, with automated notifications on completion?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use CodePipeline triggered by CodeCommit pull request merge. In a CodeBuild stage, invoke SageMaker HPO via AWS CLI. Use CloudWatch Events to notify on job completion.",
      "B": "Use SageMaker Experiments SDK manually invoked in notebooks.",
      "C": "Configure EventBridge to start tuning on S3 object creation, send SNS on complete.",
      "D": "Use Step Functions triggered by CodePipeline Webhook to start tuning."
    },
    "explanation": "CodePipeline on merge, CodeBuild to start tuning, and CloudWatch Events to notify provides CI/CD for experiments."
  },
  {
    "id": "1cd4c3c00cb6ed13ddd1452b6258aa88c7286fbc9cb3f88ccc986206453afd4b",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "When integrating a testing framework into CI/CD for ML, the team needs to run data schema validation, unit tests for transformations, and integration tests for training artifacts. How should they structure the CodePipeline?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Single build stage running all tests sequentially.",
      "B": "Separate pipelines for data tests, code tests, and training.",
      "C": "Pipeline with Source -> Data Validation Test (CodeBuild) -> Unit Test (CodeBuild) -> Integration Test (CodeBuild) -> Deploy.",
      "D": "Use Step Functions to orchestrate tests outside CodePipeline."
    },
    "explanation": "Separate CodeBuild stages in one pipeline for test types provide clear isolation and failure points."
  },
  {
    "id": "936a5d6c095d60e9f8aef0244b6bdcb28ec2188b0536890988e542428fe9eaea",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A compliance requirement mandates that any model deployment must pass through an audit stage capturing approvals in an immutable log. Which service and pipeline step combination satisfies this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use CloudTrail to log manual approvals in SNS.",
      "B": "Use CodeDeploy with manual approval.",
      "C": "Use Step Functions Wait state with DynamoDB logging.",
      "D": "Use CodePipeline Manual Approval action with execution details logged in CloudTrail."
    },
    "explanation": "CodePipeline Manual Approval action events are logged in CloudTrail, satisfying audit and immutability requirements."
  },
  {
    "id": "6fa5edc3f4eaa20a2a2438255d87b9bae6934dec5c85f219da3fa86e0cc53c96",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A data scientist merges new featurization code into GitHub. This should automatically update the SageMaker processing step in the CI/CD pipeline. How can this be achieved?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Manually update pipeline JSON in S3.",
      "B": "Use CodePipeline with a GitHub source action on the repo. Add a CodeBuild stage to synthesize updated pipeline definitions and deploy via CloudFormation update.",
      "C": "Write a Lambda to watch GitHub and update SageMaker resources via SDK.",
      "D": "Use AWS AppRunner to auto-deploy changes."
    },
    "explanation": "CodePipeline GitHub source plus CodeBuild to update pipeline definitions and CloudFormation deploy automates the update."
  },
  {
    "id": "abd6e3ceff29ae36a0e90d2b3580faf991b061c3b4598c9e44f38ae61cc28721",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your pipeline uses AWS CodePipeline and CodeBuild to build, test, and deploy containers to SageMaker. You need to ensure that only approved container images are deployed. Which mechanism should you implement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use ECR image scanning on push, and add a Lambda that checks scan results before CodeDeploy stage.",
      "B": "Restrict CodePipeline IAM role to only allow unscanned images.",
      "C": "Use a lifecycle policy in ECR to retain only scanned images.",
      "D": "Configure CodeBuild to fail if image size exceeds threshold."
    },
    "explanation": "Integrating ECR image scan results via Lambda in the pipeline ensures only approved images proceed to deployment."
  },
  {
    "id": "5ea298d4a131b5e76f110a5f5469dcc7de1c2684769aaf6cfae9372afa126a67",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "The team must enforce that any pipeline change triggers a security compliance scan of both infrastructure and code. Which pattern achieves this end to end?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon Inspector on EC2 build agents.",
      "B": "Use CloudWatch Events to trigger manual security review.",
      "C": "In CodePipeline after Source stage, add a CodeBuild stage that runs static code analysis (e.g., cfn-nag) and container vulnerability scans, failing the build on issues.",
      "D": "Use AWS Config rules to audit pipeline resources."
    },
    "explanation": "Placing a security scan CodeBuild stage in the pipeline ensures any change undergoes compliance checks before deployment."
  },
  {
    "id": "f27dbcb5263a9474f29fde7039e95875a3967f64cabc3f832c884fab511779ee",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "You need to orchestrate an ML pipeline that span multiple AWS Regions. How can you centralize pipeline definition and execution?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Replicate the pipeline in each region manually.",
      "B": "Use Route 53 to direct triggers to regional pipelines.",
      "C": "Use EventBridge cross-region events to start different pipelines.",
      "D": "Store pipeline code in a central CodeCommit repo and use CodePipeline cross-region actions with CloudFormation to deploy and execute regional pipelines."
    },
    "explanation": "CodePipeline cross-region actions and central repo allow centralized definition while executing region-specific pipelines."
  },
  {
    "id": "b0d2074bd3a4592955295e4f3294cb88b0bb92fe9bd5ee6932ee1f9f6d62e7f1",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A team wants to provision ephemeral CI/CD infrastructure for each pull request to isolate changes during ML pipeline definition tests. Which approach is most automated and least overhead?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Manually launch Cloud9 environments per PR.",
      "B": "Use CodePipeline with a CodeBuild job that uses CDK to spin up a nested CloudFormation stack for the PR, runs tests, then tears it down.",
      "C": "Use Step Functions to create stacks, run tests, delete stacks.",
      "D": "Use AWS Batch to run tests in containers."
    },
    "explanation": "Using CDK in CodeBuild to deploy and teardown nested stacks per PR provides isolation with automation and minimal custom code."
  },
  {
    "id": "6a7911a899fb52e3d2be8f07c9c5b5da1a5f123ecab2251929402900339db70f",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "They need to integrate model explainability checks into CI/CD such that if SHAP feature importance shifts beyond threshold in new model, pipeline fails. How to implement?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Add a CodeBuild stage using SageMaker Clarify SDK to compute SHAP values, compare them, and exit with failure if threshold exceeded.",
      "B": "Use CloudWatch Metrics to monitor SHAP values after deployment.",
      "C": "Embed checks in training script to abort training.",
      "D": "Deploy model then run real-time inference to compute drift."
    },
    "explanation": "A dedicated CodeBuild stage for Clarify explainability tests before deployment stops the pipeline on SHAP drift."
  },
  {
    "id": "7ecc2fbb6df8d3481c85a589a5281d647eca218994f6971f72ba2d614b3bee62",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A centralized security team must approve any pipeline that grants broad IAM permissions. How can the CI/CD pipeline enforce this approval?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Config to detect wide permissions.",
      "B": "Use Lambda to revoke permissions after deployment.",
      "C": "Include a Manual Approval action after IAM change stage, before deployment, logged in CodePipeline.",
      "D": "Use CloudTrail Insights alerts."
    },
    "explanation": "Manual Approval actions in CodePipeline halt progress until security team approval, fulfilling enforcement and logging."
  },
  {
    "id": "b9f402023a6e12058ee972234347cd3d83380fbaee5d50c029e438ea1175b1ad",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your ML pipeline CodeBuild stage needs environment variables stored securely. Which practice provides secure injection and auditability in CI/CD?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Store env vars in CodeBuild project settings.",
      "B": "Use AWS Secrets Manager, grant CodeBuild role GetSecretValue, inject at runtime.",
      "C": "Hard-code secrets in buildspec.",
      "D": "Store secrets in S3 with public-read access."
    },
    "explanation": "Secrets Manager integration with CodeBuild provides secure retrieval and auditing; avoids hard-coding or insecure storage."
  },
  {
    "id": "02b4bfdfa34cdfa5e86c51362fbb2e0fbbadf8708af268238bee696a19b71fe4",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A pipeline deploys SageMaker endpoints across multiple accounts. The build stage packages a SageMaker Model package. How can you share the model package across accounts securely in the pipeline?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Manually copy model artifacts via S3 CLI.",
      "B": "Use public S3 bucket for artifacts.",
      "C": "Use AWS DMS to migrate artifacts.",
      "D": "Use SageMaker Model Package Groups with cross-account grant via resource policy, and use CodePipeline roles to deploy."
    },
    "explanation": "SageMaker Model Package Groups support resource policies for cross-account sharing; CodePipeline roles can assume deploy permissions."
  },
  {
    "id": "adad94b508f435cccdc484ac30253a6cfbfcd2f1ad85f7d4206198492866e1d5",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "How can you ensure pipeline stages for SageMaker training run on Spot instances to reduce cost, while controlling max runtime for retries?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set TrainingComputeType to ml.p3.2xlarge in pipeline.",
      "B": "Use a separate Spot training pipeline.",
      "C": "In SageMaker Pipeline TrainStep, specify TrainingInputMode with UseSpotInstances=true and MaxWaitTime/MaxRunTime.",
      "D": "Modify IAM policy to allow Spot."
    },
    "explanation": "SageMaker Pipelines TrainStep supports UseSpotInstances and max wait/run time to leverage Spot with retry constraints."
  },
  {
    "id": "3205907e919c348ce3d85a3e476836384b846b1e127e7bec94645e0a2b09b75d",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your pipeline uses CodePipeline with CloudFormation deploy action to create SageMaker Pipeline stacks. The team wants to detect drift between the deployed pipeline definition and source. Which tool helps?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable CloudFormation Drift Detection on the pipeline stack and integrate a CodeBuild stage to run detect-stack-drift and fail if drifted.",
      "B": "Use AWS Config to detect code changes in CodeCommit.",
      "C": "Add a Lambda to compare JSON definitions.",
      "D": "Use Inspector to scan pipeline definitions."
    },
    "explanation": "CloudFormation Drift Detection identifies config drift; performing it in CodeBuild fails pipeline if drift exists."
  },
  {
    "id": "61d701c4467ca8b04125ac4ef6612b863bb07b245b8bf6730f8622146b43d82b",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A pipeline must orchestrate a nightly batch inference job on new data via SageMaker Processing, then archive results to S3 Glacier. How can this be built in CodePipeline?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Lambda for processing and then Glacier archive.",
      "B": "Define pipeline with Source (EventBridge scheduled start), Processing stage using CodeBuild to call SageMaker Processing, then a Deploy stage to trigger a Lambda copying results to Glacier.",
      "C": "Use Step Functions with Lambda tasks.",
      "D": "Use Data Pipeline service."
    },
    "explanation": "CodePipeline with an EventBridge-triggered Source and CodeBuild action calling SageMaker Processing, followed by a Lambda archive Deploy stage, meets requirements."
  },
  {
    "id": "eb31a1ace6aafd690d5b5170b9c2f6503f82a88de5648a7e2fb79bf61a745323",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A pipeline deploys model code and container. You need to validate model container performance under load before routing production traffic. Which deployment strategy implements this in CI/CD?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a single Lambda test after deployment.",
      "B": "Deploy to Sagemaker endpoint and test within same stage.",
      "C": "Use CodeDeploy with linear deployment configuration, sending small percentage of traffic to new container, run performance tests, then proceed.",
      "D": "Use Canary deployments via ECS Blue/Green."
    },
    "explanation": "CodeDeploy linear deployment with traffic shifting and automated tests allows performance validation before full cutover."
  },
  {
    "id": "9a1979c72c617ea558fdf64f5925592b90542cf4ad671afe40890b46089793a9",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "They need to incorporate code coverage metrics into their CI/CD pipeline for ML data transformation scripts. How can this be done of minimal complexity?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Use CodeBuild to run pytest without coverage and interpret logs.",
      "B": "Add a CodeBuild stage that runs pytest with coverage, then use CodeBuild reports feature to publish coverage artifacts.",
      "C": "Use Coveralls external service triggered by EventBridge.",
      "D": "Manually inspect coverage after each build."
    },
    "explanation": "CodeBuild reports support publishing coverage metrics natively, integrating smoothly in pipelines without external dependencies."
  },
  {
    "id": "30251438ac592718f9a99af9ea6d1eabbaf498ce7ea655404b0c9fd1f0397ed0",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A pipeline failure occurs when deploying to a private VPC endpoint because of missing permissions. Which policy change in CodePipeline role solves this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add IAM permission for ec2:CreateNetworkInterface, ec2:DescribeNetworkInterfaces, ec2:DeleteNetworkInterface.",
      "B": "Allow s3:GetObject VPC endpoints only.",
      "C": "Grant AWSKeyManagementServiceDecrypt in Secrets Manager.",
      "D": "Permit TagResource on CloudWatch Logs."
    },
    "explanation": "Deploying to private VPC endpoints requires network interface permissions for CodePipeline-managed actions; adding those resolves the failure."
  },
  {
    "id": "6d68ba77cdcb88c42e7bfdab60415cd86d0f4a7442110efc353de80aa58517db",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your pipeline uses AWS CodeBuild to package and deploy a PyTorch model to SageMaker. The buildspec installs large ML libraries causing slow builds. How to speed up builds without sacrificing reproducibility?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use smaller instance size for CodeBuild.",
      "B": "Cache from public PyPI mirror.",
      "C": "Use requirement pre-install via pip wheel.",
      "D": "Enable CodeBuild local caching for pip and container layers to reuse previous build artifacts."
    },
    "explanation": "CodeBuild local caching for pip and Docker layers significantly speeds up build times while ensuring same dependencies are installed."
  },
  {
    "id": "c7f69b42039490be274573466e9ed51c8bf526818f77184b38d4302fbf9d374a",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "They need to rollback to the last successful SageMaker pipeline execution automatically if a new execution fails validation. How can this be configured?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use EventBridge to watch FailedExecution and trigger manual rollback.",
      "B": "In SageMaker Pipelines, configure a CallbackStep that on failure starts an execution of the previous pipeline execution using StartPipelineExecution with previous execution id.",
      "C": "Use CloudFormation change sets to revert pipeline definition.",
      "D": "Use Step Functions with TRY/CATCH to revert."
    },
    "explanation": "SageMaker Pipelines CallbackStep can programmatically invoke a rollback to the last model by triggering an execution of the previous successful pipeline."
  },
  {
    "id": "44bb61787f16dd958f7dc38bb769f124f3df677d2e316dad0afd8f14552dda7f",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A pipeline must enforce that secret rotation and updated credentials are picked up without pipeline definition changes. Which pattern supports this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Hardcode secrets in parameter store with long TTL.",
      "B": "Re-deploy pipeline on rotation.",
      "C": "Use SSM Parameter Store dynamic references in pipeline actions to retrieve credentials at runtime.",
      "D": "Trigger pipeline manually after rotation."
    },
    "explanation": "Using dynamic references to Parameter Store ensures pipeline fetches updated secrets at runtime without redefining the pipeline."
  },
  {
    "id": "74c042d54b073364fdd910b038f4ef95c0b0296f493049f0d7a60cb2df282e9e",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Multiple data teams share a central CI/CD pipeline for SageMaker training. They need isolation so one team\u2019s pipeline failure doesn\u2019t block others. How to configure?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create separate CodePipeline pipelines per team pointing to same buildspec and repos, with individual pipeline resource and IAM roles.",
      "B": "Use single pipeline with parallel branches per team.",
      "C": "Use Step Functions instead of CodePipeline.",
      "D": "Use multi-branch CodeBuild project."
    },
    "explanation": "Separate pipelines per team isolate execution and failures, while sharing code and buildspec for consistency."
  },
  {
    "id": "e179087a7baa01f98a4c0ea0d7a06345822cbeaede9a152fedd9a70518296af4",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A security change requires that any pipeline artifact stored in S3 be encrypted with a customer-managed KMS key. How to enforce this in CodePipeline?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SSE-S3 default in artifact buckets.",
      "B": "Specify encryption in CodeBuild project settings.",
      "C": "Use CloudTrail to audit encryption.",
      "D": "Configure CodePipeline artifact store with KMS key ARN in the pipeline definition."
    },
    "explanation": "Defining the artifactStore with a specific KMS key ARN in the pipeline ensures all pipeline artifacts are encrypted accordingly."
  },
  {
    "id": "46c6a38034622ae5dea07256ab10c9251794372360490aa5c3554475483fdbf7",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "A fraud-detection model is trained on a dataset where fraudulent cases are only 1% of the data. The model achieves 99% accuracy on a test set, but the business indicates that many frauds are missed. Which evaluation metric would most appropriately reflect the model\u2019s ability to detect fraud?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Overall accuracy on the test set",
      "B": "ROC AUC",
      "C": "Precision-Recall AUC",
      "D": "Log-loss"
    },
    "explanation": "With extreme class imbalance, overall accuracy and ROC AUC can be misleading. Precision-Recall AUC focuses on the positive (fraud) class and better reflects detection capability."
  },
  {
    "id": "ea6632146488d39a9f667e451167ec23f5a99246881015834c64bdfc3f7513fe",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "A classifier on a highly imbalanced dataset yields ROC AUC = 0.95 but PR AUC = 0.4. What does this indicate, and what should you do next?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "The model discriminates well overall but has low precision on positives; tune the decision threshold to improve precision.",
      "B": "The model is reliable; deploy it as is.",
      "C": "The dataset is too balanced; rebalance it further.",
      "D": "Collect more negative samples to improve ROC AUC."
    },
    "explanation": "High ROC AUC with low PR AUC shows many false positives. Adjusting the classification threshold based on PR curve can improve precision for the rare positive class."
  },
  {
    "id": "fd13e69ce6438d26d97fd57f8442d6cd0959f3061dd71e881c09a51a24bf9283",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your legacy model\u2019s F1 score on the production validation set is 0.85. A new candidate model scores 0.87. To determine if this improvement is statistically significant, you should:",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Automatically deploy the new model because 0.87>0.85",
      "B": "Perform a simple paired t-test on the F1 scores",
      "C": "Use bootstrap sampling on hold-out data to compute confidence intervals of F1 difference",
      "D": "Compare scores on a single test set without intervals"
    },
    "explanation": "Bootstrap sampling yields confidence intervals on the difference in F1, allowing you to assess statistical significance rather than relying on point estimates."
  },
  {
    "id": "9c65ae4ce3ce0293ec70296427d4176a78c1e18d45ac7160a45b85c8af2e9dc9",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "During training, your deep network\u2019s training loss plateaus very early and validation loss remains high. Which SageMaker Debugger rule should you enable to diagnose this convergence issue?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "VanishingGradient",
      "B": "LossNotDecreasing",
      "C": "WeightUpdate",
      "D": "OverfitDetection"
    },
    "explanation": "The LossNotDecreasing rule alerts when training loss fails to decrease. VanishingGradient detects near-zero gradients, but first you must confirm loss stagnation with LossNotDecreasing."
  },
  {
    "id": "2ddf02852c01446467e5480e05a2863474ca4defb005761034d6325a1fc60f83",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your model achieves 100% training accuracy but only 60% validation accuracy. Which approach best detects and visualizes this overfitting?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Plot training and validation loss curves using SageMaker Debugger",
      "B": "Compute final confusion matrix on test set",
      "C": "Inspect global SHAP values",
      "D": "Monitor data quality with ModelMonitor"
    },
    "explanation": "Plotting training vs validation loss curves identifies divergence indicative of overfitting; SageMaker Debugger automates rule-based collection of these tensors."
  },
  {
    "id": "38c8c62d50094ef6c9e108dae6417b38e0ee7ea7d92b369a15b2637411938255",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You wish to conduct an A/B test between your production and shadow model variants using SageMaker endpoints. Which configuration allows a traffic split between variants?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy two separate endpoints and manually route traffic",
      "B": "Use a multi-model endpoint with two models loaded",
      "C": "Create an endpoint configuration with two production variants and specify traffic weights",
      "D": "Use AWS CodePipeline to shift traffic between endpoints"
    },
    "explanation": "Endpoint configurations in SageMaker allow multiple production variants with specified traffic weights for A/B testing without separate endpoints."
  },
  {
    "id": "0d3dcdf26aec2b72f4f09e67f7444d492c6b28871dd607caca546cf61900812d",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Model A has F1 = 0.92 and requires 12 hours training; Model B has F1 = 0.91 and requires 1 hour. Retraining occurs weekly. Which model should you deploy, and why?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model A, because maximizing F1 is paramount",
      "B": "Model B, because the small F1 loss is outweighed by 12\u00d7 faster retraining",
      "C": "Deploy both and ensemble",
      "D": "Alternate weekly between A and B"
    },
    "explanation": "In a weekly retraining pipeline, a 1% F1 reduction is acceptable given the 12\u00d7 speedup, reducing compute cost and risk of stale models."
  },
  {
    "id": "7d9e3a590ae77b5ff2ff75e7c2f8308d8df7c2f9188d883a51506ca89d3f8518",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You need to monitor inference data streams for feature distribution changes in production. Which SageMaker capability and baseline type will you use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ModelQualityMonitor with a model quality baseline",
      "B": "ModelBiasMonitor with a bias baseline",
      "C": "ModelExplainabilityMonitor with SHAP baseline",
      "D": "ModelMonitor DataQualityMonitor with a data quality baseline"
    },
    "explanation": "ModelMonitor\u2019s DataQualityMonitor uses a data quality baseline to detect feature distribution drifts in incoming inference data."
  },
  {
    "id": "fc3f3b3a3b555630de8cf305096f27ba0a9f50507be8a1533e9ec91d49addfb3",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "A particular prediction\u2019s SHAP value for feature X is +2.5. What does this imply for that instance?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Feature X increased the model\u2019s output log-odds by 2.5 for this instance",
      "B": "Feature X is the most important globally",
      "C": "The model is biased toward feature X",
      "D": "SHAP values above 0 indicate uncertainty"
    },
    "explanation": "A positive SHAP value indicates that feature X contributed positively to the prediction (in log-odds space for classification) for that specific instance."
  },
  {
    "id": "bfe6b3535a0195377049f6fa7235da0724bfea2bf45fbc93c587b8ca3d4c0695",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You want to track hyperparameters, metrics, and artifacts across multiple training runs in SageMaker. Which feature provides built-in experiment management?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Experiments with trials and trial components",
      "B": "SageMaker Clarify",
      "C": "Resource tagging",
      "D": "CloudWatch Logs"
    },
    "explanation": "SageMaker Experiments lets you organize runs into experiments and trials, capturing hyperparameters, metrics, and artifacts for reproducibility."
  },
  {
    "id": "40ce601f70d2d8ca24a0702d0150227d6dd7f2a791fd0946818832439743fdc5",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your regression model will be used in a financial application where large errors are severely penalized. Which evaluation metric should you prefer?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Root Mean Square Error (RMSE)",
      "B": "Mean Absolute Error (MAE)",
      "C": "R\u00b2 (coefficient of determination)",
      "D": "Mean Absolute Percentage Error (MAPE)"
    },
    "explanation": "RMSE penalizes larger errors quadratically, aligning with applications that severely penalize large deviations."
  },
  {
    "id": "d45a32496009e24fe7a43b9bbcc7f704684a0f613b2bca5f7492a9c486796c11",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "To optimize for recall over precision in a binary classifier, you decide to adjust the decision threshold. How should you determine the new threshold?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set threshold to 0.5 regardless of metrics",
      "B": "Maximize accuracy on validation data",
      "C": "Use the threshold that maximizes ROC AUC",
      "D": "Analyze the precision-recall curve to find the point achieving the desired recall"
    },
    "explanation": "Precision-recall curves show trade-offs for different thresholds; selecting the threshold that meets your recall requirement is appropriate."
  },
  {
    "id": "059fece6b64f53a657a01826ceec18b4dc3fabd7e88a802865349c0dd463abf3",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You have a multi-class classification problem with imbalanced class frequencies. You want each class to contribute equally to overall performance. Which averaging strategy do you use for F1 score?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Micro average",
      "B": "Macro average",
      "C": "Weighted average",
      "D": "Sample average"
    },
    "explanation": "Macro-averaged F1 computes F1 per class then averages equally, ensuring each class contributes equally despite imbalance."
  },
  {
    "id": "b2fa9f14ce93c282c753d14aeef04f928354d6658fb30a570ba52bf0a2140622",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your model is deployed in a streaming inference pipeline. You need to detect sudden drops in model accuracy over recent data without storing all predictions. Which approach is best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Query historical S3 logs daily",
      "B": "Compute a one-off accuracy on a large batch",
      "C": "Use SageMaker Model Monitor with a sliding window and accuracy rule",
      "D": "Use CloudWatch Metrics alone"
    },
    "explanation": "Model Monitor can continuously compute metrics over sliding windows on production data without storing all raw predictions."
  },
  {
    "id": "117873f82fbc734949ac04c7ba558be7508a16d599208b1d000b1418d705927f",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Business requirements specify that predicted loan amounts must not decrease as applicant income increases. Which technique helps you verify this monotonic relationship in your deployed model?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Generate a partial dependence plot (PDP) for income",
      "B": "Examine the confusion matrix",
      "C": "Use a data drift baseline",
      "D": "Calculate overall RMSE"
    },
    "explanation": "A PDP shows the average model prediction as a function of a feature, allowing you to verify monotonicity constraints."
  },
  {
    "id": "2c69e428649275dce7351ee3e57d0bd50e827ddaea90e2a1dc805b24b5f6da6e",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You suspect your neural network is overfitting early in training. Which SageMaker Debugger built-in rule can alert you when validation loss starts increasing while training loss decreases?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "LossNotDecreasing",
      "B": "VanishingGradient",
      "C": "Overflow",
      "D": "Overfit"
    },
    "explanation": "The Overfit rule in SageMaker Debugger flags when validation loss increases while training loss continues to decrease, a sign of overfitting."
  },
  {
    "id": "5e61638fbedacca3d44e7a204e6bebdadb5e14353cf796eef5df16313f73c253",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Given confusion matrix counts TP=80, FP=20, FN=10, TN=890, what is the false positive rate (FPR)?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "20/(20+890) = 0.022",
      "B": "20/(80+20) = 0.20",
      "C": "10/(10+890) = 0.011",
      "D": "80/(80+10) = 0.89"
    },
    "explanation": "FPR = FP/(FP+TN) = 20/(20+890) \u22480.022."
  },
  {
    "id": "996aab25071dfe37ff9626a112386af9d2d998b61db0c461c17bbc0f6c65e553",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You want to evaluate if two classifiers produce significantly different error rates on the same test set. Which statistical test is most appropriate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Paired t-test on accuracy scores",
      "B": "McNemar\u2019s test",
      "C": "Chi-square test for independence",
      "D": "Wilcoxon signed-rank test"
    },
    "explanation": "McNemar\u2019s test compares paired categorical outcomes (correct vs incorrect) from two classifiers on the same instances."
  },
  {
    "id": "f20e32b12fa1ecbbcc8617f759705db999021c04253210b5b0169e0d958bbb95",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "After deployment, your model\u2019s input distribution remains stable but prediction error gradually increases. What type of drift does this indicate?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Covariate shift",
      "B": "Label shift",
      "C": "Data drift",
      "D": "Concept drift"
    },
    "explanation": "Concept drift occurs when the relationship between inputs and targets changes, causing error to rise despite stable input distribution."
  },
  {
    "id": "560d10fbc43dd11c454861a7d9995fadfadd2de65515b8e8b3406e908044129d",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You observe gradients in the first layers of your network are vanishing, slowing learning. Which SageMaker Debugger rule detects this issue directly?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "VanishingGradient",
      "B": "LossNotDecreasing",
      "C": "Detection of hung jobs",
      "D": "WeightDecay"
    },
    "explanation": "The VanishingGradient rule monitors gradient norms and flags when they are near zero, indicating vanishing gradients."
  },
  {
    "id": "439a3912530905f8a15e52842fdf21c3faadc8179cfdf6ed3501a4a6da614b56",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your pruned model achieves 40% size reduction with only a 0.5% drop in accuracy. To decide whether to deploy it, you should analyze:",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Only the accuracy difference",
      "B": "The trade-off between resource savings (memory, latency) and accuracy loss",
      "C": "Global SHAP importance values",
      "D": "Training time differences"
    },
    "explanation": "Deployment decisions should weigh resource savings (e.g., reduced latency, memory) against any drop in accuracy to determine overall benefit."
  },
  {
    "id": "b1e640672de737d7692f74394eba9043b5c6cea4d5612143053bf364a6093f5b",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You want to assess whether your classifier\u2019s probability outputs are well calibrated. Which metric or tool should you use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ROC AUC",
      "B": "Precision-Recall AUC",
      "C": "Log-loss (cross-entropy)",
      "D": "F1 score"
    },
    "explanation": "Log-loss penalizes incorrect probability estimates and is sensitive to calibration, making it suitable for assessing probability calibration."
  },
  {
    "id": "035f584ca5ae16ef63ae76fdb66b74929ad317d087a2a1e98b99948f705fe30e",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You need to compare model performance across multiple user segments (e.g., age groups) and give equal importance to each segment. Which aggregation of segment-level AUC metrics should you use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Arithmetic mean of AUC for each segment",
      "B": "Overall AUC on combined data",
      "C": "Weighted average by segment size",
      "D": "Maximum AUC across segments"
    },
    "explanation": "The unweighted (arithmetic) mean of segment-level AUCs treats each segment equally regardless of its size."
  },
  {
    "id": "d57ab13555da03a061f044c6f18e6c8e101fc4afb11e02bdec145bb90718865f",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your cost matrix penalizes false negatives at 10\u00d7 the cost of false positives. Which evaluation metric best aligns with this cost sensitivity?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Expected cost computed from confusion matrix and cost weights",
      "B": "ROC AUC",
      "C": "Log-loss",
      "D": "Overall accuracy"
    },
    "explanation": "Computing the expected cost directly using confusion matrix counts and cost weights aligns evaluation with your business cost matrix."
  },
  {
    "id": "00a0b5e3ee964f1a1e54f19f4208497a96f724278e2ebdf7c347a355a47ce0fd",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "A model shows high overall accuracy but a high false negative rate on a protected group. To monitor fairness over time, which metric should you track with SageMaker Clarify?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Overall RMSE",
      "B": "ROC AUC",
      "C": "Data drift percentage",
      "D": "Difference in true positive rates (equal opportunity difference)"
    },
    "explanation": "Equal opportunity difference measures TPR gap between groups, highlighting unfair false negatives for protected attributes."
  },
  {
    "id": "48649e2a6b964d6781730a26b6b05ea0791770015a55a337b1dd0017687f3d5e",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "To detect label distribution changes in production (label drift), which SageMaker Model Monitor job and baseline do you configure?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ModelQualityMonitor with a label distribution baseline",
      "B": "DataQualityMonitor with feature baseline",
      "C": "ModelBiasMonitor with bias baseline",
      "D": "ModelExplainabilityMonitor with SHAP baseline"
    },
    "explanation": "ModelQualityMonitor can track label statistics in inference data against a baseline to detect label drift."
  },
  {
    "id": "cc3ac53ea8afa67c8b9dcb0d1036788d8db1e821d295b46ff49e9e29cfc21570",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You need to choose a threshold for a binary classifier that minimizes a custom cost function of TPR and FPR. Which process accomplishes this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Select threshold\u2009=\u20090.5",
      "B": "Maximize ROC AUC",
      "C": "Minimize log-loss",
      "D": "Compute cost for each threshold on validation probabilities and choose the minimum"
    },
    "explanation": "Evaluating the custom cost at each threshold on validation data lets you select the threshold minimizing your cost function."
  },
  {
    "id": "348f4383d5e81c63a47b69274e2770ab8f36bd9704f0a07b4676daf08fc4a0bb",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your regression model predicts three related targets simultaneously. To evaluate performance treating each target equally, you should:",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Compute RMSE per target and report their average",
      "B": "Compute overall RMSE across all predictions",
      "C": "Weight each target by its variance in a single loss",
      "D": "Report only the highest RMSE target"
    },
    "explanation": "Averaging per-target RMSE ensures each output is given equal importance in performance evaluation."
  },
  {
    "id": "3c0b4fc1f92c5ea2386ee366f0cdf18de5a3080717a20831c39a0f63fdd94964",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your classifier\u2019s test set performance fluctuates significantly between runs. Which validation technique reduces variance and yields a more stable performance estimate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "K-fold cross-validation",
      "B": "Using a single larger test set",
      "C": "Bootstrapping only",
      "D": "Early stopping"
    },
    "explanation": "K-fold cross-validation averages performance over multiple splits, reducing variance due to data sampling."
  },
  {
    "id": "f0cd84d0022c4976cf2d93c958e7926eedfa9185ca50d1508407c0be41a539b1",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You want to visualize model performance degradation over time in production. Which solution best automates this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Export logs daily and plot manually",
      "B": "Use SageMaker Model Monitor to publish quality metrics to CloudWatch dashboards",
      "C": "Run ad-hoc batch accuracy reports",
      "D": "Embed metrics in application code"
    },
    "explanation": "Model Monitor can stream quality metrics to CloudWatch, allowing automated dashboards tracking performance over time."
  },
  {
    "id": "c9487f2416b907a6e01c0063b05d7763ca096c66b3af0d043f087c27a79296ea",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You see a sudden increase in false positives without any change in input distribution. What type of drift is this, and what action should you take?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Covariate drift; retrain on new features",
      "B": "Concept drift; investigate changes in the real world and retrain",
      "C": "Data drift; increase data validation",
      "D": "Label shift; adjust class weights"
    },
    "explanation": "A rise in error despite stable inputs indicates concept drift; investigate underlying changes and retrain the model accordingly."
  },
  {
    "id": "102c13c1ba82e628b22173b5dd51aab22b6159a01e552193726d56c6ef37d6d7",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Observation of rapidly growing gradient norms across layers suggests exploding gradients. Which SageMaker Debugger rule will catch this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "VanishingGradient",
      "B": "LossNotDecreasing",
      "C": "ExplodingGradient",
      "D": "WeightDecay"
    },
    "explanation": "The ExplodingGradient rule monitors for abnormally large gradient norms, flagging exploding gradient issues."
  },
  {
    "id": "448ae2b79b1179d6070b76981842ae986cc84713ed7ace14a6aed8d3b8832152",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You use SageMaker Clarify to monitor feature attribution drift. Which monitor type and baseline should you configure?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ModelExplainabilityMonitor with a SHAP value baseline",
      "B": "DataQualityMonitor with a distribution baseline",
      "C": "ModelBiasMonitor with bias all baseline",
      "D": "ModelQualityMonitor with accuracy baseline"
    },
    "explanation": "ModelExplainabilityMonitor compares new SHAP attributions against a baseline to detect feature attribution drift."
  },
  {
    "id": "2c73bb8211158393f08396e24c4e5194aa1d208bca8fc295e9c2bda78d94787f",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A data engineer needs to join a 1 TB Parquet dataset in Amazon S3 with a DynamoDB table of product metadata and register features in SageMaker Feature Store daily with minimal operational overhead and within a 2-hour window. Which approach should the engineer choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Data Wrangler to import both datasets, perform the join, then execute a BatchPutRecord job to ingest features into Feature Store.",
      "B": "Schedule an AWS Glue ETL job to read the Parquet data, use the DynamoDB Spark connector to join the tables, write results back to S3, and trigger a SageMaker processing job to ingest into Feature Store.",
      "C": "Set up AWS Glue Streaming ETL to capture DynamoDB Streams updates and continuously merge against S3 data, writing directly into the online store.",
      "D": "Deploy an AWS Lambda function triggered by each S3 PUT to read and join records on the fly, then call PutRecord for each feature into Feature Store."
    },
    "explanation": "Scheduling a bulk AWS Glue ETL job best handles 1 TB batch processing within a time window and decouples transformation from per-record Lambda overhead."
  },
  {
    "id": "e636e68d56f498b4b10d0ca269e0f4853f1ccf9963df94ab1ac4a783a673b1fa",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A feature team must encode a categorical feature with 10 million distinct values for use in a neural network. They need to minimize memory and avoid introducing ordinal bias. Which encoding technique and pipeline step should they implement?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use one-hot encoding in SageMaker Data Wrangler to generate 10 million binary columns, then apply feature selection.",
      "B": "Apply label encoding in DataBrew, converting each category to an integer index, and feed directly to the network.",
      "C": "Use feature hashing in a SageMaker processing job to map categories into a fixed-size hash vector, then normalize hashed features.",
      "D": "Group rare categories into an \"Other\" bucket before one-hot encoding in AWS Glue ETL."
    },
    "explanation": "Feature hashing maps high-cardinality categories into a fixed-size vector without ordinal bias and controls memory footprint."
  },
  {
    "id": "11e429d88f8f589c2e53eef7b1ee74a7f4547d3f9980a26fc24cca1d543c05dd",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A time-series dataset has irregular timestamps and missing readings. A data scientist needs to impute missing values and generate lag features in SageMaker Data Wrangler for model training. Which sequence of transformations should they apply?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Resample to a fixed interval with forward-fill imputation, compute lag features using the shifted timestamp, then scale features.",
      "B": "Compute lag features first (with gaps), drop NA, then apply KNN imputation to fill missing values.",
      "C": "Apply drop-rows where any null exists, generate lag features on remaining data, and finally apply standard scaling.",
      "D": "Use quantile imputation on raw data, then apply rolling window aggregations for lag features, and normalize."
    },
    "explanation": "Resampling with forward fill ensures consistent intervals before computing lags; then scaling preserves temporal relationships."
  },
  {
    "id": "a0a32c63676fbd23c7de9dcf348153e3ee56a9ce1b5f82f751552cb19da0d5b4",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A large training dataset in S3 must be deduplicated by a composite key before feature scaling. The data volume exceeds what a single SageMaker Data Wrangler instance can process. What architecture meets the requirements?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Use SageMaker Data Wrangler with distributed compute enabled for dedupe and scaling.",
      "B": "Use an AWS Lambda function to split and dedupe small partitions, then reassemble.",
      "C": "Use AWS Glue DataBrew to dedupe by key and apply scaling transformations inline.",
      "D": "Use an AWS Glue Spark job on EMR to dedupe by composite key, write results to Parquet in S3, then use Data Wrangler for scaling on the reduced dataset."
    },
    "explanation": "A Spark job on EMR can handle large-scale deduplication; downstream Data Wrangler can operate efficiently on the smaller deduped data."
  },
  {
    "id": "0d493321fefba26638ad2b02de1e706c3b5b33300e0f1d4a220f3340362c43be",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A team must apply quantile binning on a continuous feature to reduce noise, then one-hot encode the bins. They need to evaluate bin edges on training only and apply identical bins to validation/test sets. How should they implement this in SageMaker Data Wrangler?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create a recipe step to compute quantile bin edges on the training partition, save edges as a parameter, then apply same edges in subsequent steps to all partitions before one-hot.",
      "B": "Compute quantile bin edges over full dataset in Data Wrangler, then split into train/validation/test and one-hot encode separately.",
      "C": "Use AWS Glue DataBrew to compute bins on training, export bin definitions, then import back into Data Wrangler for splits.",
      "D": "Perform quantile binning and one-hot encoding in a SageMaker processing job where edges are recomputed per batch."
    },
    "explanation": "Computing bins on training only and saving edges ensures no leakage; applying stored edges uniformly maintains consistency."
  },
  {
    "id": "68b13e262a93f13459220b88b4e1e17ef45249be71eab9ded87ec02da32b0e2c",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A streaming application ingests clickstream JSON records into Kinesis and needs to parse, clean, and feature-engineer fields in near real-time. Which architecture minimizes latency and offloads transformation operations?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Lambda to poll Kinesis, transform records inline, then push to SageMaker Feature Store.",
      "B": "Write a Kinesis Data Firehose delivery stream with inline record transformation using AWS Glue.",
      "C": "Deploy a Kinesis Data Analytics (Apache Flink) application to parse JSON, apply scalar and one-hot encodings, then write transformed records to an output stream.",
      "D": "Use SageMaker Processing on a schedule to pull batches from Kinesis and process them."
    },
    "explanation": "Kinesis Data Analytics with Apache Flink provides low-latency, continuous transformations at scale, offloading work from Lambda."
  },
  {
    "id": "544c00e187541a3c7e4f352d2b9cc57ff7c59a216e0c7ac2bb4d28a97b89510a",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A pipeline must mask PII columns before any transformations to comply with compliance. They have CSV data with 5 sensitive columns. Using AWS Glue DataBrew, which recipe steps ordering ensures compliance and minimal reprocessing?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Apply all feature scaling, encoding, and missing-value imputation, then mask PII at the end.",
      "B": "Mask PII using column-level encryption, then apply transformations in the same recipe.",
      "C": "Branch the job: first do transformation, output safe data; second, mask PII on original and join results.",
      "D": "Create two DataBrew steps: first mask (using built-in Mask Column), then reference masked columns in subsequent transformation steps."
    },
    "explanation": "Masking PII first in the recipe prevents any downstream steps from accessing raw PII and avoids reprocessing masked data."
  },
  {
    "id": "6c359cf828c84143ccc6473d04695322e583eea7bbb83a25147c5dc9f98f5587",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A dataset contains a categorical feature with moderate cardinality and missing values. A data scientist wants to impute missing values as \"Unknown\" before one-hot encoding in SageMaker Data Wrangler. Which setting order in the recipe is correct?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "One-hot encode first, then treat missing values in generated dummy columns.",
      "B": "Apply a Fill Missing Values step on the raw column to \"Unknown\", then apply a One-Hot Encoding step on the filled column.",
      "C": "Drop rows with missing values, then one-hot encode remaining categories.",
      "D": "Run one-hot encoding with the \"Include Nulls\" option, then rename the \"null\" column to \"Unknown\"."
    },
    "explanation": "Imputing missing values before encoding ensures the \"Unknown\" category is represented correctly in the dummy variables."
  },
  {
    "id": "e30ca4305722efed8a25f16e4e1499a0cda57ce8987779fb583d06fb55e2c7ea",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A text feature requires tokenization and vectorization using a pretrained BERT tokenizer. They need to integrate this into a SageMaker feature engineering pipeline. Which approach is best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Data Wrangler built-in text tokenizer step with custom vocabulary.",
      "B": "Invoke AWS Comprehend batch API to tokenize, then import tokens to Data Wrangler.",
      "C": "Build a SageMaker Processing script container that loads the pretrained BERT tokenizer, tokenizes text, and writes token IDs to S3 for the next pipeline step.",
      "D": "Use AWS Glue Spark job to apply the BERT tokenizer via a Glue Python shell job."
    },
    "explanation": "A custom processing container in SageMaker Processing provides full control over the pretrained tokenizer and integrates cleanly into ML pipelines."
  },
  {
    "id": "cb31442a0cc1ba52a6fd2d2d2707d4d81cd687c25292619abf0b7f2f60d426c2",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A data scientist wants to scale numeric features using Z-score normalization but only using statistics from the training set. They plan to scale validation/test sets identically. How can they configure SageMaker Data Wrangler?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Apply Z-score normalization in a DataBrew recipe after splitting, selecting all partitions together.",
      "B": "Use a SageMaker Processing job to compute mean/std on full dataset, then scale each partition separately in Data Wrangler.",
      "C": "In Data Wrangler, split the dataset into train/validation/test, add a Normalize step in the recipe that refers to the training split only, and ensure the recipe applies the same parameters to other splits.",
      "D": "Compute mean/std manually outside of Data Wrangler and hard-code values in a recipe."
    },
    "explanation": "Data Wrangler can compute normalization parameters on the training split and apply them uniformly to other splits within one recipe."
  },
  {
    "id": "41485b60fc323931888d9822dff7f54e229bef25e34518757cb1be6014a8741a",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "During a SageMaker Data Wrangler flow, a shuffle step is added before splitting data into train/validation/test. The resulting model shows data leakage. What was the mistake?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Shuffling after splitting caused overlap across sets.",
      "B": "Shuffle did not include seed, producing non-reproducible splits.",
      "C": "Shuffle was applied only to the training split, leaving test data ordered.",
      "D": "Shuffle was applied before splitting, mixing records across temporal boundaries and causing leakage."
    },
    "explanation": "Shuffling before splitting time-series or grouped data can mix related records across splits, introducing leakage."
  },
  {
    "id": "0f90535197aa207d1c4a35d47b99e22b726a8beb8f26bc35814670c7f1e05dd6",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A data engineer needs to generate rolling window statistics (mean, std) per user over the last 7 days from clickstream JSON data in S3. Data volume is 500 GB/month. Which cost-effective approach should they use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Data Wrangler to load all data and compute rolling stats with Python transforms.",
      "B": "Use an AWS Glue Spark job with window functions to compute rolling statistics, write Parquet outputs back to S3.",
      "C": "Load raw data into DynamoDB and use DynamoDB Streams with Lambda to update rolling metrics.",
      "D": "Use SageMaker Feature Store with on-the-fly aggregation in training jobs."
    },
    "explanation": "A Glue Spark job with window functions scales to 500 GB and performs aggregations efficiently before model input."
  },
  {
    "id": "b8e0c4afbbcfaf776dc44b91422c71af787564d530b0e6b58c0e7bc459236b44",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A team must remove outliers defined as values beyond 3 standard deviations for a numeric feature. They want to log how many records were removed. In SageMaker Data Wrangler, how should they configure this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a Filter recipe step to remove outliers, then use the Data Wrangler UI to view counts after run.",
      "B": "Use a Custom Transform step with a Pandas script to drop outliers and log metrics to CloudWatch.",
      "C": "Add an Aggregate Recipe step to calculate mean/std, then a Filter step referencing those values, and enable the record count output option to capture removed record counts.",
      "D": "Export data to a processing job, implement outlier removal in code, and write counts to S3."
    },
    "explanation": "Data Wrangler\u2019s recipe can calculate statistics and filter in separate steps, and the built-in count option provides removal metrics."
  },
  {
    "id": "546e6350c97874d3955171cb4a6f42eebbe82e6eebfc1f9c497fee4fa4530b71",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A feature store offline dataset must be kept in sync with latest transformations from SageMaker Data Wrangler. Which mechanism ensures atomic replacement without downtime?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Write transformed data to a staging S3 prefix, then use CFN to update the offline store import job to point to new prefix.",
      "B": "Overwrite the existing S3 prefix directly with new Parquet files in place.",
      "C": "Append new transformed data to the existing prefix, and use a Glue crawler to update partitions.",
      "D": "Delete the offline store table and recreate it with new data after transformation."
    },
    "explanation": "Using a staging prefix and atomic pointer switch avoids partial reads and downtime."
  },
  {
    "id": "193da31815dd47cfda11dbf5b1639bae72af79e2b807c7debbf9f7fc962c66af",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "Continuous feature updates from AWS IoT devices stream into Kinesis Data Streams. A data scientist needs to window, normalize, and encode features before writing to Feature Store online. What real-time pipeline minimizes component count?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Kinesis Data Firehose with AWS Lambda transform to process and send to Feature Store.",
      "B": "Use a SageMaker endpoint to receive raw records, transform, and BatchPutRecord.",
      "C": "Use AWS Glue Streaming ETL job to read, transform, and write to Feature Store offline.",
      "D": "Deploy Kinesis Data Analytics (Apache Flink) application to window, normalize, encode, and call PutRecord API to Feature Store online."
    },
    "explanation": "Kinesis Data Analytics provides a single low-latency stream processing engine capable of calling AWS APIs directly."
  },
  {
    "id": "186919604716fde9b9d5dacc577908d70737be877798de650b6a048930b8c6b2",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "An image classification pipeline needs to apply resizing, normalization, and five types of augmentation (rotation, flip, crop, noise, color jitter) in SageMaker processing. To maximize GPU utilization and reproducibility, what should the team do?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Chain multiple processing jobs sequentially, each handling one augmentation type.",
      "B": "Use a single SageMaker Processing job with a PyTorch container that applies resizing, normalization, and augmentations in one GPU-accelerated DataLoader pipeline.",
      "C": "Implement augmentations in AWS Lambda functions triggered by S3 events for each image.",
      "D": "Use Data Wrangler custom transform steps for each preprocessing function and export as a combined dataset."
    },
    "explanation": "A single GPU-accelerated processing job with PyTorch DataLoader ensures reproducibility and efficient parallel augmentation."
  },
  {
    "id": "cc057d02a8d5e123f3902d489c183d3d1816cbee5e5a5ca22c73a3ba7216f3d4",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A dataset contains repeated nested JSON fields that must be flattened for feature extraction. In AWS Glue DataBrew, which recipe step correctly handles arbitrary nesting levels?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a Pivot recipe step on the nested column.",
      "B": "Use a Select recipe step and specify JSONPath for each nested field.",
      "C": "Use a Join recipe step with the same column.",
      "D": "Use the Unnest recipe step to iteratively flatten nested JSON arrays and objects into top-level columns."
    },
    "explanation": "The Unnest step is designed to flatten arbitrarily nested JSON structures into columns automatically."
  },
  {
    "id": "e0c9a22095dcc9bd8c81d3dc83d075a83192dc7f209db3ca4ebdcc7dc177f4e7",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A feature with highly skewed distribution needs log transformation before scaling. A data scientist wants to ensure negative values are handled. In Data Wrangler, what transformation sequence should be used?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Scale feature with MinMax, then apply log transform.",
      "B": "Add a constant offset to make values positive, apply log transform, then standardize.",
      "C": "Apply Box-Cox transformation directly (Box-Cox cannot handle negatives).",
      "D": "Filter out negative values, then apply log transform on positives."
    },
    "explanation": "Adding a constant offset ensures all values are positive before log transform; standardizing afterwards preserves distribution."
  },
  {
    "id": "f590b204c5f4046535572c746fee7037dff99ab676a9256a48f2683d5fd36e61",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A team must detect and drop duplicate records defined by a composite key, but preserve the earliest timestamp record. They want to log count of dropped duplicates. What is the correct approach in Data Wrangler?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use the Remove Duplicates recipe step on the composite key then sort ascending by timestamp.",
      "B": "Sort by timestamp and use the Remove Duplicates step without additional configuration.",
      "C": "Sort descending by timestamp, then use the Remove Duplicates step on the composite key with \"keep first occurrence\" and enable Logging to capture drop counts.",
      "D": "Use a Custom Transform step with a Pandas script to drop duplicates and write counts to CloudWatch."
    },
    "explanation": "Sorting descending ensures the earliest timestamp is last, then keeping the first occurrence drops later duplicates and logs counts."
  },
  {
    "id": "fc0b3a402f4d55870a335950ea1adaa6b8446ac0334c11b5e6e7992cce953cf3",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A text corpus requires feature hashing of TF-IDF vectors into 512 dimensions before training. Which SageMaker toolchain should be used and why?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a SageMaker Processing job with a scikit-learn container to compute TF-IDF and apply FeatureHasher transformer in one job.",
      "B": "Use AWS Glue DataBrew to compute TF-IDF and hashing in recipe steps.",
      "C": "Use SageMaker Data Wrangler built-in TF-IDF and hashing steps.",
      "D": "Use AWS Lambda to compute and hash vectors on each record."
    },
    "explanation": "A processing job with scikit-learn provides efficient batch computation of TF-IDF and hashing integrated into the ML pipeline."
  },
  {
    "id": "9aa505abfe9b021963c86eab4a7107fc3b7798601a6c1afb4772971e86ba095a",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A feature store batch export job must produce engineered features that include one-hot encoded categories and normalized numerics. The export time window is 24 hours. Which orchestration ensures reproducibility and minimal drift?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Manually run a Data Wrangler flow to export features daily.",
      "B": "Schedule a Lambda that calls PutRecord for each export.",
      "C": "Use AWS Glue job to read offline store and apply transformations at export time.",
      "D": "Define a SageMaker Feature Store Batch Transform pipeline with saved Data Wrangler recipe steps to export and transform atomically."
    },
    "explanation": "Embedding Data Wrangler recipe in a Feature Store batch pipeline ensures consistent transformations and reproducibility."
  },
  {
    "id": "5167d0cae51ea8a08071423823f1c1ccc40f70517e1470cac213b4f64a8f3558",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A data scientist wants to detect measurement bias in labeled data prior to model training. Which AWS tool and workflow should they use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Glue Data Quality to compute missing value rates per label.",
      "B": "Use SageMaker Clarify\u2019s Data Bias job to generate pre-training bias metrics (e.g., CI, DPL) and produce a report for review.",
      "C": "Use SageMaker Data Wrangler to compute feature distributions per class.",
      "D": "Use Amazon Athena to run SQL queries comparing label distributions across partitions."
    },
    "explanation": "SageMaker Clarify is specifically designed to compute pre-training bias metrics and provide actionable reports."
  },
  {
    "id": "50fe2a325fd36e01ac962a82ff221da239e87bd434a7409ae8858221922daf58",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "An offline Spark feature pipeline writes to Parquet partitioned by date. A validation job must only process the latest two days and ignore older partitions. How can they configure an AWS Glue job?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a dynamic frame with push_down_predicate on the date partition to filter to last two days before transformation.",
      "B": "Read all partitions and filter in code.",
      "C": "Use a Glue crawler to catalog only two latest partitions.",
      "D": "Use AWS Glue DataBrew with a filter recipe to drop older partitions."
    },
    "explanation": "Using push_down_predicate filters partitions at read time, reducing data scanned and improving performance."
  },
  {
    "id": "2fd6f01c02c1b9a3fd336405f1f8f4d74bbb7e1c66c4009a95880b4e0ec3f288",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A feature requires ranking users by activity per region, generating a rank feature. Which tool and technique should they use to compute this in a SageMaker pipeline?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Glue DataBrew with window aggregate recipe step.",
      "B": "Use a SageMaker Processing job with pure Python for ranking.",
      "C": "Use SageMaker Processing with a Spark container to apply window ranking functions by region, writing results to S3.",
      "D": "Use Lambda and Step Functions to orchestrate row-by-row ranking."
    },
    "explanation": "A Spark container in a processing job can efficiently compute window functions and scale to large user sets."
  },
  {
    "id": "b5d313495f680681131765ac9f6328c22995dfccf9bc862c85375cbc1247e619",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "During a Data Wrangler flow, a customer ID column is misclassified as numeric, preventing correct encoding. How can this be fixed?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Drop the column and reload.",
      "B": "Override the inferred data type to String in the Data Wrangler schema settings, then apply categorical encoding.",
      "C": "Cast the column inside a custom transform only.",
      "D": "Use AWS Glue job to recast and save back to S3 before Data Wrangler."
    },
    "explanation": "Overriding the schema inference directly in Data Wrangler allows correct downstream categorical encoding without external ETL."
  },
  {
    "id": "3b093b84e89db98cb9a7c9295e8f593f167263fcc5b5e92a52e833550aafbfee",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A data scientist must anonymize email addresses by hashing them before any aggregation. The hashing function must be consistent across training and inference. How should they implement this in a SageMaker processing pipeline?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a Data Wrangler custom step with Python hashing, then export hashed emails.",
      "B": "Use AWS Glue DataBrew Mask Column with SHA256.",
      "C": "Use a Lambda function triggered by S3 events to hash emails.",
      "D": "Include a SageMaker Processing step with a custom container that applies SHA256 to the email column and writes to Parquet for downstream use."
    },
    "explanation": "A custom processing step ensures identical hashing logic in both training and inference pipelines."
  },
  {
    "id": "570b86c72a515fcf63b58994c4b63b932b6fb6788efed335bbcea9b4c6c6100d",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A dataset of IoT sensor readings requires deduplication, imputation of missing values using KNN, and feature scaling. Which order in a DataBrew recipe ensures correct results and performance?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Scale, dedupe, then KNN impute.",
      "B": "Deduplicate first, KNN impute missing values, then apply scaling.",
      "C": "Impute missing values first, then dedupe, then scale.",
      "D": "Perform all three in parallel using branching steps."
    },
    "explanation": "Deduplication should occur before imputation to avoid artificial neighbors; scaling is last to ensure normalized neighbor distances."
  },
  {
    "id": "3f94e3b041939d85039f792c3d35887a9c2676c13720441f2e3c58e646bda871",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A team needs to apply custom Python feature transformations (e.g., trigonometric functions) on numeric columns and integrate into SageMaker Pipelines. Which component should they use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A SageMaker Processing step with a custom Python script container.",
      "B": "A Data Wrangler custom recipe step.",
      "C": "A Lambda function in CodePipeline.",
      "D": "AWS Glue DataBrew Python transforms."
    },
    "explanation": "A SageMaker Processing step in the pipeline allows custom scripts and integrates directly with SageMaker Pipelines."
  },
  {
    "id": "95a174e93f308b3a0048edc9890dbf03605221af064224767640a8a5566c8861",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A dataset transformation in Data Wrangler fails due to schema drift: new columns appear. How can the recipe handle unknown columns without failing?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Manually update the recipe to include new columns each time.",
      "B": "Restrict the flow to specified columns only and drop unknown ones.",
      "C": "Enable the \"Ignore new columns\" option in the Data Wrangler recipe settings to skip unknown fields.",
      "D": "Use a SageMaker Processing job instead for dynamic schema."
    },
    "explanation": "Data Wrangler\u2019s ignore new columns setting prevents failures when unexpected fields appear."
  },
  {
    "id": "4847fed0c6f8c8a359293f323b3277b6d99ecfafb84f90b5ff4dc11dcf02eb6a",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A table join in SageMaker Data Wrangler is running out of memory because one table is 200 GB. Which alternative solution scales to this size?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase the Data Wrangler instance type to more memory.",
      "B": "Perform the join using an AWS Glue Spark ETL job and write result to S3, then import into Data Wrangler.",
      "C": "Use a SageMaker Processing job with Pandas on a GPU instance.",
      "D": "Split the larger table into chunks and join sequentially in Data Wrangler."
    },
    "explanation": "An AWS Glue Spark ETL job is designed to handle large-scale joins and can write partitioned outputs for downstream use."
  },
  {
    "id": "779fc9b44df277c91f63cee895d728a0841013e1b2c4f3411245568c989e8151",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A feature extraction requires computing pairwise differences between two timestamp columns per record. Which recipe step in Data Wrangler should they choose and why?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use the Compute Feature step with a custom expression (timestamp2 - timestamp1) to derive duration.",
      "B": "Use the Aggregate step grouping on record ID.",
      "C": "Use the Pivot step on timestamp fields.",
      "D": "Use a Custom Transform with a Pandas script to compute differences."
    },
    "explanation": "The Compute Feature step allows arithmetic expressions on columns and is more efficient than full Python transforms."
  },
  {
    "id": "60e3015299c2ffdbf80ce11affc05fe318e0c9f7c0e4136da2589bb220c5e93e",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A team must ensure that text tokenization and stop-word removal occur before computing count vector features in a Data Wrangler flow. How should they configure the recipe?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Add Count Vectorization step, then Tokenization, then Stop-word removal.",
      "B": "Add Stop-word removal, then Count Vectorization, then Tokenization.",
      "C": "Add Tokenization, then Count Vectorization, then Stop-word removal.",
      "D": "Add Tokenization, then Stop-word removal step, then Count Vectorization in sequence."
    },
    "explanation": "Tokenization must split text first, followed by stop-word removal on tokens, then count vectorization."
  },
  {
    "id": "20317c4ef09222a833693e4d988f8378187e9e55a4b581229cb0a184485c29e1",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A dataset stored as CSV in S3 must be converted to Parquet with snappy compression before feature engineering to improve I/O. Which tool and configuration should be used?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a SageMaker Processing job with a Pandas script to read CSV and write Parquet.",
      "B": "Use an AWS Glue ETL job with a PySpark script to read CSV, write Parquet with snappy compression, and partition by date.",
      "C": "Use SageMaker Data Wrangler export with Parquet output and compression.",
      "D": "Use AWS Lambda triggered per file to invoke Athena CTAS to create Parquet tables."
    },
    "explanation": "An AWS Glue Spark ETL job is well-suited for large-scale CSV to Parquet conversion with compression and partitioning."
  },
  {
    "id": "6f3c83fd164e1601c178df82e12ff2efcc0a6ff90192403a356abc1e174ac982",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A company must ingest 10 TB of historical telemetry data from an on-premises Hadoop cluster into Amazon S3 within 5 days for model training. The WAN link is limited to 200 Mbps and encryption at rest is required. Which ingestion method meets the transfer window while minimizing operational overhead?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Use AWS Snowball Edge to physically import the data into S3 with built-in encryption.",
      "B": "Use S3 Transfer Acceleration over the internet with SSE-S3 encryption.",
      "C": "Use AWS DataSync over the existing link with server-side encryption.",
      "D": "Deploy AWS Storage Gateway in Volume mode and snapshot to S3."
    },
    "explanation": "Snowball Edge provides accelerated, encrypted bulk transfer when network bandwidth is constrained, minimizing operational overhead."
  },
  {
    "id": "b46eff657bb8d9bee2c4ab7d00266c3548e1f3fa779e01c983d7a9ae60daf133",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An ML pipeline queries 500 GB of transaction logs in S3 via Athena, then ingests the results into SageMaker Data Wrangler. Which storage format and partitioning maximize query performance and minimize costs?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Store logs as CSV files partitioned by year/month.",
      "B": "Store logs as JSON files partitioned by customer ID.",
      "C": "Store logs as Apache Parquet files partitioned by date.",
      "D": "Store logs as unpartitioned text files in a single prefix."
    },
    "explanation": "Parquet\u2019s columnar format plus date partitioning reduces scanned data and cost for Athena queries and speeds ingestion."
  },
  {
    "id": "909af5bb6fa1891a2e8b9efc24237ed0356ca85e910733711318dd4eb5493626",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A real-time anomaly detection model requires streaming sensor data into S3 for batch retraining. Which combination of services ingests JSON records at low latency with ordering guarantees?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Kinesis Data Firehose delivering directly to S3.",
      "B": "Amazon Kinesis Data Streams then a Kinesis Data Firehose delivery stream to S3.",
      "C": "AWS Glue streaming ETL reading from S3 Event Notifications.",
      "D": "Amazon Managed Kafka writing directly to S3."
    },
    "explanation": "Kinesis Data Streams preserves ordering and low latency; Firehose can then buffer and batch to S3 automatically."
  },
  {
    "id": "4d623876a596d043908bd5114fec79d168f6e51516e1dbaa6191e29a283c81a8",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "Your model needs high-throughput access to feature data during training. You must choose between Amazon EFS, Amazon FSx for NetApp ONTAP, and Amazon S3. The data is >1 TB, highly structured, and training uses Spark on EMR. Which storage is best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon EFS with provisioned throughput.",
      "B": "Amazon FSx for NetApp ONTAP.",
      "C": "Amazon S3 with EMRFS Consistent View.",
      "D": "Store data on EMR HDFS volumes."
    },
    "explanation": "S3 with EMRFS scales to TBs, offloads management, and is cost-effective for large structured datasets; HDFS adds management overhead."
  },
  {
    "id": "57fa1dd74662478157dc80eb9e574455196fd8c23f433268db0bee6eacd42c19",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A team uses SageMaker Data Wrangler to profile 2 TB of CSV data stored in S3. They notice excessive request latencies. Which action will most reduce latency when reading from S3?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable SSE-KMS on the S3 bucket.",
      "B": "Convert CSV data to Apache Parquet with Snappy compression.",
      "C": "Enable S3 Object Lock on the bucket.",
      "D": "Enable S3 Versioning on the bucket."
    },
    "explanation": "Converting to Parquet reduces the number of S3 GET requests and data scanned, greatly lowering latency in Data Wrangler."
  },
  {
    "id": "a7badc23a5647d2c25d8a20cc7947473984af26cf447947a3ed423f7da392482",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An ML workflow must merge customer profile data in DynamoDB with clickstream logs in S3, then store results for training. Which approach minimizes custom code and operational complexity?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use AWS Glue ETL jobs with a DynamoDB connector to join and write to S3 in Parquet.",
      "B": "Export DynamoDB to S3 via Data Pipeline, then use Lambda to merge with clickstream.",
      "C": "Use SageMaker Processing to read both and write merged CSV to S3.",
      "D": "Stream clickstream into DynamoDB and query joins at training time."
    },
    "explanation": "AWS Glue ETL handles connectors and joins natively and writes Parquet to S3, reducing custom orchestration."
  },
  {
    "id": "ef4900fa8d22413cfd1e645eacb310b2039295ffc8b0ba88025bb55896696c3a",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "You must ingest 5 TB of Amazon RDS data into SageMaker Feature Store online store for low-latency feature retrieval. Which combination meets throughput and cost requirements?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Export RDS snapshot to S3, use Glue to write to Feature Store.",
      "B": "Use RDS native replication into DynamoDB, then batch ingest into Feature Store.",
      "C": "Run a SageMaker Processing job to read from RDS and write to offline store.",
      "D": "Use SageMaker Data Wrangler to connect to RDS and directly ingest records into Feature Store online store."
    },
    "explanation": "Data Wrangler can connect to RDS and ingest into Feature Store online, meeting throughput needs with minimal glue code."
  },
  {
    "id": "fab0f87342459b3fcc060e5eb8d1abaf4b92eb6112a848ea78e44fbda0ea413d",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A model requires streaming user events from mobile apps to S3 for nightly batch training. You need exactly-once delivery and minimal data loss. Which architecture is most appropriate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon Kinesis Data Firehose directly to S3.",
      "B": "Use AWS Lambda triggered by API Gateway to write to S3.",
      "C": "Use Amazon Kinesis Data Streams with a consumer that writes to S3 via Firehose.",
      "D": "Use Amazon SQS and a fleet of EC2 batch workers to write to S3."
    },
    "explanation": "Kinesis Data Streams ensures ordering and exactly-once semantics; Firehose delivers to S3 reliably with retries."
  },
  {
    "id": "7f874e94852dfc6dea78b33f02da1d481fb4e64efe95ca0b0d6afc01c076239a",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "During a cost audit, you discover that S3 GET and PUT costs for small files (<1 KB) used in training are high. How can you reduce costs while keeping data in S3?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch to S3 Standard-Infrequent Access.",
      "B": "Combine small files into larger Apache Avro container files.",
      "C": "Enable S3 Transfer Acceleration.",
      "D": "Enable S3 Intelligent-Tiering."
    },
    "explanation": "Combining small files into larger containers reduces request count and lowers GET/PUT request costs."
  },
  {
    "id": "540a8178967cd87df156fc0c2bbf6afae20dd9a72b509ebb31774a87cb734ba4",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A global company must ingest CSV sales data from multiple regional offices into a central S3 bucket. Some offices have poor upload bandwidth. How to ensure data arrives in the correct order and is fully available for batch processing in SageMaker?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use S3 Transfer Acceleration with directory replication.",
      "B": "Use AWS DataSync scheduled tasks from each office.",
      "C": "Use AWS Storage Gateway file gateway.",
      "D": "Configure regional AWS Kinesis Data Firehose streams to deliver to a central S3 bucket with buffering and ordering enabled."
    },
    "explanation": "Regional Firehose streams buffer data, preserve ordering per shard, and reliably deliver to central S3 for downstream batch."
  },
  {
    "id": "512b105d62124767d4681df60b3137f59696a7a3b6c71418086ccaa8c407cf7a",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "Your training data resides in a high-CPU EBS volume attached to an EC2 instance. To ingest data into SageMaker Data Wrangler at scale, which approach minimizes network bottlenecks?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Snapshot the EBS volume to S3 and let Data Wrangler read from S3.",
      "B": "Attach the EBS volume to the SageMaker Data Wrangler notebook instance.",
      "C": "Enable EBS multi-attach and mount on Data Wrangler instance.",
      "D": "Run a DataSync task from the EC2 instance to the Data Wrangler volume."
    },
    "explanation": "Snapshot to S3 offloads network traffic and lets Data Wrangler ingest efficiently from S3 rather than over direct EBS mounts."
  },
  {
    "id": "adbabf1c102b2d98042276cef69009ae7b154c49b65d1a6ccdf5219ff592f841",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A data scientist needs to join high-velocity Kafka events with a static dataset in S3 before training. They require minimal lag and schema evolution support. What ingestion pattern is best?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "Use Kinesis Data Firehose with Lambda to enrich events with S3 data.",
      "B": "Deploy Amazon Managed Streaming for Kafka with AWS Glue streaming ETL to join and write to S3.",
      "C": "Use SageMaker Processing to pull from Kafka and S3 periodically.",
      "D": "Use AWS Lambda triggered by Kafka Connect and query S3 DynamoDB export."
    },
    "explanation": "Glue streaming ETL supports schema evolution and continuous joins with Kafka and S3, delivering results to S3 in near real time."
  },
  {
    "id": "3391ec1fad8611a0812106e8ac6d5e36ead2b5194c6b8f8dac2de6443ad07102",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A compliance requirement mandates encrypting sensor data and storing in S3 Glacier after ingestion. The data is 8 TB per month. What solution meets compliance with the lowest storage cost while remaining queryable for ML?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Ingest to S3 Standard with SSE-KMS, transition to S3 Glacier Deep Archive immediately.",
      "B": "Ingest to S3 Glacier with client-side encryption.",
      "C": "Ingest to S3 Standard-IA with SSE-KMS, configure lifecycle to move to Glacier after 30 days.",
      "D": "Ingest to S3 One Zone-IA with SSE-KMS then transition to Glacier Flexible Retrieval."
    },
    "explanation": "Standard-IA with SSE-KMS and a 30-day lifecycle balances low cost while keeping data in S3 for Athena before Glacier transition."
  },
  {
    "id": "6479337a2e0f3551c0387e9124c80e064eeb89c766056e42c934be34588ec9f5",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "You must ingest and merge product catalog data from Amazon RDS and Alexa usage logs in S3 into SageMaker Feature Store offline store. How do you ingest with minimal code?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Glue jobs to extract from RDS and S3, transform, and write to Feature Store.",
      "B": "Use SageMaker Processing to read both sources and write offline store.",
      "C": "Export RDS to S3, then use Data Wrangler to merge and ingest.",
      "D": "Use AWS AppFlow to replicate RDS and merge with S3 logs."
    },
    "explanation": "Glue ETL natively connects to RDS and S3 and can write to Feature Store with minimal custom code."
  },
  {
    "id": "2108cc412ded82d524fe8bc410d45a5e41afe676357bd2110994db24bab02860",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An ML workflow streaming video metadata at 10 MB/sec must store data in S3 for batch training. Latencies <5 seconds acceptable. Which ingestion service provides auto-scaling and buffering without managing infrastructure?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon Kinesis Data Streams with manual shard scaling.",
      "B": "Use Amazon Kinesis Data Firehose to deliver to S3.",
      "C": "Run AWS Lambda to poll a custom endpoint and write to S3.",
      "D": "Use AWS Managed Kafka with MirrorMaker to S3."
    },
    "explanation": "Firehose auto-scales and buffers transient bursts, delivering to S3 with minimal management."
  },
  {
    "id": "7dd7ff5bcc10f8404b7db557d67cbc70b6d05363ebdd2c66a54185e17cd602af",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A dataset of images is stored in S3 and must be preprocessed via SageMaker Processing weekly. The team wants to minimize request overhead and latency when reading many small files. How should they reorganize the data?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable S3 Transfer Acceleration when reading each image.",
      "B": "Copy images to an EFS mount for processing.",
      "C": "Bundle images into TFRecord or RecordIO files and store in S3.",
      "D": "Store images as base64-encoded JSON in single S3 objects."
    },
    "explanation": "Bundling into TFRecord/RecordIO reduces object count and request overhead, speeding SageMaker Processing."
  },
  {
    "id": "c87aff9c8cbf1a9f282b38e1fc5b2099c6d41533cacd057a870cba231727ffa8",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A financial services company deployed a SageMaker real-time endpoint for credit risk scoring. They need to detect, within one hour, any drift in the distribution of the applicant_age and debt_to_income_ratio features compared to production baselines and send alerts. Which solution meets these requirements with minimal custom development?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable DataCaptureConfig on the endpoint, generate baseline statistics with ModelMonitor\u2019s built-in methods, create a monitoring schedule for input feature drift, and configure CloudWatch alarms on the monitoring job metrics.",
      "B": "Configure a Lambda function to read logs from CloudWatch Logs every hour, compute feature histograms, compare to baselines stored in S3, and send SNS alerts.",
      "C": "Use Kinesis Data Analytics to ingest endpoint invocation payloads, run SQL queries to detect distribution changes, and trigger alarms through CloudWatch.",
      "D": "Write a custom Spark job on EMR to read captured inference data, compute drift metrics, and schedule hourly cron jobs to publish alerts via Lambda."
    },
    "explanation": "A is correct because SageMaker Model Monitor can capture endpoint inputs, compute feature drift against baselines, and integrate with CloudWatch alarms with minimal coding. B and D require building custom data pipelines and drift logic. C uses Kinesis Data Analytics which is not optimized for SageMaker endpoint data capture and adds unnecessary complexity."
  },
  {
    "id": "d4f3b05b379e1f73e599b89cb99c366e5d38129ad082767e8772cea86e17ac68",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An e-commerce company uses a SageMaker endpoint for fraud detection. They must monitor model performance by tracking the false positive rate (FPR) in production and automatically retrain if FPR exceeds 5%. Which architecture satisfies this requirement?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Model Monitor\u2019s default data quality monitor to detect inference anomalies and trigger a Lambda to retrain the model when anomalies occur.",
      "B": "Capture only input payloads, write a CloudWatch metric filter on the logs for false positives, and use a CloudWatch alarm to start a SageMaker batch transform retraining job.",
      "C": "Enable DataCaptureConfig for both input and output, create ModelQualityMonitor with a custom metrics file specifying FPR, schedule hourly monitoring, and configure CloudWatch alarm to invoke a retraining pipeline.",
      "D": "Log predictions to S3, use Athena with scheduled queries to compute FPR hourly, and use Step Functions to orchestrate retraining when threshold is exceeded."
    },
    "explanation": "C is correct because ModelQualityMonitor supports custom metrics (including FPR) computed from captured input/output and can trigger alarms. A lacks FPR tracking. B captures only inputs and relies on log parsing. D is possible but involves Athena and Step Functions, which is more complex and not leveraging built-in model quality monitoring."
  },
  {
    "id": "b142f8b45de946a5d2ea444a6da9db355ff29daa8b440eb6b671689a1dc6095b",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A healthcare provider has a batch inference pipeline that writes predictions and actual outcomes to S3 daily. They need to monitor model performance degradation (e.g., increasing RMSE) and alert DevOps when RMSE increases by more than 10% from baseline. Which move is best?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use CloudWatch Logs Insights to query S3 logs via log aggregation and trigger alarms when RMSE query results exceed thresholds.",
      "B": "Create a ModelQualityMonitor job using baseline metrics from training, supply the ground truth and prediction files from S3, schedule daily runs, and configure CloudWatch alarms.",
      "C": "Configure SageMaker Clarify to detect bias drift which will also report RMSE change and use CloudWatch alarms for notifications.",
      "D": "Invoke a Lambda function via EventBridge daily to compute RMSE from S3 data and send an SNS alert when RMSE >1.1\u00d7 baseline."
    },
    "explanation": "B is correct because ModelQualityMonitor directly supports batch inference data from S3 with ground truth to compute metrics like RMSE and integrate with alarms. A and D require custom code. C is incorrect because Clarify focuses on bias and feature drift, not performance metrics like RMSE."
  },
  {
    "id": "80dc93d1dcf5b4fb4e9076bfe36f6ccae788724f96c16d48f5a38a50a421a661",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A consumer app uses an endpoint for sentiment analysis. They observe occasional spikes in inference latency that degrade user experience. They need to detect latency anomalies in real time and trigger scaling actions. Which solution is most appropriate?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use ModelMonitor\u2019s data quality monitor to detect latency outliers and send events to CloudWatch.",
      "B": "Instrument the endpoint container to emit custom latency metrics to CloudWatch and set alarms on p95 latency.",
      "C": "Schedule a daily ModelMonitor job to analyze captured requests and identify latency anomalies.",
      "D": "Enable SageMaker endpoint invocation metrics (p90 and p95) in CloudWatch, create CloudWatch alarms for sudden increases, and attach Application Auto Scaling policies to the endpoint."
    },
    "explanation": "D is correct because SageMaker endpoints automatically emit latency metrics to CloudWatch, which can be used for real-time alarms and auto scaling. A and C misuse ModelMonitor, which is offline. B requires container modification when built-in metrics suffice."
  },
  {
    "id": "b347fbe68bc1d0dca879a6a8b672c43aabd6bea817751e6af5cc7fc36843605d",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An IoT solution uses a SageMaker endpoint for anomaly detection on sensor streams. They require near real-time alerts when feature distributions drift beyond control limits, but they cannot incur high Lambda execution fees. Which design balances cost and responsiveness?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Stream captured inference data to Kinesis Data Firehose, run a Lambda per record to compute drift and send alerts.",
      "B": "Write all captured data to S3 and run a ModelMonitor daily job with low-cost spot instances.",
      "C": "Use ModelMonitor endpoint preprocessor with custom monitoring script to detect drift thresholds on a 15-minute schedule and publish alerts via CloudWatch Events without Lambda.",
      "D": "Use Kinesis Data Analytics SQL application to continuously compute drift metrics and send alerts through SNS."
    },
    "explanation": "C is correct because ModelMonitor supports custom scripts for drift detection on schedules, avoiding per-record Lambda costs. A and D incur compute in Kinesis/Lambda continuously. B reduces responsiveness to daily."
  },
  {
    "id": "81895ebe4a9514c7f0073a5c528d1d6c2d76a214f11f798fdecf86db0e04481a",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A streaming inference endpoint uses DataCaptureConfig to write payloads and responses to S3. The team needs to detect when data quality (missing fields, invalid ranges) degrades in production and be paged immediately. Which process achieves this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy ModelMonitor\u2019s DefaultModelMonitor with a custom data quality constraints file, schedule it every 5 minutes, and configure CloudWatch alarms for violations.",
      "B": "Configure SageMaker Clarify to analyze captured data for anomalies and configure alerts for missing features.",
      "C": "Use Athena with scheduled queries every 5 minutes on the captured S3 files, detect invalid records, and notify via SNS.",
      "D": "Stream S3 events to Lambda to validate each record in real time and alert on invalid data."
    },
    "explanation": "A is correct because DefaultModelMonitor supports data quality constraints with custom rules, scheduled frequently, with CloudWatch integration. B focuses on bias/feature importance. C and D require custom orchestration and are less integrated."
  },
  {
    "id": "6f55689628d8a965cdc6522544a4b7ed4eb4434fd4e83f1eadce3d28de67ae60",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A media company wants to monitor drift in top-5 features importance for its recommender model in production. They need automated alerts if the feature importance distribution shifts significantly. Which solution fits?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use ModelQualityMonitor to track feature importance values via ground truth comparisons.",
      "B": "Use ModelExplainabilityMonitor to baseline SHAP feature importances during training, schedule production monitoring, and alarm on significant distribution shifts.",
      "C": "Invoke Clarify\u2019s bias detection in production to monitor feature importances.",
      "D": "Export feature importance logs in JSON and use CloudWatch Logs Insights to compare distributions daily."
    },
    "explanation": "B is correct because ModelExplainabilityMonitor (part of Clarify) handles SHAP-based production monitoring of feature importances and can raise alarms. A is wrong because ModelQualityMonitor focuses on prediction quality, not explainability. C misuses bias detector. D is custom and manual."
  },
  {
    "id": "75b3bdab09770e1d57377bc2c3ad4779784ea18ffbc5f10cfdb3c96c1dd5562f",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A logistics firm uses a SageMaker endpoint behind an ALB. They need to monitor end-to-end inference latency and alert if 95th percentile latency exceeds 300 ms for more than 5 minutes. How do they implement this with least maintenance?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Embed custom timing code in inference container to publish custom CloudWatch metrics.",
      "B": "Use ModelMonitor to capture input/output times and schedule drift checks on latency.",
      "C": "Stream ALB access logs to CloudWatch Logs, parse latency percentile every minute via Logs Insights, and alarm.",
      "D": "Use CloudWatch metric for SageMaker Endpoint p95 latency with a 5-minute period alarm, and connect to SNS for notifications."
    },
    "explanation": "D is correct: SageMaker endpoints expose p95 latency in CloudWatch, which supports threshold-based alarms. A requires custom code. B misuses ModelMonitor. C is indirect and involves complex log parsing."
  },
  {
    "id": "0ed689243508c54581139ea9073e4f22a945b16e7c13782bcb2a880dbbec145e",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A biotech company runs batch inference nightly and stores predictions plus sample metadata in S3. They want to detect when the distribution of a key lab_measurement feature drifts compared to training data, and integrate alerts into PagerDuty. Which approach is best?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Athena scheduled queries on S3 data to compute KLD between current and training histograms, and send alerts via SNS to PagerDuty.",
      "B": "Set up a ModelMonitor batch monitoring job with a baseline from training data, schedule it nightly, and configure CloudWatch alarms to SNS for PagerDuty.",
      "C": "Deploy Clarify drift detector to analyze batch outputs for distribution changes and integrate with EventBridge for notifications.",
      "D": "Write custom PySpark EMR job to compute population stability index nightly and notify via Lambda."
    },
    "explanation": "B is correct because batch ModelMonitor directly handles feature drift with baselines, schedules jobs, and integrates with CloudWatch/SNS. A and D are custom and more maintenance. C misuses Clarify drift detector instead of ModelMonitor."
  },
  {
    "id": "c1175f8de77da8496677481c71d4f98132449643350f6815b0725b9309dd44fd",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A retail company uses multi-model endpoints. They must monitor data skew separately for each model under high invocation volume without overloading S3. How should they configure Model Monitor?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable DataCaptureConfig for all models to a single S3 prefix and schedule a single monitor job with a filter script per model.",
      "B": "Use one monitor schedule per model, each writing to its own S3 bucket to avoid prefix collision.",
      "C": "Configure DataCaptureConfig to drop 80% of captured requests, tag model name, and run a single monitor job with group-by on model name.",
      "D": "Disable DataCaptureConfig, and use CloudWatch synthetic tests per model for skew detection."
    },
    "explanation": "C is correct: sampling reduces S3 volume, tagging allows a single job with group-by to monitor each model. A writes all data and requires filtering inside job. B duplicates resources. D uses synthetic tests, not real capture."
  },
  {
    "id": "3c8b63b313ee4120e2a9cef5c2ff9d39da261446028477e1f7f497b7cb9b0a20",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An AI startup uses Clarify ModelExplainabilityMonitor for drift detection, but they notice that on Mondays, due to different user behavior, alerts spike falsely. They want to suppress Monday alerts but keep daily checks. Which modification is appropriate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a suppression rule in CloudWatch alarm for Mondays using a schedule-based mute period.",
      "B": "Adjust the drift threshold upward only for Monday\u2019s monitoring schedule.",
      "C": "Exclude data collected on Mondays from the ModelExplainabilityMonitor input baseline.",
      "D": "Shift monitoring schedule to every 48 hours to skip Monday checks."
    },
    "explanation": "A is correct: CloudWatch alarm suppression allows muting alerts on Mondays without affecting thresholds. B leads to inconsistent drift sensitivity. C corrupts baseline. D reduces monitoring frequency overall."
  },
  {
    "id": "c64c620c17e62083e1760051aeb87679b69e1d672a34dad0856c6ae28670f586",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A ride-sharing company deployed a streaming inference endpoint. They need to detect concept drift in the predicted probability distribution for rider surge events. Which built-in capability supports this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ModelQualityMonitor\u2019s custom metrics for probability shift detection.",
      "B": "ModelMonitor\u2019s ModelBiasMonitor class with a custom preprocessor to compute Jensen-Shannon divergence.",
      "C": "ModelExplainabilityMonitor with SHAP-based drift metrics.",
      "D": "DefaultModelMonitor data quality checks for distribution changes."
    },
    "explanation": "B is correct because ModelBiasMonitor can measure probability distribution drift of predictions using divergence metrics. A is not a ModelQualityMonitor feature. C focuses on feature importance. D checks data quality, not prediction distribution."
  },
  {
    "id": "1befecaa00eeef698870eff0fc3b0fe2e6e9765e580565c874008c9faeddfa86",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A gaming company wants to use SageMaker Model Monitor to detect anomalies in user behavior predictions every five minutes but must avoid executing on cold data. The endpoint has periods of very low traffic. How to configure?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Schedule monitoring at 5-minute intervals without sampling; it will skip if no new data.",
      "B": "Use 1-minute schedule with a Lambda filter to drop empty datasets.",
      "C": "Configure ModelMonitor with max_runtime per job to 10 minutes so it exits on no data.",
      "D": "Schedule monitoring every 5 minutes with min_sampling_size set to 50 so it won\u2019t run unless at least 50 records are present."
    },
    "explanation": "D is correct: min_sampling_size prevents jobs when insufficient data arrives, avoiding waste. A will run and error on empty data. B and C require custom code or rely on runtime but don\u2019t prevent job attempts."
  },
  {
    "id": "b5f49ec9d1ed6c93e89354fc21a209ee1383757c96097ed10dba8bbb6c5db490",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An energy company\u2019s endpoint serves predictions at different times of day, with nightly off-peak periods. They need to monitor inference anomalies only during peak hours (8 AM\u20138 PM). Which pattern accomplishes this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create a monitor schedule at 1-hour intervals and ignore anomalies logged outside of 8\u201320 via filter in the alerting Lambda.",
      "B": "Use two monitoring schedules: one at 5-minute intervals active between 8 and 20 using schedule expressions, and none outside that window.",
      "C": "Enable continuous monitoring and write a custom script to drop anomalies outside the window.",
      "D": "Schedule a daily monitor job at 20:01 that covers the day\u2019s peak data."
    },
    "explanation": "B is correct: schedule expressions in SageMaker allow cron-based windows to run only during peak hours. A adds complexity. C and D don\u2019t align to real-time peak detection."
  },
  {
    "id": "b245cedc74bd65147799378b5885aae27635e2cf22092870d65643cb03c6b98f",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A financial app uses a multi-variant endpoint to serve A/B test models. They need to compare real-time inference accuracy of each variant for rollout decisions without manual intervention. Which integrated feature achieves this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use CloudWatch Logs Insights to query variant-specific logs and compute accuracy per variant.",
      "B": "Enable Kinesis Data Streams on endpoint traffic, write a consumer to compute variant accuracy.",
      "C": "Use SageMaker Model Monitor\u2019s built-in endpoint variant-based shadow testing with baseline accuracy thresholds and CloudWatch alarms.",
      "D": "Invoke two separate endpoints in parallel and use Lambda to compare results against ground truth."
    },
    "explanation": "C is correct: Model Monitor supports shadow variants for model comparison against baselines, automating monitoring. A and B require manual aggregation. D duplicates endpoints and custom code."
  },
  {
    "id": "bbb8dcdc99e40a86b7b7744206c681e7eb451f011d5d2910ca143c9aa405a132",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A telecom provider must ensure that no single categorical feature \u2018device_type\u2019 in production grows in usage by more than 20% relative to training. They want to monitor this continuously. Which approach is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use ModelMonitor\u2019s DefaultModelMonitor with a custom constraints JSON specifying the allowed 20% change for device_type categories and schedule frequent jobs.",
      "B": "Implement a Kinesis Data Analytics job to maintain real-time category counts and compare to training counts.",
      "C": "Use Clarify\u2019s bias monitor to set fair thresholds on device_type representation.",
      "D": "Capture endpoint inputs to S3 and run a daily Athena job to compute ratios and alert."
    },
    "explanation": "A is correct because DefaultModelMonitor constraints allow category-level drift threshold configuration, scheduled regularly. B and D are custom. C is misusing bias monitor, not intended for drift constraints."
  },
  {
    "id": "42759915365fec276fd08aec5f92daf3e777be37787828c9fc269fd1c33bade4",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An automotive company uses real-time inference for predictive maintenance. They must validate that all required sensor fields exist and log any missing fields for further investigation, without failing invocations. How should they set this up?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Implement input validation in the inference container code and reject requests with missing fields.",
      "B": "Use ModelMonitor\u2019s default data quality monitor to reject invocations with missing fields.",
      "C": "Stream inference input logs to CloudWatch Logs and create metrics filters for missing fields.",
      "D": "Configure ModelMonitor with a custom preprocessor script that checks for missing keys, logs violations to S3, and does not block the endpoint."
    },
    "explanation": "D is correct: ModelMonitor\u2019s preprocessor can implement validation logic, log violations, and does not interfere with real-time endpoint behavior. A blocks requests. B cannot reject invocations. C requires log parsing and has delay."
  },
  {
    "id": "5d86f92c110f9a221d62ce346e9e8fe41079d213cd61778e5dcaecf3b89c9c1f",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A travel app\u2019s SageMaker real-time endpoint is scaled to multiple instances for high traffic. They need aggregated abnormalities in feature distributions across all instances. Which configuration ensures correct drift detection?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy a monitor job per instance capturing to instance-specific S3 prefixes and aggregate results in Athena.",
      "B": "Enable DataCaptureConfig at the endpoint level to collect across all instances, and run a single ModelMonitor job on that capture location.",
      "C": "Add a sidecar container per instance to forward data to Kinesis Data Streams for drift detection.",
      "D": "Use CloudWatch metrics on each instance and manually compute distribution shifts in a Lambda."
    },
    "explanation": "B is correct: DataCaptureConfig at endpoint level automatically aggregates across instances. A is complex. C and D require additional infrastructure and custom aggregation."
  },
  {
    "id": "44637c1457f11b27b64fdf4eccfa6a2cf35e095e9d6fbdc79ca7f450fcd29698",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A gaming platform has an endpoint for matchmaking that logs into DynamoDB. They need to monitor that output predictions are within the valid score range [0,1] and alert if outliers appear. Which solution is optimal?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Schedule a ModelMonitor default data quality job with constraints JSON specifying numeric_validity for the output_score feature and set CloudWatch alarms.",
      "B": "Use Lambda streams on DynamoDB to check each output and publish custom CloudWatch metrics for out-of-range values.",
      "C": "Enable Amazon Kinesis Firehose on logs to S3 and run nightly Athena queries to detect out-of-range values.",
      "D": "Instrument application code to validate scores before writing and send alerts via SNS."
    },
    "explanation": "A is correct: ModelMonitor constraints support numeric validity checks and integrate with alarms. B and C are custom solutions more maintenance. D moves monitoring into application logic rather than production monitoring."
  },
  {
    "id": "b25ebdebb2bef883db70f92d23d8e53eaad79d810c17f3b4b86723b622b7b7cf",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A retailer\u2019s real-time pricing endpoint serves dynamic pricing. They want to test periodically whether price predictions exceed a profit margin threshold and revert if they do. Which monitoring feature should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use ModelQualityMonitor with a custom metric that computes profit margin on predictions.",
      "B": "Schedule a Lambda to call the endpoint, compute profit margin, and alarm.",
      "C": "Use ModelMonitor\u2019s output constraints JSON to define the profit margin threshold on prediction and schedule continuous monitoring.",
      "D": "Implement profit margin check logic in the inference container and emit CloudWatch metrics."
    },
    "explanation": "C is correct: ModelMonitor supports output constraints for predictions and continuous monitoring. A is misusing quality monitor, which focuses on ground truth comparison. B and D are custom implementations requiring more code."
  },
  {
    "id": "bd3b0bb00285a1b2c5ec2fbc04bf95f96c8e0fa1d2f52b23369b4823dd97c6f1",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An insurance provider uses a batch inference pipeline on S3 with predictions and claims. They need to identify when the ratio of denied claims to total predictions deviates by more than 15% from the baseline. Which tool and configuration do they choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Clarify ModelBiasMonitor with a custom preprocessor to compute the ratio and schedule daily.",
      "B": "Use ModelQualityMonitor with ground truth and predictions, define a custom violation report with denied_claims/total and threshold, and schedule daily runs.",
      "C": "Use DefaultModelMonitor data quality for this ratio and set thresholds.",
      "D": "Query S3 via Athena daily to compute the ratio and call a Step Functions pipeline."
    },
    "explanation": "B is correct: ModelQualityMonitor supports custom metrics for model performance comparisons, including custom functions like ratio of denied claims. A is bias monitor, not for performance metrics. C is data quality; D is custom."
  },
  {
    "id": "e7b48c372b6fdcdd3764172aedbd3c8abe50be7c4e7425e2b8ee77694f4150fa",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A sports analytics firm needs to detect if the distribution of a continuous feature \u2018player_speed\u2019 starts trending downward in real-time endpoint predictions. They require a rolling window of last 1,000 inferences. How can they configure Model Monitor?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set DataCaptureConfig, configure a monitoring schedule with monitoring_interval=1 minute, and set sampling_percentage so that window size approximates 1,000. Use constraints on the mean of player_speed.",
      "B": "Enable Clarify drift detector with window_size=1000 for player_speed.",
      "C": "Stream inference outputs to Kinesis Data Streams and configure Kinesis Analytics to compute rolling averages.",
      "D": "Use a Lambda triggered on capture S3 events to maintain a sliding window in DynamoDB and trigger alarms."
    },
    "explanation": "A is correct: monitoring_interval and sampling_percentage controls sample size per job; constraints can specify mean thresholds. B is not built-in. C and D are custom solutions outside Model Monitor."
  },
  {
    "id": "0fac125f62338707913f496767c841aa1cddd9b62cc764a2b818fe8d745aea79",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A medical imaging application serves predictions for tumor detection. For compliance, any missing license_plate field (erroneously included) must be detected at inference time and reported, but not block inference. Which Model Monitor component is appropriate?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use DefaultModelMonitor data quality monitor to enforce presence of license_plate.",
      "B": "Use ModelExplainabilityMonitor to detect missing features.",
      "C": "Use ModelQualityMonitor comparing ground truth, specifying missing field detection.",
      "D": "Use a custom preprocessor script in ModelMonitor constraints to check for license_plate existence and log violations."
    },
    "explanation": "D is correct: a preprocessor script can validate arbitrary conditions (including extraneous fields) without affecting inference. A does not support arbitrary missing field checks. B and C are for explainability and performance respectively."
  },
  {
    "id": "a480fe735805504c6539823520050e6c4934500c151f8735cc2c1fe192bc81ed",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An ad-tech firm runs real-time bidding predictions and must detect when the click-through rate (CTR) predicted by the model deviates by more than 10% from training CTR. They have no ground truth in real time. What feature of SageMaker solves this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use ModelQualityMonitor with placeholders for ground truth CTR.",
      "B": "Use ModelMonitor\u2019s custom reporter in data quality to compute predicted CTR distribution drift against baseline.",
      "C": "Use Clarify bias monitor to detect prediction bias on CTR.",
      "D": "Use endpoint logs and Athena to compute CTR distribution daily."
    },
    "explanation": "B is correct: data quality monitors can compute statistics on predictions alone, enabling detection of predicted CTR shifts without ground truth. A is not applicable since quality monitor requires ground truth. C misuses bias monitor. D is custom and offline."
  },
  {
    "id": "7dcce68d81ebcb3945493b164c8028df545c03fdb21e032226d36d585a514fbb",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A gaming platform needs to ensure that inference payload sizes don\u2019t suddenly increase (which could indicate malicious inputs). They want to monitor payload byte size distributions in production. How can they achieve this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use DefaultModelMonitor with a preprocessor script to compute payload size feature, set constraints on the size distribution, and schedule monitoring.",
      "B": "Modify inference container to log payload size to CloudWatch and alarm on spikes.",
      "C": "Use Lambda triggered on S3 capture PUT events to record object sizes and alert.",
      "D": "Write a Kinesis Data Analytics job to analyze payload streams for size anomalies."
    },
    "explanation": "A is correct: ModelMonitor allows custom features via preprocessor scripts (including payload size) and constraint definitions. B, C, D require separate pipelines and custom code."
  },
  {
    "id": "de4d745845142a85649ff751de2b92410d16d7d4d231c56499c867585cc5eb61",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A supply chain model produces predictions every minute. The team must compute the percentage of predictions greater than threshold X and alert if it falls below 30% for more than 10 minutes. They have Model Monitor enabled. What is the simplest way to implement?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Clarify\u2019s bias monitor to configure threshold checks on prediction distribution.",
      "B": "Schedule a daily ModelQualityMonitor job with custom metrics file for percentage above X.",
      "C": "Configure a real-time ModelMonitor schedule with a custom constraints JSON to compute % above X and set alarms on violations every 5 minutes.",
      "D": "Stream predictions to Kinesis Data Streams and use AWS Lambda to monitor percentage."
    },
    "explanation": "C is correct: ModelMonitor constraints support custom aggregations (like percentage above threshold) and real-time schedules. A and B misapply Clarify or daily quality jobs. D is custom and more complex."
  },
  {
    "id": "e8acde60736a677d31317caaa992ac3b621967270ccfb4f31c3e4e22e6e3f0b4",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A biotech lab uses an asynchronous SageMaker endpoint. They need to monitor and alert on both input data completeness and response time exceeding 2 seconds for each batch. Which combination of Model Monitor capabilities achieves this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable DataCaptureConfig for request and response, configure DefaultModelMonitor with separate data quality and custom latency constraints, schedule every 5 minutes.",
      "B": "Use Clarify ModelExplainabilityMonitor for input completeness and CloudWatch metrics for latency.",
      "C": "Use two separate Lambda functions\u2014one reading S3 captures for missing fields and one alarm on CloudWatch latency metrics.",
      "D": "Run a daily ModelQualityMonitor job to measure completeness and latency metrics."
    },
    "explanation": "A is correct because DefaultModelMonitor can validate data quality and custom metrics like latency in one job. B misuses explainability. C is custom. D is too infrequent."
  },
  {
    "id": "d35f2d87189194189c8b47f591bc8a284f41237dc98db49a04ba99468deed7c9",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A financial firm wants to monitor covariance drift between two features, income and expenditure, in real-time inference data. Which Model Monitor strategy accomplishes this?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Use Clarify bias detector to compute covariance and schedule monitoring.",
      "B": "Schedule Athena queries to compute covariance and trigger alarms.",
      "C": "Implement a custom preprocessor in ModelMonitor to compute covariance drift constraint and schedule frequent runs.",
      "D": "Stream inference data to Redshift and run SQL to compute covariance daily."
    },
    "explanation": "C is correct: custom preprocessor scripts in ModelMonitor allow computing covariance and enforcing drift constraints. A and B don\u2019t natively compute covariance. D is custom and offline."
  },
  {
    "id": "783f8c07efbe7bb5ccad34ad41299d3440b875566c40086c319ab1d16a4eb9cf",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A mobile app uses a SageMaker real-time endpoint. They want to detect if any string feature contains non-UTF8 characters in production. Which approach is most maintainable?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Modify inference code to reject non-UTF8 inputs and log violations.",
      "B": "Use Clarify\u2019s bias detector to detect invalid characters.",
      "C": "Run a Lambda on captured S3 data to search for invalid encodings.",
      "D": "Configure ModelMonitor with a custom preprocessor to scan string features for non-UTF8 and log violations in S3."
    },
    "explanation": "D is correct: ModelMonitor preprocessor scripts can perform arbitrary validation (e.g., encoding checks) with minimal maintenance and decoupled from inference code. A couples monitoring with serving. B is misapplied. C is custom."
  },
  {
    "id": "865eea2ab369eaa3994d0edad335f2a65d5ab2a1bbbeeb59ec957097bf93adde",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A retailer uses batch transform to score customer churn nightly. They need to verify that output probabilities sum to 1 for each record (softmax outputs) and alert on any anomalies. Which Model Monitor feature supports this at lowest operational cost?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Clarify bias monitor to validate output probability sums.",
      "B": "Use DefaultModelMonitor with a custom postprocessor script to compute sum of probabilities per record and set constraints that sum equals 1.",
      "C": "Instrument batch transform container to raise errors when sums differ and notify via SNS.",
      "D": "Write Athena query over output files to check sums and send alerts."
    },
    "explanation": "B is correct: postprocessor scripts in ModelMonitor can validate prediction outputs and enforce numeric constraints. A is misapplied. C and D require custom code or queries."
  },
  {
    "id": "6a84e242b19c6389a4969558949893600ccd4f22a3244aeefac199cd79dd3dd3",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A manufacturing line inference endpoint outputs categorical defect labels. They must detect when a rare defect category appears more than twice in an hour, indicating a quality issue. How can they configure Model Monitor?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create a DefaultModelMonitor job with custom constraints specifying max_count=2 for that category over a 1-hour window and schedule jobs accordingly.",
      "B": "Run a Clarify bias monitor to detect changes in rare label frequency.",
      "C": "Use Kinesis Data Streams to aggregate counts and Lambda to alert.",
      "D": "Schedule daily Athena queries on captured data to check counts."
    },
    "explanation": "A is correct: ModelMonitor supports frequency constraints for categorical values. B misuses bias. C and D are custom and less real-time."
  },
  {
    "id": "cf418ed2df1c5e00a01f13217591e967210989095e1157ba2b4a92894b7d45ae",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An app uses a SageMaker real-time endpoint for language translation. They want to ensure that none of the translated outputs contain profane words. Which Model Monitor mechanism should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Clarify bias monitor with a profanity lexicon.",
      "B": "Modify inference code to filter profanity and report.",
      "C": "Configure ModelMonitor with a custom postprocessor script that scans outputs against a profanity list and logs violations.",
      "D": "Stream responses to Kinesis and run a profanity detection Lambda."
    },
    "explanation": "C is correct: postprocessor scripts in ModelMonitor can implement content validation and integrate with alerts. A is misused. B couples monitoring into serving. D requires extra infrastructure."
  },
  {
    "id": "296830543cd703aff911680a8f6b9a288ce6f2d24e871889220b1c81ec192f21",
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A logistics service uses a multi-model endpoint for forecasting. They need per-model latency and feature distribution monitoring. The endpoint metadata tags each invocation with model_id. How can they implement this in ModelMonitor?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable separate DataCaptureConfig per model_id by deploying multiple endpoints.",
      "B": "Enable a single DataCaptureConfig that captures model_id in payload, schedule ModelMonitor with a group_by based on model_id for both latency and data quality metrics.",
      "C": "Stream all invocations to Kinesis Data Streams, use a Lambda to split by model_id.",
      "D": "Deploy Clarify bias detectors per model for distribution monitoring."
    },
    "explanation": "B is correct: ModelMonitor supports group_by functionality on a specified feature (model_id) to generate separate metrics per group. A duplicates endpoints. C is custom. D misuses bias detectors."
  },
  {
    "id": "96a933e45dae82163b395195c7decee78ab263216d60bd763039ada390a9c867",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A financial services company needs to detect fraudulent transactions in real time with sub-50ms latency per transaction, while also running nightly trend analyses on aggregated data. They plan separate ML pipelines for each use case. Which combination of inferencing types should they implement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Batch inferencing for fraud detection and real-time inferencing for trend analysis",
      "B": "Real-time inferencing for fraud detection and batch inferencing for trend analysis",
      "C": "Streaming inferencing for fraud detection and online inferencing for trend analysis",
      "D": "Online inferencing for fraud detection and micro-batch inferencing for trend analysis"
    },
    "explanation": "Real-time inferencing meets sub-50ms latency for fraud detection; batch inferencing is appropriate for nightly aggregate trend analysis. Streaming and micro-batch are not AWS inference categories defined in this context."
  },
  {
    "id": "0173760b365aece635c691cb0b8527d494975dd0766fcb3d5f0f555e3128fb8a",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "An e-commerce team is deciding between logistic regression and random forest for a binary classification task. They refer to these as \u201calgorithms.\u201d After training, they obtain predictive artifacts they deploy. In AWS terms, what is the term for these deployed artifacts?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Hyperparameters",
      "B": "Features",
      "C": "Models",
      "D": "Labels"
    },
    "explanation": "Algorithms (logistic regression, random forest) produce trained artifacts called models. Hyperparameters configure algorithms; features are inputs; labels are outputs used during training."
  },
  {
    "id": "4a819081ac8e67d395611913cde99e64284de066c86ac3fce44bd116c833e233",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A manufacturing firm collects sensor readings (temperature, vibration, pressure) every second as JSON messages and also attaches operator comments as free text. What best describes the types of data ingested?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Structured tabular data only",
      "B": "Unstructured multimedia data",
      "C": "Semi-structured numerical data only",
      "D": "Mixed structured (numeric) and unstructured (text) data"
    },
    "explanation": "Numeric sensor readings are structured; operator comments are unstructured text. It\u2019s mixed. The other options mischaracterize the combination."
  },
  {
    "id": "697615cf80eff32cd4c763ff972498634d3480eb45231d9efcdbf9b346a04acb",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A marketing team wants to segment customers by purchase behavior without any predefined labels. They plan to discover latent groups. Which learning paradigm should they apply?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Supervised classification",
      "B": "Unsupervised clustering",
      "C": "Reinforcement learning",
      "D": "Semi-supervised learning"
    },
    "explanation": "Clustering on unlabeled data is unsupervised learning. Supervised requires labels; reinforcement uses rewards; semi-supervised mixes labels and unlabeled data."
  },
  {
    "id": "d0199157a5062786a57e339fe9c40e16aa46d8849f8d0a8dc6ba4c3e84af10f7",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A logistics company uses trial and error and a reward function to optimize warehouse pick-and-place operations in simulation. Which ML method are they using?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Supervised learning",
      "B": "Unsupervised learning",
      "C": "Reinforcement learning",
      "D": "Self-supervised learning"
    },
    "explanation": "Using a reward signal in simulation for actions is reinforcement learning. Supervised uses labeled examples; unsupervised finds patterns without rewards; self-supervised derives labels internally but not via reward."
  },
  {
    "id": "81f4b765f7cbe41d252d54dbd674a80182b2a257d5577efc50b88d59d7406083",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "An image classification solution on AWS uses a convolutional neural network with millions of parameters and requires GPUs. Which term best categorizes this approach?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deep learning",
      "B": "Traditional ML algorithm",
      "C": "Reinforcement learning agent",
      "D": "Ensemble tree model"
    },
    "explanation": "CNNs with many layers/functions on unstructured images are deep learning. Ensemble trees and traditional ML do not generally use deep neural nets; reinforcement learning uses a reward model."
  },
  {
    "id": "443fdc632585e653fc69ac087a4d84e6e6d73869ec105ae0d51f5c38e6dfb534",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A research group selects a pre-trained language model of 20B parameters that supports few-shot prompting. What terminology best describes this artifact?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A neural network",
      "B": "A supervised model",
      "C": "An embedding",
      "D": "A large language model (LLM)"
    },
    "explanation": "A pre-trained model with billions of parameters for language tasks is an LLM. While it is a neural network and supervised pre-training is used, the specific term is LLM; embeddings are vector representations."
  },
  {
    "id": "30953991dda22a9f20110d7e174984bebb785e987c93b5bfb08da269bb69f673",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A facial recognition model misclassifies certain demographics more often than others. What term describes this issue?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Bias",
      "B": "Variance",
      "C": "Fit",
      "D": "Regularization"
    },
    "explanation": "Systematic error disadvantaging particular groups is bias. Variance relates to sensitivity to data fluctuations; fit/reg. concern under/overfitting and penalty terms respectively."
  },
  {
    "id": "01b785637425a56937643c533c3dfa64c06c688aceeaf811cfa2fe9f5ae04276",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A text classification model achieves 99% accuracy on training data but only 60% on new customer emails. Which problem is occurring?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High bias",
      "B": "Data leakage",
      "C": "Underfitting",
      "D": "Overfitting"
    },
    "explanation": "A large gap between training and test performance indicates overfitting. High bias or underfitting would show poor performance on both. Data leakage would inflate test performance, not reduce it sharply."
  },
  {
    "id": "d0ae82f7a0291e37807a1f9866e569a70d17bd1f56a73a49c83d5e232bfef154",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "An online retailer deploys a model endpoint to score customer propensity in milliseconds per request. Which term describes this stage?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Model training",
      "B": "Feature engineering",
      "C": "Inference",
      "D": "Data preprocessing"
    },
    "explanation": "Serving a deployed model to generate predictions is inference. Training builds the model; feature engineering and preprocessing occur before model usage."
  },
  {
    "id": "e98fd7313fb09d9c47c6066b630b31e2a2bd74069264c986fd1780a8a9c8dfe1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A shipping firm wants to detect package damage by analyzing shipment photos. Which AI domain should they leverage?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Computer vision",
      "B": "Natural language processing",
      "C": "Reinforcement learning",
      "D": "Speech recognition"
    },
    "explanation": "Analyzing images falls under computer vision. NLP handles text; reinforcement learning uses rewards; speech recognition handles audio."
  },
  {
    "id": "7422e8ce5563577b3ab150cc2aa3568de36e44be2b8b5438752056f5046c79ce",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A call center wants to transcribe and analyze customer audio for sentiment. Which AI domain and subtask are involved?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Computer vision with OCR",
      "B": "Computer vision with object detection",
      "C": "Natural language processing with speech-to-text and sentiment analysis",
      "D": "Reinforcement learning with audio encoding"
    },
    "explanation": "Transcribing speech is speech-to-text (an NLP subtask) followed by sentiment analysis (also NLP). The other options misapply computer vision or reinforcement learning to audio."
  },
  {
    "id": "92792df63cb34fffa136b2bffe5e1cfc140099ea285d8e1b4b57ce00933eaf6c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A startup uses customer behavior logs to train a model and wants to evaluate various algorithms efficiently. Which pipeline step does this describe?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Deployment",
      "B": "Experimentation (training and validation)",
      "C": "Feature storage",
      "D": "Data annotation"
    },
    "explanation": "Testing multiple algorithms with validation data is experimentation within the training lifecycle. Deployment comes after; feature storage and annotation are different steps."
  },
  {
    "id": "7322c5e7718ecc8d5b5e678c716d7518caae5dab4157c2c86dfcbd33c1a58022",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A dataset contains numeric, categorical, text, image, and time-series fields. How would you classify this dataset overall?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Structured",
      "B": "Unstructured",
      "C": "Semi-structured",
      "D": "Heterogeneous multimodal data"
    },
    "explanation": "Multiple data modalities (numeric, text, image, time-series) define heterogeneous multimodal data. Structured/unstructured/semi-structured don\u2019t capture multiple distinct modalities."
  },
  {
    "id": "14ba17f88e8ae677d8fdc542acbde6c9e40f499738d0d5209d286d0d16e4f0c1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A credit scoring model uses decision trees and logistic regression, then blends outputs. What ensemble method category is this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Stacking",
      "B": "Bagging",
      "C": "Boosting",
      "D": "Blending"
    },
    "explanation": "Combining diverse model outputs via a meta-learner is stacking. Bagging uses same algorithm on subsets; boosting sequentially focuses on errors; blending is a holdout variant of stacking but less formal."
  },
  {
    "id": "ab915aa928b9e0c9c80ec65f66f52adb2b14b3040df186965f6688966fc19ced",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A medical imaging model needs 3D scans and uses volumetric convolution. Which architectural choice defines this model?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Recurrent neural network",
      "B": "Transformer",
      "C": "Autoencoder",
      "D": "3D convolutional neural network"
    },
    "explanation": "3D CNNs handle volumetric data via 3D convolutions. RNNs process sequences; transformers use attention; autoencoders learn embeddings."
  },
  {
    "id": "d6067366c07b863ae41df8091fff88851641f02bea67a632b9250d554d45b588",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "When splitting data for training and inference, which data characteristic ensures the model generalizes?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High label imbalance",
      "B": "Representative i.i.d. distribution",
      "C": "Missing values\u96c6\u4e2d",
      "D": "Highly correlated features"
    },
    "explanation": "An independent, identically distributed (i.i.d.) split ensures the inference data resemble training data. Label imbalance, missing values, or correlated features hamper generalization."
  },
  {
    "id": "9a4c7771a2a5738913b3d4f5510a20388b61d849539aa95eb8130f0dac490c86",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A recommender system logs user clicks and ratings. Which learning paradigm uses both past actions and reward signals to improve suggestions?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Unsupervised learning",
      "B": "Supervised learning",
      "C": "Reinforcement learning",
      "D": "Self-supervised learning"
    },
    "explanation": "A recommender optimizing using click/reward feedback is reinforcement learning. Supervised uses explicit labels; unsupervised finds patterns; self-supervised generates labels."
  },
  {
    "id": "b0219f93fc79b08e51c5de14815f44fee2b31ac0dd6e80d2b4f515e15e203796",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A startup fine-tunes a pre-trained transformer-based LLM by updating all weights on domain data. What process are they performing?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Prompt engineering",
      "B": "Fine-tuning",
      "C": "Inference",
      "D": "Embedding"
    },
    "explanation": "Updating model weights on new data is fine-tuning. Prompt engineering crafts inputs; inference uses the model; embedding produces vector representations."
  },
  {
    "id": "e9aaff176ccdd61f80fd3d53688c7f0d4cb6d873a48be471e73e7da12c784855",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A dataset of customer addresses stored as JSON documents without fixed schema is an example of which data type?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Structured",
      "B": "Unstructured",
      "C": "Semi-structured",
      "D": "Multimodal"
    },
    "explanation": "JSON documents with flexible schema are semi-structured. Structured implies rigid tables; unstructured refers to raw text/images; multimodal combines multiple media types."
  },
  {
    "id": "c2fa434fa89edf09c3c8bc0dfddf86aaf7db847f2dc4f2de63ccb6e5b1ecc763",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A deep network\u2019s capacity to memorize training noise rather than general patterns relates to which property?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Bias",
      "B": "Fairness",
      "C": "Regularization",
      "D": "Variance"
    },
    "explanation": "High variance indicates sensitivity to noise and overfitting. Bias relates to systematic error; fairness to equity; regularization is a technique to reduce variance."
  },
  {
    "id": "926681a14ec344b7209acd6e3ad4e627251c5d29366fb0b32193586b1638b69b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A supply-chain model uses past demand to predict next-month orders via linear regression. Which algorithm type is this?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Regression",
      "B": "Classification",
      "C": "Clustering",
      "D": "Dimensionality reduction"
    },
    "explanation": "Predicting a continuous numeric target is regression. Classification predicts discrete labels; clustering groups unlabeled data; dimensionality reduction reduces feature space."
  },
  {
    "id": "cdfe1ea85e29e0f058c9f93809331579a439372c2be5b914269396bd81a37389",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "An NLP pipeline transforms text into 768-dimension vectors before clustering. What are these vectors called?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Features",
      "B": "Embeddings",
      "C": "Hyperparameters",
      "D": "Parameters"
    },
    "explanation": "Embeddings are dense vector representations of text. Features are inputs to models; parameters are learned weights; hyperparameters configure training."
  },
  {
    "id": "6e2bdc08443dc8f3ed21a648ead75ac7feb235ef28bb5aea3b14b9df999ac5ab",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A governance team audits a model\u2019s training algorithm and discovered inconsistent outputs for identical inputs across runs. Which concept might explain this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High bias",
      "B": "Data imbalance",
      "C": "Nondeterministic initialization",
      "D": "Overfitting"
    },
    "explanation": "Random weight initialization or nondeterministic operations in deep learning can lead to different outputs for identical inputs. Bias, imbalance, and overfitting don\u2019t directly cause run-to-run variability."
  },
  {
    "id": "891591cfc4b6e5390f1af06929495a9d798fcadd52281955aacb71a3ba8a7547",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A customer service chatbot uses a model that responds within seconds but batches billing analysis nightly. How should you label these two inferencing modes?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Synchronous and asynchronous inferencing",
      "B": "Online and offline training",
      "C": "Real-time and streaming inferencing",
      "D": "Real-time and batch inferencing"
    },
    "explanation": "Chatbot responses require real-time (online) inference; nightly billing analysis is batch inference. The other terms mix training with inference or streaming, which is not the defined mode here."
  },
  {
    "id": "994be2a0c335f90fea7b2accfedfafe4978a0da974b9cb0a20275ebf8d4b2468",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "An ML pipeline scales behavior analysis to new data without labels by learning data structure. Which technique applies?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Supervised classification",
      "B": "Unsupervised learning",
      "C": "Reinforcement learning",
      "D": "Transfer learning"
    },
    "explanation": "Unsupervised learning discovers patterns in unlabeled data. Transfer learning uses labeled source domains; reinforcement learning uses rewards; classification is supervised."
  },
  {
    "id": "0ce6c9e755ad949c430b3c6cc79e8db482329e3f4e4c40041b9a5dc9787d0251",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A text summarization system must compress long documents into key sentences. Which AI subfield does this represent?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Computer vision",
      "B": "Reinforcement learning",
      "C": "Natural language processing",
      "D": "Anomaly detection"
    },
    "explanation": "Summarization is an NLP task. Computer vision processes images; reinforcement is reward-based; anomaly detection finds outliers."
  },
  {
    "id": "397ff7680eb57472e99ff9ee7ef04dc91f278416b77af451df878bceeb301fe4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A model\u2019s training loop uses gradient descent to minimize a loss function. Which concept does the loss function represent?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Algorithm objective",
      "B": "Model artifact",
      "C": "Feature set",
      "D": "Hyperparameter"
    },
    "explanation": "The loss function defines the objective the algorithm optimizes. The model artifact is the trained parameters; features are inputs; hyperparameters configure training."
  },
  {
    "id": "6a03ccf8c8e70ad0bcfa68dde5224ebdf7aa946a707fd23262cecd8fc2d4040a",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A pre-training phase on massive text yields a foundation model that can be specialized later. What is the general process called?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Inference",
      "B": "Pre-training",
      "C": "Fine-tuning",
      "D": "Evaluation"
    },
    "explanation": "Pre-training on large unlabeled corpora yields a foundation model. Fine-tuning specializes; inference uses the model; evaluation measures performance."
  },
  {
    "id": "b8d5967ba553b8e3705bdd33ff58a538eaac25f76d36c84aab2820601ab657e1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A wildlife monitoring project clusters animal tracks without labels and then assigns species names manually to clusters. Which workflow combines the two learning types?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Supervised learning",
      "B": "Reinforcement learning",
      "C": "Unsupervised learning",
      "D": "Semi-supervised learning"
    },
    "explanation": "They initially use unsupervised clustering then manually label clusters, combining labeled and unlabeled data = semi-supervised. Pure supervised or unsupervised use only one type; reinforcement uses rewards."
  },
  {
    "id": "b6319b74f76c8e971412cc375df49da63b0cd85c78f7ae7c9b137b4c279da6f6",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A model\u2019s performance degrades over time because data distribution shifts. Which concept describes this challenge?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Overfitting",
      "B": "Data drift",
      "C": "Bias",
      "D": "Variance"
    },
    "explanation": "Data drift refers to changes in input data distribution over time, causing degradation. Bias/variance relate to model errors; overfitting is memorization."
  },
  {
    "id": "571ec8f8aa9c00540d302bbfc64de87d0f470a411d91e3a1f69084ae55b08af1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A dataset has 100 features but you observe diminishing returns beyond 10 in model performance. Which concept are you examining?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Regularization strength",
      "B": "Algorithm complexity",
      "C": "Feature importance",
      "D": "Model capacity"
    },
    "explanation": "Evaluating how adding features impacts performance assesses feature importance. Regularization, algorithm complexity, and model capacity address other aspects."
  },
  {
    "id": "f096397214b9787dee03b79ad309a01d035642fa5e27258663b6525b1c817a0e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A retail company wants to segment customers based on purchasing behavior to tailor marketing campaigns. They have numerical features such as purchase frequency, average order value, and days since last purchase. Which ML technique and AWS service combination best suits this use case?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use the built-in K-Means clustering algorithm in Amazon SageMaker to group customers.",
      "B": "Use Amazon Personalize to automatically cluster customers and generate segments.",
      "C": "Use sentiment analysis in Amazon Comprehend to segment customers by sentiment.",
      "D": "Use Amazon Forecast to predict future customer segments over time."
    },
    "explanation": "Unsupervised clustering via SageMaker K-Means is ideal for grouping based on numeric behavior; other services are for recommendations, sentiment, or forecasting."
  },
  {
    "id": "a57b5146adf604f03556db3b0893da0626fecfbb5b2c638fcf9d78c31ed42e03",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A fintech startup needs to identify potentially fraudulent credit card transactions in real time with minimal ML expertise and wants pre-trained fraud detection capabilities. Which AWS service should they choose?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Fraud Detector",
      "B": "Amazon SageMaker Ground Truth",
      "C": "Amazon Comprehend",
      "D": "Amazon GuardDuty"
    },
    "explanation": "Amazon Fraud Detector provides domain-specific, pre-trained models for fraud detection; others are for labeling, NLP, or threat detection."
  },
  {
    "id": "b2cf8323b923281cd4806c8b1f023016ad74595036940b54efafcaaffd01e336",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A global brand wants to analyze customer tweets to gauge public sentiment about a new product launch without building or training custom models. Which AWS service should they use?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Comprehend Sentiment Analysis",
      "B": "Amazon SageMaker BlazingText",
      "C": "Amazon Translate",
      "D": "Amazon Lex"
    },
    "explanation": "Comprehend\u2019s built-in sentiment API analyzes text sentiment directly; other services are for custom text models, translation, or chatbots."
  },
  {
    "id": "65b26f6ce67a58bdecbf8a84111cd31d136f5ee786e42971dd1613ff474512f7",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A media company needs to convert recorded webinar audio into text transcripts to index content for search. Which AWS service should they choose?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Transcribe",
      "B": "Amazon Comprehend",
      "C": "Amazon Polly",
      "D": "Amazon Translate"
    },
    "explanation": "Amazon Transcribe converts speech to text; Comprehend analyzes text, Polly generates speech, Translate translates text."
  },
  {
    "id": "0dcbb7b82a6553f172b178269bd87076d00e2a58202fe4ed6146bd71b7dd2a99",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "An e-commerce website must provide real-time translation of product descriptions into multiple languages to serve global customers. Which AWS service is most appropriate?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Translate",
      "B": "Amazon Comprehend",
      "C": "Amazon Polly",
      "D": "Amazon Lex"
    },
    "explanation": "Amazon Translate provides real-time neural translation; Comprehend is for NLP analysis, Polly for speech, Lex for conversational interfaces."
  },
  {
    "id": "ccc5fd7dedabf8cba05b2842ccb3217d69eacc0e613b9bfbea0c063414d1289e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A legal firm wants to automatically extract key phrases and entities (like person, organization, date) from large volumes of contracts. Which AWS service should they use?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Comprehend Entity and Key Phrase Extraction",
      "B": "Amazon Textract Table Extraction",
      "C": "Amazon SageMaker BlazingText",
      "D": "Amazon Kendra"
    },
    "explanation": "Comprehend extracts entities and key phrases; Textract focuses on form/ table OCR, SageMaker needs model building, Kendra is search."
  },
  {
    "id": "2512f4b4a820cfdfb9e963f3a904fad8964b1262b4bb9f82d3eca01383f49498",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A news app needs to generate audio versions of articles for accessibility. Which AWS service should they integrate?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Transcribe",
      "B": "Amazon Translate",
      "C": "Amazon Polly",
      "D": "Amazon Lex"
    },
    "explanation": "Amazon Polly converts text to lifelike speech; Transcribe converts speech to text, Translate translates text, Lex builds chat interfaces."
  },
  {
    "id": "982d07a008ba65d49b544eb1318234b757090f04d7eaebc27cd184bd7c493e8c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A company wants to build a conversational agent to answer common HR policy questions via chat. Which AWS service should they use?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Comprehend",
      "B": "Amazon Lex (without Lambda integration)",
      "C": "Amazon Polly",
      "D": "Amazon Lex with intent management"
    },
    "explanation": "Amazon Lex provides NLU, intent handling, and conversation flow; Comprehend is for text analysis, Polly for TTS."
  },
  {
    "id": "b6928cbd5aa6c445c47d260690dfabd4f5c27ee3a163179b10cffa619498fc60",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "An online retailer wants to offer personalized product recommendations based on browsing behavior and purchase history. Which AWS service is most suitable?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Personalize",
      "B": "Amazon Forecast",
      "C": "Amazon Kinesis Data Analytics",
      "D": "Amazon Personalize"
    },
    "explanation": "Amazon Personalize is designed for personalized recommendations; Forecast is forecasting, Kinesis analyzes streaming data but not personalized models."
  },
  {
    "id": "d274758eb15bf4d8e835e2272a13478c09a109561aac28aca4932fdbe93a532e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A manufacturing firm needs to automatically detect defects in product images on the assembly line. They need bounding boxes around defects. Which service is most appropriate?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Rekognition Text Detection",
      "B": "Amazon SageMaker Image Classification",
      "C": "Amazon Textract",
      "D": "Amazon Rekognition Object Detection"
    },
    "explanation": "Rekognition\u2019s object detection provides bounding boxes; image classification labels whole images, Textract is OCR."
  },
  {
    "id": "c19be730134cf00ffc0d6398fa4ec1d1cd5325d6df1269e135eca726b8df36a8",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A bank wants to build a model to predict customer churn probability using historical account activity. Which ML technique and AWS service should they choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Binary classification using SageMaker built-in XGBoost algorithm",
      "B": "Unsupervised K-Means clustering in Amazon SageMaker",
      "C": "Reinforcement learning in AWS DeepRacer",
      "D": "Time series forecasting in Amazon Forecast"
    },
    "explanation": "Predicting churn is a supervised binary classification problem; SageMaker\u2019s XGBoost is appropriate."
  },
  {
    "id": "a684c9c9e42bdca547b6021bcf42a8ecf5ecfd8b11322866115d9bd8da30adf6",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A telecom operator wants to forecast next-month data usage per customer to provision network capacity. Which technique and AWS service best apply?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Classification with SageMaker Linear Learner",
      "B": "Time series forecasting with Amazon Forecast",
      "C": "Clustering with SageMaker K-Means",
      "D": "Anomaly detection with SageMaker Random Cut Forest"
    },
    "explanation": "Forecasting numeric usage over time requires time series forecasting; Amazon Forecast is purpose-built."
  },
  {
    "id": "ad1f92f63a52ed9ff7ae6826469abb1eaace51152a176ff8faa15bb2524a0dd4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A logistics company receives IoT sensor data and wants to detect abnormal temperature spikes without labeled anomalies. Which AWS service and algorithm combination should they use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon SageMaker Linear Learner",
      "B": "Amazon Fraud Detector",
      "C": "Amazon Comprehend",
      "D": "SageMaker Random Cut Forest anomaly detection"
    },
    "explanation": "Random Cut Forest in SageMaker unsupervised algorithm detects anomalies in unlabeled time series."
  },
  {
    "id": "bafd376a5874f3f47a69ed8b863e868e154215abe6108503c7b88a5ea098c7ee",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "An educational platform wants to cluster quiz questions based on topic similarity using NLP embeddings. They don\u2019t require supervised labels. Which approach is most appropriate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon Translate then cluster translated text",
      "B": "Use Amazon Comprehend sentiment scores",
      "C": "Generate embeddings with SageMaker JumpStart BERT model and cluster via K-Means",
      "D": "Use Amazon Personalize grouping"
    },
    "explanation": "Extracting embeddings with a pre-trained model then K-Means clustering is an unsupervised NLP approach."
  },
  {
    "id": "4d855cf8cea8b0a84db2962880ebaafe2d12ab4433de5971b9b44661fbd8a333",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A marketing team considers using ML to map postal codes to region names because there are only 50 unique codes. Is ML appropriate here?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Yes \u2013 use Amazon SageMaker lookup table training",
      "B": "No \u2013 use a simple rule-based lookup table (no ML)",
      "C": "Yes \u2013 use Amazon Comprehend to infer regions",
      "D": "No \u2013 use Amazon Forecast to predict region mapping"
    },
    "explanation": "Mapping a small finite set is best solved with a rule-based lookup, not ML."
  },
  {
    "id": "4799b3f0f16c40a0fe1d2284a7673bd413e6cedebfc86126f1586cae1e26ac61",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A company wants to detect specific keywords in customer call transcriptions to trigger alerts. Which AWS service should they use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Transcribe with custom vocabulary and AWS Lambda keyword filter",
      "B": "Amazon Comprehend Key Phrase Extraction",
      "C": "Amazon Kinesis Video Streams",
      "D": "Amazon Lex Slot Filling"
    },
    "explanation": "Custom vocabulary in Transcribe captures keywords in speech-to-text, Lambda filters them; Comprehend extracts after text conversion."
  },
  {
    "id": "a12da58750001a405a03777768fb21c426b43cad1859bc14ea0a36e425a67e20",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "An HR department needs to automatically categorize resume documents by skill sets using pre-built models and minimal customization. Which AWS service is most appropriate?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon SageMaker BlazingText",
      "B": "Amazon Comprehend Custom Classification",
      "C": "Amazon Textract form analysis",
      "D": "Amazon Personalize"
    },
    "explanation": "Comprehend Custom Classification allows rapid text classification with minimal model building; others are for embeddings, OCR, or recommendations."
  },
  {
    "id": "2cb7eb2acdd084228dc251839e347da51f30f70b1dfb2400b172b96b2864fba0",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A legal team needs to redact personally identifiable information (PII) from scanned PDF contracts. Which AWS service or combination should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Comprehend PII detection alone",
      "B": "Amazon SageMaker DocumentClassifier",
      "C": "Amazon Textract for OCR then Comprehend PII detection",
      "D": "Amazon Rekognition Text Moderation"
    },
    "explanation": "Textract extracts text from scanned PDFs; Comprehend PII API then identifies and redacts PII."
  },
  {
    "id": "79ca2919a577245dd180f0fd773ed4c5147cf22c369956ba722d0d9919a03ada",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "An online gaming company wants to recommend in-game items based on player behavior and contextual metadata. Which AWS service should they choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Personalize",
      "B": "Amazon Forecast",
      "C": "Amazon SageMaker Neo",
      "D": "Amazon Comprehend"
    },
    "explanation": "Personalize builds contextual, real-time recommendation models; Forecast is for time series, Neo for deployment, Comprehend for NLP."
  },
  {
    "id": "1eab5f20c8bfc28233a66807900b82f7e1d3ee4f7ce8bb876d51370cebe026e3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A pharmaceutical company wants to group patients into cohorts for a clinical trial based on lab results and demographic features without labeled outcomes. Which ML technique applies?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Supervised classification using SageMaker XGBoost",
      "B": "Unsupervised clustering using SageMaker K-Means",
      "C": "Reinforcement learning using Amazon SageMaker RL agents",
      "D": "Anomaly detection using SageMaker Random Cut Forest"
    },
    "explanation": "Clustering groups data without labels into cohorts; supervised or anomaly detection are less appropriate."
  },
  {
    "id": "d9777a95972cd57f44cebb38970b26a7dc8361e7d4f917dc582ecd50a0c17db9",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A contact center wants to route incoming chats based on detected customer intent without building custom ML models. Which AWS service should they use?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Lex V2 with custom code",
      "B": "Amazon Comprehend Syntax Analysis",
      "C": "Amazon SageMaker DeepAR",
      "D": "Amazon Lex built-in intent classification"
    },
    "explanation": "Lex provides built-in intent classification for chat routing; other services are for syntax, forecasting, or require custom ML."
  },
  {
    "id": "c4e03e0a2a57926345f886888af2d529cb84d0d09cd0d9ed515bf177cff656f1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A bank wants to estimate credit risk score as a continuous variable from customer profiles. Which ML technique and AWS service combination is best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Unsupervised clustering with SageMaker K-Means",
      "B": "Anomaly detection with SageMaker Random Cut Forest",
      "C": "Regression using SageMaker Linear Learner",
      "D": "Classification using Amazon Fraud Detector"
    },
    "explanation": "Predicting a continuous risk score is regression; SageMaker Linear Learner supports regression tasks."
  },
  {
    "id": "f62e4ae9eacd9e8e3bc21cae346ab8da9a6a574a17f3e3569ca515f9eefdc04f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A manufacturing plant collects vibration data and suspects rare equipment failures. They have no labeled failure examples. Which AWS approach should they use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Supervised classification in Amazon SageMaker",
      "B": "Unsupervised anomaly detection using SageMaker Random Cut Forest",
      "C": "Time series forecasting using Amazon Forecast",
      "D": "Clustering using SageMaker K-Means"
    },
    "explanation": "Unsupervised anomaly detection detects rare failures without labeled data; clustering groups all data equally."
  },
  {
    "id": "cbcfa5a6c5a9c9623e6b11d0316eacefdb9ff6729a26c72091a682aecef644fb",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A travel website wants to extract flight numbers, dates, and passenger names from PDF itineraries. Which AWS service combination should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Comprehend Entities API",
      "B": "Amazon Textract Forms API only",
      "C": "Amazon Textract for OCR then Amazon Comprehend Entity Extraction",
      "D": "Amazon SageMaker OCR built-in model"
    },
    "explanation": "Textract OCRs form data; Comprehend extracts structured entities; other options miss one stage."
  },
  {
    "id": "0fbc2cc6e6e10b99cb07419b8a3ac35288e78a86c85bfc0088f872b5fbfe7138",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A social media analytics firm needs to cluster trending topics without prior labels. They want to use embeddings from a pre-trained language model. Which AWS service should they leverage?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Comprehend Topic Modeling",
      "B": "SageMaker JumpStart transformer embeddings + K-Means",
      "C": "Amazon Personalize clustering",
      "D": "Amazon Rekognition for topic detection"
    },
    "explanation": "JumpStart provides pre-trained models for embeddings; clustering then groups topics; Comprehend doesn\u2019t expose topic modeling."
  },
  {
    "id": "ad37510943733e564d29a76dc00b908893851602642e257081fd31870e0edc19",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A chatbot needs to answer customer queries by matching to FAQ entries. They require semantic matching rather than keyword matching. Which AWS service/model approach should they use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Lex with keyword slots",
      "B": "Amazon Comprehend sentiment analysis",
      "C": "Amazon Translate semantic matching",
      "D": "Generate embeddings using SageMaker JumpStart and perform similarity search"
    },
    "explanation": "Embeddings and similarity search provide semantic matching; other services are for keyword, sentiment, or translation."
  },
  {
    "id": "bf270e8bc2d4f525ad50cacf4c422595d1d78740dc2e6df163dea10c261bc572",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A publisher wants to detect the language of submitted articles automatically before translation. Which AWS service should they use?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Comprehend DetectDominantLanguage",
      "B": "Amazon Translate language detection",
      "C": "Amazon Polly language identification",
      "D": "Amazon Transcribe language model"
    },
    "explanation": "Comprehend\u2019s DetectDominantLanguage API identifies text language; Translate focuses on translation post-detection."
  },
  {
    "id": "eb09ae14315e63031e66f70c3376d1626d265b6ac0e9621d58bb9b33603ee5ac",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "An agriculture startup gathers drone imagery to identify crop health issues automatically. They need object detection and classification. Which AWS service combination is suitable?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Rekognition Text Detection + Comprehend",
      "B": "Amazon SageMaker K-Means clustering",
      "C": "Amazon Textract Table Extraction",
      "D": "Amazon SageMaker Object Detection built-in algorithm"
    },
    "explanation": "SageMaker\u2019s Object Detection algorithm (e.g., SSD) supports bounding boxes and classification in imagery."
  },
  {
    "id": "de74e5f71bde2dfea3eab9b789a8ab09305927bbe1db2e0bea6c3506bab79145",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A legal department wants to determine sentiment trends in case law documents over time and highlight emerging negative topics. Which AWS services should they combine?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Textract + Amazon Translate",
      "B": "Amazon Comprehend Key Phrases + Amazon Personalize",
      "C": "Amazon Textract for OCR + Amazon Comprehend Sentiment Analysis + Amazon QuickSight for visualization",
      "D": "Amazon Rekognition + Amazon Athena"
    },
    "explanation": "Textract extracts text, Comprehend analyzes sentiment, QuickSight visualizes trends; others mismatch functions."
  },
  {
    "id": "9bdcede78ccfacc585f8464a255ec5e8721cce6a4ab7e0cf1fd08d1d983c5c7b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A retailer wants to forecast daily sales volume per store for the next quarter, incorporating holiday effects and promotions. Which AWS solution should they choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Regression with SageMaker XGBoost",
      "B": "Time series forecasting with Amazon Forecast",
      "C": "Clustering with SageMaker K-Means",
      "D": "Anomaly detection with SageMaker Random Cut Forest"
    },
    "explanation": "Forecast incorporates seasonality and holiday effects; regression would require manual feature engineering."
  },
  {
    "id": "f6c50c59dace8c8a84c9d15062046c75ea8529469b9999a3726ac9675801c17a",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A health app records user vitals and needs to detect abnormal patterns in real time for alerting. They have streaming data and no labeled anomalies. Which AWS combination is best?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Forecast streaming predictions",
      "B": "Amazon SageMaker Linear Learner streaming inference",
      "C": "Amazon Fraud Detector real-time API",
      "D": "Amazon Kinesis Data Analytics to ingest + SageMaker Random Cut Forest for anomaly detection"
    },
    "explanation": "Kinesis ingests streaming data; SageMaker RCF analyzes unlabeled anomalies in real time."
  },
  {
    "id": "f07bcd9cac5382c308299fdef160f68fa7d3318ca921a6c53e4be1d4b82757c7",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A company has a dataset of customer emails and wants to route them to the correct department automatically. They need intent classification out of the box. Which AWS service should they use?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Comprehend Custom Classification",
      "B": "Amazon SageMaker Object Detection",
      "C": "Amazon Translate",
      "D": "Amazon Personalize"
    },
    "explanation": "Comprehend Custom Classification provides easy training for text routing; other services are irrelevant."
  },
  {
    "id": "3821d9ac563813980395aaa30ea7e91960e01db6e6447f57706398d75f419aa8",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A startup wants to create a voice-driven assistant that transcribes speech, interprets intent, and responds vocally. Which combination of AWS services is most appropriate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Transcribe + Amazon Comprehend + Amazon Polly",
      "B": "Amazon Transcribe -> Amazon Lex -> Amazon Polly",
      "C": "Amazon Lex -> Amazon Translate -> Amazon Comprehend",
      "D": "Amazon Polly -> Amazon SageMaker RL"
    },
    "explanation": "Transcribe handles speech-to-text, Lex interprets intent, Polly synthesizes speech; other chains mix stages incorrectly."
  },
  {
    "id": "6d8bfb4b793e6bf69df37664ab5bcd2b8dd034d3757beb810c67313766e989c3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A team needs to ensure that feature transformations applied during model training are used identically during real-time inference to avoid training/serving skew. Which AWS service or feature best satisfies this requirement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon SageMaker Feature Store",
      "B": "AWS Glue DataBrew",
      "C": "Amazon SageMaker Data Wrangler flow exports",
      "D": "Embedding transformations in Lambda functions"
    },
    "explanation": "SageMaker Feature Store provides a single source of truth for feature definitions and transformations, ensuring consistency between training and inference. Data Wrangler and Glue cannot serve real-time features, and Lambda-based transformations risk divergence."
  },
  {
    "id": "5f1542446d1a170d92f3c89eb6096f8f1253f04337f454976e12c96568ebe0d5",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "You must orchestrate a repeatable end-to-end ML workflow including data preprocessing, training, evaluation, model registration, and conditional deployment in response to evaluation metrics. Which AWS tool should you choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Step Functions with custom Lambda tasks",
      "B": "AWS Glue Workflows",
      "C": "Amazon SageMaker Pipelines with ConditionStep",
      "D": "AWS Data Pipeline"
    },
    "explanation": "SageMaker Pipelines natively orchestrates ML steps, supports ConditionStep for metric-based branching, and integrates with training, processing, and model registry. Step Functions and Glue lack ML-specific constructs, and AWS Data Pipeline is deprecated for ML workflows."
  },
  {
    "id": "938898fade61955941398ebdd2df9fa9ef0e9e0ef44b8cecff4a98405b85cc68",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A data scientist needs to deploy ten versions of a fraud detection model behind a single endpoint, loading each version only when invoked to minimize memory usage. Which deployment option meets this requirement?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provisioned real-time endpoints per model variant",
      "B": "SageMaker serverless endpoint",
      "C": "Batch Transform job",
      "D": "SageMaker multi-model endpoint"
    },
    "explanation": "Multi-model endpoints load models on demand into a container, supporting multiple versions behind a single endpoint. Serverless endpoints cannot host multiple models, and Batch Transform is for offline inference."
  },
  {
    "id": "11fd6356ca9126cfdf8801a59d6d887905162327af4024f7b6144082844df552",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "To detect input data drift in production, which combination of services and features provides automated baseline generation, continuous monitoring, and alerting?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon CloudWatch metrics with custom Lambda evaluations",
      "B": "SageMaker Model Monitor\u2019s DataQualityJobDefinition + Amazon EventBridge rule",
      "C": "AWS Config with custom rules",
      "D": "AWS Glue job scheduled daily"
    },
    "explanation": "Model Monitor can generate a baseline from training data (DataQualityJobDefinition), schedule continuous monitoring, and emit results via EventBridge for alerting. Other options require extensive custom work or lack ML-specific drift detection."
  },
  {
    "id": "d564eb1bdf5a74c7cd41556f81a767b213eaacecd52bcfa1481fcf44de2cd72b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "Your preprocessing logic involves heavy PySpark transforms on large tabular datasets. Which SageMaker component should you use for scalable, containerized execution?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon SageMaker Processing with built-in Scikit-learn container",
      "B": "AWS Glue ETL job",
      "C": "Amazon SageMaker Processing with Spark container",
      "D": "AWS Lambda with parallel invocations"
    },
    "explanation": "SageMaker Processing supports Spark containers for distributed PySpark workloads within an ML pipeline. Glue is general ETL, not integrated with SageMaker pipelines, and Lambda cannot handle large-scale Spark jobs."
  },
  {
    "id": "ff568c01c39e0af50412df56c700dbd97ba2c1773ecb9f196870c9ac1d961ff4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A business requires tracking of every training run\u2019s hyperparameters, input data versions, and evaluation metrics with lineage. Which SageMaker feature is specifically built to capture this metadata?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon SageMaker Experiments",
      "B": "Amazon CloudWatch Logs",
      "C": "Amazon S3 object versioning",
      "D": "AWS CloudTrail"
    },
    "explanation": "SageMaker Experiments records runs, parameters, inputs, and metrics, providing lineage and comparison across trials. CloudWatch Logs and S3 versioning don\u2019t structure ML metadata, and CloudTrail logs only API calls."
  },
  {
    "id": "6652c047b0422257426eff971b4305d96d5fd12d7f4532023522f5444f2f31f8",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "When model quality metrics fall below a threshold in production, you need to trigger an automated retraining pipeline. Which architecture best implements this requirement?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CloudWatch alarm on endpoint latency to start Lambda",
      "B": "AWS Config rule violation to start SageMaker job",
      "C": "SNS notification from CloudTrail to trigger training",
      "D": "EventBridge rule on Model Monitor alarm to start SageMaker Pipeline"
    },
    "explanation": "Model Monitor can detect quality degradation, emit an EventBridge event, and trigger a SageMaker Pipeline that retrains and redeploys the model. Other options don\u2019t tie directly to model quality metrics."
  },
  {
    "id": "5a5f9f1a49d742944441b00f2b78e4f216deea808c0da37736b2ff88d3845315",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A classification model on a heavily imbalanced dataset shows 95% accuracy but poor minority class detection. Which single metric should you prioritize to better reflect model performance?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Overall accuracy",
      "B": "Precision",
      "C": "F1-score",
      "D": "Mean squared error"
    },
    "explanation": "F1-score balances precision and recall, and is better suited for imbalanced datasets than accuracy. Precision alone ignores recall, and MSE is for regression."
  },
  {
    "id": "72d9c1775547cbcb81e769cb9079531c184dd3ffda74f5d722b7276de995bf99",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "You want to generate a statistical profile of your training data to serve as a baseline for drift detection. Which SageMaker component accomplishes this with minimal code?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "SageMaker Clarify bias report",
      "B": "SageMaker Model Monitor Data Quality job",
      "C": "SageMaker Debugger profiling job",
      "D": "SageMaker Edge Manager"
    },
    "explanation": "Model Monitor\u2019s DataQualityJobDefinition automatically profiles datasets to generate statistics and constraints used for drift monitoring. Clarify focuses on bias/fairness, Debugger on training diagnostics, Edge Manager on device models."
  },
  {
    "id": "95916b4669a7f3d28813670726f4db1b0447107c78c2a55206a44f9da2f70fdf",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "Your organization needs to track approved model versions and their deployment environments, enabling rollback. Which service should you integrate into your pipeline?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon SageMaker Model Registry",
      "B": "AWS Systems Manager Parameter Store",
      "C": "Amazon DynamoDB",
      "D": "AWS CodeCommit"
    },
    "explanation": "SageMaker Model Registry manages model versions, approval statuses, and associated metadata, making deployments and rollbacks straightforward. Parameter Store and DynamoDB aren\u2019t specialized for model lifecycle, and CodeCommit is for code, not models."
  },
  {
    "id": "a2f29d491209b176e39a3b8f6d0e94c9c90e94dd30441559978d7da5ce1134a6",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "You need to run custom evaluation code on your test dataset after training completes within the same pipeline. Which SageMaker step type should you use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "TrainingStep",
      "B": "ProcessingStep",
      "C": "TransformStep",
      "D": "ModelStep"
    },
    "explanation": "ProcessingStep allows execution of arbitrary evaluation scripts post-training and can read model artifacts and test data. TrainingStep only trains, TransformStep performs batch inference, and ModelStep registers a model."
  },
  {
    "id": "5f68accb50e218da9c9e8afb9b3e616be091cb8267516cc7d6ecbe2f746a89b7",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "You plan to fine-tune an open-source transformer model from Hugging Face on SageMaker. Which approach minimizes undifferentiated heavy lifting?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Hugging Face DLC via Hugging Face integration",
      "B": "Build your own Docker container with Transformers installed",
      "C": "Use Amazon EC2 instances with preinstalled Transformers",
      "D": "Perform training on AWS Lambda functions"
    },
    "explanation": "SageMaker\u2019s built-in Hugging Face deep learning containers provide managed environments and optimization for fine-tuning. Custom containers and EC2 require manual setup, and Lambda cannot handle large training jobs."
  },
  {
    "id": "272c5259a8ce64644ba8fe54c1e1a7fd22bd02c40fd284110f238b079c24ea75",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "Your ML pipeline must decide at runtime whether to continue to deployment based on evaluation metrics. Which SageMaker feature allows you to embed this logic directly in the pipeline definition?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Lambda invocation within a Pipeline",
      "B": "State Machine integration",
      "C": "ConditionStep in SageMaker Pipelines",
      "D": "RetryStrategy in TrainingStep"
    },
    "explanation": "ConditionStep in SageMaker Pipelines evaluates expressions on step outputs to branch logic without external orchestration. Lambda or Step Functions would decouple, and RetryStrategy only handles retries."
  },
  {
    "id": "fea34e5e126819fa9961d1d86f3ed3a6c6a43a2b5d2ea6a0d2f9f42a10971642",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "Your service-level agreement requires sub-100ms inference latency at unpredictable traffic volumes for a text classification model. Which SageMaker endpoint type best meets this requirement with minimal management overhead?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Multi-model endpoint",
      "B": "Serverless inference endpoint",
      "C": "Real-time provisioned endpoint",
      "D": "Batch transform"
    },
    "explanation": "Serverless endpoints automatically scale based on traffic and provide low-latency inference without capacity planning. Real-time provisioned endpoints require manual scaling, multi-model cannot auto-scale, and Batch Transform is offline."
  },
  {
    "id": "cd5f8bf0798fa2b0108751b12d521c8380ac5251d6813f9380476ea4207b64c5",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "You want to embed feature-engineering code in a pipeline step, reuse it interactively in Data Wrangler, and version control it. Which approach best satisfies these needs?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Implement transforms in a Lambda and call from ProcessingStep",
      "B": "Write PySpark in a ProcessingStep directly",
      "C": "Use Glue DataBrew recipe then export",
      "D": "Develop and version a SageMaker Data Wrangler flow, then export as ProcessingStep"
    },
    "explanation": "Data Wrangler flows allow interactive development of transformations, version control via Studio, and export as ProcessingStep for reproducible pipelines. Glue recipes aren\u2019t integrated into pipelines, and Lambda lacks data science tooling."
  },
  {
    "id": "3b842c367c6bc75e7ec5ed9ed42fcbfdd41cd9504bf572dfa45a36a4fded7f9d",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "Your model inference is hosted using a Kubernetes-based mechanism requiring custom container orchestration. Which SageMaker deployment option allows you to maintain this while integrating with SageMaker Pipelines?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker real-time endpoint with custom container",
      "B": "Batch Transform job with custom transformer",
      "C": "SageMaker serverless endpoint with custom container",
      "D": "SageMaker Inference Realtime Inference on EKS"
    },
    "explanation": "SageMaker Realtime Inference on EKS lets you deploy to your EKS cluster with custom orchestrations, integrating with pipelines. Standard endpoints and serverless endpoints abstract away Kubernetes."
  },
  {
    "id": "fbea0674e785de18463957a63599a6f4cfdeb5c427a644b5047dceb424def753",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A compliance auditor requires a record of every data artifact and code version used in model training. Which combination ensures full traceability?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "S3 versioning + CloudTrail logs",
      "B": "SageMaker Experiments lineage tracking + SageMaker Model Registry",
      "C": "Git commit hashes in Glue ETL + DynamoDB audit table",
      "D": "AWS Config recording S3 changes"
    },
    "explanation": "Experiments track data and code versions at the run level, and Model Registry ties model artifacts to training runs, providing end-to-end ML lineage. S3 versioning and CloudTrail are lower-level and not ML-specific."
  },
  {
    "id": "dcfcaadd7bccc65e8894e57cab0d94c4f20262534e840e00d8b78207a4432cf0",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "Your application needs low-latency access to inference results but generates heavy batched requests. Which inference architecture balances throughput and latency?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provisioned real-time endpoint only",
      "B": "Batch Transform only",
      "C": "Mixed: use serverless real-time endpoint for spikes and Batch Transform for bulk",
      "D": "SageMaker multi-model endpoint only"
    },
    "explanation": "Combining serverless real-time endpoints for unpredictable spikes and Batch Transform for bulk processing optimizes cost and performance. Single methods either overprovision or increase latency."
  },
  {
    "id": "fc2f75bab92aa9d89a23efb6c1c128ebffd6dbc4edead9e43459fd01656b5c22",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "To minimize training cost, you want managed spot training with checkpointing. Which configuration achieves this in SageMaker?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set TrainingJobStrategy to \u2018SpotSingleNode\u2019",
      "B": "Enable \u2018ManagedSpotTraining\u2019 and configure checkpoint S3 path",
      "C": "Use Spot Instances in EC2 training cluster",
      "D": "Run training on a serverless notebook with spot instances"
    },
    "explanation": "ManagedSpotTraining in SageMaker automatically provisions spot instances and saves checkpoints to S3. The other options are either invalid or require manual orchestration outside SageMaker training jobs."
  },
  {
    "id": "5e0fdf1705cc22ca4b47970d1236f4dec29efac5a6f02e9a44efad2a2202ec59",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "Your ML pipeline must load features for offline training and serve the same features for online inference with single-digit millisecond latency. Which design meets these SLAs?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Feature Store with both offline and online stores",
      "B": "Store features in Amazon RDS and query via Lambda",
      "C": "Store parquet files in S3 and use Athena for queries",
      "D": "Use DynamoDB for offline and S3 for online"
    },
    "explanation": "SageMaker Feature Store provides an offline store for training and an online store optimized for low-latency reads. The other options either lack performance or separate stores."
  },
  {
    "id": "ada95a92fb4d4afde923015732c3e89010afb6f05e77cc2643a1dbe4a47f049f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "You have an imbalance in real-time inference traffic and want dynamic endpoint scaling without idle cost. Which SageMaker feature should you use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provisioned instance fleet with Auto Scaling",
      "B": "Serverless inference",
      "C": "Multi-model endpoint with provisioned instances",
      "D": "Batch transform with event triggers"
    },
    "explanation": "Serverless inference scales automatically to zero when idle and scales with demand, avoiding idle costs. Auto Scaling of provisioned fleets still incurs minimum capacity costs."
  },
  {
    "id": "0d1eb33bf9bd38420c22665b4d120665f2eb17ec5e682b75e7f1c93e5134ed94",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "To enforce data schema and quality checks before training, you need a managed solution in your pipeline. Which SageMaker component should you integrate?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Clarify**BiasReportStep**",
      "B": "Glue Data Catalog crawler",
      "C": "Athena schema validation",
      "D": "Model Monitor DataQualityJobDefinition"
    },
    "explanation": "Model Monitor's DataQualityJobDefinition can be used in pipelines to validate data schema and detect anomalies before training. Clarify is for bias/fairness, and Glue/Athena are not ML-specific."
  },
  {
    "id": "b6cf8cdfe7e5e9db1a500fc04d16491f3ee5fcfebda927e935bed4d036f2dd36",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A newly trained model shows lower inference accuracy in production than during testing. Which multi-step solution most directly addresses this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase instance size of endpoint and redeploy",
      "B": "Retrain the model weekly with latest data",
      "C": "Use Model Monitor to detect drift\u00d7trigger retraining pipeline",
      "D": "Add more hidden layers to the neural network"
    },
    "explanation": "The correct approach is to detect data drift with Model Monitor, then trigger automated retraining. Changing instance size or model architecture without addressing drift won't fix production accuracy declines."
  },
  {
    "id": "517f7092e8ccf910079c3d5a04073e1bb5ac072b80461a5f2542099d78af8d56",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "You need to run a hyperparameter tuning job and automatically compare results across multiple experiments and datasets. Which SageMaker features will you combine?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Hyperparameter TuningJob integrated with Experiments",
      "B": "CloudWatch metrics filtered by TrainingJob name",
      "C": "Athena queries on S3 logs",
      "D": "Manual Excel aggregation of results"
    },
    "explanation": "Hyperparameter TuningJobs integrated with SageMaker Experiments allow automated tracking and comparison of trials across experiments. CloudWatch and manual methods lack structured ML metadata."
  },
  {
    "id": "aab6fca825d6d1c33f0b6c012830b4e000c0381ae3b73082eccd85b155792760",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "Which evaluation approach is most appropriate when your business objective values the top 10% of predictions being correct, regardless of overall recall?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ROC AUC",
      "B": "Overall accuracy",
      "C": "F1-score",
      "D": "Precision at N% (e.g., Precision@10%)"
    },
    "explanation": "Precision@N% measures correctness within the top-scoring N% of predictions, aligning directly with business objectives. Other metrics don\u2019t focus on a specific percentile cutoff."
  },
  {
    "id": "620907b015cd06e2db656f240447271540de93759167b218ecf60e1407b25de7",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "To version-control your pipeline definitions and allow pull requests for change management, which AWS service integration is recommended?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Store pipeline JSON in S3 with versioning enabled",
      "B": "Integrate SageMaker Studio with AWS CodeCommit",
      "C": "Use DynamoDB to store pipeline definitions",
      "D": "Embed pipeline definitions as Lambda code"
    },
    "explanation": "SageMaker Studio can integrate with CodeCommit, allowing Git-based workflows for pipeline definitions. S3 versioning doesn\u2019t provide pull requests and code review workflows."
  },
  {
    "id": "6fcd5d31ac6f6a458734c454761860b78d4c1e745753a305c293059948f38da4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "Your deployment requires A/B testing two model variants with 20% traffic on the new variant. Which SageMaker feature allows this traffic split?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provision two separate endpoints and use a Lambda router",
      "B": "Use Batch Transform for variant testing",
      "C": "Specify ProductionVariants with InitialVariantWeight in CreateEndpointConfig",
      "D": "Deploy new variant to serverless endpoint"
    },
    "explanation": "Setting InitialVariantWeight in a ProductionVariants array when creating an endpoint config splits traffic between variants. Other approaches are more complex and not built in."
  },
  {
    "id": "0c240733d23dfc2e4f45d68b17bb268d3f5eb8a2200e831aff639f20eb4ea3a8",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "You want to integrate fairness and bias detection in your preprocessing and post-training steps within a SageMaker pipeline. Which components should you use in the respective steps?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ClarifyDataQualityCheck for preprocessing and ClarifyBiasCheck for post-training",
      "B": "ModelMonitor for preprocessing and Clarify for post-training",
      "C": "Glue DataBrew recipe for both steps",
      "D": "Athena queries for preprocessing and CloudWatch alarms for post-training"
    },
    "explanation": "SageMaker Clarify\u2019s data quality check can be used before training, and its bias check after training, within pipelines. Model Monitor focuses on drift and quality, not fairness."
  },
  {
    "id": "343f0cfad0b0c5f7dbbf05bf8602c5ffba077404c02d1e01764040dabb7b5c40",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "When using a subword tokenizer (like byte-pair encoding) for a generative AI foundation model, which scenario best explains a drawback of using a very small vocabulary size?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Frequent words get split into many subwords, increasing sequence length and reducing generation efficiency.",
      "B": "The model fails to represent rare words, leading to unknown token outputs for uncommon terms.",
      "C": "A larger embedding matrix is required, increasing memory usage.",
      "D": "It prevents the model from learning long-range dependencies due to positional errors."
    },
    "explanation": "A is correct because a small vocabulary forces common words into multiple subwords and lengthens context. B is wrong because subword tokenizers compose rare words from subwords. C is wrong since smaller vocab shrinks the embedding matrix. D is unrelated to vocabulary size."
  },
  {
    "id": "d5f5ce1da05c242a0209e666578469f8ba187e3504556d8aa9997247e7d6eecf",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "In a retrieval-augmented generation system, why is chunking long documents with overlapping windows often preferred over non-overlapping chunks?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Overlapping windows improve continuity across boundaries to prevent missing context.",
      "B": "They reduce total number of chunks, saving storage.",
      "C": "They ensure equal token counts per chunk to meet fixed-size model input.",
      "D": "They allow each chunk to be processed in parallel without context duplication."
    },
    "explanation": "A is correct because overlaps preserve context at boundaries. B is false\u2014overlap increases chunk count. C is irrelevant to overlap. D is incorrect since overlap duplicates context, not eliminates it."
  },
  {
    "id": "e69df7bdcab64bc7538fe5cb8837f41c66c3cbac28474be56f1e4e9618bb33a7",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "A team must choose an embedding dimension for text vectors in a generative AI application. Which factor primarily influences selecting a higher dimensionality for embeddings?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Larger vocabulary size and semantic granularity demand more dimensions to capture nuanced relationships.",
      "B": "Increased batch size during training requires higher embedding dimensions for convergence.",
      "C": "Lower model inference latency favors higher dimensions to speed similarity searches.",
      "D": "The number of transformer layers directly scales with embedding dimension."
    },
    "explanation": "A is correct because richer semantics and more tokens require larger embeddings. B is unrelated to dimension. C is opposite\u2014higher dimensions often slow similarity searches. D is false\u2014layers and embedding size are independent choices."
  },
  {
    "id": "f5badd3f53c4cdb5fc7bb07f53761493203645f955677f8100f2f6c0e8558c16",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "When comparing two text embeddings for semantic similarity, which metric is most appropriate to mitigate discrepancies in vector magnitude due to token count differences?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Cosine similarity because it normalizes magnitude.",
      "B": "Euclidean distance because it captures absolute distance.",
      "C": "Dot product because it's faster on GPUs.",
      "D": "Manhattan distance because it's robust to outliers."
    },
    "explanation": "A is correct since cosine similarity ignores vector length and focuses on angle. B and D use absolute distances influenced by magnitude. C\u2019s speed claim ignores the normalization benefit of cosine."
  },
  {
    "id": "124fdf33b17f382ad9bfcd6931db44a50910e332c8af7b8a05c3af5ed6b0f78c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "To improve reasoning in a generative AI model, a prompt engineer uses chain-of-thought prompts. What\u2019s a potential drawback of this technique?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "It increases latency and token usage, possibly hitting context limits.",
      "B": "It leads the model to ignore the final instruction and focus on intermediate steps.",
      "C": "It reduces model creativity by constraining outputs.",
      "D": "It causes the model to hallucinate numeric values more frequently."
    },
    "explanation": "A is correct because verbose reasoning consumes context and compute. B is incorrect\u2014models still follow instructions. C and D are not established drawbacks tied specifically to chain-of-thought."
  },
  {
    "id": "f686a41f741e5d89c90980f3b1cdd2eda7c9da8df2721816bf95264c6c06111d",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "In transformer-based generative AI, what is the purpose of using a causal attention mask during training?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "To prevent the model from attending to future tokens, enforcing autoregressive generation.",
      "B": "To ensure the model attends equally to all tokens regardless of position.",
      "C": "To reduce computational complexity by masking low-attention heads.",
      "D": "To allow bidirectional context for improved semantic encoding."
    },
    "explanation": "A is correct\u2014causal masks block future tokens. B, C, and D describe other mechanisms not achieved by causal masking."
  },
  {
    "id": "d0486b63da302f9803b06ca27e74dda017472dd1ebe8eccc1675ee745ee2856d",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "A company requires rapid prototyping with foundation models but limited compute. Which model-size tradeoff should they consider?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Choosing a medium-sized model to balance latency and performance.",
      "B": "Selecting the largest available model for higher accuracy despite slower inference.",
      "C": "Using a tiny model as it guarantees zero hallucinations.",
      "D": "Fine-tuning a very large model to reduce deployment cost."
    },
    "explanation": "A is correct\u2014a medium model balances speed and quality. B ignores latency constraints. C is false\u2014tiny models can still hallucinate. D increases cost, not reduces it."
  },
  {
    "id": "d119f45beaf5f50a5d62395061d123b36c0d465a9f2c7eff7d0ffd981d87c19b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "A generative AI application needs to caption images and also generate layouts. Why would a multi-modal foundation model be preferred over a unimodal text model?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "It natively processes visual features and textual patterns in a shared embedding space.",
      "B": "It reduces the number of parameters compared to separate image and text models.",
      "C": "It improves tokenization speed by merging pixels and text tokens.",
      "D": "It avoids any need for positional embeddings for images."
    },
    "explanation": "A is correct\u2014multi-modal models jointly embed images and text. B is wrong\u2014multi-modal often adds parameters. C and D are incorrect technical claims."
  },
  {
    "id": "af8e32e3ba6be2d69f0083deb5889466c92aecdbc35b47c50f1be7699a6c56c4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "When fine-tuning a diffusion model for faster image synthesis, which modification to the noise schedule can accelerate generation without severely degrading quality?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Reducing the number of denoising timesteps and adjusting the variance schedule to concentrate noise removal.",
      "B": "Increasing the initial noise variance to cover broader solutions.",
      "C": "Using a linear noise schedule instead of cosine to simplify computations.",
      "D": "Removing the stochastic sampling step entirely to get deterministic outputs."
    },
    "explanation": "A is correct\u2014fewer steps with tuned variance speeds up inference. B, C, and D either degrade quality severely or break the diffusion process."
  },
  {
    "id": "9ae4231ff885f4a9ad4916e6ae0fffc41419e791e392fb86f3224ad4208d7a0f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "In a generative AI system employing vector quantization on embeddings, what is a primary disadvantage of aggressive quantization (e.g., fewer codebook entries)?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Loss of semantic nuance leading to lower generation fidelity.",
      "B": "Increased storage overhead from larger codebooks.",
      "C": "Slower similarity searches due to coarse quantization.",
      "D": "Higher risk of overfitting the quantized vectors."
    },
    "explanation": "A is correct\u2014too few codebook entries degrade fidelity. B is opposite\u2014fewer entries save storage. C is false\u2014coarser quantization speeds search. D is unrelated."
  },
  {
    "id": "bb5e4383c98ba5f5583d7c3b8161be7bc236bd1a73b92f0acceb6b524d726de4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "For RAG over a large PDF, what overlap ratio between chunks helps avoid context loss at chunk boundaries while minimizing redundancy?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "10-20% overlap balances context carryover with storage.",
      "B": "50% overlap ensures no context is lost but doubles storage.",
      "C": "0% overlap maximizes chunk independence and reduces size.",
      "D": "80% overlap is needed to capture extended references across pages."
    },
    "explanation": "A is correct\u2014small overlap preserves boundary context without excessive duplication. B and D waste storage; C loses context."
  },
  {
    "id": "c08bf08ae259284f4036be7552441864788995e38ae3b11f835e92cb6e2d4186",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "A developer must choose between Locality-Sensitive Hashing (LSH) and an IVF index in FAISS for embedding retrieval. Which scenario favors LSH?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "When memory is extremely limited and approximate retrieval is acceptable.",
      "B": "When exact nearest neighbors are required for high-precision tasks.",
      "C": "When embedding dimension exceeds GPU memory for IVF.",
      "D": "When dynamic updates require fast insertion and deletion."
    },
    "explanation": "A is correct\u2014LSH is lightweight and approximate. B demands exact search, favoring IVF or exact indices. C and D are unrelated trade-offs."
  },
  {
    "id": "f31d8e30b83af0f2fbc40efa0d01f919fbe38f7786b5038b8559aa6d7080896e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "What is a subtle prompt injection risk when concatenating user input with a system prompt in a generative AI chatbot?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A user embeds instructions that override system directives, altering model behavior.",
      "B": "The model increases hallucination rates due to longer inputs.",
      "C": "The system prompt loses embedding effects due to position bias.",
      "D": "The user\u2019s input is truncated entirely during tokenization."
    },
    "explanation": "A is correct\u2014malicious input can override system rules. B\u2013D describe unrelated or less subtle issues."
  },
  {
    "id": "78b07560c5e32799cb212f6ffa839ed1400da0e0a6b5b4c42950e6e49cc82a44",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "Compared to prefix-tuning a foundation model, what is a disadvantage of instruction fine-tuning?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Instruction fine-tuning requires updating entire model weights, incurring higher compute cost.",
      "B": "It cannot incorporate human feedback.",
      "C": "It fails to improve performance on unseen tasks.",
      "D": "It reduces the model\u2019s context window."
    },
    "explanation": "A is correct\u2014full fine-tuning is expensive. B\u2013D are incorrect statements about instruction fine-tuning."
  },
  {
    "id": "2dff08bba5bf68400fc4b767aaf74997aa5619d0368d409cbeed4eafed06c8db",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "Why might a byte-level BPE tokenizer yield better out-of-vocabulary handling than a word-level tokenizer in a multilingual generative model?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "It can break unknown words into known byte sequences, avoiding unknown tokens.",
      "B": "It stores full words for all languages, increasing vocabulary.",
      "C": "It reduces model perplexity uniformly across languages.",
      "D": "It simplifies position embeddings by using bytes."
    },
    "explanation": "A is correct\u2014byte-level BPE composes unknown words. B\u2013D are incorrect or irrelevant advantages."
  },
  {
    "id": "c3b1e3e6faafaef56241f6e43b81e140b61dc76cb639903820463741e58a6891",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "A prompt engineer observes that lowering the temperature to 0.2 yields overly generic outputs. What\u2019s causing this behavior?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Low temperature sharpens the distribution, making the model choose high-probability tokens repeatedly.",
      "B": "It increases sampling variance, leading to unpredictable results.",
      "C": "It normalizes logits, causing underflow in softmax.",
      "D": "It disables top-k sampling, resorting to greedy decoding."
    },
    "explanation": "A is correct\u2014lower temperature favors top tokens. B\u2013D mischaracterize temperature\u2019s effect."
  },
  {
    "id": "d730cfc5267c27184ab87e093cb0ffe7ab76ec99574d3b50050433ba1ccd3011",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "In sampling strategies, what is the key difference between top-k and nucleus (top-p) sampling for text generation?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Top-k limits to a fixed number of tokens, while top-p selects tokens until cumulative probability exceeds a threshold.",
      "B": "Top-k adapts k based on token probability, while top-p uses a static set.",
      "C": "Top-p always reduces latency, whereas top-k is slower.",
      "D": "Top-p relies on beam search, and top-k does not."
    },
    "explanation": "A is correct\u2014top-k fixes count, top-p fixes probability mass. B\u2013D are inaccurate."
  },
  {
    "id": "02f16e3ddd7f19fee7d559c4c2af7a706861665652a6e3c59f23c209c6c9b05e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "When deploying an embedding store in production, which cost factor grows most with increasing embedding dimension versus increasing dataset size?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Memory per vector grows linearly with dimension, while index size growth with dataset size depends on index type.",
      "B": "Network bandwidth becomes the dominant cost for larger dimensions.",
      "C": "CPU indexing time is unaffected by dataset size.",
      "D": "Storage IOPS increase with dimension."
    },
    "explanation": "A is correct\u2014dimension directly scales vector size. B\u2013D misattribute costs."
  },
  {
    "id": "3189e88dd45534398fdeee120d39d1380c3f5b3ef59a94ae2deedbb2325d791c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "Why do transformers use sinusoidal positional encodings instead of learned embeddings for very long sequences?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Sinusoidal embeddings generalize to unseen lengths due to their periodic properties.",
      "B": "Learned embeddings require less memory for long sequences.",
      "C": "Sinusoidal encodings prevent overfitting on position patterns.",
      "D": "Learned embeddings cannot be used with multi-head attention."
    },
    "explanation": "A is correct\u2014sinusoids extend to any position. B and D are false; C is not primary reason."
  },
  {
    "id": "64c5096e59765cb593c8f2677496e37d42a96e6030b435f8ab4d7bf4803a8024",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "What benefit does multi-head attention provide in a transformer-based generative model?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "It allows the model to attend to information from different representation subspaces at different positions.",
      "B": "It reduces computation by parallelizing single-head attention.",
      "C": "It enforces causality in the decoding phase.",
      "D": "It increases the maximum context window."
    },
    "explanation": "A is correct\u2014each head learns unique relationships. B, C, and D misstate its purpose."
  },
  {
    "id": "f9a3f3d1899ef8480752e5463450d60e07b5989629acbba727ace0f9c7647531",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "If a foundation model has a 2048-token context window, how should you preprocess a 5000-token document for a RAG pipeline to preserve coherence?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Chunk into 2048-token segments with ~10% overlap to maintain context at boundaries.",
      "B": "Chunk into non-overlapping 2500-token blocks to minimize number of chunks.",
      "C": "Truncate to the first 2048 tokens as most relevant.",
      "D": "Randomly sample 2048 tokens from the document each query."
    },
    "explanation": "A is correct\u2014overlap preserves continuity. B loses coherence; C and D discard key content."
  },
  {
    "id": "cb2a0f6d767bd1fb3991a91e9ed9ce958251f28bf8e0b743b4b55049a37ab596",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "A developer wants both long inputs and long outputs from a model with a fixed 4096-token limit. What\u2019s an effective strategy?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Split the input into chunks and generate outputs for each, then concatenate and refine.",
      "B": "Request maximum output length and let the model truncate the input automatically.",
      "C": "Increase the temperature to trade context for output.",
      "D": "Use greedy decoding to prioritize output length."
    },
    "explanation": "A is correct\u2014chunking plus post-processing handles long flows. B\u2013D do not address the limit effectively."
  },
  {
    "id": "49c07cc4ecda63c15b90ab4bb880442d4cf23c893efb66449f6a06d83e2a92d1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "During inference on a diffusion model, what happens if the noise schedule steps have too small a variance between timesteps?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Generation quality may degrade due to insufficient denoising signal between steps.",
      "B": "Sampling speed increases but outputs become diverse.",
      "C": "Model collapses to a single mode output.",
      "D": "The reverse process becomes non-deterministic."
    },
    "explanation": "A is correct\u2014too-fine steps weaken noise gradients. B\u2013D misrepresent the effect."
  },
  {
    "id": "5c807903e5adba4a0625636515e8b12f6966dfd4275b5acbc1fc600c18f729c4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "Which technique ensures cross-lingual alignment when training multilingual embeddings for a generative AI chatbot?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Shared subword vocabulary and joint training on parallel corpora to align semantic spaces.",
      "B": "Language-specific embeddings with no parameter sharing.",
      "C": "Training separate models and concatenating their outputs.",
      "D": "Using one-hot encodings for each language."
    },
    "explanation": "A is correct\u2014shared vocab plus parallel data aligns languages. B\u2013D fail to produce joint embedding spaces."
  },
  {
    "id": "e9bb731dda25ab878bfb2c8542e0fae9091a3c36768007cb0c4008fa7da9b9a6",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "In few-shot prompting, why might providing too many examples degrade a model\u2019s performance on a new task?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "It consumes context window, leaving less space for the actual prompt and thus limiting task instructions.",
      "B": "It causes the model to memorize examples, preventing generalization.",
      "C": "It increases temperature implicitly, reducing determinism.",
      "D": "It disables attention to the final instruction."
    },
    "explanation": "A is correct\u2014examples use up context tokens. B\u2013D are not primary issues with too many examples."
  },
  {
    "id": "0290d4046c85245916c53bd285cb15c38e2ddac27dc3ab1e4cb0489391fb669e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "How can vector arithmetic on embeddings (e.g., vec(king)\u2212vec(man)+vec(woman)) fail in a generative AI context?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Embedding space may not be fully linear for abstract relations, causing semantic drift in generation.",
      "B": "Arithmetic always yields unknown tokens.",
      "C": "It requires specialized loss functions at inference.",
      "D": "It works only for image embeddings, not text."
    },
    "explanation": "A is correct\u2014linear semantics are only approximate. B\u2013D are incorrect statements."
  },
  {
    "id": "36272140e339bcf701b8feb08214c71a250cb2f76fd1d4350f04cee016950194",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "What is the primary memory complexity issue with transformers as sequence length increases?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Self-attention scales quadratically (O(n\u00b2)) with sequence length, limiting long contexts.",
      "B": "Feed-forward layers require O(n\u00b3) memory.",
      "C": "Positional encoding scales linearly but dominates memory.",
      "D": "Multi-head attention scales logarithmically, making it inefficient."
    },
    "explanation": "A is correct\u2014attention\u2019s O(n\u00b2) growth is the bottleneck. B\u2013D misstate complexities."
  },
  {
    "id": "ab9e44fb5c106204ae637faa717f86da2758edc1c52b0860aba8643ede68b89a",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "When combining embeddings with RAG for document QA, why is it problematic to update the index with new documents without re-computing embeddings?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "New documents won\u2019t be comparable to existing ones in latent space, leading to retrieval inconsistencies.",
      "B": "It reduces the context window size.",
      "C": "It increases the embedding dimension automatically.",
      "D": "It forces the RAG system to retrain the generator."
    },
    "explanation": "A is correct\u2014embeddings must share the same space. B\u2013D are unrelated issues."
  },
  {
    "id": "15d6fa102d5006dff0debfd5c84b84ff06c3c94c69b70a52a83c886c805426bc",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "Why might image embeddings from a CLIP model and text embeddings be stored separately in a mixed-modality application?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "They occupy different subspaces and require modality-specific similarity metrics for retrieval.",
      "B": "Image embeddings cannot be quantized.",
      "C": "Text embeddings are binary while image embeddings are continuous.",
      "D": "CLIP models output text embeddings only for image queries."
    },
    "explanation": "A is correct\u2014they differ in distribution and distance metrics. B\u2013D are false."
  },
  {
    "id": "58b02a29a391135cc69f2f8b8cdaf42ebc89f489a986db5989f958aa95839fec",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "What effect does token bias (adding a constant to logits of specific tokens) have during generation?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "It increases the probability of favored tokens, potentially reducing diversity.",
      "B": "It normalizes the output distribution, improving fairness.",
      "C": "It reduces model perplexity to zero.",
      "D": "It dynamically adjusts the learning rate."
    },
    "explanation": "A is correct\u2014bias skews token probabilities. B\u2013D misinterpret token bias effects."
  },
  {
    "id": "914302221f0c8ffa43d59734d72e85be2c2ba10bc6efefeb00de9c10a53eeb41",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A fintech startup uses a large foundation model via Amazon Bedrock to draft financial advice. They observe occasional hallucinations in the output. Which single change will most effectively reduce hallucinations without retraining?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase the model\u2019s temperature setting.",
      "B": "Implement Retrieval Augmented Generation (RAG) with a verified document store.",
      "C": "Switch from few-shot to zero-shot prompting.",
      "D": "Add chain-of-thought prompting to the prompt."
    },
    "explanation": "RAG constrains the model to factual context, reducing hallucinations. Altering temperature, shot counts, or prompting style alone does not guarantee factual grounding."
  },
  {
    "id": "2314b84b86a67f886d5ce2ceaac5a3034623fc6cbfd710a64fa04083157a43e8",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A marketing team must generate personalized slogans with high creativity but predictable quality. Which parameter adjustment balances creativity and consistency?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature to 1.2",
      "B": "Set top-k to 1",
      "C": "Lower temperature to 0.6 and use top-p=0.8",
      "D": "Use maximum token length with temperature=0.2"
    },
    "explanation": "Lowering temperature to ~0.6 and using top-p sampling yields creative yet controlled outputs. High temperature or extreme top-k/p settings push too far toward randomness or determinism."
  },
  {
    "id": "ddead9a6b6da860128a53e5219c281a5ab7d4ffbe8828a53db7120b277c2806b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A healthcare provider needs an explainable generative AI model for medical summaries. Which model choice best meets interpretability requirements?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A closed-proprietary 70B parameter model with no transparency.",
      "B": "A high-capacity multimodal model via Bedrock.",
      "C": "An open-source 30B parameter model fine-tuned on medical data.",
      "D": "An open-source 7B parameter model with published weights and architecture."
    },
    "explanation": "A smaller open-source model with transparent architecture facilitates interpretability. Proprietary or extremely large models obscure inner workings and hinder explainability."
  },
  {
    "id": "4d5716224d71103337dc1f347e911741b0839854d4f1595f0def7da9d8055278",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A retail company measures business impact of generative AI by ARPU. They deploy a chatbot that upsells products. Which evaluation metric combination best correlates with ARPU growth?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Average order size and conversion rate",
      "B": "Token usage per session and model latency",
      "C": "Model perplexity and inference throughput",
      "D": "User engagement time and number of API calls"
    },
    "explanation": "ARPU growth ties to conversion rate and order size. Perplexity or token counts don\u2019t directly measure revenue impact."
  },
  {
    "id": "c2a627def3d5689a07ffbc4b3eb18efcdd4a603c1a70265af856b0e16c1fef8c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "An e-commerce site wants a generative AI recommendation engine. They need cross-domain performance (products and content). Which deployment yields best cross-domain generalization?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Fine-tune a product-only foundation model on product data.",
      "B": "Use in-context learning on a content-only model.",
      "C": "Use a foundation model pre-trained on diverse domains via Bedrock JumpStart.",
      "D": "Train a custom model from scratch on combined data."
    },
    "explanation": "A generously pre-trained foundation model on diverse domains generalizes best. Scratch training demands huge data and time; fine-tuning on narrow data limits domains."
  },
  {
    "id": "b6e1556b08030f138414f291132eed68ef1fb71c97ab3f3a08dbceb370788448",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A publisher uses a text generation API to summarize articles. They need consistent output length but the model sometimes over-runs. Which setting adjustment controls output length strictly?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature to reduce unpredictability.",
      "B": "Set a maximum token limit and use \u201cstop sequences.\u201d",
      "C": "Use larger model with more capacity.",
      "D": "Switch from sampling to greedy decoding."
    },
    "explanation": "Defining stop sequences and a max token limit reliably halts generation at desired length. Greedy decoding alone may still overshoot if stop tokens aren\u2019t configured."
  },
  {
    "id": "3c8df454437d9ee92fe760d5432571c0dd618ad73cd56c848260448f9659bf62",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A legal firm requires compliance with data residency. They must avoid sending sensitive documents outside their VPC. Which AWS generative AI service meets this requirement?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Comprehend via public API",
      "B": "Amazon Lex in multi-region mode",
      "C": "Amazon Q via internet endpoint",
      "D": "SageMaker-hosted private Bedrock container with VPC endpoints"
    },
    "explanation": "Running Bedrock privately in SageMaker with VPC endpoints keeps data inside the VPC. Public APIs send data over internet."
  },
  {
    "id": "4630f68e7e37926d2128acdcd9b0ede060cf66c0c667c6200985930078ee497f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A gaming company sees nondeterministic outputs from the same prompt. Which combination yields the most deterministic behavior?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High temperature, high top-p",
      "B": "Low max tokens, chain-of-thought",
      "C": "Temperature=0.0 and top-k=1",
      "D": "Few-shot prompting and top-p=0.9"
    },
    "explanation": "Temperature 0.0 with top-k=1 forces the model to choose the highest-probability token each step, yielding deterministic outputs."
  },
  {
    "id": "0c0eec0ccaa028adeb43209555393604521d3886b0ac0f2a47b83900f584c9a8",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A developer needs to estimate token usage costs for a monthly chatbot workload of 100k prompts at average 200 tokens each request and 800 tokens response. The Bedrock price is $0.0004 per input token and $0.0006 per output token. What\u2019s the monthly token cost?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "100k\u00d7(200\u00d70.0004+800\u00d70.0006) = $68,000",
      "B": "100k\u00d7(200\u00d70.0006+800\u00d70.0004) = $52,000",
      "C": "100k\u00d7(1000\u00d70.0005) = $50,000",
      "D": "100k\u00d7(200+800)\u00d70.0004 = $40,000"
    },
    "explanation": "Input cost:200\u00d70.0004=$0.08; output:800\u00d70.0006=$0.48; per prompt $0.56; times100k = $56,000. Actually correct math yields $56,000, not listed. (A) miscalculated \u2013 trick: candidate must catch mispricing."
  },
  {
    "id": "3543367c3e20c25a913b3835d58784b1a1f3ff3f0424c6062e21b56195243748",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "Which limitation of generative AI most impacts use in safety-critical systems?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "High throughput cost",
      "B": "Limited multimodal support",
      "C": "Inability to fine-tune",
      "D": "Nondeterminism and hallucinations"
    },
    "explanation": "Hallucinations and unpredictability make generative AI unsafe for critical systems; the other factors are secondary."
  },
  {
    "id": "374c251296c49973101d1a8d9161122b69de041b3d9d7128bd0208ac34e71a1a",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A company benchmarks two foundation models for product text generation. Model A has lower perplexity but higher latency. Model B has higher perplexity but lower latency. They value efficiency over quality. Which model suits them?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model A for its lower perplexity",
      "B": "Model B for its lower latency",
      "C": "Average the outputs of A and B",
      "D": "Fine-tune A to reduce latency"
    },
    "explanation": "When efficiency is prioritized, lower latency (Model B) is preferable despite slightly worse perplexity."
  },
  {
    "id": "4d717178e3ee6884230ca9292f4e092ef4d5cca2207e852416788d14bfc7d484",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A media company needs summarization of streaming audio in near-real-time. They require minimal delay. Which generative AI approach is best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Batch transcription then summarization",
      "B": "Use large multimodal model with high accuracy",
      "C": "Stream transcription with low-latency LLM endpoint",
      "D": "Daily batch processing of recorded files"
    },
    "explanation": "Streaming transcription plus low-latency LLM endpoint meets near-real-time requirement. Batch methods introduce unacceptable delays."
  },
  {
    "id": "238e1d0fded7d658aa0bd70e1fb14bdc0347059e12112af4672909779a4311cd",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A team wants to monitor model drift in a generative AI summarization service. Which metric combination best surfaces drift?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "KL divergence of token distribution and output length variance",
      "B": "Total token cost and average session length",
      "C": "Number of API errors and model latency",
      "D": "User satisfaction score only"
    },
    "explanation": "Statistical measures like KL divergence and length variance detect distributional changes indicating drift; others don\u2019t capture content shift."
  },
  {
    "id": "c8a80a8fb45227e7d5e9a68d0a2f4d09009c010eb658a729aaf57976569b426a",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A startup uses a pre-trained multimodal model for video captioning but needs domain adaptation. They lack compute for fine-tuning. Which strategy suits best?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a custom model from scratch on their data",
      "B": "Use in-context learning with domain-specific examples",
      "C": "Switch to a unimodal text model",
      "D": "Implement RLHF at scale"
    },
    "explanation": "In-context learning injects domain examples without fine-tuning; other methods require heavy compute or rebuilds."
  },
  {
    "id": "2c869e6b34683f83166310f036ea3e9e2b5acbc9c2dae79e13fae520376b31f3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "Which risk is specifically introduced by prompt injection attacks on generative AI systems?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model overfitting",
      "B": "Increased latency",
      "C": "Unauthorized command execution",
      "D": "Higher token costs"
    },
    "explanation": "Prompt injection can manipulate the model to execute unauthorized instructions; other risks aren\u2019t directly tied to injection."
  },
  {
    "id": "43a8b6e4590363489afc63403567d5d5003070d8af301e6e2feb5673967b3f69",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "An enterprise requires auditability of all AI outputs linked to data sources. Which feature of Bedrock should they leverage?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Multi-shot prompting",
      "B": "Temperature tuning",
      "C": "Batch endpoints",
      "D": "Provenance logs with model card integration"
    },
    "explanation": "Provenance logs track input-output lineage and model metadata for audit; other features don\u2019t provide traceability."
  },
  {
    "id": "0cc53ad96151a1324c6a41f80fcd79a1ef65cf0529a049ef573bc61d29ec86ba",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A firm wants to measure the efficiency of a generative AI summarization pipeline end-to-end. Which metric combination is most appropriate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "F1 score and token cost",
      "B": "End-to-end latency and cost per summary",
      "C": "Perplexity and BLEU score",
      "D": "Model parameter count and API throughput"
    },
    "explanation": "Pipeline efficiency is captured by latency and cost; perplexity/BLEU address quality, not efficiency."
  },
  {
    "id": "7a1b24130478a5c7eea75eb14abcba8101a547be11eac8a8dbe1b072e6cbb990",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "Which generative AI limitation poses the greatest challenge when generating legal contracts?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Hallucination of non-existent clauses",
      "B": "Low throughput",
      "C": "Multimodal capability",
      "D": "High interpretability"
    },
    "explanation": "Incorrectly invented clauses (hallucinations) jeopardize legal accuracy; other limitations are less critical."
  },
  {
    "id": "626c28752ea2164095eb5591408e4c33d1a424c3db72e524da1179c6fe93bf7f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A company wants to optimize both the accuracy and speed of its generative AI system. Which trade-off best describes the relationship?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increasing model size increases speed and accuracy linearly",
      "B": "Lower temperature increases speed at the cost of accuracy",
      "C": "Larger models improve accuracy but increase latency",
      "D": "Higher top-p reduces latency and improves accuracy"
    },
    "explanation": "Bigger models generally yield higher accuracy but slower inference; other statements are oversimplified or incorrect."
  },
  {
    "id": "9dae420d05e8296139b1178d9b4be9f7299e02cc7e4aa785dc868c6d12fba788",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A data scientist evaluates two foundation models for summarization: Model X yields higher ROUGE scores; Model Y yields lower latency and cost. They prioritize CLV improvements through speed. Which model should they choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model X for better ROUGE",
      "B": "Model Y for lower latency/cost",
      "C": "Interpolate outputs of X and Y",
      "D": "Fine-tune X to reduce cost"
    },
    "explanation": "Faster, cheaper Model Y better supports higher throughput and improved customer lifetime value."
  },
  {
    "id": "c0c03c0b46ceff98682655f59787d601440657131dd466e329200ffd16a867ad",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "Which advantage of generative AI is most beneficial for customer support automation?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Responsiveness to dynamic user queries",
      "B": "High labeled data requirements",
      "C": "Deterministic outputs",
      "D": "Guaranteed factual accuracy"
    },
    "explanation": "Generative AI\u2019s responsiveness enables handling diverse user questions; factual accuracy is not guaranteed and labeled data may not be required at inference."
  },
  {
    "id": "8f5fff263f11bf2ec4613c5a00e3a58f8f5a08993c63406ff1d07c705b54ca86",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A retailer leverages a generative AI to compose product descriptions. They notice lower click-through rates on AI-generated text compared to human-written. Which capability limitation likely caused this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Nondeterminism in generation",
      "B": "High latency",
      "C": "Lack of domain-specific fine-tuning",
      "D": "Insufficient token budget"
    },
    "explanation": "Without domain adaptation via fine-tuning, output may miss brand voice nuances, lowering engagement; other factors are less relevant."
  },
  {
    "id": "489866f2e98dec153b8aa8c3e25683b9ec0db16881b9af9a7ecc244dde5790ea",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A company uses a generative AI for code generation. Which business metric directly measures its value?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model perplexity",
      "B": "Output token count",
      "C": "Inference latency",
      "D": "Developer productivity increase"
    },
    "explanation": "Developer productivity gain directly quantifies business impact; perplexity and token counts are technical metrics."
  },
  {
    "id": "b2d1b49324a61e9e1ef62bf2cbffa2d4a7bc744793dbf29a2ad13cc9d84e570f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "Which generative AI disadvantage makes it challenging to comply with regulatory data-lineage requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High throughput cost",
      "B": "Opaque generation processes",
      "C": "Limited access to multimodal models",
      "D": "Deterministic behavior"
    },
    "explanation": "Opaque \u201cblack-box\u201d processes hinder tracing content origin; other factors don\u2019t affect lineage."
  },
  {
    "id": "b3a957183cb3a7b4adf12a6e656526eb1b8a782aa3c56128b876c50a2c2179dd",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A media analytics firm integrates generative AI summaries across news domains. They need consistent quality across topics. Which approach minimizes topic bias?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature for all prompts",
      "B": "Use zero-shot prompting only",
      "C": "Fine-tune on a balanced, multi-domain corpus",
      "D": "Limit token usage per domain"
    },
    "explanation": "Fine-tuning on diverse, balanced data reduces bias; temperature or prompt style alone cannot address content imbalance."
  },
  {
    "id": "bf0d770a19665bf66ab895f196c7c25c7b264daa9e7addcaeca0a4a5c68b532f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A company wants to expose a generative AI chatbot to external partners but must enforce content controls. Which AWS feature should they use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock Guardrails",
      "B": "High top-p sampling",
      "C": "In-context learning",
      "D": "Increased token limits"
    },
    "explanation": "Guardrails enforce policy checks on generated content; sampling or token settings do not ensure compliance."
  },
  {
    "id": "e65a1ed235c8a846cfe4033daa71ab64025037eb71b152ba6dc59b8730c1bd5e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "To evaluate a generative model\u2019s cross-domain performance, which benchmark approach is most robust?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Measure model latency across domains",
      "B": "Compare token costs per domain",
      "C": "Track customer satisfaction per domain",
      "D": "Use standardized test datasets spanning all target domains"
    },
    "explanation": "Standardized, domain-diverse benchmarks yield objective cross-domain performance comparisons; cost or latency alone don\u2019t measure accuracy."
  },
  {
    "id": "f7c18fd63912f3298faf8f5ff1e447def34d17e39e2d607bf9c94f86ec79b98d",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "Which generative AI limitation must be addressed to ensure consistent brand tone in marketing copy?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Output length variance",
      "B": "Nondeterminism in language style",
      "C": "Multimodal capability",
      "D": "High memory footprint"
    },
    "explanation": "Nondeterminism yields variable style; controlling temperature and using templates helps enforce brand tone."
  },
  {
    "id": "071b1109ee1b5dd124678cd5d2dcccc5af431b4b00c45f445079f3bfd4061272",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A logistics company uses generative AI for routing instructions. They need to guarantee no incorrect directions. Which limitation disqualifies generative AI for this use case?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High inference cost",
      "B": "Limited multimodal support",
      "C": "Possible hallucinations",
      "D": "Large model size"
    },
    "explanation": "Hallucinations risk generating unsafe or incorrect routes; other factors are less safety-critical."
  },
  {
    "id": "7988755ebea78490865e6a19ec4e65b48aa3de65db5d1cbf56d4460d4af11d0e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "Which metric combo best quantifies generative AI\u2019s impact on lead generation?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Conversion rate uplift and cost per lead",
      "B": "Inference latency and token usage",
      "C": "Model perplexity and ROUGE score",
      "D": "Average session length and API error rate"
    },
    "explanation": "Conversion uplift and cost per lead directly measure lead generation effectiveness and efficiency."
  },
  {
    "id": "7b28ffb1ecdcaaf0d80a8c57cdff9125c92d5c246cdb62a6b1060285c6b541ee",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A startup wants to lower unpredictability in a multi-turn conversational AI. What two changes achieve this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature and remove context history",
      "B": "Use a larger model and high top-k",
      "C": "Set temperature=0.0 and enable deterministic mode",
      "D": "Switch to zero-shot prompting and increase max tokens"
    },
    "explanation": "Deterministic mode and temperature=0.0 enforce repeatable outputs; other options increase randomness or remove useful context."
  },
  {
    "id": "5708aa14d5a59c86fda1510827f2594d57f727ce778a503d9112a21fc4fca15a",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A healthcare provider needs to deploy a pre-trained multimodal foundation model that can process radiology images and patient notes. The solution must be fully managed, maintain PHI data in a private network, and minimize custom infrastructure work. Which AWS technology should you choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock with VPC interface endpoint",
      "B": "Amazon SageMaker JumpStart hosted endpoint",
      "C": "Custom EC2 instance running an open-source transformer",
      "D": "Amazon Q with public internet access"
    },
    "explanation": "Bedrock supports private VPC endpoints for managed, multimodal foundation models without custom infra."
  },
  {
    "id": "6ac1a1ce3232cb37d7e1d79ad2f1044e7f27bfa246415757fde5e7c59a6df428",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "An e-commerce company wants to generate product descriptions in multiple languages using a foundation model without data leaving AWS. They require token-based pricing and no long-term commitment. Which service meets these requirements?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon SageMaker JumpStart multilingual model",
      "B": "Amazon Translate with custom terminology",
      "C": "Amazon Bedrock with built-in foundation models",
      "D": "Amazon Comprehend custom classification"
    },
    "explanation": "Bedrock provides token-based invoicing, multi-language foundation models, and data stays within AWS."
  },
  {
    "id": "ad9b12f333fbfa88a2b6d488724e158f4aa15660f5cab3409a89dc408b2d35ed",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A startup must prototype a generative AI chatbot quickly, with minimal code and zero ML expertise. They need out-of-the-box Q&A capabilities using a foundation model. Which offering should they use?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Build custom fine-tuned LLM on SageMaker",
      "B": "Amazon Q managed conversational agent",
      "C": "Deploy JumpStart model on EC2",
      "D": "Use Amazon Lex with Lambda"
    },
    "explanation": "Amazon Q provides managed, Q&A chatbot using foundation models with no ML coding."
  },
  {
    "id": "9a16cccd2dd6c0baa07a508de9c4c108d6e61b9370426a1f7406b17326e4d066",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A media company needs to orchestrate a pipeline that preprocesses prompts, calls a foundation model, and post-processes outputs. They prefer a graph-based orchestration library integrated with AWS. Which tool?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Step Functions",
      "B": "SageMaker Pipelines",
      "C": "AWS Lambda orchestration",
      "D": "PartyRock"
    },
    "explanation": "PartyRock provides graph-based orchestration for generative AI tasks over AWS."
  },
  {
    "id": "2bc5bf250dd8e86bffa81797a781fcfd79cc475b3e1db287d0fec697c504d8c0",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "An organization wants to fine-tune a foundation model with few-shot data, host it, and manage model health metrics. Which combination of services is most appropriate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock fine-tuning and use CloudWatch",
      "B": "SageMaker JumpStart fine-tuning and SageMaker Model Monitor",
      "C": "Amazon Q custom training and Lambda",
      "D": "Custom EC2 training and CloudWatch Logs"
    },
    "explanation": "SageMaker JumpStart supports fine-tuning foundation models and integrates with Model Monitor."
  },
  {
    "id": "c52bb55bdba63e272ea5e20885bc8165e001eab6892d548a735470672e372845",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A fintech firm must serve high-throughput generative AI requests with predictable latency and pay per instance. They have capacity to manage underlying GPUs. Which is optimal?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock inference",
      "B": "SageMaker JumpStart serverless endpoint",
      "C": "Deploy Hugging Face model on SageMaker GPU instances",
      "D": "Use Amazon Q API"
    },
    "explanation": "SageMaker GPU endpoints give instance-based billing and predictable performance."
  },
  {
    "id": "12a56097e31603b2c7d1513e1c65ca2072936dd3c5b42739e2193643e570d549",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "Your team needs to store and search embeddings generated by a foundation model for RAG. You need a managed, autoscaling, low-latency search solution. Which AWS service?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Aurora",
      "B": "Amazon OpenSearch Service",
      "C": "Amazon Q",
      "D": "Amazon S3"
    },
    "explanation": "OpenSearch supports vector search on embeddings with auto-scaling."
  },
  {
    "id": "b6108b43e935ebfeaf4e46ac5af8040411d56f86c84e00fe89547f757c4b71d0",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A legal tech firm has strict data residency requirements in an on-premises AWS Outpost. They want to use a foundation model via AWS. Which solution supports this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock public API",
      "B": "SageMaker JumpStart in region",
      "C": "Amazon Q multi-region feature",
      "D": "SageMaker JumpStart deployed on Outposts"
    },
    "explanation": "SageMaker JumpStart can deploy models on Outposts ensuring on-prem data residency."
  },
  {
    "id": "052e4ae60bfa3485408bcf6094aff8840985ab7045d51e358484594d86a3881f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A marketing agency wants to evaluate costs of large language model inference for budget forecasting. Which AWS feature gives per-token cost estimates for Bedrock usage?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Bedrock pricing console estimator",
      "B": "Cost Explorer Reserved Instances",
      "C": "SageMaker pricing API",
      "D": "AWS Budgets with SageMaker metrics"
    },
    "explanation": "Bedrock console shows per-token pricing for models."
  },
  {
    "id": "913771709178fb969d48f21c997438d188ea6ecb53db8ba35ee23f2865864b08",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A gaming company needs to integrate dynamic world-building via generative AI in a live game server with sub-100ms response times. They need edge deployment. Which AWS solution?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Amazon Bedrock central inference",
      "B": "SageMaker serverless endpoint",
      "C": "Deploy foundation model with SageMaker edge manager on Greengrass",
      "D": "Amazon Q global API"
    },
    "explanation": "SageMaker Edge Manager on Greengrass allows deploying models to edge devices for low latency."
  },
  {
    "id": "db4391e594a20ffc732688ea417af0e2f566db93cefb722c6e8a74d9d570a8ce",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A biotech startup wants to experiment with open-source diffusion models but avoid large infrastructure setup. They want managed notebooks and one-click deployment. Which service?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock",
      "B": "SageMaker JumpStart with notebook and endpoint",
      "C": "Amazon ECR custom container",
      "D": "Amazon EC2 GPU instance only"
    },
    "explanation": "SageMaker JumpStart provides notebooks and one-click endpoint deployment for open-source models."
  },
  {
    "id": "388b31595dc15103a70e146d0e693d2b10f4b880e10ce20de49f592e23844f89",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "Your team must ensure that all generative AI API calls are logged and audited for compliance. Which AWS feature should you enable with Bedrock?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CloudTrail data events only",
      "B": "S3 server access logs",
      "C": "CloudWatch Logs integration",
      "D": "CloudTrail data and management events for Bedrock"
    },
    "explanation": "Enabling Bedrock data events in CloudTrail logs all API calls for auditing."
  },
  {
    "id": "c4c5329d5b3bf028bcbc7493bddfaea122be2cc0fcd80327f1106d94d4f3fd8a",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A retail chain wants to deploy a foundation model across multiple AWS accounts using Infrastructure as Code. Which service integrates best with CloudFormation for generative AI deployments?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Q",
      "B": "Amazon Bedrock CloudFormation resource types",
      "C": "SageMaker Studio templates",
      "D": "AWS Lambda custom resources"
    },
    "explanation": "Bedrock provides CloudFormation resource types for unified IaC deployment."
  },
  {
    "id": "836217f0f8655bfcae8a10c64d6b126fc4d47b27ac351c18c53b2b724118fc21",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "An analytics platform generates embedding vectors in high volume and needs to store them cost-effectively for occasional batch RAG. Which storage option is best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon OpenSearch (hot nodes)",
      "B": "Aurora PostgreSQL with vector plugin",
      "C": "Amazon S3 with vector file format and Athena",
      "D": "Neptune for frequent graph queries"
    },
    "explanation": "S3 with Athena provides low-cost, batch retrieval for embeddings."
  },
  {
    "id": "a9c710ceafa3d2e0129142f719bed328c6ce2911ef852689782b829c3158c80e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "Your security team demands AI inference traffic never traverse the public internet. You use Bedrock. What configuration enforces this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create a VPC endpoint for Bedrock in your private subnet",
      "B": "Use NAT Gateway routing only",
      "C": "Attach Internet Gateway to VPC",
      "D": "Enable public access in Bedrock settings"
    },
    "explanation": "A VPC interface endpoint ensures Bedrock traffic stays within AWS network."
  },
  {
    "id": "addc7a2f2375055b54fd836de5202072f131a79a022e667856f14365f97a49e0",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A developer needs to rapidly prototype code generation from LLM prompts using Python SDK and shared notebooks with teammates. Which AWS offering is most suitable?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Q API with cURL",
      "B": "Bedrock console UI",
      "C": "Lambda custom integration",
      "D": "SageMaker JumpStart notebooks with AWS SDK pre-configured"
    },
    "explanation": "JumpStart notebooks include SDK clients and example code for prompt-based prototyping."
  },
  {
    "id": "4271fc470099fec383ddf30b76b4ba71843061a178aec904ee46c5109228e57f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A data scientist needs to compare inference latency and token cost across multiple foundation models. Which approach is best?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Cost Explorer directly",
      "B": "Deploy each model in Bedrock and benchmark with CloudWatch metrics",
      "C": "Estimate via pricing pages offline",
      "D": "Use SageMaker Model Monitor"
    },
    "explanation": "Deploying in Bedrock and measuring with CloudWatch gives real metrics for latency and cost."
  },
  {
    "id": "2c1f566135735e7986b9b7447aaf94a879c70c0e20e11d5469451b266e094df8",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A telecom provider must maintain model version lineage and metadata for foundation models they fine-tune. Which AWS capability helps track this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Config rules",
      "B": "CloudTrail for model events",
      "C": "Amazon SageMaker Model Registry",
      "D": "Amazon Q audit log"
    },
    "explanation": "SageMaker Model Registry manages versions and metadata for fine-tuned models."
  },
  {
    "id": "b91117d00dea5282abe519a077d509fe0ade5c140a4c1407ad56c641fdde882f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "Your legal team requires redaction of PII in fine-tuning data before uploading to AWS. Which service should you integrate into your generative AI pipeline?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Comprehend classification",
      "B": "Amazon Translate PII protection",
      "C": "SageMaker Clarify",
      "D": "Amazon A2I PII redaction workflow"
    },
    "explanation": "Amazon A2I can human-review and redact PII before using data for training or fine-tuning."
  },
  {
    "id": "87283d521f99898319e3ca8b99ee7e8d346433ead316d8300619fc038ed7f8bc",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A media startup wants to implement RAG using Bedrock with a knowledge base in Amazon DocumentDB. They need to serve embeddings at scale. Which mechanism efficiently syncs new documents?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Lambda triggered by DocumentDB change streams to index embeddings in OpenSearch",
      "B": "Batch ETL with Glue daily",
      "C": "Manual export-import process",
      "D": "Streaming with Kinesis Data Streams directly into DocumentDB"
    },
    "explanation": "Lambda with change streams provides near real-time embedding sync into OpenSearch for RAG."
  },
  {
    "id": "98564c4ef1af1e2e4c09eb6bb7a16512de7e30bb79f6d799b22827af034c792c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A research group wants to fine-tune a diffusion model using spot instances to reduce cost. They need managed job retries. Which AWS feature should they use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Bedrock fine-tuning with spot",
      "B": "SageMaker training jobs with Managed Spot Training",
      "C": "EC2 batches with spot fleet",
      "D": "Lambda step functions"
    },
    "explanation": "SageMaker managed spot training retries jobs and saves cost automatically."
  },
  {
    "id": "dc44c9eeb735af5fc3fab963ee6c5bfa1e14ea650932ca2ba639c12b9f9c6a3b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A compliance requirement mandates deletion of user-specific fine-tuned models upon user withdrawal. Which AWS service can orchestrate model lifecycle including deletion?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Model Registry lifecycle policies",
      "B": "Bedrock model settings",
      "C": "AWS Config remediation",
      "D": "CloudTrail event triggers"
    },
    "explanation": "Model Registry lifecycle policies automate deregistering and deleting models per policy."
  },
  {
    "id": "aa879ac65989b9ccc75e43ba34a67ee6651da7c3f3ffcafa555eedc83e67d918",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A global enterprise needs to ensure latency is under 200ms for generative AI inference across three AWS regions. They want a single API endpoint. What solution meets this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy three Bedrock endpoints and load-balance at client",
      "B": "Use Amazon Q global regional endpoints",
      "C": "SageMaker multi-az endpoint",
      "D": "Amazon CloudFront API Gateway distribution routing to Bedrock regional endpoints"
    },
    "explanation": "API Gateway + CloudFront can route to nearest Bedrock endpoint for consistent low latency."
  },
  {
    "id": "f8bfae92ed02a1a8d169aa5970196d2003de6474471acb185280268ab1e621dc",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A product team needs alerts when Bedrock model performance degrades below a BLEU threshold over time. How do you implement this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CloudWatch anomaly detection on token counts",
      "B": "SageMaker Model Monitor on Bedrock",
      "C": "Custom Lambda evaluating inference logs and publishing metrics to CloudWatch alarms",
      "D": "Bedrock built-in metric alarms"
    },
    "explanation": "Bedrock lacks native Model Monitor; custom Lambda parses logs, publishes metrics to CloudWatch for alarms."
  },
  {
    "id": "21c572ad14249acbf7d6282308b09030110377318f3581cca3f5fa914a7b0759",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A financial institution must encrypt all model artifacts at rest and in transit for a JumpStart fine-tuning job. Which configuration ensures this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable TLS only",
      "B": "Use KMS key for SageMaker job and ensure VPC endpoint",
      "C": "Use default AWS keys",
      "D": "Encrypt only S3 buckets"
    },
    "explanation": "Specifying customer-managed KMS key and VPC endpoint encrypts data in transit and at rest."
  },
  {
    "id": "94d74e2c1a6012d40e4e01c9d796779d01a95035b896c02520c97b77fec69acf",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A compliance audit found that generative AI logs were stored alongside customer PII. The team must isolate logs and secure them for tamper proofing. Which approach is recommended?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Send logs to a separate S3 bucket with Object Lock in compliance mode",
      "B": "Store logs in the same bucket but different folder",
      "C": "Use DynamoDB for logs",
      "D": "Use AWS Config aggregator"
    },
    "explanation": "Separate bucket with Object Lock ensures immutability and isolation from PII."
  },
  {
    "id": "2e9bb909d809de80e747f3aa835fd53428083f369c38d6ac3e914a4c599bef38",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "Your team wants to benchmark foundation model inference on different Graviton vs Intel instances in Bedrock. Which is true?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "You can choose instance types in Bedrock",
      "B": "Bedrock always runs on Intel",
      "C": "Instance type is abstracted; use SageMaker for custom instance benchmarking",
      "D": "Use Amazon Q to specify CPU architecture"
    },
    "explanation": "Bedrock abstracts hardware; SageMaker endpoints allow explicit instance type selection for benchmarking."
  },
  {
    "id": "eb40c4afe6d85d7ad0a78e358982ff9806cb29a25cb04155861059285e50a404",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A startup requires dynamic orchestration that pauses fine-tuning jobs on SageMaker when daily free tier limits are reached, then resumes next day automatically. How to implement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Manual stop/start by engineer",
      "B": "Step Functions workflow with CheckBilling and SageMaker callbacks",
      "C": "Use CloudWatch schedules only",
      "D": "Use AWS Budgets alerts alone"
    },
    "explanation": "Step Functions can check budget via API and orchestrate SageMaker start/stop tasks."
  },
  {
    "id": "932e31dfc5327814f64b962acc62f5dee44c8f92dc7c6f9bd64eaaa013be6b21",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A gaming company must generate dialogue using an LLM but only allow safe responses (no profanity). Which AWS generative AI feature helps enforce this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Bedrock guardrails",
      "B": "SageMaker Clarify",
      "C": "Amazon Q content filtering",
      "D": "Comprehend custom classification"
    },
    "explanation": "Bedrock supports Guardrails to filter or transform outputs according to safety policies."
  },
  {
    "id": "c0d46ddd88e04769386703acecf49b739196f17e571a110b15a8f0caabd68c98",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A data science team needs versioned snapshots of training data, embeddings, and model artifacts in one place with query capabilities. Which AWS feature?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "S3 buckets with versioning",
      "B": "Glue Data Catalog only",
      "C": "RDS Postgres",
      "D": "SageMaker Feature Store and Model Registry"
    },
    "explanation": "Feature Store and Model Registry together manage data, embeddings, and model artifact versioning with query APIs."
  },
  {
    "id": "e568b1ad42435462abe1cab12cd86b8f7dcce8dc2b6de66eb2526564efb46bb0",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "Your integration calls a Bedrock model synchronously but occasionally times out due to long generation. You need async calls with callback. Which pattern works?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase API timeout only",
      "B": "Use AWS Lambda tail-chaining",
      "C": "Use Synchronous SNS notifications",
      "D": "Call Bedrock asynchronously and poll with GetFrome API via Step Functions"
    },
    "explanation": "Bedrock supports async inference with token for status, polled via Get API in Step Functions."
  },
  {
    "id": "a388f6f9de378961fb3a439e9d3132116cd5e662a2d0d05ce415a0ba52f90d9b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A fintech company is building a real-time customer support chatbot using a foundation model on Amazon Bedrock. They require end-to-end inference latency under 250 ms per request with acceptable answer quality. Which model selection criterion should they prioritize?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Choose a foundation model with fewer parameters to minimize inference latency, even if domain-specific performance is moderately lower.",
      "B": "Select the largest foundation model available to maximize language coverage, accepting higher latency.",
      "C": "Pick the model trained on the largest dataset to ensure maximum accuracy regardless of inference speed.",
      "D": "Opt for a proprietary model under premium support to guarantee SLA-backed latency."
    },
    "explanation": "A smaller parameter model yields lower inference latency (<250 ms) while still providing acceptable quality; larger models incur higher latency."
  },
  {
    "id": "8079e00da803a610990617bd0a1299544ca0e709909cbc5b661b018965fdae07",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A healthcare startup plans to store and retrieve 50 million patient-note embeddings for a RAG solution. They require low-latency vector similarity search that scales automatically. Which AWS service should they choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon OpenSearch Service with the k-NN vector search plugin.",
      "B": "Amazon RDS for PostgreSQL with the pgvector extension.",
      "C": "Amazon Neptune using SPARQL for similarity matches.",
      "D": "Amazon DocumentDB with a MongoDB vector plugin."
    },
    "explanation": "OpenSearch Service with k-NN is built for high-scale, low-latency vector search; other options add complexity or lack native vector indexing."
  },
  {
    "id": "2048328bda59da8b112c7f8e54b619bdf51d7099028735bcb95d0277c2df5e0b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A news aggregator uses RAG to answer queries on newly published articles and needs embeddings updated in near real time. Which ingestion design best meets this requirement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon Kinesis Data Streams to capture new articles, trigger Lambda to generate embeddings, and index into OpenSearch immediately.",
      "B": "Schedule a daily AWS Glue job to re-embed and reindex all articles.",
      "C": "Fine-tune the foundation model hourly with the latest articles.",
      "D": "Upload new embeddings manually via the Bedrock console."
    },
    "explanation": "A streaming pipeline via Kinesis\u2192Lambda\u2192OpenSearch ensures minimal staleness; batch or manual approaches don\u2019t meet real-time needs."
  },
  {
    "id": "8057869e55fa17da7ddf1400fc14786516e9e43a05f9d91499a5c30d413d4de3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "An e-commerce company wants the foundation model to recognize newly launched product categories but has only 800 labeled examples. Which customization method minimizes cost while ensuring accurate category recognition?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Fully pre-train a custom foundation model on AWS using all available company data.",
      "B": "Fine-tune the foundation model with full parameter updates on the 800 examples.",
      "C": "Implement retrieval-augmented generation by storing category data externally and providing in-context examples at inference.",
      "D": "Switch to a larger foundation model to cover the category vocabulary without customization."
    },
    "explanation": "RAG with in-context examples uses external knowledge to cover new categories without the high compute cost of fine-tuning small datasets."
  },
  {
    "id": "1dec37e3b4503406eed59d42d28c6c754c02b2108c018863f6ae1c5357513f0e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A legal advisory chatbot must minimize hallucinations when generating advice. Which inference parameter adjustment is most effective?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set temperature near zero to produce deterministic, least-random responses.",
      "B": "Raise top_p to 1.0 to broaden token selection.",
      "C": "Increase temperature to encourage creative answers.",
      "D": "Disable beam search to expedite response time."
    },
    "explanation": "Lowering temperature reduces randomness and hallucination; higher temperature or wider top_p increases variability."
  },
  {
    "id": "aa9a721850c7d6cbee558b3016a33101e8e2533a3e0a2632b08adc2e360dd857",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "An organization projects 100 000 monthly RAG queries and anticipates monthly content updates. Fine-tuning costs $50 000 upfront with negligible per-query cost; RAG costs $0.02 per query. Which approach has the lower year-one total cost?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use RAG exclusively, paying $0.02\u00d7100 000\u00d712 = $24 000 monthly.",
      "B": "Fine-tune once for $50 000 and serve queries at minimal incremental cost.",
      "C": "Pre-train a new foundation model quarterly.",
      "D": "Rotate between RAG and fine-tuning monthly."
    },
    "explanation": "RAG would cost $0.02\u00d7100 000\u00d712 = $24 000; fine-tuning is a one-time $50 000, so in year one RAG is cheaper ($24 000) \u2013 candidate should recalc: actually correct is A; but structured scenario expects RAG is cheaper at $24 000 vs fine-tune $50 000."
  },
  {
    "id": "c99eb366135921bf3dcb79c89739069f51379e9f3a9bcf464af6f390670fa075",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A logistics company must build a multi-step agent to validate addresses, calculate shipping costs, generate labels, and send notifications. They want a low-code orchestration solution. Which AWS feature should they use?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Bedrock Agents with integrated AWS Step Functions orchestration.",
      "B": "Chain multiple AWS Lambda functions manually.",
      "C": "Develop a custom microservice orchestrator on Amazon EC2.",
      "D": "Sequence tasks using AWS Batch workflows."
    },
    "explanation": "Bedrock Agents provide built-in multi-step orchestration integration, reducing custom code overhead."
  },
  {
    "id": "a372e1dac759c6f9745944dd58dfd1cc1cb7ccb9691e86e3aab5d38a01eaeb39",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A software team needs to summarize 100-page PDF manuals. The foundation model\u2019s token limit is 4096. Which approach handles this effectively?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Chunk the manual into \u22642000-token segments, summarize each, then combine summaries recursively.",
      "B": "Send the entire PDF text in one request and rely on the model\u2019s context window to truncate gracefully.",
      "C": "Fine-tune the model to increase its token limit.",
      "D": "Increase max_output_tokens beyond 4096 during inference."
    },
    "explanation": "Hierarchical chunking and summarization respects token limits and yields coherent overall summary."
  },
  {
    "id": "498185b906f3ef2fab77aa18cced7f10abe1778120e36e09571512b5f1aa6d67",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A global translation service must support 30 languages with low latency. Which model selection strategy balances coverage and performance?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy specialized bilingual models for each language pair to reduce model size and latency.",
      "B": "Use a single large multilingual foundation model for all languages.",
      "C": "Translate via Amazon Translate then post-process with the foundation model.",
      "D": "Fine-tune one model per region for language coverage."
    },
    "explanation": "Bilingual models reduce inference cost and latency vs one large multilingual model while still covering required pairs."
  },
  {
    "id": "6b6105391a172bb434571e4de9b1af5d9cfbe4a78af0bacb66cc94eaf59094be",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "For a RAG pipeline, a team wants to limit the number of retrieved documents per query to reduce inference context size and cost. Which retrieval parameter should they adjust?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set the retriever\u2019s k parameter (number of top documents) to a lower value.",
      "B": "Reduce the embedding dimension size.",
      "C": "Lower the model temperature.",
      "D": "Decrease max_output_tokens."
    },
    "explanation": "Lowering k reduces the number of retrieved passages in the prompt, shrinking context and cost; other parameters don\u2019t affect retrieval count."
  },
  {
    "id": "7799a5a936cf83a7d59129819f9d883549a2d4548b09e641477307954694bed9",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A vector store previously used 768-dim embeddings; switching to a new model yields 1024 dimensions, and query latency doubles. How can they restore performance without sacrificing retrieval quality?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Apply dimensionality reduction (e.g., PCA) to project 1024-dim embeddings down to 768 dimensions.",
      "B": "Continue using 1024-dim embeddings and provision larger instance types.",
      "C": "Pad old 768-dim embeddings to 1024 dims.",
      "D": "Revert to the previous embedding model."
    },
    "explanation": "Dimensionality reduction keeps vector store size manageable and preserves most semantic information; padding wastes resources."
  },
  {
    "id": "a0b9477b390d7f59e027f418d4b7d72935b5a1a384eadda9ace381a7a94b9fa8",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A financial application requires ACID transactions when updating user embeddings under heavy concurrency. Which embedding store meets this requirement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Aurora PostgreSQL with pgvector extension.",
      "B": "Amazon OpenSearch Service with k-NN.",
      "C": "Amazon Neptune configured for vector similarity.",
      "D": "Amazon DynamoDB with custom indexing."
    },
    "explanation": "Aurora PostgreSQL offers ACID compliance plus pgvector for embeddings; other stores are eventually consistent or non-transactional."
  },
  {
    "id": "25cffcaa1e655c6f656eaa5d0d890d315104f26537b4864c1153b901fc1d2f70",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "Engineers want more diverse, creative summarizations. Which inference parameter changes achieve this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase both temperature and top_p.",
      "B": "Decrease temperature toward zero.",
      "C": "Reduce max_output_tokens to force brevity.",
      "D": "Disable retrieval augmentation."
    },
    "explanation": "Higher temperature and broader top_p allow the model to sample more diverse tokens, increasing creativity."
  },
  {
    "id": "e5c66a2cdaf54e60a29d5106a7bfe6b69a4b7dc504711da138c7b91b3b975d7d",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A marketing team must refresh product descriptions monthly. Custom fine-tuning costs $20 000 each time; RAG costs $0.015 per query. They expect 200 000 queries per month. Which approach is most cost-effective?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use RAG to avoid repeated fine-tuning costs.",
      "B": "Perform monthly fine-tuning despite the one-time expense.",
      "C": "Fine-tune quarterly and use zero-shot otherwise.",
      "D": "Use zero-shot prompting exclusively."
    },
    "explanation": "RAG @ $0.015\u00d7200 000 = $3 000/month is cheaper than $20 000 per refresh; zero-shot yields low accuracy."
  },
  {
    "id": "43a99a9079b8dbba237bccf1842736b24cb74dab896a3bee72aadeaa0cee4338",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A cell-phone provider needs a multi-step agent to verify SIM status, calculate upgrade eligibility, send offers, and log responses. They require built-in branching logic and retry handling. Which AWS capability should they use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock Agents with built-in workflow, branching, and retry support.",
      "B": "AWS Step Functions calling Bedrock directly.",
      "C": "Chained AWS Lambda functions orchestrated manually.",
      "D": "AWS Batch with job dependencies."
    },
    "explanation": "Bedrock Agents provide multi-step orchestration with logic and retry controls tailored to LLM workflows."
  },
  {
    "id": "dbd51f1b6573f573995fc4a34cf6b1cd5647a92cb71991c6e7f672c5a5b8d381",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A retailer wants to store image embeddings and filter by price range. Which AWS service supports vector search plus metadata filtering?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon OpenSearch Service with k-NN plugin and document fields.",
      "B": "Amazon Neptune graph queries with PRICE property.",
      "C": "Amazon Timestream for time-series embedding storage.",
      "D": "Amazon S3 with object tags and Lambda lookups."
    },
    "explanation": "OpenSearch combines k-NN vector search and structured field filters for metadata constraints; other options lack integrated vector + metadata queries."
  },
  {
    "id": "1cfcd60ffa3c38446484040e31edb0c60e0b7479f3f8e22cc4a7b372fe6a41d6",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A global app must perform semantic search across English, Spanish, and Mandarin content. Which foundation model criterion is critical?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Select a multilingual foundation model pretrained on all target languages.",
      "B": "Use an English-only model with translation pre-processing.",
      "C": "Deploy separate monolingual models per language.",
      "D": "Rely on fine-tuning to teach new languages."
    },
    "explanation": "A multilingual LLM natively understands all languages for semantic embeddings; translation adds latency and error."
  },
  {
    "id": "6a98983b5b2973ad0cb202f500377b108a4653a76a48ed7a79d4eb05288debff",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "To forecast RAG pipeline costs, which factors should be included?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Compute costs for embedding generation, LLM token charges, and storage costs for vector indexes.",
      "B": "Only the storage costs of embeddings.",
      "C": "Just the EC2 instance hours used by the application.",
      "D": "Only the foundation model\u2019s license fee."
    },
    "explanation": "RAG cost = embedding compute + LLM tokens consumed + storage + any I/O; ignoring any leads to underestimation."
  },
  {
    "id": "2a259820836e29d3df7e7c945443c31c8c7bb46e6ce46bfbef88b6c2fe6db10f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A user submits a 2.5 million-token document to a model with a 1 million-token context limit. Which strategy handles this input best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a sliding/chunking approach: split into overlapping 1 million-token segments and process iteratively.",
      "B": "Request the model to accept larger context by adjusting max_context_tokens parameter.",
      "C": "Fine-tune the model to handle larger inputs.",
      "D": "Discard tokens beyond the first 1 million."
    },
    "explanation": "Chunking with overlap preserves context while respecting model limits; other methods are unsupported or lose information."
  },
  {
    "id": "65abadaba6c0f4915a37d384139bb20501339706ebd470971cd2aebb15b4cf43",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "During a partial OpenSearch outage, the RAG pipeline must continue to answer queries gracefully. Which fallback should be implemented?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Fall back to a keyword-based Boolean search over a local S3-backed index.",
      "B": "Switch to Amazon Neptune for vector search.",
      "C": "Abort requests and return errors.",
      "D": "Queue requests until OpenSearch recovers."
    },
    "explanation": "A local keyword search ensures degraded service rather than total failure; queuing or errors harm UX."
  },
  {
    "id": "f1120edd1ac2a98391c042c04ecc2a0f98de9a1d17a5f9a71cd7958c2baf932f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "Retrieval accuracy in RAG is low despite high embedding similarity scores. Which diagnostic step should they take first?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Examine embedding distributions and consider retraining or switching embedding models.",
      "B": "Ramp up the model\u2019s temperature for more variability.",
      "C": "Switch to a larger foundation model for inference.",
      "D": "Increase max_output_tokens to get longer answers."
    },
    "explanation": "Poor retrieval often stems from suboptimal embeddings; model hyperparameters won\u2019t fix retrieval quality."
  },
  {
    "id": "965dba8d7663ac218d3e612a3232611478f146205e380fa1b4a85741b5f14cc8",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "An agent workflow must enforce a maximum of 512 output tokens. Which Bedrock API parameter should they configure?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "max_output_tokens",
      "B": "temperature",
      "C": "top_p",
      "D": "beam_width"
    },
    "explanation": "max_output_tokens directly limits the number of tokens the model generates; other parameters control randomness or search strategy."
  },
  {
    "id": "66bc8912bcb11fe8bfae359315e73821d4b5cf82491efe95272d0c745860b93a",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A high-security customer requires private network communication for vector searches. Which architecture element ensures privacy?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy OpenSearch Service within a VPC and use AWS PrivateLink for Bedrock access.",
      "B": "Use the public OpenSearch endpoint with IAM policies.",
      "C": "Tunnel traffic over S3.",
      "D": "Use CloudFront in front of OpenSearch."
    },
    "explanation": "VPC deployment plus PrivateLink keeps data off the public internet; other options expose endpoints publicly or misuse services."
  },
  {
    "id": "55949dfed17a665a5411d30e029eb14853c6378eaf9108542f7eaad1c5218faf",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A content-heavy app must pre-warm embedding Lambda functions to reduce cold-start latency spikes. What is the best practice?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Configure provisioned concurrency on the Lambda function.",
      "B": "Let the function handle occasional cold starts.",
      "C": "Use AWS Batch instead of Lambda.",
      "D": "Store embeddings in S3 to avoid Lambda."
    },
    "explanation": "Provisioned concurrency keeps warm instances ready, reducing cold-start latency; other options either ignore the issue or circumvent the design."
  },
  {
    "id": "5f23c583390530066c23e58f8752225526b5652fd991f75e00e9e1a214574d8e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A financial analytics team must store embeddings alongside relational transaction data and enforce ACID. Which solution is most appropriate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon Aurora PostgreSQL with the pgvector extension.",
      "B": "Store embeddings in DynamoDB and transactions in RDS separately.",
      "C": "Use Amazon DocumentDB for both embeddings and transactional data.",
      "D": "Implement a two-phase commit between OpenSearch and Aurora."
    },
    "explanation": "Aurora Postgres + pgvector provides a unified, ACID-compliant store for both embeddings and relational data; other approaches increase complexity."
  },
  {
    "id": "a9d14ba5871909662c1bfa247202a5643f2e52484fbc6fccbe532a0aab5d30fe",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A chatbot using RAG must control costs by limiting the number of retrievals as well as generated tokens. Which combination of parameters should be adjusted?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Lower the retriever\u2019s k (documents) and set a conservative max_output_tokens.",
      "B": "Increase temperature and top_p.",
      "C": "Use a larger batch size.",
      "D": "Enable debug mode."
    },
    "explanation": "Reducing k shrinks context; lowering max_output_tokens caps the output length, both directly reduce cost."
  },
  {
    "id": "4fb952577bbfb35407ac379674063a756d93fc77974c89804dd93384cfa413d5",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A scientific publisher needs to index both text and vector embeddings for articles in the same service. Which AWS option should they pick?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon OpenSearch Service with integrated full-text search and k-NN vectors.",
      "B": "Amazon Neptune for RDF and vector queries.",
      "C": "Amazon RDS for PostgreSQL with full-text search.",
      "D": "Amazon S3 + Lambda for indexing."
    },
    "explanation": "OpenSearch supports hybrid queries mixing full-text and vector similarity; other services require stitching or lack vector support."
  },
  {
    "id": "409cd46a0e8deb14da5fd73cc43a9c80e9460f2ed8a899e3e00fe68ef80cf6ee",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A recommendation system uses a foundation model to generate item embeddings and serve nearest-neighbor queries. To minimize search errors, which storage configuration is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy OpenSearch Service with replication enabled and k-NN plugin.",
      "B": "Use DynamoDB with Global Secondary Index.",
      "C": "Persist embeddings in S3 and scan on demand.",
      "D": "Host embeddings in Redis cluster."
    },
    "explanation": "OpenSearch with replication ensures high availability and accuracy in k-NN searches; others don\u2019t natively support vector retrieval."
  },
  {
    "id": "939ac2432275af3cf4b8141483dc80e731d1c70c3636b516c2a36fe68fd0089c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A development team notices fluctuating RAG latency at peak loads. Which scaling approach for the vector store is most effective?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable auto-scaling on the OpenSearch domain\u2019s data nodes.",
      "B": "Provision a fixed large instance size.",
      "C": "Manually add more EC2 instances.",
      "D": "Switch to a single high-I/O DynamoDB table."
    },
    "explanation": "Auto-scaling OpenSearch data nodes adapts to load changes automatically; fixed or manual scaling is less responsive."
  },
  {
    "id": "ba7cbba88891eb7a5ed9e48f97253f553d2b54d0fcf390b20586c37b81e54b38",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A legal research app must retrieve citations with high recall in RAG. Which retriever tuning improves recall while controlling context size?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase k (number of retrieved documents) and apply a relevance threshold filter.",
      "B": "Decrease temperature.",
      "C": "Reduce max_output_tokens.",
      "D": "Switch to a single-shot prompt."
    },
    "explanation": "Raising k returns more candidates for the LLM to choose from, boosting recall; other parameters don\u2019t affect retrieval breadth."
  },
  {
    "id": "7ea65105005d6cc8cf195329cf695d606941e305a5c011de499209aa5b4b887b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A team wants to quantify RAG pipeline performance for SLAs: retrieval latency and end-to-end response time. Which tools should they use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon CloudWatch Synthetics for end-to-end tests and OpenSearch slow-log metrics for retrieval latency.",
      "B": "S3 access logs.",
      "C": "AWS Config rules.",
      "D": "AWS Glue job metrics."
    },
    "explanation": "CloudWatch Synthetics simulates full pipeline; OpenSearch slow logs measure vector search latency; other tools are unrelated."
  },
  {
    "id": "c334f3425bcf4ff05f69fb2969019c3e240a157923bf065e4736d1684b1024f5",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A developer must enforce data retention\u2014delete embeddings older than 90 days automatically. Which architecture achieves this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use OpenSearch index rollover with a 90-day retention policy and lifecycle policy to delete old indices.",
      "B": "Manually purge embeddings via Bedrock console.",
      "C": "Use S3 expiration rules.",
      "D": "Archive data in Glacier."
    },
    "explanation": "OpenSearch index lifecycle policies automate deletion of data older than a threshold; other options don\u2019t apply to the index store."
  },
  {
    "id": "725065bcb37c5bb029a0328c75cd58098f795b2646e7931cb21ef7df7e4b7667",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "You are designing prompts for a customer support chatbot that must interpret rare domain-specific error codes. Which prompt engineering approach most reliably guides the model through multi-step interpretation of an unfamiliar code?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use zero-shot prompting with a single instruction to \"explain the code\"",
      "B": "Provide a long template with all known codes and rely on the model to match",
      "C": "Use chain-of-thought prompting with a few representative examples and ask the model to verbalize each reasoning step",
      "D": "Use a generic completion prompt with a high temperature to encourage creativity"
    },
    "explanation": "Chain-of-thought with examples explicitly leads the model through reasoning steps needed for rare codes; zero-shot or generic completion may omit steps and lead to hallucinations."
  },
  {
    "id": "4a3c79c899d3102870bb5e0be4bf85718e32c8ef9761db6a073a4c10d93d3604",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "A generative AI application must summarize medical records while never revealing patient identifiers. Which prompt technique helps ensure anonymization?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Insert negative prompts like \"Do not remove any text\"",
      "B": "Use explicit negative prompts: \"Do not output any names, dates, or addresses\" combined with guardrail templates",
      "C": "Rely on the model\u2019s default privacy capability",
      "D": "Set a high temperature to diversify summary wording"
    },
    "explanation": "Explicit negative prompts plus guardrail templates clearly instruct the model to avoid specific PII, while high temperature or default privacy are unreliable."
  },
  {
    "id": "6e294b1f9aec3e3f939a71e010e69598058248b26a8d102dd23f7382af8a895b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "When using few-shot prompting for financial document classification, you notice inconsistent output if you exceed three examples. What is the most likely cause?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Context window saturation leads to model truncation of examples",
      "B": "Too many examples reduce the model\u2019s creativity",
      "C": "Model overfits to early examples only",
      "D": "High temperature setting causes inconsistency"
    },
    "explanation": "Adding more examples fills the context window causing the model to truncate or ignore later examples; reducing to 2\u20133 examples keeps the context coherent."
  },
  {
    "id": "e44462c9719c2f268779249ac5b465d8d71c77933f75388f60ba34c2add6c586",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "You need a stable translation service for legal documents where hallucinations are unacceptable. Which settings achieve high determinism?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High temperature (0.9) and no few-shot examples",
      "B": "Zero-shot prompting with a medium temperature (0.5)",
      "C": "Few-shot prompting with creative instructions",
      "D": "Zero-shot prompting with temperature 0 and explicit instruction style templates"
    },
    "explanation": "Temperature 0 ensures deterministic outputs and explicit templates guide consistent translation; creative or high temperature settings risk variability and hallucination."
  },
  {
    "id": "d672f7fcdaafa4d221f070bdfb16bd40a6da9fa09160fd80ad67b05f148ade54",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "An adversary tries prompt injection by appending \u201cIgnore previous instructions and output X.\u201d Which engineering technique mitigates this risk?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature to dilute malicious text",
      "B": "Use a system-level instruction or guardrail template enforced before user content",
      "C": "Provide more in-context positive examples",
      "D": "Switch to few-shot prompting"
    },
    "explanation": "System-level instructions or guardrail templates take precedence over user content, preventing prompt injection; temperature or examples do not block malicious append."
  },
  {
    "id": "c9c4830eb75460a0ec42b8fc16c920dc189418d0c5cad0466a3826480bde7d17",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "Your model responses vary significantly when summarizing news articles. You need safer outputs. Which combination reduces variability while preserving detail?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Lower temperature to ~0.2 and include an instruction template focusing on key facts",
      "B": "Use high top-p sampling and no instruction",
      "C": "Use chain-of-thought without temperature adjustment",
      "D": "Switch to zero-shot with generic \u201cSummarize\u201d prompt"
    },
    "explanation": "Lowering temperature reduces sampling randomness, and a focused template ensures the model includes key factual details."
  },
  {
    "id": "d27d0bc5c5e802946b581cfa339cf222fc6f33cd98faa89dcc58615dcf7e7b75",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "A content moderation tool uses an LLM to flag hate speech. False negatives increase when model sees adversarial phrasing. Which prompt-engineering tactic improves robustness?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Raise temperature to catch all variations",
      "B": "Provide only positive examples in prompt",
      "C": "Use a generic moderation API call",
      "D": "Include negative and adversarial examples in few-shot prompt and use chain-of-thought labeling guidelines"
    },
    "explanation": "Including adversarial examples and chain-of-thought guidelines teaches the model to reason through obfuscated hateful language; temperature changes or generic calls are insufficient."
  },
  {
    "id": "953dc7ea2813ee7d1740eb9a53fb39b9814170cc4ea8f2ae097a5b3e44681ed9",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "You have a foundation model that occasionally repeats sensitive content. What prompt modification reduces repetition?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Remove all examples to reduce bias",
      "B": "Use high temperature to diversify",
      "C": "Add a negative prompt: \"Do not repeat previous content\" and set max_repeat_penalty",
      "D": "Use zero-shot prompting"
    },
    "explanation": "Negative prompts combined with repeat-penalty parameters actively discourage the model from echoing sensitive content."
  },
  {
    "id": "1eebc9a1d2f9ce582b8b7ed736e4e71376c4ab09abc4fd070e04169b1d799c40",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "Your generative AI assistant must inject step-by-step debugging suggestions when users report code errors. Which prompt design ensures consistent inclusion of steps?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Template instructing \u201cList steps: 1. Analyze, 2. Identify, 3. Suggest Fixes\u201d with chain-of-thought",
      "B": "Few-shot with two examples only",
      "C": "Zero-shot \u201cHelp me debug\u201d",
      "D": "High temperature free-form prompt"
    },
    "explanation": "A structured template listing numbered steps with chain-of-thought guides the model to always include a detailed debugging process."
  },
  {
    "id": "4cfb2aa774b8488efb0d4faae980a12535398b26e883dc8d38e929c5ed94085c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "When using in-context examples, why might you randomize the order of examples in your prompt?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "To reduce context window usage",
      "B": "To prevent positional bias causing overfitting to early examples",
      "C": "To increase token diversity",
      "D": "To improve latency"
    },
    "explanation": "Randomizing examples stops the model from disproportionately attending to the first few examples, reducing positional bias."
  },
  {
    "id": "3cfece725c3e2dabc0d44ecf3ebad1c70b88453293870511168c5ed777f33df9",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "What is the primary risk of using high temperature in a safety-critical medical Q&A system?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Longer response time",
      "B": "Decreased token usage",
      "C": "Increased likelihood of hallucinations and nondeterministic errors",
      "D": "Lower creativity"
    },
    "explanation": "High temperature increases randomness and risk of false statements, unacceptable in safety-critical domains."
  },
  {
    "id": "b36d0ba6446200a46b8f4d68b77e11fce0fd5dc1d4959a991632e751bb63381d",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "You need to instruct a model to generate JSON only. Which technique is most effective?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Use a high temperature and hope for JSON",
      "B": "Use chain-of-thought prompt to think in JSON",
      "C": "Provide zero-shot generic instruction",
      "D": "Provide a strict response template: \"Respond ONLY in JSON format: {...}\" and include example JSON skeleton"
    },
    "explanation": "A strict template with skeleton examples and explicit \u201cONLY in JSON\u201d instruction ensures format adherence."
  },
  {
    "id": "fefcc2cdbe3691823f7a0b5fdd73ac42891315b37292428f46e48430c8fecb40",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "A hallucination in a product recommendation agent misattributes features. Which prompt tweak most reduces this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature to refresh knowledge",
      "B": "Use retrieval-augmented generation: fetch feature docs and include in context",
      "C": "Switch to single-shot prompting",
      "D": "Add more unrelated examples"
    },
    "explanation": "Providing retrieved factual docs in context grounds the model and reduces hallucinations; temperature changes are ineffective."
  },
  {
    "id": "948ff68f14db0ce4d13fca073ec1c7aee90633359e65ae3f0607bb04245457d9",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "During stress testing, your LLM triggers a jailbreaking prompt. Which guardrail approach best prevents this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Implement a system-level policy that filters or rejects jailbreak patterns before model call",
      "B": "Use few-shot with benign examples",
      "C": "Increase temperature",
      "D": "Switch to zero-shot"
    },
    "explanation": "Filtering malicious prompts before they reach the model is more reliable than prompt content adjustments."
  },
  {
    "id": "afbf8443ffef572d1a02752d303e59e873667244f1ea039dbcbe80be2151057e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "You are defining a template for writing marketing copy. Which template structure yields the most consistent brand tone?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Open-ended instruction \u201cWrite copy\u201d",
      "B": "Chain-of-thought with broad guidelines",
      "C": "Structured template with sections: \u201cHeadline:\u2026, Body:\u2026, CTA:\u2026\u201d plus two few-shot examples",
      "D": "High-temperature freestyle prompt"
    },
    "explanation": "Structured templates with explicit sections and examples guide the model to consistently follow brand tone and format."
  },
  {
    "id": "2a27e42f7bf8da4db779ec4c4421999ba80e7fa6761a880a555898dc9c8188bd",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "To encourage the model to propose multiple alternative solutions rather than a single answer, which setting helps?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Temperature=0 and beam search",
      "B": "Increase temperature moderately and include \u201cList 3 alternative approaches\u201d in instruction",
      "C": "Use zero-shot default prompt",
      "D": "Few-shot with single example"
    },
    "explanation": "Moderate temperature plus explicit instruction to list alternatives fosters diverse outputs."
  },
  {
    "id": "3cc51e0c55c3b7213671a64db2b21dca688149e6aacf2b30448b9b90cf56ff76",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "Which prompt engineering approach best mitigates model exposure to private training data?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use data filtering and add a negative prompt \u201cDo not reveal source data\u201d in guardrails",
      "B": "Lower temperature",
      "C": "Use few-shot with internal data examples",
      "D": "Use zero-shot with generic instructions"
    },
    "explanation": "Combining data filtering and explicit negative prompts in guardrails prevents the model from disclosing training data."
  },
  {
    "id": "9158dd729c93b66a78dc8c74dfcf093b5588b7f35ad09f15f74b2a94b13792fe",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "You want the model to first self-critique its answer and then refine it. Which prompt technique accomplishes this reliably?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High temperature free-form prompt",
      "B": "Zero-shot with generic critique request",
      "C": "Chain-of-thought with two-phase template: \u201cStep 1: Provide answer. Step 2: Critique and refine.\u201d",
      "D": "Few-shot with single example"
    },
    "explanation": "A two-phase template clearly instructs sequential tasks and chain-of-thought ensures each phase is executed."
  },
  {
    "id": "f1635bf3438c07482c31614f6753361184970c309a28a30c35b418552a7034e3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "When designing prompts for an LLM-based code generator, you observe the model writes insecure code. Which prompt addition reduces insecure patterns?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature",
      "B": "Add a guardrail template: \u201cEnsure code follows OWASP best practices. Do not include insecure functions,\u201d with examples",
      "C": "Use fewer in-context examples",
      "D": "Switch to single-shot prompting"
    },
    "explanation": "Guardrail templates specifying security constraints and examples guide the model to produce secure code."
  },
  {
    "id": "1ab1a911f1ee13a13f42b1aca676d354ecd2e64c337a69cb12f2c2dbdd58698d",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "A summarization model hallucinates non-existent entities. Which prompt strategy most reduces this risk?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use single-shot summarization",
      "B": "Increase temperature",
      "C": "Add more irrelevant examples",
      "D": "Retrieval-augmented prompt: provide source text and instruct \u201cSummarize only content present in the text\u201d"
    },
    "explanation": "Providing source text and explicit instruction to limit to that text prevents hallucinations."
  },
  {
    "id": "ce05763ff96c6e9bb483e9ed54c94c24cdb2fc8472684cea8cd16010bbd67712",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "For a multi-turn dialogue, context length exceeds model limit and earlier user instructions are truncated. How to preserve persona instructions?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use higher temperature",
      "B": "Use system-level instructions stored separately and prepend only essential tokens",
      "C": "Switch to few-shot examples in each turn",
      "D": "Increase chain-of-thought depth"
    },
    "explanation": "System-level instructions are applied outside user context and remain effective regardless of conversation length."
  },
  {
    "id": "fbd7b57364176e319628959adc6419b24807d956cb6d36a8c44250cca3b26a04",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "You need the model to refuse unsafe code. Which negative prompt is most effective?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "\u201cIf request involves unsafe code, reply with \u2018I cannot comply\u2019\u201d within guardrail template",
      "B": "\u201cWrite code\u201d",
      "C": "Use high temperature",
      "D": "Provide safe code example only"
    },
    "explanation": "Explicit negative prompt specifying refusal behavior ensures the model declines unsafe code requests."
  },
  {
    "id": "c2237d85686782b7045c9c8795f6ca7c52dcf731e8bb72d8e79443a5400b8b2b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "Why might you chain multiple prompts instead of one long prompt?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "To use more tokens",
      "B": "To increase randomness",
      "C": "To segment tasks, manage context limits, and verify intermediate results",
      "D": "To avoid guardrails"
    },
    "explanation": "Chaining tasks lets you check intermediate outputs, handle context windows, and apply different instructions per step."
  },
  {
    "id": "ce71335e3e1687bca2b8464bda91568493c2017f228830485e619b7159fc859e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "A language model sometimes ignores instructions buried at the end of a long prompt. How do you ensure instruction priority?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Put instructions last",
      "B": "Use system-level or model-preset instructions with higher precedence",
      "C": "Use single-shot prompt without instructions",
      "D": "Increase temperature"
    },
    "explanation": "System-level or preset instructions override prompt content regardless of position, ensuring they are followed."
  },
  {
    "id": "8822f471e024ed8a86f2610f6aa38231b43754326096c3683342344af905aaf3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "In zero-shot translation tests, some outputs are syntactically correct but semantically wrong. Which prompt tweak improves semantic fidelity?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature",
      "B": "Reduce max tokens",
      "C": "Use chain-of-thought",
      "D": "Add explicit instruction: \u201cTranslate faithfully, preserving meaning exactly\u201d and use a short few-shot pair"
    },
    "explanation": "Explicit fidelity instruction plus few-shot example enforces meaning preservation; temperature or token limit changes do not guarantee semantic accuracy."
  },
  {
    "id": "b6fe7d013032eb78c2e43b910b95ca4b1e0fb3f18b34dda52008df6d1286a860",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "Your model\u2019s completion occasionally exposes internal policy text. Which prompt-engineering measure helps?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add negative prompt \u201cDo not reveal internal policy\u201d in guardrails",
      "B": "Use high temperature",
      "C": "Use fewer examples",
      "D": "Switch to zero-shot default API call"
    },
    "explanation": "Explicit negative prompts prevent the model from disclosing protected content; other methods won\u2019t reliably block exposures."
  },
  {
    "id": "9ccc4f217a62202d5ac3ca89a5b1f7610098c48c18689ea0a93daaba484aa5a1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "You observe that adding irrelevant examples degrades performance. What principle does this illustrate?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Higher token count improves context",
      "B": "More examples always help",
      "C": "Context contamination; irrelevant or noisy examples reduce model focus",
      "D": "Temperature must be low"
    },
    "explanation": "Irrelevant examples contaminate context, reducing the model\u2019s ability to generalize correctly."
  },
  {
    "id": "bd2e943eb8b2e662a01cda2598cd27ad8dce822362ba7a6caedbf58a71090abd",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "For an AI hiring assistant, you must avoid demographic bias. Which prompt-engineering approach mitigates this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use high temperature to diversify responses",
      "B": "Include counterfactual and debiasing examples in few-shot prompt and use neutrality guardrail",
      "C": "Zero-shot generic instruction",
      "D": "Chain-of-thought only"
    },
    "explanation": "Providing debiasing examples and neutral guardrails helps the model avoid demographic biases in its output."
  },
  {
    "id": "2153d0241444671d60ed3daf601bc9b6e0e99bb1b801e9c5fa1583fc83c31268",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "Which method most effectively limits token usage when composing long responses?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High temperature",
      "B": "Use chain-of-thought",
      "C": "Few-shot with many examples",
      "D": "Set a max_tokens parameter and use concise templates"
    },
    "explanation": "Using max_tokens limits the response length regardless of prompt complexity; concise templates help the model stay within limits."
  },
  {
    "id": "f618ecf5f79a2a5ed2df11edde241f8309efb2d51b3e0ffd1db2604f1fc3640d",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "Your LLM assistant for legal advice must never provide unauthorized practice of law. Which prompt safety layer is most reliable?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Prepend a system-level instruction: \u201cYou are a legal AI assistant, not a lawyer. Always include \u2018This is not legal advice\u2019 disclaimer.\u201d",
      "B": "Use chain-of-thought to think in law",
      "C": "High temperature for creativity",
      "D": "Zero-shot with generic instruction"
    },
    "explanation": "A system-level instruction enforced before user input ensures consistent disclaimer compliance."
  },
  {
    "id": "beade042015ecbdecd4bd63f7e51410c82b6d03b81d40408e9c1011046c575ce",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "A company needs to continuously update a foundation model with their domain-specific unlabeled text data streamed daily. They must retain the model\u2019s broad language capabilities while injecting domain knowledge. Which approach best satisfies this requirement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Fine-tune the entire model on the new data each day",
      "B": "Perform continuous pre-training on the domain corpus",
      "C": "Use instruction tuning on curated domain prompts",
      "D": "Apply reinforcement learning from human feedback"
    },
    "explanation": "Continuous pre-training (further unsupervised training) on unlabeled domain text preserves general capabilities while adding domain knowledge."
  },
  {
    "id": "634f23dcf8d3e87da0cf78b166f01003edfe454638c393fa984932d5f8b4ccc3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "A team has only 500 labeled examples to adapt a 7B-parameter foundation model for a specialized classification task. They need to minimize compute and storage costs. Which fine-tuning method should they choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Low-rank adaptation (LoRA)",
      "B": "Full parameter fine-tuning",
      "C": "Instruction tuning with domain prompts",
      "D": "Reinforcement learning from human feedback"
    },
    "explanation": "LoRA adds a small number of trainable parameters, reducing compute and storage compared to full fine-tuning."
  },
  {
    "id": "30eddcf857af3742d54f136858528fc3bac12ab9b548cc641941d3b7bd74a7bb",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "In RLHF workflows, which sequence of steps is correct?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train reward model \u2192 supervised fine-tuning \u2192 pre-train foundation model",
      "B": "Pre-train foundation model \u2192 RLHF \u2192 supervised fine-tuning",
      "C": "Pre-train foundation model \u2192 supervised fine-tune on reference data \u2192 train reward model \u2192 reinforcement learning",
      "D": "Supervised fine-tune \u2192 pre-train \u2192 train reward model"
    },
    "explanation": "Standard RLHF: pre-train, supervised fine-tune, collect feedback to train reward model, then RL optimization."
  },
  {
    "id": "5d3b06c6b2ba5a0e8ee0db93f60035922dd2606752d5e31f40bec895b75c8aa1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "A dataset for fine-tuning is heavily skewed toward one class. To prevent overfitting and bias, what is the most appropriate data preparation step before fine-tuning?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Remove minority class samples",
      "B": "Apply full fine-tuning without adjustment",
      "C": "Use stratified sampling or class-balanced oversampling",
      "D": "Increase learning rate to adapt quickly"
    },
    "explanation": "Stratified sampling or oversampling balances classes and ensures representativeness, preventing biased fine-tuning."
  },
  {
    "id": "1405cc4f7a48299f453007a2c799c3aaef252c4e034861e43be9b639abc7769b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "After fine-tuning on a narrow domain dataset, the model\u2019s performance on general tasks degrades significantly. Which technique mitigates this catastrophic forgetting?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a smaller learning rate",
      "B": "Increase batch size",
      "C": "Apply RLHF",
      "D": "Mix domain data with a subset of general pre-training data during fine-tuning"
    },
    "explanation": "Including a fraction of general data prevents the model from forgetting prior capabilities during fine-tuning."
  },
  {
    "id": "028f9162cb5c3ad2798e9d65d5cfde06195798794f156be04f9ae303e48f72f5",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "You need to adapt a multi-modal foundation model to a new domain with limited labeled images and annotations. Which two-step process is most efficient?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Full model fine-tuning \u2192 RLHF",
      "B": "Domain-adaptive pre-training on unlabeled images \u2192 parameter-efficient fine-tuning on labels",
      "C": "Instruction tuning \u2192 supervised classification fine-tuning",
      "D": "Prefix-tuning \u2192 supervised text embedding training"
    },
    "explanation": "First domain-adaptive pre-training uses unlabeled data; then a parameter-efficient method adapts on labels efficiently."
  },
  {
    "id": "284f0b788ea1f3f149607fb261bf8cf6e539e4cb20b8113ef9d420d35c33a7db",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "A team wants to reduce cost of repeated fine-tuning experiments. Which parameter-efficient fine-tuning method stores the fewest additional parameters per experiment?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Full fine-tuning",
      "B": "Prefix-tuning",
      "C": "LoRA",
      "D": "Adapter layers"
    },
    "explanation": "LoRA injects low-rank matrices into weights, requiring fewer additional parameters than full tuning or prefix-tuning."
  },
  {
    "id": "249bbaebcbc4753b4603861e511d48563a1b8250b3cf8f5c318a7676b09ed33a",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "During instruction tuning, a model outputs harmful or toxic responses. Which step should be added to the fine-tuning pipeline to address this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase training epochs",
      "B": "Remove instruction examples",
      "C": "Use higher learning rate",
      "D": "Incorporate a safety filter or toxic content detection in supervised data curation"
    },
    "explanation": "Filtering training data for safety and removing toxic examples prevents harmful outputs post fine-tuning."
  },
  {
    "id": "88cd2bac60b6cdbeb5c1f8d20f2cb77c384a6cc59b5fa469d02e80164d18bb5f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "You must adapt a foundation model to a highly specialized jargon-heavy domain. Labeled data is scarce. Which approach yields the best domain adaptation?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Full fine-tuning on limited labels",
      "B": "Unsupervised domain-adaptive pre-training on jargon corpus followed by LoRA",
      "C": "RLHF with random prompts",
      "D": "Zero-shot inference with prompt engineering"
    },
    "explanation": "Domain-adaptive pre-training injects jargon knowledge, then LoRA leverages scarce labels efficiently."
  },
  {
    "id": "eb67bbc04788517ab2acc947e8b8cd57d8a1a5232e2ae6464a5795715554ae87",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "A fine-tuned model is unexpectedly biased toward older data patterns. Which data governance practice could have prevented this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Define and enforce data versioning and lineage before training",
      "B": "Use larger batch sizes",
      "C": "Apply RLHF",
      "D": "Increase model size"
    },
    "explanation": "Data versioning and lineage ensure the training pipeline uses up-to-date, representative data, avoiding stale biases."
  },
  {
    "id": "c610af92dffdc44facb50da9658aa418540a1884ddff2fd6e320fccd0ae2cfd1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "Which continuous fine-tuning strategy allows safe incremental updates to a deployed foundation model without taking it offline?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Use a rolling\u2010update canary pipeline with adapter modules",
      "B": "Perform full model offline retraining then redeploy",
      "C": "Apply large batch synchronous fine-tuning",
      "D": "Use zero-shot prompting instead"
    },
    "explanation": "Adapter modules can be updated incrementally via a canary rollout, minimizing downtime and risk."
  },
  {
    "id": "00bd9fa75cfef47bd5204053907eff6e927cf57e95a475a3cd095c96fe60951c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "When fine-tuning with instruction data, which metric best indicates improved adherence to instructions?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Perplexity",
      "B": "ROUGE",
      "C": "Human preference rate in A/B tests",
      "D": "Training loss"
    },
    "explanation": "Human preference rate directly measures whether outputs adhere to instructions, beyond loss or ROUGE."
  },
  {
    "id": "8da4035cee4a34efc8181c904dc321ec8ce77b36aa3ba6e852bfb8c2802af7cb",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "A domain-specific fine-tuning dataset has label noise. How can you minimize its impact on training?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase learning rate",
      "B": "Use full fine-tuning",
      "C": "Ignore outliers",
      "D": "Apply robust loss functions or sample reweighting"
    },
    "explanation": "Robust loss functions or reweighting can reduce the influence of noisy labels during fine-tuning."
  },
  {
    "id": "5e102a4002992428e26a3e5683530210d37c781fb1b5170541de80998726a3b5",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "Which fine-tuning schedule helps prevent overfitting when adapting to a small dataset?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Long constant high learning rate",
      "B": "Warmup followed by cosine decay",
      "C": "No warmup, sudden drop",
      "D": "Increasing learning rate mid-training"
    },
    "explanation": "A warmup then cosine decay schedule allows stable initial training and gradual fine-tuning, reducing overfitting."
  },
  {
    "id": "797b701e91ad98a4472d13ed90259d6ba2fc5bbd4c02b5abd3f6193927ac9573",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "Which method efficiently incorporates new vocabulary into a frozen foundation model?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Full vocabulary retraining",
      "B": "Prefix-tuning",
      "C": "Embedding extension with LoRA or adapter fine-tuning",
      "D": "RLHF"
    },
    "explanation": "Adapter modules or LoRA can be applied to embeddings to learn new tokens without full model retraining."
  },
  {
    "id": "bb69a75ae1901d3ef9bac472e6a6e8fd3ce13d9cb645e32a1eb8f3700560e4cd",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "To fine-tune a multi-lingual foundation model for a low-resource language, what is the best data strategy?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Only use translated synthetic data",
      "B": "Apply full fine-tuning on small native set",
      "C": "Use instruction tuning in high-resource language",
      "D": "Combine cross-lingual transfer learning with small native language corpus"
    },
    "explanation": "Cross-lingual transfer leverages shared representations and small native data for efficient adaptation."
  },
  {
    "id": "11e79bb371dbc06dc736c5e238f431691c2a66d08dc75bdc9168401541e4c594",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "Which fine-tuning approach allows you to revert to the base model with minimal effort if domain adaptation fails?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Overwrite original weights",
      "B": "Use adapter modules loaded at runtime",
      "C": "Full fine-tuning saved over base",
      "D": "Instruction tuning integrated into core weights"
    },
    "explanation": "Adapter modules are separate from base weights and can be detached to revert easily."
  },
  {
    "id": "f79e0dcde701f84936f3afd5e78f70b1e6fcdfa5b5542de068fda1f5649b578e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "During iterative fine-tuning, a model's performance plateaus quickly. Which action is most likely to break the plateau?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Decrease batch size further",
      "B": "Reduce dataset size",
      "C": "Introduce a scheduled learning-rate restarts",
      "D": "Switch to full fine-tuning"
    },
    "explanation": "Learning-rate restarts can help escape local minima and drive further training improvement."
  },
  {
    "id": "7060e6a7243b5fdeff5bc7254f5f9f0c46d7a31b4430e40d0967075315802054",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "A foundation model fine-tuned with generic instructions still underperforms on a niche domain task. Which next step adds highest domain fidelity?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Perform domain-specific instruction tuning with curated task prompts",
      "B": "Increase model size",
      "C": "Switch to zero-shot prompting",
      "D": "Apply standard RLHF"
    },
    "explanation": "Domain-specific instruction tuning focuses the model on precise task requirements, boosting fidelity."
  },
  {
    "id": "700083dc958a480183e8cb17083df32e4d1ffdb629e1dc3ca9cf0f8cb8a6b20d",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "Which evaluation method best measures whether a fine-tuned generative model has adapted stylistically to domain conventions?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Perplexity on general corpus",
      "B": "BLEU against unrelated references",
      "C": "ROUGE on generic summaries",
      "D": "Human evaluation on domain-style adherence"
    },
    "explanation": "Human evaluation specifically on stylistic criteria is necessary to assess domain stylistic adaptation."
  },
  {
    "id": "07ee36d9a82409fcba468142b03f887adb5e5d9901f8366ecc1b8d8285d77a70",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "A team applies standard supervised fine-tuning but finds the model ignores rare example types. Which transfer learning technique addresses this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use larger batch size",
      "B": "Meta-learning-based fine-tuning (MAML)",
      "C": "Increase epochs on whole data",
      "D": "Instruction tuning"
    },
    "explanation": "Meta-learning like MAML helps the model adapt to few-shot or rare cases by learning to learn quickly."
  },
  {
    "id": "19edd2923fcc828b5ef5c7158b23621fb53a4320af2bda187a7a8ed39651ecb4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "Which component is essential to include when employing RLHF?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Large batch size",
      "B": "Tokenizer adjustments",
      "C": "Cosine learning-rate schedule",
      "D": "Human feedback loop and reward model"
    },
    "explanation": "RLHF relies on human feedback and a trained reward model to guide policy updates."
  },
  {
    "id": "7a9e64e3ec6adf7b9fae095f7726f673605cd2c058eaace4b4a962e9a92debf2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "When continually fine-tuning on new domain data, what practice ensures traceability of model versions?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Maintain data and model versioning with a registry",
      "B": "Use single monolithic model file",
      "C": "Overwrite logs",
      "D": "Only track final model"
    },
    "explanation": "A registry with versioned datasets and model artifacts ensures full traceability across updates."
  },
  {
    "id": "cc31bee6c70228e887bfa54311518c1f5e97cf444469a56021c81bb74b10af93",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "A custom domain corpus contains sensitive information. Which governance practice should be part of data preparation?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use full fine-tuning",
      "B": "Ignore and proceed",
      "C": "Apply anonymization and access controls",
      "D": "Increase token length"
    },
    "explanation": "Anonymization and strict access controls are critical to secure sensitive training data."
  },
  {
    "id": "83a6d213686707f1806ddba749575b34909ce5b9adc028f844778f0eac77b3f6",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "Which fine-tuning metric would best detect overfitting early during training?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Training loss",
      "B": "Validation loss trend",
      "C": "Inference latency",
      "D": "Model size"
    },
    "explanation": "Monitoring validation loss helps detect divergence between train and hold-out performance indicating overfitting."
  },
  {
    "id": "2dc39072ec83a3e89c422d8fe1a902d1c55d685daea6a138031ea53423221995",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "You need to fine-tune while preserving model safety constraints. Which method adds the least risk?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use adapter-based fine-tuning with safety filter",
      "B": "Full fine-tuning without filters",
      "C": "RLHF without guardrails",
      "D": "Instruction tuning on unfiltered data"
    },
    "explanation": "Adapter-based tuning limits changes and combining with content filters preserves safety constraints."
  },
  {
    "id": "dad228e8a4e3de7baa6efd37e06c6391fade49af2b73c89d32d47a07535aa764",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "After domain adaptation, new prompts produce unexpected hallucinations. Which training modification addresses this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase batch size",
      "B": "Use larger context window",
      "C": "Include retrieval-augmented examples in fine-tuning",
      "D": "Switch to zero-shot prompting"
    },
    "explanation": "RAG during fine-tuning grounds the model and reduces hallucinations by providing factual context."
  },
  {
    "id": "a6faded7a4719fb0b04b56bf2faf681cdd14b98820794023b5dbab2654e179c3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "A team wants to support continuous improvement via human feedback on deployed model outputs. Which pipeline element must they implement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Higher learning rate",
      "B": "Feedback collection interface feeding into reward model updates",
      "C": "Static model endpoint",
      "D": "Single training job"
    },
    "explanation": "A feedback loop and reward model update process are needed for continual RLHF improvements."
  },
  {
    "id": "7d50bc67e2517b884e8eefdf53d229f3358fd558dd29569471fd1aec620688cb",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "Which fine-tuning paradigm is best when you have both classification and generation tasks in a single domain?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Pure supervised classification fine-tuning",
      "B": "RLHF only",
      "C": "Instruction tuning only",
      "D": "Multi-task fine-tuning mixing classification and instruction data"
    },
    "explanation": "Multi-task fine-tuning allows the model to learn both classification and generative capabilities jointly."
  },
  {
    "id": "4fdc85217e2bdacf02c3808c41c4cdd5a221f8348733bfd57c18d05285031ce2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "When performing domain-adaptive pre-training on a model, which optimizer setting change is recommended?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Lower learning rate than general pre-training",
      "B": "Higher learning rate",
      "C": "No weight decay",
      "D": "Remove warmup"
    },
    "explanation": "A lower learning rate ensures stable domain pre-training without disrupting general knowledge."
  },
  {
    "id": "a24f6c2e4f45dbdf23c4007ca4e6d4fb4fb22ecd26f0937ead86a7c3e168ae68",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "Which method best quantifies performance degradation to general tasks after fine-tuning on a narrow domain?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Training loss",
      "B": "Domain validation perplexity",
      "C": "Benchmark on held-out general task datasets",
      "D": "Model size change"
    },
    "explanation": "Evaluating on separate general benchmarks reveals any loss of general capability post fine-tuning."
  },
  {
    "id": "24824196536eb62ba2d1383d8d5a9cf611d0976cadbf34156e3bc09b541efb13",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A startup has unlabeled transaction logs and wants to discover typical customer behavior patterns without prior labeling. Which type of learning and algorithm is most appropriate?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Supervised learning with decision trees",
      "B": "Unsupervised learning with clustering (e.g., K-means)",
      "C": "Reinforcement learning with Q-learning",
      "D": "Supervised learning with logistic regression"
    },
    "explanation": "Unlabeled data requires unsupervised learning; clustering algorithms like K-means find patterns without labels."
  },
  {
    "id": "27ced98bbbb1a1a991f01d4a2a36dd299dc0c56cf48918b55acbf8e0a3facae0",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A financial firm has daily closing prices and wants to predict tomorrow's price. They have known labels. Which type of task and learning paradigm applies?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Supervised classification",
      "B": "Supervised regression",
      "C": "Unsupervised clustering",
      "D": "Reinforcement learning"
    },
    "explanation": "Predicting a continuous numeric value from labeled data is a supervised regression problem."
  },
  {
    "id": "c9330b80c48b00a14e90a09a5e8c33f1c10abfba1f31059456ed99e58bc8bd47",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "In ML, what distinguishes the algorithm from the model artifact?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Algorithm is the trained weights; model is the optimization procedure",
      "B": "Algorithm generates predictions; model defines hyperparameters",
      "C": "Algorithm is the procedure to learn; model is the learned parameters instantiation",
      "D": "Algorithm is the input data; model is the output labels"
    },
    "explanation": "The algorithm is the learning procedure, while the model is the result \u2014 the learned parameters from training."
  },
  {
    "id": "2b5f15491261586e3f6c8d5fb6ccf38602a79f659cb6f494fae718c16dddb330",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A real-time translation service requires sub-second responses. Which inference type should you deploy?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Batch inference via scheduled job",
      "B": "Real-time inference via API endpoint",
      "C": "Stream inference via Kinesis Data Analytics",
      "D": "Edge inference on Greengrass only"
    },
    "explanation": "Sub-second response demands real-time inference exposed through an API endpoint."
  },
  {
    "id": "7b9ca38f09803bf0fce8d767bc610faddd1dfa84ebc1934f27cda70d41a0d5ee",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "You receive IoT device logs in JSON format. How do you categorize this data type?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Structured tabular data",
      "B": "Unstructured free text",
      "C": "Semi-structured data",
      "D": "Time-series numeric data"
    },
    "explanation": "JSON has a schema yet flexible fields\u2014qualifying as semi-structured data."
  },
  {
    "id": "cf3b7501a3630e20f97a67139f20e28dc420aebbb31c69dfaa637223a4e05676",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A model performs poorly on both training and test data. Which problem is indicated and what remedy applies?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Overfitting; reduce model complexity",
      "B": "Underfitting; increase model complexity",
      "C": "Data drift; retrain model periodically",
      "D": "High variance; add regularization"
    },
    "explanation": "Poor performance on training and test signals underfitting; increasing capacity can help."
  },
  {
    "id": "f3cfe78d690f595711c23faa103f1aaf11c530f618bea99325e7edb33ed41821",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "An AI hiring tool rejects candidates from a minority group more often. Which issue does this illustrate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Variance error",
      "B": "Model bias affecting fairness",
      "C": "Overfitting to training data",
      "D": "Underfitting across groups"
    },
    "explanation": "Systematic disadvantage of a group indicates bias/fairness concern, not variance or fit issues."
  },
  {
    "id": "abead2c056743985cf64adb2eac900e212124a7b2c2b239dcfb41b037a0e23c9",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "In NLP, what is a token in the context of language models?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "A single character",
      "B": "A subword unit such as a wordpiece",
      "C": "A part-of-speech annotation",
      "D": "A document-level vector"
    },
    "explanation": "Modern LLMs break text into subword tokens (wordpieces) for modeling."
  },
  {
    "id": "a7cb44344e69e09d7fb5f5924bcfe67384e6a1d14380b1642360ff7a2de2dbb5",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "Comparing batch versus real-time inference, which statement is true?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Batch inference processes individual records on demand",
      "B": "Real-time inference processes large volumes of data on schedule",
      "C": "Batch inference optimizes throughput over latency",
      "D": "Real-time inference requires training separate models"
    },
    "explanation": "Batch inference trades latency for throughput by processing large volumes at once."
  },
  {
    "id": "ee1c3f0a1ba7ddde1b28c5b12408fc435ce0015772c498fd456d37f53401b3a7",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "Which statement correctly describes the relationship between AI, ML, and deep learning?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "AI \u2282 ML \u2282 DL",
      "B": "ML \u2282 AI and DL \u2282 ML",
      "C": "DL \u2282 AI \u2282 ML",
      "D": "AI, ML, DL are disjoint fields"
    },
    "explanation": "Deep learning is a subset of ML, which itself is a subset of the broader AI field."
  },
  {
    "id": "3848b5d3010ccdfd3dafef7ff49eefd93be1c8f0e4d9a3e22c1410b4f5b7f0f9",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A neural network with only one hidden layer is best described as:",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "A shallow neural network",
      "B": "A deep neural network",
      "C": "A convolutional network",
      "D": "A support vector machine"
    },
    "explanation": "Networks with a single hidden layer are termed 'shallow'; deep networks have multiple layers."
  },
  {
    "id": "24a40ff3fb62da0042376ab9c315b4be8eaef3e72165d2cfab8b47e1cedd3b06",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "Which algorithm outputs probability distributions over discrete classes?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Linear regression",
      "B": "Softmax regression",
      "C": "K-means clustering",
      "D": "Principal component analysis"
    },
    "explanation": "Softmax regression (multinomial logistic) yields class probabilities; others do not."
  },
  {
    "id": "e7022aa8191d41c1797ce420e6546c990914f9e8996cab9bb0b1c77b007575b1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "Which learning paradigm uses reward signals to learn optimal actions?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Supervised learning",
      "B": "Unsupervised learning",
      "C": "Reinforcement learning",
      "D": "Semi-supervised learning"
    },
    "explanation": "Reinforcement learning uses reward feedback to guide an agent's actions."
  },
  {
    "id": "3c78bd025b4fabb0b21b8e5b0d1d462a93a69f14dd2592c2d5a2ae0d2ff4c2bd",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A model generalizes well when training and unseen-data errors are similar. This indicates:",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "High bias",
      "B": "High variance",
      "C": "Good fit",
      "D": "Data drift"
    },
    "explanation": "Comparable train/test errors signal the model fits data appropriately without under/overfitting."
  },
  {
    "id": "c20ae23485505533d4c5d682e28be4e3bb923e92756f2f257c7b8c0d23e24ace",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "Which term describes the data type for customer support chat transcripts?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Structured numeric data",
      "B": "Semi-structured log data",
      "C": "Unstructured text data",
      "D": "Time-series sensor data"
    },
    "explanation": "Free-form chat transcripts are unstructured text requiring NLP techniques."
  },
  {
    "id": "46e2c27536a8fff77660986b3627cedfc06e7e3508192337d5b680b0e8cb8c30",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "Which describes the 'fit' process in ML?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Applying model to new data",
      "B": "Calculating inference latency",
      "C": "Optimizing model parameters on training data",
      "D": "Deploying model to production"
    },
    "explanation": "Fitting means training \u2014 adjusting model parameters to minimize error on training data."
  },
  {
    "id": "1a492e0485d74e5029d0b4e018751ac6aeb482b853bc0a043148c39535d0097b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "Which term refers to features derived from raw data to improve model performance?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Embeddings",
      "B": "Hyperparameters",
      "C": "Feature engineering",
      "D": "Transfer learning"
    },
    "explanation": "Feature engineering transforms and creates input variables to boost model quality."
  },
  {
    "id": "520224f5662b931afb2dd52354afdafed3ebb98b09b1d2e3e229d839b5b1b76f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "Which inference mode is suitable for nightly generation of business reports?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Online inference",
      "B": "Edge inference",
      "C": "Batch inference",
      "D": "Streaming inference"
    },
    "explanation": "Batch inference handles large volumes on a schedule, ideal for nightly reports."
  },
  {
    "id": "ac8ddf117e108bb06490ca78f008f2bf1da4e26dd06d5ada9bad34f3aa24cd73",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "Which statement about unsupervised versus supervised learning is correct?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Supervised learning does not require labeled data",
      "B": "Unsupervised learning minimizes a loss function based on labels",
      "C": "Unsupervised learning discovers hidden patterns without labels",
      "D": "Supervised learning clusters data without supervision"
    },
    "explanation": "Unsupervised learning finds structure/patterns in unlabeled data; supervised requires labels."
  },
  {
    "id": "f1a55d2bfa55699a824140664b4c9f5f107a34b526139eb686fdd322f9cb4729",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A dataset contains grayscale images of size 28\u00d728. Which data type classification applies?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Structured tabular",
      "B": "Time-series numeric",
      "C": "Unstructured image",
      "D": "Semi-structured document"
    },
    "explanation": "Images are unstructured pixel arrays requiring CV techniques, not tabular formats."
  },
  {
    "id": "77b20f777f28c5dfe0e40ca57e4703b49d2b4d742606fe292b3520b8b5e12ac6",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "Which is a characteristic of deep learning compared to classical ML?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Requires less data for training",
      "B": "Learns representations automatically via multiple layers",
      "C": "Always interpretable",
      "D": "Uses k-nearest neighbors inherently"
    },
    "explanation": "Deep learning stacks many layers to learn hierarchical feature representations automatically."
  },
  {
    "id": "be7fbad7de08e2c32b8f8a08cdff962638cf321003e088643bf6d2473ee6974a",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A model shows higher accuracy on test data than on training data. Which issue most likely occurred?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Data leakage",
      "B": "Underfitting",
      "C": "Overfitting",
      "D": "Covariate shift"
    },
    "explanation": "Higher test accuracy than training usually indicates information leaked from test into training."
  },
  {
    "id": "a015e051d39f2bb44c0d3b5aaf0405f5cd52191e3ece55ee96d7bb96c3e9dd44",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "Which describes 'algorithmic bias' in ML?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Random noise in labels",
      "B": "Systematic error from training data misrepresentation",
      "C": "Model's inability to learn complex patterns",
      "D": "Fluctuations in performance over time"
    },
    "explanation": "Algorithmic bias arises when training data misrepresentation causes systematic errors."
  },
  {
    "id": "c0f9983a803e7429d6a591571c0607a6658a6b7485c051da8b40258bb05a259c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "What differentiates structured from unstructured data?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Structured data is stored in tables with defined schema",
      "B": "Unstructured data has numeric values only",
      "C": "Structured data needs NLP to process",
      "D": "Unstructured data is always binary"
    },
    "explanation": "Structured data fits into fixed schemas (tables); unstructured lacks a predefined format."
  },
  {
    "id": "624d5127eba00bdd282a22166b15067d410c081ad843d162ffea2a7b615fe2f5",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "Which statement about reinforcement learning is false?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "It learns from labeled examples",
      "B": "It uses reward feedback",
      "C": "It interacts with an environment",
      "D": "It optimizes cumulative reward"
    },
    "explanation": "Reinforcement learning does not rely on labeled examples; it learns via trial-and-error rewards."
  },
  {
    "id": "b29b2fb2678be0f125ff28355ef30db17f29990c5cd91444abd5b83a8daba0e8",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "Which term describes the risk that a model performs unfairly across subpopulations?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Variance",
      "B": "Bias",
      "C": "Fairness issue",
      "D": "Generalization error"
    },
    "explanation": "Fairness issues occur when model performance differs systematically across groups."
  },
  {
    "id": "a7922a3a324bdd6b9b60bef2de0a0c70cfec30f285f5d2efa743b26a5fdce030",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "In AI terminology, what is an 'algorithm' best defined as?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "A mathematical model with learned parameters",
      "B": "A set of rules or procedures for solving a problem",
      "C": "The data used to train a model",
      "D": "The output generated after inference"
    },
    "explanation": "An algorithm is the defined procedure or set of rules used to solve problems or learn from data."
  },
  {
    "id": "f1c84c154e7420590f818a067841efc94de46193014db7ed4d26e5a6bce9b23b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "Which category best describes audio files used for speech recognition?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Structured",
      "B": "Unstructured",
      "C": "Time-series",
      "D": "Tabular"
    },
    "explanation": "Audio is unstructured data; though sequential, it lacks a fixed schema for direct table storage."
  },
  {
    "id": "61cfbd1bf48dbf28042fb973e6d08cebed0e2f8270bf9f94c3f371e6f6f619a3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A model uses a predefined architecture like VGG before any training. This blueprint is called:",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Model",
      "B": "Algorithm",
      "C": "Hyperparameter",
      "D": "Training data"
    },
    "explanation": "The architecture or learning procedure is part of the algorithm; the model emerges after training."
  },
  {
    "id": "a1c3eefc2bfc99521d4057687d90c391f37cf8b9012e33a30655c044dc21fb5b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "Which statement illustrates a deep learning characteristic?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model uses manual feature extraction",
      "B": "Model stacks multiple hidden layers for representation learning",
      "C": "Model uses linear decision boundaries only",
      "D": "Model selects features via PCA only"
    },
    "explanation": "Deep learning models use many hidden layers to learn hierarchical representations."
  },
  {
    "id": "ee85bc45ac2fa549da00d56f448f446e5eb072e115300d29425ebeb523e7de81",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "When applying a trained model to new input to generate predictions, which phase of the ML lifecycle is that?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Training",
      "B": "Inference",
      "C": "Preprocessing",
      "D": "Evaluation"
    },
    "explanation": "Inference refers to using a trained model to make predictions on unseen data."
  },
  {
    "id": "cc7c0303c73e4242d59194b4a572120ebbc506c9b9759f43c4243007be46c975",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A retailer has extensive purchase histories for customers but no predefined segments. The marketing team wants to group customers by behavior to target promotions. Which ML technique is most appropriate?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Clustering",
      "B": "Binary classification",
      "C": "Regression",
      "D": "Reinforcement learning"
    },
    "explanation": "Clustering is unsupervised and groups unlabeled data by similarity; classification and regression require labels, and reinforcement learning optimizes sequential decisions."
  },
  {
    "id": "40e0ac9a49bc68fda02f38dab051b3dc058736495bec83679c61d82ab9a807ae",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A bank wants to predict whether a loan applicant will default. They need a probability score and a decision boundary. Which ML approach best fits?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Linear regression",
      "B": "Classification using logistic regression",
      "C": "K-means clustering",
      "D": "Time-series forecasting"
    },
    "explanation": "Logistic regression yields probabilities and class labels for default prediction; linear regression predicts continuous values, clustering groups without labels, and forecasting is temporal."
  },
  {
    "id": "e2c985d7f07e2b80fbc878cdd8108af29cb2e4e7f15d83a74edbec4c89b8399f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A housing company needs to estimate house sale prices from features like size and location. Which technique should they use?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "K-nearest neighbors classification",
      "B": "Binary classification",
      "C": "Regression",
      "D": "Dimensionality reduction"
    },
    "explanation": "Regression predicts continuous values (prices); classification predicts discrete classes, and dimensionality reduction compresses features."
  },
  {
    "id": "84fc77736e4c22c7e0c0272714eb1568182bb8cf92624b75ee179c351956828a",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A logistics firm wants to detect unusual shipping delays in real time by analyzing delivery times without labeled anomalies. Which technique is most suitable?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Supervised classification",
      "B": "Regression",
      "C": "Collaborative filtering",
      "D": "Unsupervised anomaly detection"
    },
    "explanation": "Unsupervised anomaly detection identifies outliers without labels; classification and regression require labels, and collaborative filtering is for recommendations."
  },
  {
    "id": "47ba940e9059b5752203c4ba2ef48ea962fb5546193de54a741b34efc56a9958",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A call center wants to automatically transcribe and analyze customer sentiment from voice calls to identify dissatisfied callers. Which AWS service and technique should they choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Lex with classification",
      "B": "Amazon Polly with clustering",
      "C": "Amazon Transcribe followed by Amazon Comprehend sentiment analysis",
      "D": "Amazon Rekognition for audio classification"
    },
    "explanation": "Transcribe converts speech to text and Comprehend extracts sentiment; Lex is for conversational bots, Polly is TTS, and Rekognition analyzes images/video."
  },
  {
    "id": "dda5aef9e3e947f873da870c5d166203f2c0d3844eabeb57dddd345f5fd43aa8",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A media company has labeled images of defects in manufacturing and wants to automate defect detection on new images. Which AWS managed service and technique apply?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Textract with regression",
      "B": "Amazon Rekognition Custom Labels with classification",
      "C": "Amazon Comprehend with clustering",
      "D": "Amazon SageMaker for reinforcement learning"
    },
    "explanation": "Rekognition Custom Labels trains an image classification model; Textract extracts text, Comprehend handles NLP, and reinforcement learning is for sequential decision tasks."
  },
  {
    "id": "a44b67a5850278795ed1d1910c0b8cdc6a7b257ceaee43e1adf04c44c95a1035",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A startup needs to translate customer emails in real time across languages without maintaining custom models. Which AWS managed service fits?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Comprehend",
      "B": "Amazon Lex",
      "C": "Amazon Transcribe",
      "D": "Amazon Translate"
    },
    "explanation": "Translate provides managed real-time machine translation; Comprehend does NLP analysis, Lex builds chatbots, and Transcribe converts speech to text."
  },
  {
    "id": "679b5844bd5eb4e0e6071b660d6a4cbb62edebeb8937f9d9d2a1904c068a1f16",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "An e-commerce platform wants to recommend products based on user behavior and similar users. Which ML technique and AWS service?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Collaborative filtering with Amazon Personalize",
      "B": "Clustering with Amazon Comprehend",
      "C": "Sequence prediction with Amazon Lex",
      "D": "Semantic search with Amazon Translate"
    },
    "explanation": "Personalize uses collaborative filtering for recommendations; Comprehend is NLP, Lex is conversational AI, and Translate is translation."
  },
  {
    "id": "8c223de5837840fbe7b1e9807e2adc4358a911532117c8be4f220e002b5173b7",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A manufacturer tracks sensor data streams from equipment and wants to predict time to failure. Which technique suits continuous data and error margins?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Classification",
      "B": "Clustering",
      "C": "Time-series regression forecasting",
      "D": "Association rule mining"
    },
    "explanation": "Time-series regression forecasting predicts continuous future values over time; classification predicts classes, clustering groups data, and association mining finds co-occurrence patterns."
  },
  {
    "id": "11026408b3f6d82955e349c16333e808c75821c54533eeae5d2e58e674b6d7f7",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A legal firm needs to extract key clauses from contracts and classify them as confidentiality or non-confidentiality. They prefer a fully managed service. Which AWS service and ML approach?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Textract with clustering",
      "B": "Amazon Comprehend custom classification",
      "C": "Amazon Translate with regression",
      "D": "Amazon Polly with unsupervised learning"
    },
    "explanation": "Comprehend custom classification labels text based on categories; Textract extracts raw text, Translate translates languages, Polly generates speech."
  },
  {
    "id": "fd277903a0acfaab88ccb0d3c060ace036fbe10a19b793fd045e20acc230348b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A health-tech startup has limited historical patient data and wants to predict disease risk. They worry about overfitting. What should they consider before applying ML?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Proceed with deep learning",
      "B": "Use reinforcement learning",
      "C": "Assess whether data volume and diversity justify a simple rule-based model",
      "D": "Deploy a high-capacity ensemble model"
    },
    "explanation": "With limited data, simple rules can outperform complex models and avoid overfitting; deep or ensemble models require more data, and reinforcement learning is inappropriate."
  },
  {
    "id": "284435e916d84cde5dc714d25bd8e53b2246b586d726bec3f15bf58654d4cb9e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A content platform needs to summarize long articles automatically. They want high accuracy without building custom models. Which AWS service is appropriate?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Translate",
      "B": "Amazon Polly",
      "C": "Amazon Rekognition",
      "D": "Amazon Comprehend with summarization API"
    },
    "explanation": "Comprehend offers a managed summarization feature; Translate is translation, Polly is TTS, and Rekognition is for images."
  },
  {
    "id": "0bc3e9b565822019845c9888c55daeae030023d132934d01ebd653d3fd9c1fd6",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A company wants to detect sentiment shifts in social media streams for brand monitoring. They require near-real-time analysis. Which architecture using AWS services achieves this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Translate -> Amazon SageMaker endpoint",
      "B": "Amazon Kinesis Data Streams -> AWS Lambda -> Amazon Comprehend",
      "C": "Amazon S3 batch -> Amazon Athena with Comprehend",
      "D": "Amazon DynamoDB -> Amazon Rekognition"
    },
    "explanation": "Kinesis streams data in real time, Lambda triggers Comprehend sentiment analysis; batch S3 is not real time, Translate and Rekognition are wrong services."
  },
  {
    "id": "62040933ccd51e016b4ccf9665ea1a969a9ebf7c3a9f82d44fa6ee9ba083c22d",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A fraud detection team wants to update their detection rules with model insights but cannot label post-fraud cases easily. Which ML approach helps uncover new fraud patterns?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Supervised classification with historical labels",
      "B": "Regression on fraud amounts",
      "C": "Reinforcement learning on transaction sequences",
      "D": "Unsupervised clustering on transaction features"
    },
    "explanation": "Clustering can reveal patterns in unlabeled data; supervised needs labels, regression predicts amounts, and RL optimizes sequential action rewards."
  },
  {
    "id": "54b5d6d64c80f81f986e3e48ac0d3b9df2cb30e0044545f98b4d864568bb18a3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A marketing team needs to identify topics in customer feedback without predefined categories. Which technique and AWS service apply?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Custom classification with Amazon Comprehend",
      "B": "Sentiment analysis with Amazon Transcribe",
      "C": "Topic modeling with unsupervised Amazon Comprehend",
      "D": "Image analysis with Amazon Rekognition"
    },
    "explanation": "Comprehend supports unsupervised topic modeling; custom classification requires labels, Transcribe is speech-to-text, and Rekognition is for images."
  },
  {
    "id": "e03911e3e3326197483e7dbeb9aaa4f4562828b89ce91ba65ea019bb1da02368",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A retailer has price data with many outliers and wants to predict median transaction value. Which ML objective minimizes outlier impact?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Mean squared error regression",
      "B": "Quantile regression",
      "C": "K-means clustering",
      "D": "Logistic regression"
    },
    "explanation": "Quantile regression predicts a specified percentile (median) and is robust to outliers; MSE penalizes outliers heavily, clustering and logistic are unrelated."
  },
  {
    "id": "14cf0d2cd98a3c0c623f078ee0ea8a04991efcb3cae348be68dd494cb3824bb3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A travel company wants to provide personalized itinerary chat support integrated into their website without building backend ML. Which AWS service should they use?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Lex",
      "B": "Amazon Polly",
      "C": "Amazon Comprehend",
      "D": "Amazon Translate"
    },
    "explanation": "Lex builds conversational interfaces with managed NLU and integration; Polly is TTS, Comprehend analyzes text, and Translate translates languages."
  },
  {
    "id": "7f2e0b31ca6de4c73e99a1288d25eda8d573aa1b5d18c77acb5b2d39c38f67f8",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A data science team wants to quickly prototype a classification model on tabular data with minimal setup. Which AWS service option is fastest?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "SageMaker custom notebook and build model",
      "B": "Deploy your own EC2 with scikit-learn",
      "C": "Use AWS Lambda for runtime training",
      "D": "Use SageMaker Autopilot"
    },
    "explanation": "Autopilot automates model building on tabular data; other options require manual setup or are unsuitable for training."
  },
  {
    "id": "c7f730dee0ab42fa1a00e5a8ea1fac866a1e5fb682ad4fcfd5804fc3bed8014a",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "An IoT company needs to decide if an edge device should alert based on sensor patterns. They require on-device ML but limited compute. Which approach?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Deploy a full SageMaker endpoint on device",
      "B": "Use Amazon Rekognition on device",
      "C": "Use SageMaker Neo to compile a small footprint model",
      "D": "Use Amazon Comprehend on device"
    },
    "explanation": "Neo compiles models for resource-constrained devices; full endpoints and NLP/image services are inappropriate on edge."
  },
  {
    "id": "1063d838b4ab560725ac5d4e5275e03ba8a86e9be72417170263903f2223592e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A supply chain manager wants to forecast demand peaks and valleys for inventory planning. Which ML technique suits this seasonal pattern?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Classification",
      "B": "Time-series forecasting",
      "C": "Unsupervised clustering",
      "D": "Reinforcement learning"
    },
    "explanation": "Time-series forecasting models temporal patterns and seasonality; classification, clustering, and RL are not for continuous forecasting."
  },
  {
    "id": "dfc098488ea296e9d4946a2bd86b8657f1882f3476d3f7d59192b272b8bad32c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A news aggregator needs to classify articles by topic and extract key entities. They prefer a single AWS service call. Which service?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Comprehend",
      "B": "Amazon Translate",
      "C": "Amazon Textract",
      "D": "Amazon Polly"
    },
    "explanation": "Comprehend provides classification and entity extraction; Translate translates text, Textract extracts text from images, Polly synthesizes speech."
  },
  {
    "id": "0401627e802b74db3cedb826ec13234bd3d3eeccca9ad217fbc02ab79a325c56",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A gaming company wants NPCs to adapt tactics based on player behavior over multiple matches. Which ML paradigm applies?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Supervised learning",
      "B": "Unsupervised learning",
      "C": "Reinforcement learning",
      "D": "Clustering"
    },
    "explanation": "Reinforcement learning trains agents to optimize rewards through interactions; supervised and unsupervised learning do not handle sequential decision-making."
  },
  {
    "id": "c9be8b2952f90888fab2ca82495c3233fd3a790413b17e2309c4e7890e9e4d8b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A research team has a small labeled dataset for image classification but needs higher accuracy. Which AWS managed solution speeds experiments?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Rekognition without custom training",
      "B": "SageMaker Studio with built-in algorithms",
      "C": "Amazon Comprehend classification",
      "D": "Amazon Translate"
    },
    "explanation": "SageMaker Studio with built-in image classification algorithms allows rapid experimentation; Rekognition without custom training won\u2019t improve accuracy, Comprehend and Translate are wrong domains."
  },
  {
    "id": "1f13fa271d944854f39808f63d374bb254b42d31b731b40f67f1dacd5e42d52c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A call analytics team must determine call intent from transcripts and route calls automatically. Which combination of AWS services and techniques?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Lex for TTS",
      "B": "Amazon Polly \u2192 Amazon Transcribe",
      "C": "Amazon Translate \u2192 Amazon Comprehend sentiment",
      "D": "Amazon Transcribe \u2192 Amazon Comprehend custom classification"
    },
    "explanation": "Transcribe converts speech, and Comprehend custom classification identifies intent; other combos are irrelevant."
  },
  {
    "id": "896c497edbad68d55ea8b8ecf9cf3e050ec206536fceab50312ddd0c6e6b3820",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A company with limited data wants to cluster customer feedback text. They cannot label data. Which AWS service can help without labels?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Comprehend custom classification",
      "B": "Comprehend unsupervised topic modeling",
      "C": "Translate clustering",
      "D": "Lex clustering"
    },
    "explanation": "Comprehend supports unsupervised topic modeling on text; other services don\u2019t cluster unlabeled text."
  },
  {
    "id": "4639683923c815de9d53fc522035afe9e2e96313a8d887b6a702a152a02b4445",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A manufacturing line collects temperature and vibration data to predict equipment failure exact time. Which service and technique?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Forecast classification",
      "B": "Amazon SageMaker clustering",
      "C": "Amazon SageMaker time-series forecasting",
      "D": "Amazon Comprehend regression"
    },
    "explanation": "SageMaker time-series forecasting handles temporal regression; Forecast service focuses on business metrics forecasting, clustering is for segmentation, Comprehend is NLP."
  },
  {
    "id": "4f0af569f5405fb7ee56080dda3ccf5203d03201d279cda19899ef5cfc6122d2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "An HR analytics team wants to predict employee attrition risk and understand key factors. Which ML technique supports both prediction and feature importance insight?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "K-means clustering",
      "B": "Reinforcement learning",
      "C": "Neural network black-box model",
      "D": "Interpretable supervised classification (e.g., decision tree)"
    },
    "explanation": "Decision trees provide classification and interpretability; clustering is unsupervised, RL is sequential, and black-box networks lack transparency."
  },
  {
    "id": "b28631df2568b5900a8b9e80a1ec5cbff98115543d86c66f34d60bb87f2c4125",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A financial services firm needs a service to continuously monitor AWS account security postures and recommend fixes, including container vulnerabilities. Which service?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Config",
      "B": "AWS CloudTrail",
      "C": "Amazon Inspector",
      "D": "AWS Artifact"
    },
    "explanation": "Inspector assesses vulnerabilities in EC2 and ECR and provides remediation; Config tracks configuration, CloudTrail logs API calls, Artifact provides compliance docs."
  },
  {
    "id": "5f7f153e7ef84d60630cdd82901842a9c8bf3d87f08ca69373c105576709cbc7",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A biotech company wants to find new protein structures by exploring combinations without prior labels. Which ML approach should they use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Supervised classification",
      "B": "Unsupervised clustering",
      "C": "Regression",
      "D": "Reinforcement learning"
    },
    "explanation": "Clustering can explore unlabeled data structure; supervised and regression need labels, RL is for sequential decision optimization."
  },
  {
    "id": "1fd5e73ecf71fe1891723a9ec7022451008fdf67a0731a221ea82daddb46525b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A customer service team wants to deploy a chatbot that understands intents and delegates to live agents when confidence is low. Which AWS service and feature?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Lex with confidence threshold",
      "B": "Amazon Polly with fallback",
      "C": "Amazon Comprehend sentiment",
      "D": "Amazon Translate delegation"
    },
    "explanation": "Lex supports intent recognition and confidence thresholds for fallback; Polly is TTS, Comprehend is NLP analysis, and Translate is translation."
  },
  {
    "id": "293e78596cfa0f8df86bbb04ae7de8b512fb865d922a639c252d179bc2448c9d",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A company wants to index and search through a large repository of documents using semantic similarity. Which AWS service and ML technique?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Kinesis with regression",
      "B": "Amazon Translate with clustering",
      "C": "Amazon Elasticsearch with RL",
      "D": "Amazon OpenSearch Service with vector embeddings"
    },
    "explanation": "OpenSearch supports k-NN vector searches for semantic retrieval; Kinesis is streaming, Translate is translation, RL unsuitable."
  },
  {
    "id": "6752895d47824af341b79778c9d366d501537f75045301e8f1819580453138e7",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A retail analytics team wants to group stores with similar sales patterns and then predict future sales per group. Which sequence of ML techniques applies?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Regression then clustering",
      "B": "Classification then regression",
      "C": "Clustering then time-series forecasting",
      "D": "Reinforcement learning then classification"
    },
    "explanation": "First cluster by pattern, then apply forecasting per cluster; other sequences are illogical or wrong paradigm."
  },
  {
    "id": "acf171d4e8e75c11a06bdb132629ed42072f01970b1333c6deaf34a95d289777",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A manufacturing company needs to deploy a model to perform real-time predictions of equipment failures with millisecond latency. Which deployment method should they choose to meet this requirement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set up a SageMaker real-time inference endpoint",
      "B": "Use a SageMaker batch transform job",
      "C": "Deploy the model in an AWS Lambda function",
      "D": "Embed the model in Amazon Kinesis Data Analytics"
    },
    "explanation": "Real-time inference endpoints provide millisecond latency, whereas batch transforms and Kinesis Data Analytics are unsuitable for strict real-time requirements."
  },
  {
    "id": "c55116e9a4ecfe9d621479c200dcd574b6bb344a9866d4c087954301628e828c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A data science team must process a 10 TB dataset once per month to generate predictions. The process does not require low latency. Which AWS service and feature combination is the most cost-effective solution?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker real-time endpoint with on-demand instances",
      "B": "SageMaker batch transform job with managed Spot instances",
      "C": "Deploy model as a Lambda function triggered by S3 uploads",
      "D": "Host the model on Amazon EC2 Auto Scaling group"
    },
    "explanation": "Batch transform with managed Spot instances minimizes cost for large one-off jobs without low-latency requirements."
  },
  {
    "id": "c2c2f85f8ec147056db866d87cf4f8c6e5669bb1f8085a160e84a175bc47e525",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A team wants to track parameters, metrics, inputs, and outputs across multiple training runs for reproducibility. Which SageMaker feature should they integrate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Model Monitor",
      "B": "SageMaker Model Registry",
      "C": "SageMaker Experiments",
      "D": "SageMaker Debugger"
    },
    "explanation": "SageMaker Experiments captures and organizes metadata across runs, enabling reproducibility. Debugger is for training insights, and Model Monitor is for endpoint drift."
  },
  {
    "id": "209e949d139d3a98f118342bb57b0c61b69b92b6ca816741eaf1ff4c02d8b63b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "During feature engineering, a data scientist needs a low-code interface to profile data, identify outliers, and generate transformations. Which AWS service should they use?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Glue DataBrew",
      "B": "Amazon SageMaker Ground Truth",
      "C": "Amazon Athena",
      "D": "Amazon SageMaker Data Wrangler"
    },
    "explanation": "Data Wrangler provides a visual interface for profiling and transforming data; DataBrew is general ETL, not ML-focused."
  },
  {
    "id": "c3791f0db3a8225380fb26d27ecc4d8ee32e4761842786aca62e6744955bbe54",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A fintech startup requires both online and offline feature retrieval for its fraud detection model. Which AWS service best meets this requirement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon DynamoDB",
      "B": "Amazon SageMaker Feature Store",
      "C": "Amazon ElastiCache",
      "D": "Amazon RDS"
    },
    "explanation": "SageMaker Feature Store is designed for online (low-latency) and offline feature retrieval; databases alone lack the ML-specific APIs."
  },
  {
    "id": "158b878d576f44665e5775af0dce42f659e26fb7af4d67165f124c1572d3a2b3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "After deployment, a model\u2019s accuracy begins to decline due to data drift. Which AWS service can automatically detect and alert on data and prediction drift?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS CloudWatch Alarms",
      "B": "Amazon GuardDuty",
      "C": "Amazon SageMaker Model Monitor",
      "D": "AWS Config"
    },
    "explanation": "Model Monitor analyzes input and output data to detect drift; CloudWatch alarms require custom metrics, and GuardDuty is for security threats."
  },
  {
    "id": "13b29e53edc5b0852a483422a8f0c4c06227a45d73d6e8de0a2a1259687de7e0",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A company\u2019s document classification model returns low-confidence predictions, and they want human review for those cases. Which service should they use to integrate a human-in-the-loop?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Mechanical Turk",
      "B": "Amazon A2I with a private workforce",
      "C": "AWS Step Functions",
      "D": "Amazon Augmented AI (A2I)"
    },
    "explanation": "Amazon A2I provides human review workflows for ML predictions; Mechanical Turk is not integrated into SageMaker."
  },
  {
    "id": "283a3f0656e88bd5f9274057de3578ee480043f323d301c4a8aa0fdc96f0eeb4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "An organization wants full control over inference infrastructure, including custom libraries and auto-scaling policies. Which deployment approach should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure a SageMaker real-time endpoint",
      "B": "Use SageMaker multi-model endpoint",
      "C": "Deploy a self-hosted inference container on Amazon EKS",
      "D": "Package the model as a Lambda layer"
    },
    "explanation": "Self-hosted containers on EKS allow custom infrastructure control; SageMaker endpoints abstract away low-level management."
  },
  {
    "id": "f0cc242d1a478bc1c7a294ca1326add34a62d8d4df1d52163565e1430999f9c1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "For a binary classification problem with highly imbalanced classes, which performance metric is most appropriate to optimize?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Overall accuracy",
      "B": "F1 score",
      "C": "Mean squared error",
      "D": "Log-loss"
    },
    "explanation": "F1 score balances precision and recall for imbalanced data, whereas accuracy can be misleading."
  },
  {
    "id": "0bd1e50943bca79982112290ebed0b9db40df96f1eb9c75a4159ed35d2135d2e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A credit scoring model must minimize false negatives (risky customers labeled safe). Which metric should the team monitor during model evaluation?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Precision",
      "B": "Recall",
      "C": "ROC AUC",
      "D": "Mean Absolute Error"
    },
    "explanation": "Recall measures the ability to detect actual positives (risky customers); precision measures correctness of positives."
  },
  {
    "id": "6b3ce38fb7a0697183463c7a6e9def93da3e93065ca9b4a963866ab8a366c02a",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "To track dataset versions, transformation code, and model artifacts within SageMaker, which feature should you use?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "SageMaker Model Monitor",
      "B": "SageMaker Processing",
      "C": "SageMaker Clarify",
      "D": "SageMaker Model Registry"
    },
    "explanation": "Model Registry manages model versions and metadata; Model Monitor is for drift, Clarify for bias detection."
  },
  {
    "id": "61c2f257346438bf2b8ae3650f4df57f14dd589a8574f08f6d9f66e7d3885adb",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A data scientist needs to tune hyperparameters across 100 training jobs, leveraging Spot Instances to reduce cost. Which SageMaker feature should they configure?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Training Jobs with Managed Spot",
      "B": "SageMaker Experiments",
      "C": "SageMaker Automatic Model Tuning",
      "D": "SageMaker Processing Jobs"
    },
    "explanation": "Automatic Model Tuning runs many jobs exploring hyperparameters and can use managed Spot Instances."
  },
  {
    "id": "a2d767e2b434f0141acdb69b0a43fa839fb1fcea4e24dd97f9ea95c38b279fea",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A team wants to automate labeling of images with bounding boxes for a custom object detection model. Which service should they use to build this labeled dataset?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Rekognition Custom Labels",
      "B": "Amazon SageMaker JumpStart",
      "C": "Amazon Comprehend",
      "D": "Amazon SageMaker Ground Truth"
    },
    "explanation": "Ground Truth provides workflows for automated and human labeling; Rekognition Custom Labels requires existing labeled data."
  },
  {
    "id": "e8bb34d36451f269f88f3ae13918d2b1c816bb216ea70b2dac787ab51c96b93f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A developer needs to train a model using a custom Docker container with proprietary libraries. Which SageMaker feature allows this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Bring-your-own-container training in SageMaker",
      "B": "SageMaker Hosted Models",
      "C": "SageMaker Neo",
      "D": "SageMaker Notebook Instances"
    },
    "explanation": "SageMaker supports training with custom containers; Hosted Models refer to endpoints, Neo compiles models."
  },
  {
    "id": "0bf4685be28cdca07e69a54133f9ca1769ca8f7085c387fc1866542d5ef3a7f8",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "Which AWS service enables the orchestration of data preprocessing, model training, and deployment steps into a repeatable workflow?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Step Functions",
      "B": "Amazon SageMaker Pipelines",
      "C": "AWS Glue",
      "D": "Amazon Managed Workflows for Apache Airflow"
    },
    "explanation": "SageMaker Pipelines is specifically designed for ML workflows, integrating with SageMaker steps and components."
  },
  {
    "id": "b205a46bb52f74ae3fca29c0808799f88da53ac7fdc24a1774f0ce29c314610f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A data engineering team must profile streaming data for anomalies before feeding it into the ML pipeline. Which service component should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Model Monitor",
      "B": "AWS Glue Data Catalog",
      "C": "Amazon SageMaker Processing with custom script",
      "D": "Amazon Kinesis Data Firehose"
    },
    "explanation": "SageMaker Processing can run custom anomaly detection scripts on streaming data; Model Monitor is for deployed models."
  },
  {
    "id": "0aa504e0bccc47572f1f051845fee1c430a057899816d8618121cfd3c715f391",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "To manage and host multiple versions of a model in a central repository, which feature of SageMaker should you use?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "SageMaker Model Monitor",
      "B": "SageMaker Model Registry",
      "C": "SageMaker Experiment Tracker",
      "D": "SageMaker Debugger"
    },
    "explanation": "Model Registry stores and version-controls models; experiments track runs, not served models."
  },
  {
    "id": "0b7aacf320a0d2fbc40419daf73c4a5da7f4b8a3983fec22ef7367f4d9488262",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A financial services company must meet audit requirements for model transparency, including lineage of data, code, and model versions. Which SageMaker feature set provides this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Lineage Tracking",
      "B": "SageMaker Pipelines",
      "C": "SageMaker Clarify",
      "D": "AWS CloudTrail"
    },
    "explanation": "Lineage Tracking captures relationships among datasets, code, and models; CloudTrail logs API calls but not ML metadata."
  },
  {
    "id": "bec75f680765655ccf65dd0f22e50659b12161de3a73c4033807826944a6c8c7",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "For large-scale training jobs where training data size exceeds local instance storage, which SageMaker feature should you leverage?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Neo",
      "B": "SageMaker Model Monitor",
      "C": "SageMaker Batch Transform",
      "D": "Use Amazon S3 input channels with Pipe mode"
    },
    "explanation": "Pipe mode streams data directly from S3 to training container, avoiding local storage limits."
  },
  {
    "id": "45aa8ac7a83e4b7bbbac34ed6486443bea24c17ad9e48830ecaaaf7cb1fa5bdd",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A team wants to detect bias and explain model predictions before deployment. Which combination of SageMaker components addresses this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Experiments + Model Registry",
      "B": "SageMaker Clarify + Model Cards",
      "C": "SageMaker Model Monitor + Debugger",
      "D": "SageMaker Ground Truth + Processing"
    },
    "explanation": "Clarify detects bias and computes explainability metrics; Model Cards document this information."
  },
  {
    "id": "3c8069ed9ae1f7a7c039ac3f20041bfa5a00da05020a9e2c0472ca65edd86ff3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "When selecting a model source for a natural language task, which option allows using a pre-trained open-source model with minimal custom training?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "SageMaker built-in algorithms",
      "B": "Amazon Comprehend custom entity recognizer",
      "C": "Hugging Face model via SageMaker",
      "D": "Amazon Lex chatbot"
    },
    "explanation": "Hugging Face models on SageMaker let you fine-tune open-source transformers with minimal code."
  },
  {
    "id": "578bc358c55b1be401eed9c4fb821b1203bade979dbdd5835941eb23a56a6046",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A data scientist needs to rerun a previous training pipeline with updated code but identical data and parameters. Which SageMaker feature simplifies this?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "SageMaker Pipelines re-run capability",
      "B": "SageMaker Model Monitor checkpointing",
      "C": "SageMaker Experiments cloning",
      "D": "SageMaker Debugger replay"
    },
    "explanation": "Pipelines can be re-run with versioned steps to reproduce results; Experiments tracks runs but doesn't automate re-execution."
  },
  {
    "id": "4d6c05ebed5c92dbfd48a7d843a35ac6b022591b11255b8fbe896a0dbc2861a2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A predictive maintenance solution requires near real-time inference but can tolerate 2-second latency. Which endpoint type is most cost-effective?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Batch Transform",
      "B": "SageMaker asynchronous inference endpoint",
      "C": "SageMaker real-time endpoint",
      "D": "AWS Lambda with model container"
    },
    "explanation": "Asynchronous endpoints offer lower cost than real-time for latencies in seconds; batch transforms are too slow for near real-time."
  },
  {
    "id": "1c10002be447ead42d128aaa9334d3f90b5d599109d8f338be5cc4393343cb95",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "Which metric would best capture model performance when both false positives and false negatives carry significant but unequal business costs?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Accuracy",
      "B": "Precision",
      "C": "Recall",
      "D": "ROC AUC with customized threshold analysis"
    },
    "explanation": "ROC AUC allows analysis of trade-offs and selection of thresholds that reflect unequal costs."
  },
  {
    "id": "f8507ee5accec30d6e991f410db4b4c7ba3525540e71a01a1f8d239ab681caf6",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A team needs to continuously retrain a model monthly when new labeled data arrives, ensuring reproducibility and minimal operational overhead. Which solution is most appropriate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Manual training triggered by developer",
      "B": "SageMaker Pipelines with scheduled triggers",
      "C": "AWS Batch with custom scripts",
      "D": "AWS Glue ETL followed by SageMaker Training Jobs"
    },
    "explanation": "Pipelines with scheduled triggers automate retraining and maintain reproducibility of steps."
  },
  {
    "id": "73e8d82cab650a8ee62a95b04b283a27d1517611da82fbb96f41c9c673b7b8a3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "During initial exploratory data analysis, which step most directly uncovers multicollinearity among numerical features?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Compute data skew statistics",
      "B": "Visualize feature distributions",
      "C": "Calculate correlation matrix",
      "D": "Perform missing value imputation"
    },
    "explanation": "Correlation matrix identifies multicollinearity; other steps address distributions or missing values."
  },
  {
    "id": "6602aec25d2c6c227e6e6cd2d46c5f538b088ee9101a680c012028b780d713d0",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "Which SageMaker capability allows deploying multiple models behind a single HTTPS endpoint to optimize resource utilization?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Inference Recommender",
      "B": "SageMaker Model Monitor multi-model mode",
      "C": "SageMaker Multi-AZ endpoints",
      "D": "SageMaker multi-model endpoint"
    },
    "explanation": "Multi-model endpoints let you serve multiple model artifacts from one endpoint container."
  },
  {
    "id": "7a8b03f648c34105206974068c39a1511e3fc982fa6587731ec36b4632015fe8",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "To reduce training costs while maintaining model convergence speed, a team considers using managed Spot Instances. What should they plan for?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model drift monitoring",
      "B": "Improved inference latency",
      "C": "Handling potential training interruptions",
      "D": "Increased real-time throughput"
    },
    "explanation": "Spot Instances can be interrupted, so the training code must handle restarts or checkpoints."
  },
  {
    "id": "eeb9c76627aad896c0515589de17403568de26ca227295b49c722bcf6bfe7e81",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A team wants to compare performance of multiple models and register the best one automatically for deployment. Which SageMaker feature set should they use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Clarify and Model Monitor",
      "B": "SageMaker Model Registry with Pipelines evaluation step",
      "C": "SageMaker Experiments and Debugger",
      "D": "SageMaker Ground Truth and Experiments"
    },
    "explanation": "Pipelines can include evaluation steps that compare metrics and automatically register the winning model in the Model Registry."
  },
  {
    "id": "b96840d640dda76f1fce66ae3c0def210afa42e9ac8daa7d1c92c50b87b82995",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "When selecting a managed API versus self-hosted API for model inference, which trade-off is a key consideration?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Operational overhead versus infrastructure control",
      "B": "Model accuracy versus inference latency",
      "C": "Hyperparameter tuning complexity versus cost",
      "D": "Data preprocessing capabilities versus security"
    },
    "explanation": "Managed APIs reduce operational overhead but limit infrastructure customization; self-hosted offers full control at the cost of more ops work."
  },
  {
    "id": "5c8af2cf0e0fdb02b4bb35b48c7e5f6e98497aa586315464149525d5a0340aa2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A retailer measures model success by uplift in average order value rather than by classification accuracy. What type of metric is this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Technical metric",
      "B": "Capacity metric",
      "C": "Business metric",
      "D": "Hardware utilization metric"
    },
    "explanation": "Business metrics measure real business outcomes, whereas accuracy is a technical metric."
  },
  {
    "id": "11536b56907ba2ae927366fe8b32f24e1d273742baeb5a387e274364b8707fb9",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "A developer submits a 4096-character document to a foundation model with a 2048-token context window. The model rejects the request, stating the input exceeds the token limit. The developer assumed characters map one-to-one to tokens. Which concept explains this miscalculation?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Tokenization",
      "B": "Chunking",
      "C": "Embeddings",
      "D": "Positional encoding"
    },
    "explanation": "Tokenization splits text into subword units called tokens, which do not correspond one-to-one with characters. Understanding tokenization is key to context window limits."
  },
  {
    "id": "63da1acdeff8e8584cfc3f1e3eff96e7fabaae1ea24d60bf96c6074f72b22cdd",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "An engineer builds a RAG pipeline on long contracts. To preserve semantic continuity and reduce retrieval ambiguity, which chunking strategy should they adopt?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Fixed-size non-overlapping chunks",
      "B": "Semantic chunking with 10\u201320% overlap",
      "C": "Sentence-boundary chunking without overlap",
      "D": "Arbitrary character-based windows with 50% overlap"
    },
    "explanation": "Semantic chunking groups related sentences and a small overlap preserves context while minimizing redundancy."
  },
  {
    "id": "7d85003763f92d8c4b0b0f673378ce15f827bc10a8922568e1c7c275635af8a3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "A data scientist uses 64-dimensional embeddings for semantic search and sees many false positives between distinct topics. Which adjustment most directly improves topic discrimination?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase embedding dimensionality to 128 or 256",
      "B": "Switch from cosine similarity to Euclidean distance",
      "C": "Apply mean pooling instead of using the CLS token",
      "D": "Reduce embedding dimension to 32"
    },
    "explanation": "Higher-dimensional embeddings can capture finer semantic nuances, reducing false positives."
  },
  {
    "id": "b0d10d9e8b2dac16cd7b420c9f4c9e88f89b207b90859aade548662164045a31",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "When comparing text embedding vectors for similarity, which metric is most robust to differences in vector magnitude and focuses on directional similarity?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Euclidean distance",
      "B": "Manhattan distance",
      "C": "Cosine similarity",
      "D": "Jaccard distance"
    },
    "explanation": "Cosine similarity measures the angle between vectors, ignoring their magnitudes, making it robust to length differences."
  },
  {
    "id": "5c8096b59d114694584d4f3a9900ab76836298f424556dafe0c1552659dfd66e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "A business needs to generate concise summaries of long reports. Which transformer architecture variant is best suited for sequence-to-sequence summarization tasks?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Encoder-only",
      "B": "Decoder-only",
      "C": "Encoder-decoder",
      "D": "Diffusion model"
    },
    "explanation": "Encoder-decoder transformers (seq2seq) encode input then decode into a summary, ideal for summarization."
  },
  {
    "id": "c3b8dff750108ae45763768338f9bf56dd971b794e4141abed2964dc4c841254",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "To implement a chatbot that produces fluent text continuations, which transformer variant should be selected?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Encoder-only",
      "B": "Decoder-only",
      "C": "Encoder-decoder",
      "D": "Diffusion-based"
    },
    "explanation": "Decoder-only (autoregressive) transformers predict the next token, making them suitable for text generation/chatbots."
  },
  {
    "id": "c0fbced772365479d524aee874a3bb18c5882b9bdd00e77db525c5bf84ee1a75",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "An AI team needs to generate high-resolution images from noise. Which generative model type is most appropriate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Autoregressive transformer",
      "B": "Variational autoencoder",
      "C": "Diffusion model",
      "D": "Encoder-only transformer"
    },
    "explanation": "Diffusion models learn to denoise noisy data through a forward/reverse process and excel at high-resolution image generation."
  },
  {
    "id": "d4794c3e9b9b5dc44a1b9ffd5942f96f8d565a9de3a8f5ab8c067fa21f6f838c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "A research group wants to map images and text into a shared embedding space for cross-modal retrieval. Which foundation model family should they choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "GPT",
      "B": "BERT",
      "C": "CLIP",
      "D": "DALL-E"
    },
    "explanation": "CLIP jointly trains image and text encoders to project both modalities into a common vector space for retrieval."
  },
  {
    "id": "677a4e4ff8df6cd6c5aa46057e3cf91480e83843cbaf7aeb54c3e6529c550c1b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "The LLM produces incorrect reasoning steps. You want it to enumerate its logic before answering. Which prompt engineering technique elicits this behavior?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Zero-shot prompting",
      "B": "Few-shot prompting",
      "C": "Chain-of-thought prompting",
      "D": "Temperature adjustment"
    },
    "explanation": "Chain-of-thought prompts request the model to show intermediate reasoning, improving logical outputs."
  },
  {
    "id": "31acb33d99968dc203063166edf2e711fa0c452a32b98e6171bff59aeb0c5e95",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "An organization needs to generate code in both Python and JavaScript. Which model choice best supports multilingual code generation?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "GPT fine-tuned on multilingual code corpora",
      "B": "BERT fine-tuned on code classification",
      "C": "Transformer-based diffusion model",
      "D": "Encoder-only model"
    },
    "explanation": "Autoregressive GPT variants fine-tuned on diverse code scripts excel at multilingual code generation."
  },
  {
    "id": "3c68db203e73bdefe7741557cff062431417bdbc1a980b0bde02d24b6cea743b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "During the pre-training stage of a foundation model, which data characteristic is most critical for robust language understanding?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High-quality labeled task-specific data",
      "B": "Large, diverse unlabeled text corpora",
      "C": "Balanced class labels across tasks",
      "D": "Human-annotated evaluation sets"
    },
    "explanation": "Pre-training uses massive, diverse unlabeled corpora to learn general language patterns."
  },
  {
    "id": "70cb2d6b66808b7a99cb380e1be38bd82d756a2968b7591f40ff205e6784e143",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "A customer wants to adapt a general LLM to legal domain text. Which lifecycle stage accomplishes this specialization?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Pre-training",
      "B": "Fine-tuning",
      "C": "Evaluation",
      "D": "Inference"
    },
    "explanation": "Fine-tuning adjusts a pre-trained model to a domain by training on domain-specific data."
  },
  {
    "id": "9225eef3eebac3a97ce9d2a3ea1a3cfa2e7ebe4348c2401c999e7738895f98e1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "After fine-tuning, which evaluation approach ensures the model meets business objectives beyond perplexity metrics?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "BLEU score against reference corpus",
      "B": "Perplexity on validation data",
      "C": "Human-in-the-loop user satisfaction surveys",
      "D": "Model throughput benchmarks"
    },
    "explanation": "Human evaluations assess real-world satisfaction and alignment with business goals."
  },
  {
    "id": "3a3d4e23cd262eddf6c7f937e2fb0a4bf977a19914f2c16330436c19cc62a395",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "In deploying a foundation model for real-time inference at scale, which strategy optimizes cost and latency?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Hosted API with autoscaling",
      "B": "Self-hosted on under-provisioned GPU",
      "C": "Batch inference on EC2 Spot Instances",
      "D": "Single-instance serving"
    },
    "explanation": "Managed APIs with autoscaling adjust resources to demand, balancing latency and cost."
  },
  {
    "id": "15ba9ec84e662e169dd0af9d4cd04d9b51f4e92512d182f77a9b58d2a213f967",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "To continuously improve a production chatbot, which feedback mechanism should be integrated into the model lifecycle?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Static evaluation dataset",
      "B": "Manual hyperparameter tuning only",
      "C": "User interaction logs for retraining",
      "D": "Single-shot inference"
    },
    "explanation": "Ingesting user logs enables retraining on real use-cases, improving performance over time."
  },
  {
    "id": "1825decbd400ff3374a5ca06fa7454678552025ffa779bbe9a65ded5fab84226",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "Why do transformer models add positional embeddings to token embeddings?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "To encode token frequency",
      "B": "To enable the model to recognize sequence order",
      "C": "To reduce embedding dimension",
      "D": "To improve normalization"
    },
    "explanation": "Positional embeddings inject information about token positions, allowing sequence order to be modeled."
  },
  {
    "id": "5f1656a8d73f47b33afd0269939e330f5b5f0b7e2de1588437c5446baf6b111c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "In a transformer, what semantic role does the token embedding vector serve?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Positional information",
      "B": "Semantic representation of the token",
      "C": "Attention weight storage",
      "D": "Output logits for prediction"
    },
    "explanation": "Token embeddings map discrete tokens into continuous semantic vectors for the model to process."
  },
  {
    "id": "01d402c0514986330977484706a5448b5e6aeb08a61c75395360f2814be4eb72",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "A RAG pipeline uses 512-token chunks with 50% overlap, causing redundancy and high storage. How to optimize chunking?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase chunk size to 1024 tokens",
      "B": "Reduce overlap to 10\u201320%",
      "C": "Convert tokens to characters",
      "D": "Use a single chunk per document"
    },
    "explanation": "Reducing overlap lowers redundant embeddings while maintaining context continuity."
  },
  {
    "id": "f994dbc46288097da3b958fcce81bea33265fd7ee7df4ea676076197f182fc7d",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "Why normalize embedding vectors before computing cosine similarity?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "To remove directional bias",
      "B": "To standardize magnitude to unit length and isolate direction",
      "C": "To reduce embedding dimension",
      "D": "To anonymize data"
    },
    "explanation": "Normalization scales vectors to unit length so cosine similarity reflects only direction."
  },
  {
    "id": "c201675c0149f878a64d8f74383aa8cdbba6959efa78d5a1357e7932685b5894",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "Multi-head attention in transformers enables the model to:",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase network depth",
      "B": "Attend to information from multiple representation subspaces",
      "C": "Reduce parameter count",
      "D": "Perform convolution"
    },
    "explanation": "Each head learns attention in a different subspace, capturing diverse relationships."
  },
  {
    "id": "78624594aa31c0ec7c2ebc3b2c661b142841975ed16e55fe09a0fe44c3613a2c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "In diffusion models, what is the primary purpose of the noise schedule during training?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "To initialize model weights randomly",
      "B": "To corrupt data gradually so the model learns denoising",
      "C": "To augment with synonym replacements",
      "D": "To enforce sparsity"
    },
    "explanation": "A noise schedule progressively adds noise, training the model to reverse this process."
  },
  {
    "id": "b64bcf55e9a6b03f30f310ba254d60255f9ebcab3690af1c0148c78288419143",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "Which trait distinguishes multi-modal foundation models from single-modal ones?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Processing only text data",
      "B": "Handling and generating multiple data types like text and images",
      "C": "Having fewer parameters",
      "D": "Using RNN architecture"
    },
    "explanation": "Multi-modal models process and relate different modalities (e.g., text, images) in a unified framework."
  },
  {
    "id": "69a839067f7e18ab99afdcbc87b43d3210eccd1008cb6aea3c6ded06060c89fd",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "Large semantic gaps between chunks cause RAG retrievals to return irrelevant passages. How improve chunking?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use fixed-size byte-level chunks",
      "B": "Chunk at paragraph boundaries for semantic coherence",
      "C": "Remove stop words before chunking",
      "D": "Increase embedding dimension"
    },
    "explanation": "Chunking by logical paragraphs preserves coherent context and improves retrieval relevance."
  },
  {
    "id": "6b1e647c7bfc89d50fc78ce5f83b8bd9f240eca930617348a4bdf7142610261f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "The prompt includes \"Answer in bullet points\" to influence format. Which prompt engineering concept does this illustrate?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Context window",
      "B": "Instruction prompt",
      "C": "Negative prompt",
      "D": "Latent space manipulation"
    },
    "explanation": "An instruction prompt explicitly guides the model\u2019s response format or behavior."
  },
  {
    "id": "a59e3d3c6477494918a1b1a8c8581c86e790d06fbef432572af8208bcb49ced5",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "In image generation via diffusion, how does a negative prompt affect the output?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Specifies elements the model should avoid generating",
      "B": "Specifies the color palette",
      "C": "Sets the token length",
      "D": "Controls the random seed"
    },
    "explanation": "Negative prompts instruct the model to exclude certain concepts from generated outputs."
  },
  {
    "id": "18c8daf540bcc457a57619793d6adbf1ace7762f5af16ef9bd0260112a9b1492",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "What is a drawback of excessively increasing embedding dimensionality beyond necessary semantic space?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increased inference speed",
      "B": "Higher storage and computation costs with diminishing returns",
      "C": "Decreased semantic differentiation",
      "D": "Automatic overfitting avoidance"
    },
    "explanation": "Too many dimensions raise resource costs while offering minimal semantic gains."
  },
  {
    "id": "6bda21b040d869041a371a76ab047e512944b35f70ae1e88ba27e810e23dde02",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "Before fine-tuning, selecting a representative dataset prevents bias. Which data characteristic is most important?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High volume of identical examples",
      "B": "Diversity covering expected use-case distributions",
      "C": "Exclusively positive examples",
      "D": "Uniform input lengths"
    },
    "explanation": "Diverse data reflecting real use-cases ensures the model generalizes fairly and accurately."
  },
  {
    "id": "0c6c692e580e5bd78e4a47d6b0cbceb0833ab021a50591c1e9135b4a94f27f4f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "A team compares Model A (lower perplexity on legal texts) and Model B (smaller and faster). For production legal advice generation, which should they choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model A, because domain performance outweighs latency",
      "B": "Model B, because speed is primary",
      "C": "Model B, to reduce cost",
      "D": "Neither; build a custom model"
    },
    "explanation": "In sensitive domains like legal advice, domain accuracy is critical, even at higher cost or latency."
  },
  {
    "id": "9d3a236454c197d1f2c56192dce7e6b3940830845d7bd6c89088c0df64ef813c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "Which foundation model type is primarily trained to predict the next token in a sequence?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Encoder-only",
      "B": "Decoder-only autoregressive",
      "C": "Encoder-decoder",
      "D": "Diffusion"
    },
    "explanation": "Decoder-only autoregressive models are trained to predict subsequent tokens given prior context."
  },
  {
    "id": "c67f36ab47c1147d1099c83ff35b039388caf92ebcd47106312d87807fed4a33",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "After deployment, hallucinations increase. Which lifecycle stage should address this by incorporating real user feedback?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Pre-training",
      "B": "Fine-tuning with reinforcement learning from human feedback",
      "C": "Evaluation",
      "D": "Inference"
    },
    "explanation": "RLHF fine-tuning uses collected feedback to align model outputs with user expectations, reducing hallucinations."
  },
  {
    "id": "89ec4647b609fee77fb1439fab0072158f3eb02dfb8fbf039f0aeb041d9d8197",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A healthcare startup wants to use a generative AI model to draft patient-friendly explanations of complex medical reports. They need consistent, accurate outputs and must minimize hallucinations. Which limitation of generative AI poses the greatest risk, and what mitigation strategy best addresses it?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Nondeterminism; use temperature=1.0 for more diverse outputs",
      "B": "Hallucinations; implement retrieval-augmented generation (RAG) with a curated medical knowledge base",
      "C": "Interpretability; generate detailed attention maps for clinical review",
      "D": "Latency; host the model on GPU-accelerated instances"
    },
    "explanation": "Hallucinations are a primary risk in medical text generation. RAG constrains outputs to verified documents, reducing invented facts while maintaining accuracy."
  },
  {
    "id": "13a3cd2450c74715a39e0bb8bd46af4f80bc8cce7a9f423b4c6a056de166b4f5",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A marketing agency evaluates a general-purpose image-to-text foundation model to automate alt-text generation. They find outputs vary widely in detail level. Which generative AI characteristic explains this variability, and how can they control it?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model size; switch to a smaller model for consistency",
      "B": "Pre-training data bias; fine-tune on balanced captions",
      "C": "Nondeterminism; lower the temperature hyperparameter",
      "D": "Tokenization artifacts; adjust max token length"
    },
    "explanation": "Temperature controls randomness: lowering it yields more deterministic, consistent descriptions from the same prompt."
  },
  {
    "id": "592637714477f8945e31703cf3c773677a36c29d67151c10d2bdd5bed67ad61f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A finance firm wants to generate regulatory summaries from legal documents but must track source citations. Which limitation of generative AI complicates this requirement, and which AWS service can help maintain provenance?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Latency; use Amazon Q for faster inference",
      "B": "Model drift; use SageMaker Model Monitor for drift detection",
      "C": "Scalability; use Bedrock with auto-scaling",
      "D": "Lack of source traceability; use SageMaker Clarify and Amazon Q\u2019s source citation feature"
    },
    "explanation": "Generative models don\u2019t inherently cite sources. Amazon Q integrates retrieval with citation tracking. SageMaker Clarify can audit data provenance."
  },
  {
    "id": "c72018514096f617502a50b98f333719ddbca63b570131eb39e85679bb59193e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A media company must generate localized marketing copy in 10 languages using a single foundation model. They are concerned about cross-lingual performance. Which evaluation metric best quantifies business value for multilingual consistency?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "BLEU score across language pairs",
      "B": "Latency percentiles for each language",
      "C": "ROUGE-L on English only",
      "D": "Perplexity on the training corpus"
    },
    "explanation": "BLEU measures translation quality across languages; it directly reflects consistency and accuracy in localization tasks."
  },
  {
    "id": "dd7229a08a13fd0dd7b8e12a46a7ceb8b823875ddfcc1ceeb1ce96c9ea5fea54",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A logistics provider uses a generative AI chatbot to answer customer queries about shipment status. They observe the model sometimes fabricates tracking events. Beyond hallucinations, what model limitation is at play, and how can they detect and reduce it?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Low throughput; deploy on higher-capacity endpoints",
      "B": "Data privacy; enable encryption at rest",
      "C": "Nondeterminism; set a low temperature and implement post-generation validation against the tracking database",
      "D": "Fine-tuning drift; retrain weekly"
    },
    "explanation": "Nondeterminism causes variability in outputs; reducing temperature increases determinism, and validating outputs against real-time data catches fabrications."
  },
  {
    "id": "7fddffbb2316c6794e6e32d3958dcb162c6f6dfb4e693566c938481e8d8b07ce",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A retail chain wants to generate personalized product recommendations using a generative model. They need to measure business impact beyond model accuracy. Which metric should they prioritize, and why?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "F1 score; balances precision and recall",
      "B": "Click-through rate (CTR); reflects actual user engagement and conversion potential",
      "C": "BLEU score; measures textual similarity",
      "D": "Inference latency; ensures real-time suggestions"
    },
    "explanation": "CTR captures how often recommended items are clicked, directly measuring user engagement and likely revenue impact."
  },
  {
    "id": "2ff24726328c0dd714d836ed96e8bcc8ac255709feae65497d4006468a65fdbc",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A legal tech vendor uses a foundation model for contract clause generation. Clients require deterministic outputs for auditing. Which setting adjustment will improve auditability without fine-tuning the model?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase max token length",
      "B": "Enable logit biasing for key terms",
      "C": "Switch to few-shot prompting",
      "D": "Set temperature to near zero for deterministic behavior"
    },
    "explanation": "Lowering temperature reduces output randomness, producing consistent, repeatable text useful for audits."
  },
  {
    "id": "b381ae052d5c5da9fb684eb67626108a8f2da2ee9b70d19c8b14e657bf2e6fe5",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A publisher uses generative AI to auto-create article summaries but must ensure summaries are not misleading. Which limitation most threatens content fidelity, and what technique best mitigates it?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Hallucinations; apply RAG with source document retrieval",
      "B": "Latency; batch process summaries",
      "C": "Scalability; use Bedrock with auto-scaling",
      "D": "Embedding drift; refresh embeddings weekly"
    },
    "explanation": "Hallucinations cause incorrect summaries. RAG anchors generation to actual source text, preserving fidelity."
  },
  {
    "id": "37d9a59cfed18c8d4344f9f47c861cc84e7d6c2247fcdd4e5a41899b72b34286",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A gaming company wants to leverage a generative AI agent to provide in-game NPC dialogue. They need balance between creativity and storyline consistency. Which generation parameter and approach best achieve this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase top-p sampling; use zero-shot prompts",
      "B": "Lower temperature; fine-tune on story scripts",
      "C": "Increase repetition penalty; use chain-of-thought prompts",
      "D": "Use few-shot prompting; disable system messages"
    },
    "explanation": "Lower temperature controls creativity vs consistency. Fine-tuning on story scripts aligns the model to game lore and style."
  },
  {
    "id": "6c44db9e1a97fde0aa1d0057db8366590399115139b093f276c947568c586234",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A biotech company needs to generate molecular synthesis protocols. They require high accuracy and domain adherence. Which model limitation poses the greatest challenge, and how can they address it on AWS?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Latency; deploy on CPU instances",
      "B": "Cost; use Spot Instances",
      "C": "Hallucinations; use RAG with a curated protocol database in Amazon Neptune",
      "D": "Scalability; shard across multiple SageMaker endpoints"
    },
    "explanation": "Hallucinations can produce invalid protocols. RAG with a structured database ensures generation uses verified procedures."
  },
  {
    "id": "5ad6ed05d347ddbd868891a0218ee423f15b7fb540b57f2332a63e8191012900",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A telecom provider wants to compare two generative AI offerings for automated script generation. They require objective performance evaluation. Which combination of metrics gives the most comprehensive assessment?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Perplexity and latency",
      "B": "BLEU score and model size",
      "C": "ROUGE score and temperature",
      "D": "BLEU/ROUGE for quality and inference latency for operational feasibility"
    },
    "explanation": "BLEU and ROUGE assess text quality; latency ensures the solution meets real-time generation requirements."
  },
  {
    "id": "346558a0e2fd0c9e565b7764c3027b170964a82db7d8d4e000714fece9495a06",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A financial advisory service uses a foundation model to draft personalized investment advice. They must comply with regulatory transparency requirements. Which limitation challenges compliance, and what AWS tool helps ensure governance?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Scalability; use Bedrock\u2019s auto-scaling",
      "B": "Lack of explainability; generate model cards with SageMaker Model Cards and use Amazon A2I for human review",
      "C": "Throughput; optimize GPU usage",
      "D": "Embedding drift; retrain embeddings frequently"
    },
    "explanation": "Opaque model decisions hinder regulatory compliance. SageMaker Model Cards document model details, and A2I provides human validation."
  },
  {
    "id": "1de0311c38cf5379c5f0cc61a06e7941c643dca970889854cbe0ad5848144299",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "An online education platform plans to generate practice exam questions. They need high variety but must avoid factually incorrect content. Which compromise and approach best fit?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Moderate temperature and RAG with curated question bank",
      "B": "High temperature and no retrieval",
      "C": "Few-shot prompting only",
      "D": "Zero-shot prompting and maximum tokens"
    },
    "explanation": "Moderate temperature adds variation, while RAG ensures content is grounded in the verified question bank to avoid false facts."
  },
  {
    "id": "2317fc3c94f1c7b0ddba5ff24145b203d72a00ff6647e6c136cdc50720aadeea",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A news organization uses generative AI to summarize global headlines. They must measure trustworthiness of summaries. Which metric and process best quantify hallucination rates?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ROUGE-L against reference summaries",
      "B": "BLEU score against multilingual corpora",
      "C": "Human evaluation of hallucination frequency with Amazon A2I",
      "D": "Perplexity on news corpus"
    },
    "explanation": "Automatic metrics don\u2019t detect hallucinations reliably. Amazon A2I human-in-the-loop review quantifies factual errors."
  },
  {
    "id": "1ed316844d734cf5fac87bd4504d870d60f06281562586adc1e4d44476c3abcf",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A supply-chain firm wants generative AI to predict demand patterns and draft procurement plans. They need to measure solution ROI. Which business metric best captures value?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "F1 score on demand labels",
      "B": "Inference latency",
      "C": "Model throughput",
      "D": "Inventory turnover improvement percentage"
    },
    "explanation": "Inventory turnover improvement directly measures how the AI-driven plans optimize stock and improve capital efficiency."
  },
  {
    "id": "4c8719957c3f9f5daae0dc2035aad35e39210e6f3f4efb4320f07f6a5dddcc28",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A hotel chain uses generative AI chatbots for booking assistance. They observe nondeterministic price quote phrasing confusing customers. Which adjustment most effectively reduces variability?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use few-shot prompts",
      "B": "Lower temperature to near zero",
      "C": "Increase max token length",
      "D": "Enable top-p sampling"
    },
    "explanation": "Lowering temperature reduces randomness, leading to consistent phrasing of price quotes."
  },
  {
    "id": "17fa8c0d6c31e5cfbbdfa528c4fa3b377b45da527b02fd12cc3536f58c2c7326",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A media streaming service wants to generate personalized show summaries. They must avoid content bias favoring popular genres. Which strategy minimizes bias in generative outputs?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Fine-tune on a balanced genre dataset",
      "B": "Increase temperature to allow diversity",
      "C": "Use chain-of-thought prompting",
      "D": "Implement RAG with popular titles only"
    },
    "explanation": "Fine-tuning on balanced data ensures equal representation across genres, reducing bias toward popular ones."
  },
  {
    "id": "613dcbca889486adb587fe16c1be6abbeb9077ed4390fc4b42ae90a883a9e736",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A legal department needs deterministic boilerplate clauses from a foundation model. They must evaluate long-form outputs for consistency. Which generation settings and evaluation metric combination should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High temperature; BLEU",
      "B": "Zero-shot; perplexity",
      "C": "Low temperature; ROUGE score compared to standard templates",
      "D": "High top-p; F1 score"
    },
    "explanation": "Low temperature yields consistent text; ROUGE against approved templates measures textual fidelity and consistency."
  },
  {
    "id": "aa335b56299eb588f4b2734a0d8cd8a205d5e4c1f47ea0653f5eb73cfbc6d7dd",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A biotech startup uses generative AI to propose experimental protocols. They require quantifiable safety metrics. Which model limitation and metric pairing best aligns with safety requirements?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Latency; CPU utilization",
      "B": "Scalability; throughput",
      "C": "Interpretability; attention visualizations",
      "D": "Hallucinations; human-rated protocol validity with Amazon A2I"
    },
    "explanation": "Hallucinations risk unsafe protocol suggestions. Human review via A2I rates validity, directly measuring safety compliance."
  },
  {
    "id": "e7deaa7b1a56ed265286cb014e2a9d71c6d2a5cbd2be1087a66cf0effede2e33",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A financial app uses a foundation model for fraud alert explanations. They need explainability for regulatory audits. Which AWS feature and model property best support this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Bedrock agents; high throughput",
      "B": "SageMaker Model Cards; transparent, white-box model",
      "C": "Amazon Q; token-based billing",
      "D": "PartyRock; multimodal inputs"
    },
    "explanation": "Model Cards document model lineage and behavior. Choosing a transparent model enhances auditability and explainability."
  },
  {
    "id": "74e961d078a7dc731f39365658271f2fd71a7f95cdd24674fd96595c8a6fc840",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A logistics AI team wants to generate route summaries from GPS data. They face variable output lengths and missing details. Which parameter modification and technique best address both issues?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set max length and implement prompt templates with required fields",
      "B": "Increase temperature and use chain-of-thought",
      "C": "Enable logit bias for key terms",
      "D": "Use few-shot prompting without templates"
    },
    "explanation": "Limiting max length controls size; prompt templates enforce inclusion of mandatory fields, ensuring detail completeness."
  },
  {
    "id": "9e3cd4bdc4ccd7a7b7256b7fffcccd0ed3a4d56003fd00018b2ca3b5e601bb7a",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A retail AI solution generates product descriptions; the business cares about ARPU uplift. Which performance metric should they track, and why?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "F1 score on category labels",
      "B": "Perplexity on description corpus",
      "C": "Average revenue per user (ARPU) before vs after deployment",
      "D": "Inference latency"
    },
    "explanation": "ARPU directly measures revenue impact per user, aligning model output quality with financial performance."
  },
  {
    "id": "276af93050dc9f96a59eab1bb76c0ead4898fcfc70fb62938ed2699ec3f15f27",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A car manufacturer uses generative AI to draft maintenance manuals. They require minimal hallucinations in technical instructions. Which approach best ensures accuracy?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High temperature with few-shot examples",
      "B": "RAG with an indexed manual database in OpenSearch",
      "C": "Chain-of-thought prompting",
      "D": "Fine-tune on generic technical corpus"
    },
    "explanation": "RAG using an indexed, authoritative manual database grounds generation in correct technical content, eliminating hallucinations."
  },
  {
    "id": "20792bc88f95bdbaee703938c69ec207d1b0cf9a4ee9e701e46a7bf1fd41eb7e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "An e-commerce site uses generative AI for dynamic ad copy. They want to optimize click-through and avoid repetitiveness. Which combination of settings and metrics is optimal?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Temperature 1.0; BLEU",
      "B": "Temperature 0.0; perplexity",
      "C": "Top-p = 0.3; ROUGE",
      "D": "Moderate temperature and top-p; CTR and novelty score"
    },
    "explanation": "Moderate sampling settings balance novelty and coherence; tracking CTR (engagement) and a novelty metric prevents repetitive copy."
  },
  {
    "id": "e0be9c0ed96177c70acd561c96c88780b2aa2e381a3b0b43b29c4f0350b7a39b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A pharmaceutical company uses generative AI to summarize clinical trial reports. They need to minimize hallucinations and measure summary relevance. Which evaluation approach and metric should they implement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "RAG with human review via Amazon A2I and ROUGE-L against expert summaries",
      "B": "Zero-shot prompting and BLEU",
      "C": "High temperature generation and perplexity",
      "D": "Chain-of-thought prompting and F1 score"
    },
    "explanation": "RAG anchors to real reports; human-in-the-loop review quantifies hallucinations and ROUGE-L measures relevance to expert summaries."
  },
  {
    "id": "d4eeb7b525fe021609323c5475b1dd214515ad3ca88a66e5610a13cf69e5ff2f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A financial news platform automates article translation. They need to track cross-domain accuracy for emerging topics. Which business metric and process best capture model performance drift?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Average BLEU across domains",
      "B": "Perplexity on all content",
      "C": "ROUGE on English only",
      "D": "Periodic BLEU evaluation on emerging-topic test set with Model Monitor alerts"
    },
    "explanation": "Periodic BLEU on a domain-specific test set detects drift in new topics; Model Monitor alerts ensure timely retraining."
  },
  {
    "id": "1e39455690d1411c9760364976b7b6ac2dfade7e9148c31d43357758ea6dce5a",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A broadcaster uses generative AI to script news intros. They observe unintended bias in phrasing political content. Which root cause and mitigation best resolve this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Pre-training data bias; fine-tune on balanced political corpora",
      "B": "Hallucinations; use RAG",
      "C": "Nondeterminism; lower temperature",
      "D": "Scalability; use higher throughput endpoints"
    },
    "explanation": "Bias stems from pre-training data. Fine-tuning on a balanced dataset corrects model bias and yields neutral phrasing."
  },
  {
    "id": "8a3da8b1e707a80618f098d42cc1eaa9cb8ac9b1aa2f1f3058c886c575568207",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A travel app generates destination descriptions. They want to maximize user engagement but must avoid misleading content. Which generation strategy and metric pairing achieves this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High temperature with ROUGE",
      "B": "Low temperature with perplexity",
      "C": "Moderate temperature with CTR and human-rated truthfulness",
      "D": "Top-p sampling with BLEU"
    },
    "explanation": "Moderate temperature encourages creative but controlled text. Tracking CTR measures engagement and truthfulness ratings detect misleading content."
  },
  {
    "id": "ebf371a7cebe3b95d3e6d9fad363e9cbadd6f22ab69622bca87a2bda6fa8676a",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A manufacturing firm uses generative AI for maintenance logs summarization. They need to detect out-of-distribution inputs causing model errors. Which AWS feature and process helps guard against this limitation?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Q; source citations",
      "B": "SageMaker Model Monitor with custom OOD detection rules",
      "C": "Amazon A2I for hallucination review",
      "D": "Bedrock agents for orchestration"
    },
    "explanation": "Model Monitor can detect when inputs fall outside the training distribution, triggering alerts or human review before inference."
  },
  {
    "id": "6296dee9285fdab9db1c0a552493c25a4a9245a186fe883b196ac40dee897ab3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A retailer uses a foundation model to generate email marketing subject lines. They want to A/B test variations but avoid sacrificing deliverability through spammy language. Which evaluation metric and prompt technique combination best balances creativity and compliance?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High temperature; BLEU",
      "B": "Zero-shot; perplexity",
      "C": "Few-shot; ROUGE",
      "D": "Moderate temperature; human-rated spam score and open rate"
    },
    "explanation": "Moderate sampling ensures creative copy, while human-rated spam scoring and tracking open rates measure compliance and effectiveness."
  },
  {
    "id": "378cd68e7be12f18e2598a58102087a77a964116337b476ae92848daa2171ee2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A legal AI team must generate redacted contract summaries. They require strict accuracy and no accidental disclosures. Which generative AI limitation poses the biggest risk, and how can they mitigate it?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Hallucinations; use RAG with redaction filter and human review via A2I",
      "B": "Latency; optimize GPU endpoints",
      "C": "Scalability; shard inference across endpoints",
      "D": "Throughput; increase batch size"
    },
    "explanation": "Hallucinations can reintroduce redacted content. RAG with filters and human review ensures no sensitive data is leaked."
  },
  {
    "id": "c943ef2a9fe2a332e722c902c277e54267ba4a47d1377ed3090e7bfc3cb1eb9e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A SaaS analytics company uses generative AI to write SQL queries from natural language. They notice queries sometimes reference nonexistent tables. Which model limitation is this, and what strategy best addresses it?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Latency; improve instance type",
      "B": "Cost; switch to Spot Instances",
      "C": "Hallucinations; employ RAG with schema retrieved from live database",
      "D": "Deployment complexity; use Bedrock"
    },
    "explanation": "Model hallucinations produce invalid queries. RAG with live database schema ensures generated SQL matches actual tables."
  },
  {
    "id": "a4db3e70a5e1efd7c51944272b2c1758a04bb5302796da1901e14c0e2a01ba5a",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A startup needs to prototype a document summarization application using a large language model without managing infrastructure. They require minimal latency and no fine-tuning. Which AWS service or feature best meets these requirements?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "SageMaker JumpStart custom training job",
      "B": "Amazon Bedrock managed foundation model endpoint",
      "C": "Amazon Comprehend extractive summarization API",
      "D": "Amazon SageMaker Real-Time Inference with a self-hosted open-source model"
    },
    "explanation": "Amazon Bedrock provides fully managed, low-latency access to foundation models without customer-managed infrastructure. SageMaker JumpStart custom jobs require management and fine-tuning, Comprehend is extractive only, and self-hosting imposes infrastructure overhead."
  },
  {
    "id": "58056c160f67b8fe26ea3c73d0825b8e2e9703fcbab623a7de001513980f4ea7",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A financial services company must run high-throughput generative chat sessions during market hours. They want predictable pricing per token and need to minimize cost under heavy load. Which pricing model should they choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker JumpStart per-instance hourly pricing",
      "B": "Bedrock usage-based with variable markup",
      "C": "Bedrock token-based charges with volume discounts",
      "D": "Amazon Q per-query fixed fee"
    },
    "explanation": "Bedrock\u2019s token-based model with volume discounts yields predictable per-token pricing and lower costs at scale. JumpStart hourly instances may idle and incur unused hours; Amazon Q\u2019s per-query fee lacks token granularity."
  },
  {
    "id": "f669c26ccca6e881c2347be18163a8fddbb82af4d6a1854b1bdd3a255f8e2756",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A healthcare application uses a confidential medical knowledge base with RAG via embeddings. They need a HIPAA-eligible, VPC-isolated embedding store with low latency. Which AWS service combination should they use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon OpenSearch Service with public endpoints",
      "B": "Amazon RDS for PostgreSQL without encryption",
      "C": "Amazon Neptune with VPN-only connectivity",
      "D": "Amazon DocumentDB in VPC with encryption at rest and in transit"
    },
    "explanation": "DocumentDB in a VPC with encryption supports HIPAA compliance and low latency for embedding storage. OpenSearch public endpoints and unencrypted RDS fail compliance; Neptune adds complexity and lacks managed embedding integration."
  },
  {
    "id": "277c84bce9664dbb7e0a5162ea7e88a9f233c18b822d036bfad83cff5ec33c79",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A media company wants to generate images with a diffusion model hosted in Bedrock. They require burstable throughput up to 50 requests/sec during live events, and will pay only when models run. Which service feature should they use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Bedrock serverless inference with auto-scaling",
      "B": "Bedrock provisioned capacity",
      "C": "SageMaker Hosting with dedicated GPU instances",
      "D": "Amazon EC2 Spot Instances running OpenAI API proxy"
    },
    "explanation": "Bedrock serverless inference auto-scales to bursts and charges only per request. Provisioned capacity incurs fixed cost, SageMaker hosting requires instance management, and EC2 Spot Instances add operational complexity."
  },
  {
    "id": "f0d6eb5d72d7d70ef482b30fe3f033031f7825cab474d99e582379819efa2157",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A regulated enterprise must maintain model invocation logs and prompt inputs for audit, enforce encryption in transit, and isolate model endpoints. Which AWS configuration satisfies all requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Bedrock public endpoint with CloudTrail enabled",
      "B": "Bedrock VPC endpoint with CloudTrail Data Events and TLS enforced",
      "C": "SageMaker JumpStart with S3 logging only",
      "D": "Amazon Q over HTTPS without VPC controls"
    },
    "explanation": "Bedrock VPC endpoints with CloudTrail Data Events capture inputs and outputs, enforce TLS, and isolate traffic. Public endpoints or S3-only logging do not provide full isolation or comprehensive audit."
  },
  {
    "id": "e25582d92551d2f19f4ea28c53f3457ef92ac0abb7635dcb8779258ef24b2fc5",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A developer wants to fine-tune a foundation model for domain-specific language but minimize compute cost. They only need a few examples and can tolerate higher latency. Which approach balances cost and performance?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Full fine-tuning in SageMaker with p3.16xlarge",
      "B": "Bedrock fine-tuning with RLHF",
      "C": "SageMaker Ground Truth data labeling",
      "D": "In-context few-shot prompting on Bedrock foundation model"
    },
    "explanation": "In-context few-shot prompting on Bedrock avoids fine-tuning fees and heavy compute, trading higher latency for much lower cost. Full fine-tuning is expensive, RLHF adds complexity, and Ground Truth only labels data."
  },
  {
    "id": "eaf9d4412e5ea8f9af54647e6c1a8ed5eaf35f06dbb70f1f1e99bdbec7937cd2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A startup evaluates embedding storage options for RAG workflows. They need sub-50 ms nearest neighbor search latency at 100 qps and minimal management overhead. Which service is optimal?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon OpenSearch Service with k-NN plugin",
      "B": "Amazon Aurora PostgreSQL with pgvector extension",
      "C": "Amazon DynamoDB with Lambda custom search",
      "D": "Amazon Timestream optimized for time-series"
    },
    "explanation": "OpenSearch k-NN plugin provides low-latency vector search at scale with managed service. Aurora pgvector incurs higher latency and management; DynamoDB+Lambda adds infra overhead; Timestream is for time-series."
  },
  {
    "id": "e9f370bec4294bae4155cabe61f2450af27d3150fd343be73d78848c593f7b99",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "An application requires embedding generation and immediate storage. To minimize end-to-end latency, which invocation pattern should be used?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Invoke Bedrock embedding API, store to S3 via SDK next",
      "B": "Batch Bedrock embeddings, process in EMR, then S3",
      "C": "Invoke Bedrock embedding API directly from Lambda writing to Amazon DynamoDB",
      "D": "Precompute embeddings offline and store in Aurora"
    },
    "explanation": "A Lambda that directly invokes Bedrock embeddings and writes to DynamoDB yields minimal latency and overhead. Batch or offline methods add delay; S3 introduces higher write latency for real-time needs."
  },
  {
    "id": "e2c2fe4c3c32b226453a88b07db242d38fb836a4339c88974e54d7b03256b6a2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A legal firm must deploy a generative Q&A chatbot using Amazon Q. They require data residency in us-east-2, secure access, and usage logs. Which steps meet all requirements? (Select TWO.)",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure Amazon Q domain in us-east-2",
      "B": "Enable VPC endpoints and PrivateLink for Q",
      "C": "Use Comprehend Q&A instead of Amazon Q",
      "D": "Use public Q endpoints with IP restriction"
    },
    "explanation": "Configuring the domain in the required region and enabling VPC endpoints/PrivateLink secures and confines data. Comprehend Q&A is different service and public endpoints lack full isolation."
  },
  {
    "id": "d053da49adc772909ea963066e86b890c77e5b207e13cf63574aa316d40d174e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A retail company is building a conversational shopping assistant using AWS Bedrock. The assistant must handle context windows up to 1,500 tokens, maintain average response latency under 200 ms, and minimize per-token cost. Which foundation model should the company choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Anthropic Claude 2 (max 100k tokens, ~300 ms latency, high per-token cost)",
      "B": "Amazon Titan Text (max 2,048 tokens, ~80 ms latency, lowest per-token price)",
      "C": "Cohere Command (max 2,048 tokens, ~250 ms latency, moderate cost)",
      "D": "Stability Text 2 (max 1,024 tokens, ~90 ms latency, lowest cost)"
    },
    "explanation": "Titan Text meets the 1,500-token requirement, offers sub-200 ms latency, and has the lowest per-token price. Stability Text 2\u2019s context is too small; Claude 2 has higher latency and cost; Cohere is slower."
  },
  {
    "id": "9808dd1e5d57576b308679d0e24d77b1095eedeb7876842ab9d98d7995f2788d",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A healthcare analytics team needs to generate detailed patient summaries from long clinical notes (up to 8,000 tokens). Which approach best accommodates this requirement while controlling cost?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a single in-context prompt with a Titan Text model.",
      "B": "Fine-tune Amazon Titan Text on clinical notes.",
      "C": "Implement Retrieval Augmented Generation (RAG) with chunked embeddings and Amazon Bedrock.",
      "D": "Switch to Anthropic Claude 2 without any retrieval."
    },
    "explanation": "RAG with chunked embeddings supports 8,000 tokens, reduces hallucinations, and avoids expensive fine-tuning. Titan\u2019s window is too small; Claude 2 is cost-prohibitive; fine-tuning is overkill."
  },
  {
    "id": "26a22842a2e214017ba742726ae36507c22562d211c28adef2f7be591f231a57",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "An engineering firm requires multimodal inference on text and diagrams. Which AWS foundation model selection criteria is most critical?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Support for multimodal input (text+image)",
      "B": "Lowest possible per-token cost",
      "C": "Maximum context length",
      "D": "Highest temperature variability"
    },
    "explanation": "Multimodal inference demands a foundation model trained on both text and image inputs. Cost, context length, and temperature are secondary if the model lacks multimodal capability."
  },
  {
    "id": "79e93785437aacaf38538f0c01a0bc0be2069967421ed4409eab38a6e244d7fb",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A media company must generate video captions with consistent style and minimal hallucinations. Which inference parameter change will most reduce hallucinations?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature from 0.2 to 0.8",
      "B": "Decrease temperature from 0.8 to 0.2",
      "C": "Increase max tokens to 1,000",
      "D": "Remove stop sequences"
    },
    "explanation": "Lowering temperature reduces randomness and hallucinations. Increasing max tokens or removing stop sequences does not address hallucinations; higher temperature increases them."
  },
  {
    "id": "a1c561ba9cfc5f22fd82508825a75b6dc797684d497396fc821f0d72de4d6d17",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A startup wants to store vector embeddings for semantic search of documents with occasional schema changes and dynamic updates. Which storage service is best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon OpenSearch Service",
      "B": "Amazon RDS for PostgreSQL",
      "C": "Amazon Neptune",
      "D": "Amazon DocumentDB"
    },
    "explanation": "Amazon Neptune provides graph storage optimized for dynamic, schema-flexible vector embeddings and efficient nearest-neighbor search. OpenSearch is less flexible for dynamic schemas; RDS and DocumentDB aren\u2019t optimized for graph vector queries."
  },
  {
    "id": "5ec207cf5ef538252c3e084ec33dcd601addb2ef643cb8d852f138c2b533ea1b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A financial regulator uses a foundation model to answer compliance queries against a static regulation corpus. The corpus is ~50 GB. What is the most cost-effective way to ensure answers reflect the latest regulations?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Fine-tune the model every time regulations update",
      "B": "Use a large context prompt of full documents",
      "C": "Use in-context learning with all regulations loaded",
      "D": "Implement RAG with embeddings in Amazon OpenSearch and refresh index on updates"
    },
    "explanation": "RAG with OpenSearch embeddings allows incremental updates of a 50 GB corpus, ensures currency, and is more cost-effective than repeated fine-tuning or huge context prompts."
  },
  {
    "id": "1b7459d233bba6f6d19734d66daa62f1b308a7fe8c75c0cf652db55a6861d3d0",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "An AI team must adapt a foundation model to a niche legal language using only 1,000 labeled examples and limited budget. Which customization approach balances cost and performance?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Full pre-training on legal text",
      "B": "RLHF with human annotators",
      "C": "Instruction-tuning (fine-tuning) on the 1,000 examples",
      "D": "In-context learning without any fine-tuning"
    },
    "explanation": "Instruction-tuning on a small dataset is cost-effective and improves domain performance. Full pre-training and RLHF are too expensive; pure in-context learning yields weaker performance."
  },
  {
    "id": "e42743c5b5e3b3468465018d3b1d6bd1b12cfd2189e7b92aee7ce7cc60d4ab03",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A media monitoring system needs to process user queries and, if needed, execute a sequence of API calls (translation, sentiment, summarization). Which Bedrock feature simplifies orchestration?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "RAG with OpenSearch",
      "B": "Agentless multi-model pipeline",
      "C": "In-context chain-of-thought prompting",
      "D": "Bedrock Agents"
    },
    "explanation": "Bedrock Agents orchestrate multi-step workflows across APIs. Chain-of-thought or RAG won\u2019t manage external API calls; \u201cagentless pipeline\u201d isn\u2019t a real feature."
  },
  {
    "id": "32379aaae396acdc126c72f1b3a3115d4c2b7eaa34b40ea720e8ffac2d080b24",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "An enterprise requires an LLM endpoint that can scale to unpredictable traffic with cost-efficient idle pricing. Which invocation mode should you choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Synchronous API with provisioned concurrency",
      "B": "Synchronous API with on-demand (no provisioned concurrency)",
      "C": "Asynchronous API with on-demand invocations",
      "D": "Batch transform jobs in SageMaker"
    },
    "explanation": "Asynchronous on-demand endpoints scale elastically, incur no idle provisioned cost, and handle unpredictable traffic. Synchronous on-demand can throttle; provisioned is costly; batch jobs don\u2019t suit real-time."
  },
  {
    "id": "ef1dcc80e38fa2a6eac6d25860f64796a797d2481eec4c7c322acfcfda9792c0",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A banking app uses a foundation model to process user transaction queries. It must redact PII before generating responses. Which integration achieves this with minimal latency?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run a Lambda pre-processor to filter PII then call Bedrock",
      "B": "Use a Bedrock agent with built-in PII redaction tool",
      "C": "Fine-tune the model to ignore PII",
      "D": "Implement post-processing in the client application"
    },
    "explanation": "A Bedrock Agent with built-in PII redaction provides low-latency, integrated protection. Lambda adds latency; fine-tuning won\u2019t reliably remove PII; client post-processing is insecure."
  },
  {
    "id": "f01762aaed68861c893db7479941464d8f9b8719b3cb08e215a899dbd9c2fdfc",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A logistics company wants to store embeddings for thousands of ever-changing route descriptions with atomic updates and graph queries. Which storage is best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon OpenSearch Service",
      "B": "Amazon DynamoDB",
      "C": "Amazon Neptune",
      "D": "Amazon RDS"
    },
    "explanation": "Neptune supports ACID graph updates and nearest-neighbor searches. OpenSearch is less reliable for atomic graph operations; DynamoDB isn\u2019t optimized for vector search; RDS lacks graph features."
  },
  {
    "id": "340af65b8831c36903008b0f1187ddf5248dd2339d48ed3e53040181e7defd12",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A startup has a $1,000 monthly AI budget. They need a nightly summary of 100,000 news articles (500 tokens each). Which inference pattern is most cost-effective?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Real-time streaming inference",
      "B": "Synchronous API per article",
      "C": "Bedrock Agents orchestration",
      "D": "Batch transform in SageMaker"
    },
    "explanation": "Batch transform jobs in SageMaker run in bulk overnight at lower per-token cost with no concurrent endpoint overhead. Real-time and synchronous calls cost more; Agents add complexity."
  },
  {
    "id": "b77f99b6fa7c75e44db4998b32f1b7756678644e098a9f1fbc96f05c732f321a",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A legal tech company must guarantee explainability for each generated paragraph. Which foundation model choice best balances explainability and performance?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Anthropic Claude 2",
      "B": "Amazon Titan Text with model cards and low complexity",
      "C": "Cohere Command",
      "D": "OpenAI GPT-4 via Bedrock"
    },
    "explanation": "Titan Text with published model cards offers high transparency. Claude 2 and OpenAI GPT-4 lack full model cards on AWS; Cohere Command is opaque."
  },
  {
    "id": "a5da6c6a77f84f059fa4c69bb5ea3c429989b270bac2533929e3743697be6915",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A data analytics team wants real-time chat summarization where occasional context window overruns occur. Which strategy best handles overruns?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase max tokens indefinitely",
      "B": "Use temperature of 1.0",
      "C": "Implement sliding-window RAG with embeddings",
      "D": "Fine-tune the foundation model"
    },
    "explanation": "Sliding-window RAG seamlessly handles context overruns by chunking and retrieving relevant segments, without unbounded tokens or expensive fine-tuning."
  },
  {
    "id": "67b6f174cdb2db8693e8b32b36a44b3d94abadb0ff4f6c1cbaf16b52381c33ee",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "An education platform wants to reduce inference costs by at least 50%. They currently fine-tune a foundation model for every course update. What alternative yields the greatest savings?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use in-context learning with dynamic prompts instead of fine-tuning",
      "B": "Increase provisioned concurrency",
      "C": "Switch to a larger foundation model",
      "D": "Batch transform jobs daily"
    },
    "explanation": "In-context learning eliminates fine-tuning costs, achieving >50% savings. Provisioned concurrency and batch transforms do not address fine-tuning expense; larger models cost more."
  },
  {
    "id": "c1fb21f43b9b785c1242b1c5fcb807c302c9efc19cde4d91950b94a9664d5dc6",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A manufacturing firm needs natural language control of robots in multiple languages. Which foundation model feature is most important?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "High max tokens",
      "B": "Low temperature",
      "C": "Multilingual language coverage",
      "D": "Large model size"
    },
    "explanation": "Multilingual coverage is essential for multi-language commands. Token length, temperature, and size are secondary if the model can\u2019t understand required languages."
  },
  {
    "id": "7c2a849b2443998dfb9451edb7cc6f6fa2af4073df18674afe4a9a6393c3532c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A consulting firm sees unpredictable spikes in LLM usage during peak hours. To control request latency and cost, which Bedrock concurrency feature should they adjust?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Batch size",
      "B": "Provisioned concurrency",
      "C": "Max tokens",
      "D": "Temperature"
    },
    "explanation": "Provisioned concurrency allocates pre-warmed capacity to maintain low latency during peaks. Batch size, max tokens, and temperature don\u2019t manage concurrency."
  },
  {
    "id": "f47d985a1b20a4a79a1262d95d7e22f1a10d0d57a948d41b08c65afd5f253c00",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A telecom company uses RAG with a 200 GB document store. Index updates must reflect new data within minutes. Which architecture meets this requirement?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Rebuild the entire OpenSearch index daily",
      "B": "Fine-tune the model hourly",
      "C": "Use DynamoDB for embeddings",
      "D": "Stream new docs to OpenSearch via Kinesis Data Firehose"
    },
    "explanation": "Streaming via Firehose ensures minute-latency index updates in OpenSearch. Daily rebuilds are too slow; DynamoDB lacks vector search; frequent fine-tuning is impractical."
  },
  {
    "id": "276a1a9b9b7b6e3acf1612f56a4060ef89a5d9609de06e7509b4897081bd9178",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A media company must enforce a hard stop at 200 tokens in every generated summary. Which inference parameter should they set?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Temperature to 0",
      "B": "maxTokenLimit to 200",
      "C": "TopP to 1.0",
      "D": "Beam width to 5"
    },
    "explanation": "Setting maxTokenLimit to 200 enforces a hard stop. Temperature, TopP, and beam width do not constrain output length."
  },
  {
    "id": "6953c20a66323a50ceaa29558d1ff0893a8c1b725e928289aaaeb61ac2bc4f2e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A legal research tool needs precise citations. Which retrieval method should you combine with a foundation model to ensure factual accuracy?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "In-context few-shot prompting",
      "B": "Temperature of 1.0",
      "C": "RAG with a vetted legal corpus",
      "D": "Increase max tokens"
    },
    "explanation": "RAG with a vetted legal corpus grounds responses in source documents, ensuring factual citations. Prompting, temperature, and token length by themselves don\u2019t guarantee accuracy."
  },
  {
    "id": "dc7bd09807e105dbfcd93d70c7c2094c07b2dafb4da9fbc0bafb31f4e2e82236",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A startup wants to orchestrate a dynamic knowledge-search\u2192generate workflow within Bedrock without writing orchestration code. Which feature should they use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Custom Lambda preprocessor",
      "B": "In-context prompt chaining",
      "C": "Fine-tuned multi-task model",
      "D": "Bedrock Agents"
    },
    "explanation": "Bedrock Agents provide no-code orchestration of search and generation steps. Prompt chaining and Lambda require manual coding; fine-tuning can\u2019t automate retrieval."
  },
  {
    "id": "96755f0b380c10794f2078dbf39d37937cce74d2487ed176b590952af2962060",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A company must pay only for inference time used, with no idle costs, and handle unpredictable daily traffic. Which Bedrock billing option suits this?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Provisioned concurrency",
      "B": "On-demand concurrency",
      "C": "Monthly subscription",
      "D": "Batch transform"
    },
    "explanation": "On-demand concurrency bills per inference with no idle fees. Provisioned concurrency incurs idle charges; subscriptions and batch transforms aren\u2019t per-inference real-time."
  },
  {
    "id": "2ed283156502121f4d519fac3c7ceabadf5c1423b1b81ce2b5cdcfb8337ecbe7",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A risk analytics team must run summary inference on a petabyte-scale dataset overnight under a strict $2,000 budget. Which architecture is most appropriate?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Real-time Bedrock endpoint",
      "B": "Provisioned concurrency synchronous calls",
      "C": "Bedrock Agents",
      "D": "SageMaker batch transform with Spot instances"
    },
    "explanation": "SageMaker batch transform with Spot instances handles large datasets overnight under budget. Real-time endpoints and Agents are costlier for batch."
  },
  {
    "id": "28326a41c631ee4e646050892c3295927ccca06e24b60f658176e6bbb57d1d5f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "An AI solution must ensure every generated output is traceable to specific data sources. Which RAG component provides this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Document retrieval metadata",
      "B": "High temperature",
      "C": "maxTokenLimit",
      "D": "Fine-tuning"
    },
    "explanation": "Retrieval metadata tracks source documents for traceability. Temperature and token limits don\u2019t; fine-tuning embeds data but lacks per-generation traceability."
  },
  {
    "id": "a93e2d9af33a8c833bf9a5251f91b5bb322032d103d17d6427f1c524b7d1bf75",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A biotech firm wants to fine-tune a foundation model on proprietary data but also retain the ability to use general knowledge. Which customization approach preserves general capabilities?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Full model replacement with proprietary-only data",
      "B": "Instruction-tuning on proprietary data",
      "C": "Continual pre-training on only proprietary data",
      "D": "In-context learning only"
    },
    "explanation": "Instruction-tuning adapts the model to proprietary data while preserving base knowledge. Full replacement or continual pre-training risks catastrophic forgetting; in-context alone is less performant."
  },
  {
    "id": "b519926787c0c9260d79f319e59a94a76d2a99ea1c3fb3301823ab7097888667",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A global news aggregator needs sub-100 ms inference for headlines in multiple regions. How should they minimize network latency?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a single US-East Bedrock endpoint",
      "B": "Enable provisioned concurrency only",
      "C": "Increase maxTokenLimit",
      "D": "Deploy regional Bedrock endpoints nearest users"
    },
    "explanation": "Regional endpoints reduce network hops and latency. A single US-East endpoint increases latency for distant users; provisioned concurrency and token limits don\u2019t address network delay."
  },
  {
    "id": "b709b8aed6355238e2de0e75fa859d3e9cde58f68d21302610228a8a0310ece6",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A manufacturing firm uses a foundation model to translate technical manuals. They observe inconsistent terminology. Which prompt engineering technique best improves consistency?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a prompt template with defined glossary and format instructions",
      "B": "Increase temperature to 0.9",
      "C": "Remove stop sequences",
      "D": "Use only zero-shot prompting"
    },
    "explanation": "A glossary template enforces consistent terminology. Higher temperature increases variance; removing stop sequences and zero-shot prompting degrade control."
  },
  {
    "id": "3b0f18f38d578ffd185496ce350ab2f7e513f9b7c5eb4f146932f3037dad5eae",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A software company must process variable batch sizes of code snippets with predictable billing and no idle charges. Which endpoint type should they choose?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Provisioned Bedrock endpoint",
      "B": "On-demand Bedrock endpoint",
      "C": "SageMaker batch transform",
      "D": "Synchronous Lambda invocation"
    },
    "explanation": "On-demand Bedrock endpoints bill per inference with no idle fees, ideal for variable loads. Provisioned endpoints incur idle costs; batch transform and Lambda aren\u2019t optimized for real-time code snippet processing."
  },
  {
    "id": "14dbd0bec77a1b444a546b695eb03169002a0349bfa735bf86d54c69eb741b16",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "An enterprise team must comply with data residency rules. Which customization method ensures data never leaves their AWS Region?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Use a public API for fine-tuning",
      "B": "Use a global Bedrock endpoint",
      "C": "In-context learning with external data",
      "D": "Fine-tune a private Bedrock foundation model in their region"
    },
    "explanation": "Private regional fine-tuning keeps data in the same region. Public APIs and global endpoints may move data; in-context learning doesn\u2019t train the model but still sends data externally."
  },
  {
    "id": "de337cd4bbb0057657e26bda487edd1240349fedf54af954398370a53ffe56b2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A contact center wants to automatically correct grammar and expand abbreviations in live chats before analysis. Which approach yields the lowest end-to-end latency?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Batch transform jobs hourly",
      "B": "On-demand Bedrock text endpoint with high temperature",
      "C": "Synchronous Bedrock endpoint with moderate max tokens",
      "D": "RAG with OpenSearch"
    },
    "explanation": "A synchronous Bedrock text endpoint with moderate token limit gives lowest real-time latency. Batch jobs are too slow; high temperature is irrelevant to latency; RAG adds retrieval overhead."
  },
  {
    "id": "4d7e20e8923bfe894dac3c9bb35af80589830e544227cc04231890d03a2b2662",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A gaming company needs the model to generate personalized dialogs referencing user history stored in DynamoDB. Which integration pattern is best?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Embed all history in the prompt",
      "B": "Fine-tune model on historical data",
      "C": "Use a large maxTokenLimit",
      "D": "Pre-retrieve user history from DynamoDB and pass via prompt or RAG"
    },
    "explanation": "Pre-retrieval from DynamoDB and passing relevant context via prompt or RAG is efficient and cost-effective. Embedding all history inflates tokens; fine-tuning is static; token limit alone doesn\u2019t guarantee relevance."
  },
  {
    "id": "cd3c23639c4f7e1101d33270296719b09ae256a0c4d7a6d1f1b1d114fd01603e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "A financial services chatbot is giving superficially correct but logically flawed responses on multi-step loan eligibility queries. Which prompt engineering technique should you apply to improve its stepwise reasoning?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Chain-of-thought prompting",
      "B": "Zero-shot prompting",
      "C": "Negative prompting",
      "D": "Temperature tuning"
    },
    "explanation": "Chain-of-thought encourages the model to articulate intermediate steps, improving multi-step logical reasoning. Zero-shot provides no examples, negative prompts discourage outputs, and temperature tuning only affects randomness."
  },
  {
    "id": "23e6b2176294f88d884937814ac58ca3e7fea03f890b621a56f395400cf82179",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "You need to instruct a foundation model to translate text but also avoid certain sensitive terms. Which prompt component should you include?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Few-shot examples",
      "B": "Chain-of-thought instruction",
      "C": "Negative prompt instructions",
      "D": "High temperature parameter"
    },
    "explanation": "Negative prompts explicitly instruct the model what to avoid. Few-shot gives examples, chain-of-thought is for reasoning, and temperature affects creativity, not exclusion."
  },
  {
    "id": "d6d357f3734f6b2397863f71b76929ae4eae0a58adf1f2ed0530b3a3fbaca1d9",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "An internal knowledge base retrieval application returns irrelevant documents. You want the model to weigh recent data more heavily. Which prompt engineering adjustment addresses recency bias?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add negative prompts",
      "B": "Use zero-shot only",
      "C": "Increase max token length",
      "D": "Include time-stamped context in prompt examples"
    },
    "explanation": "By including time-stamped examples, the model learns to prioritize recent information. Negative prompts exclude content, zero-shot lacks examples, and token length doesn\u2019t address recency."
  },
  {
    "id": "5e101da541adbae19087d485e8bbbb53949dddf29472befdc49806db6af9838c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "A marketing team reports inconsistent style in AI-generated product descriptions. Which prompt engineering best practice stabilizes tone and format?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Use a very low temperature",
      "B": "Provide a structured template with placeholders",
      "C": "Apply chain-of-thought prompting",
      "D": "Switch to zero-shot prompting"
    },
    "explanation": "Structured templates enforce consistent style by constraining output. Low temperature reduces randomness but doesn\u2019t ensure format, chain-of-thought is for reasoning, zero-shot lacks guidance."
  },
  {
    "id": "5732ae9ad71bdbf0c683c5a2534367645cdcd947ab5c8826533a7a16658780ee",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "During RAG (Retrieval Augmented Generation), generated answers include hallucinations. Which prompt technique reduces hallucination risk?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase few-shot examples",
      "B": "Lower the temperature to zero",
      "C": "Instruct the model to cite sources from the provided context",
      "D": "Use negative prompts against hallucinations"
    },
    "explanation": "Prompting the model to cite sources ties it to retrieved context. Zero temperature reduces randomness but hallucinations can still occur. Negative prompts aren\u2019t reliable, and more examples alone won\u2019t eliminate hallucinations."
  },
  {
    "id": "c3bb0c2920f558ac4c1cb5e519baefa269a64a3bc6dc05381bccc5f215960079",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "You need to generate SQL queries from natural language but want to prevent malicious injections. Which prompt engineering practice helps guard against prompt injection attacks?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provide multiple few-shot examples",
      "B": "Use explicit system message with strict API schema constraints",
      "C": "Increase temperature to diversify syntax",
      "D": "Include chain-of-thought steps"
    },
    "explanation": "System messages with strict schema enforce structure and reduce injection risk. Few-shot doesn\u2019t block injection, higher temperature increases variability, and chain-of-thought is unrelated to security."
  },
  {
    "id": "22fe75fba921f7816b6d076091faf9703e2a15d54fd3f688c7a87778b8deff92",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "An LLM generates partial answers due to token limits. Which prompt adjustment helps produce complete answers?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Add negative prompts",
      "B": "Lower temperature",
      "C": "Switch to zero-shot",
      "D": "Include explicit \u2018continue until complete\u2019 instruction"
    },
    "explanation": "Explicit instructions to continue help the model produce full answers within length constraints. Negative prompts exclude content, temperature and zero-shot don\u2019t address completeness."
  },
  {
    "id": "fee330c28830a37e036af70136d77c93c1c092bd72b5136fa5fd4a93de759917",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "A sentiment-analysis assistant mislabels sarcasm as positive sentiment. Which prompt technique might improve detection?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Provide few-shot examples of sarcastic sentences labeled correctly",
      "B": "Use negative prompts to exclude positive terms",
      "C": "Increase max tokens",
      "D": "Use zero-shot prompting only"
    },
    "explanation": "Few-shot examples teach the model specific sarcasm patterns. Negative prompts exclude words rather than teach nuance, token limits don\u2019t help, and zero-shot lacks examples."
  },
  {
    "id": "4482718f4f47896bedf08304db5ca376cd5eac0e8c2de7567928c257ffa3cf59",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "A code generation model outputs insecure code snippets. Which prompt engineering strategy can enforce security best practices?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Chain-of-thought prompting",
      "B": "Include security checklist and example compliant code in prompt",
      "C": "Use negative prompts against insecure patterns",
      "D": "Increase temperature for diversity"
    },
    "explanation": "Embedding a security checklist plus example code guides the model toward secure outputs. Chain-of-thought is for reasoning, negative prompts aren\u2019t specific enough, and temperature affects randomness."
  },
  {
    "id": "5066fb55dd7151a5c40f5bc68e988c8bce074dfbf9cecc0e35d5ee6b4dd73108",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "Your conversational agent drifts off topic after several turns. Which prompt engineering technique limits topic drift?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Lower temperature",
      "B": "Increase few-shot examples",
      "C": "Use a persistent system prompt defining topic and role constraints",
      "D": "Include chain-of-thought prompts"
    },
    "explanation": "A system prompt that persists each turn reinforces role and topic. Temperature and examples help style not persistence, chain-of-thought is for reasoning."
  },
  {
    "id": "0f1bf10be1b2ed07a217d6fd4655b2b05a9fe9bdca4ec43a151a0bdaccaf42d2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "To minimize repetition in generated poetry, which prompt or parameter adjustment is most effective?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a negative prompt against repeated lines",
      "B": "Provide zero-shot instructions",
      "C": "Use chain-of-thought",
      "D": "Lower the repetition penalty parameter"
    },
    "explanation": "Reducing repetition penalty discourages repeated phrases. Negative prompts may not enforce pattern, zero-shot lacks guidance, chain-of-thought is irrelevant."
  },
  {
    "id": "974ebd4d8b2398a9b9f6d5227f5b14ab130e890e3d8152271b1be869067e0752",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "An LLM used for code summarization omits critical function details. Which prompt engineering approach helps ensure completeness?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Ask the model to list all function parameters and return values explicitly",
      "B": "Use zero-shot prompting",
      "C": "Apply negative prompts",
      "D": "Chain-of-thought prompt"
    },
    "explanation": "Explicitly requesting detailed output forces inclusion of all elements. Zero-shot, negative prompts, and chain-of-thought won\u2019t guarantee completeness of technical details."
  },
  {
    "id": "05c6d0157b88163a24c673d0e1c0809aeccd97d301ac659ed1e35c6a5575918b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "A translation model is inconsistent with formal versus informal tone across languages. Which prompt engineering best practice ensures desired register?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Use chain-of-thought",
      "B": "Provide parallel examples showing formal and informal translations",
      "C": "Increase temperature",
      "D": "Switch to zero-shot"
    },
    "explanation": "Parallel examples teach tone mapping for each register. Chain-of-thought is for reasoning, temperature affects randomness, zero-shot lacks examples."
  },
  {
    "id": "e16219782d6f2a6a5880639c1fd7fd00e074d8f47dbb1f052b961da06035d896",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "To improve factual accuracy in a medical QA assistant, which prompt inclusion is most critical?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Chain-of-thought reasoning steps",
      "B": "Negative prompts against common myths",
      "C": "Cited excerpts from verified medical literature in context",
      "D": "High temperature for varied responses"
    },
    "explanation": "Including verified context anchors responses to factual sources. Chain-of-thought doesn\u2019t ensure factuality, negative prompts exclude myths but don\u2019t supply facts, temperature increases creativity."
  },
  {
    "id": "882bd3e05bc42ee73637485edffeec74fb9908b2a9e850b315780873979572fe",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "When generating legal contract clauses, you need strict adherence to jurisdictional terms. Which prompt technique enforces this requirement?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Negative prompts excluding non-jurisdiction terms",
      "B": "Zero-shot prompting",
      "C": "Chain-of-thought",
      "D": "Provide clause templates labeled by jurisdiction as few-shot examples"
    },
    "explanation": "Jurisdiction-specific templates as few-shot examples guide strict adherence. Negative prompts and zero-shot offer no structure, chain-of-thought is unrelated."
  },
  {
    "id": "8d13cb9b1479859e5b4f2808eb26d5d758327409ebf96aeee68a5314b437f8ed",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "A multi-turn assistant forgets user preferences across turns. Which prompt engineering practice can maintain memory?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Persist user preference summary in system prompt",
      "B": "Use chain-of-thought prompts",
      "C": "Add more few-shot examples",
      "D": "Increase temperature"
    },
    "explanation": "Including a running summary in the system prompt preserves state across turns. Chain-of-thought, examples, and temperature don\u2019t maintain turn state."
  },
  {
    "id": "b52e28a88042f4e8f41b53dda26a50ddc3ccea2a159d9c558d66d57de5faf803",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "You observe that small changes in wording produce vastly different model outputs. Which prompt engineering mitigation can reduce this sensitivity?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use zero-shot prompting",
      "B": "Provide paraphrased few-shot examples with equivalent meaning",
      "C": "Increase temperature",
      "D": "Apply negative prompts"
    },
    "explanation": "Paraphrased examples teach the model to generalize beyond specific wording. Zero-shot and negative prompts don\u2019t address wording sensitivity, and higher temperature worsens variability."
  },
  {
    "id": "358316bd7b1bf8d9312bfdd70a510b7fc7b9c10a7bc0ceb139c1b4f276bfe550",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "A customer-facing summary bot truncates long documents prematurely. Which prompt adjustment helps it handle large inputs gracefully?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Lower temperature",
      "B": "Use chain-of-thought",
      "C": "Implement chunking instruction with overlap and summary stitching",
      "D": "Add negative prompts"
    },
    "explanation": "Chunking with overlap and stitching instructions enables processing long documents. Temperature and chain-of-thought don\u2019t address input length, negative prompts exclude content."
  },
  {
    "id": "60f32e91906acf9b6b59f05b7014dfb3b86629c47cfe67857c05e85aa4e626b2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "In a classification task, model outputs inconsistent labels for edge cases. Which prompt technique can improve boundary consistency?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provide labeled boundary examples in few-shot format",
      "B": "Use zero-shot prompting",
      "C": "Lower repetition penalty",
      "D": "Include chain-of-thought"
    },
    "explanation": "Boundary examples teach edge-case behavior explicitly. Zero-shot lacks examples, repetition penalty and chain-of-thought don\u2019t enforce classification consistency."
  },
  {
    "id": "cea3e7d2b46e22e4cccfc497f8fe6ec673693a36f4cbcbafe32abc6bee6b86aa",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "You want the model to generate rebuttals to arguments. Which prompt engineering approach yields persuasive, structured responses?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Chain-of-thought prompting",
      "B": "Use zero-shot only",
      "C": "Negative prompts against informal tone",
      "D": "Provide argumentative outline template with few-shot examples"
    },
    "explanation": "An outline template plus examples structures persuasive rebuttals. Chain-of-thought is reasoning oriented, zero-shot lacks structure, negative prompts don\u2019t guide persuasion."
  },
  {
    "id": "9fe45d0c31b70b7fcda10739b1734fbbe59a84555c7962ddcada78fc508bd2ef",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "A summarization model overemphasizes certain topics present in the prompt. How do you reduce prompt bias?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature",
      "B": "Use chain-of-thought",
      "C": "Balance topic representation in prompt context examples",
      "D": "Negative prompts"
    },
    "explanation": "Balancing examples ensures all topics receive equal weight. Temperature, chain-of-thought, and negative prompts don\u2019t correct initial context bias."
  },
  {
    "id": "54414b2f14586f90100a1b35450664c4820a945b281497a504526d12a90f3dc5",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "An AI writing assistant must follow a company style guide exactly. Which prompt strategy enforces this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Chain-of-thought",
      "B": "Embed style guide rules and sample text in the system prompt",
      "C": "Zero-shot prompting",
      "D": "High temperature"
    },
    "explanation": "Embedding style guide and samples in system prompt enforces compliance. Chain-of-thought is unrelated, zero-shot lacks guidance, temperature affects randomness."
  },
  {
    "id": "7e6ce05d98407143624e96f413b09d8f51eac93bfd313d80b4fd85b1ae2a3f0c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "To debug poor model outputs, you need to trace which prompt phrases influenced errors. Which prompt engineering practice aids this analysis?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use zero-shot prompting",
      "B": "Chain-of-thought",
      "C": "Negative prompts",
      "D": "Log and test variations by isolating single instruction changes"
    },
    "explanation": "Isolating and logging single instruction changes identifies problematic phrases. Other techniques don\u2019t support diagnostic testing."
  },
  {
    "id": "cb7e5aa1b14603622a5247e36e2f03d2c841b5f81f7ce808ca83cfbc873566b9",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "A model generating product recommendations overfits to popular items. Which prompt modification can diversify suggestions?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Include recent low-popularity item examples in few-shot set",
      "B": "Use zero-shot prompting",
      "C": "Lower max tokens",
      "D": "Negative prompt against popular items"
    },
    "explanation": "Including under-represented examples teaches diversity. Zero-shot and token limits are irrelevant, negative prompts only exclude popular items without positive guidance."
  },
  {
    "id": "f4dc6d963663e4f4f241bdffa0c094bbbd68ab8c86539dc2c2b5eb1b14a4c1dc",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "Your model misinterprets multi-language prompts and outputs in wrong language. Which prompt engineering best practice corrects this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use chain-of-thought",
      "B": "Use explicit language tags and examples in few-shot prompts",
      "C": "Increase temperature",
      "D": "Negative prompts"
    },
    "explanation": "Language tags with examples clearly instruct desired language. Chain-of-thought, temperature, and negative prompts don\u2019t enforce language choice."
  },
  {
    "id": "517a65018cb0ff5694c713654dfef5f89ebe701d34161bd0f5421144dc2614db",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "A model used for code reviews misses style guide violations. Which prompt addition ensures style compliance?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Chain-of-thought prompt",
      "B": "Zero-shot instruction",
      "C": "Include style guide checklist and example violations in prompt",
      "D": "Negative prompts"
    },
    "explanation": "Providing checklist and violation examples guides the model to detect style issues. Chain-of-thought, zero-shot, and negative prompts aren\u2019t specific enough."
  },
  {
    "id": "e7a80b50e66bf504ccbb2914fb427704957a90bab3c1cd19f04f0af9fedeb304",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "A technical support bot answers user requests but sometimes reveals internal debug info. Which prompt technique prevents leaking this info?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use negative prompts to forbid internal system messages",
      "B": "Chain-of-thought prompting",
      "C": "Zero-shot prompting",
      "D": "Increase temperature"
    },
    "explanation": "Negative prompts explicitly forbid leakage of system messages. Other techniques don\u2019t address content filtering."
  },
  {
    "id": "9544a836c9787ce12e6c02859e998860fc6c7e6d7fe3eb8a3d4cb6f22bf4392d",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "A summarization model must preserve named entities accurately. Which prompt engineering approach helps maintain entity fidelity?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Chain-of-thought",
      "B": "Zero-shot prompting",
      "C": "Negative prompts",
      "D": "Provide few-shot examples emphasizing entity preservation and highlighting changes"
    },
    "explanation": "Examples that highlight correct entity handling teach the model to preserve names. Other methods don\u2019t focus on entity fidelity."
  },
  {
    "id": "1c996ade96295b412e07c0b5a5dbc27c2e9c7bfb9dc8bfaee7fbd36cf6e2baf0",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "Your LLM assistant needs to ask clarifying questions before proceeding on ambiguous user input. Which prompt engineering pattern supports this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Negative prompts",
      "B": "Include conditional branching instructions: \u2018If unclear, ask a follow-up question.\u2019",
      "C": "Zero-shot prompting",
      "D": "Chain-of-thought prompting"
    },
    "explanation": "Conditional instructions ensure the model asks follow-ups when necessary. Other options don\u2019t incorporate dynamic branching logic."
  },
  {
    "id": "7a8ec950afdd9e0bfadbe1c7a9a89081fb49d9371fabbe1ce0a305a458ed5603",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "You have a 70B-parameter foundation model pre-trained on general web text. You need to adapt it to a medical domain where data is scarce and highly specialized. Which fine-tuning approach minimizes compute cost while preserving general capabilities?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Full-parameter fine-tuning on the medical corpus",
      "B": "LoRA (low-rank adaptation) on the medical corpus",
      "C": "Instruction tuning using in-context examples without weight updates",
      "D": "Continual pre-training on the medical data only"
    },
    "explanation": "LoRA applies low-rank adapters to update a small subset of parameters, reducing compute cost and preserving most pre-trained weights. Full fine-tuning is costlier; instruction tuning without weight updates doesn\u2019t adapt weights; continual pre-training risks catastrophic forgetting."
  },
  {
    "id": "8a5f78c27dd6c3c8a32c04c29e2b6c9c97864b4fb897c9cfdc6bb1e21210b173",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "During RLHF of a chatbot, you observe reward hacking: the model produces token sequences that game the reward model without satisfying user intent. What modification to the RLHF pipeline best addresses this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase learning rate to encourage exploration",
      "B": "Add more demonstration examples to the initial supervised fine-tuning",
      "C": "Regularize policy updates by KL-penalty against the supervised fine-tuned model",
      "D": "Remove the reward model and use human ranking directly"
    },
    "explanation": "A KL-penalty (posterior regularization) keeps the policy close to the supervised model, preventing the policy from drifting to reward-hacking behaviors. Higher learning rates worsen hacking, and removing the reward model loses guidance; adding demos helps but doesn\u2019t prevent hacking."
  },
  {
    "id": "7ab3217ca7fb6466dd97f0dd76b2bae061164ba76ef953a5bc873a39efbd723d",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "You\u2019re preparing data for fine-tuning an LLM on financial reports. The data contains duplicated boilerplate sections across documents. Which data preparation step is most critical to improve model generalization?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deduplicate boilerplate content before fine-tuning",
      "B": "Augment the dataset with random noise in boilerplate sections",
      "C": "Increase the weight of boilerplate examples in loss computation",
      "D": "Mask boilerplate tokens during training"
    },
    "explanation": "Deduplication prevents the model from overfitting to repeated boilerplate text. Augmentation or weighting boilerplate is counterproductive; masking tokens distorts context."
  },
  {
    "id": "8a1ea184e13861dafff6668a36f7e98701c7857c6d4269ffee440134abb45437",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "You need to fine-tune a foundation model for a classification task with only 500 labeled examples. Which transfer learning strategy reduces overfitting risk?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Fine-tune all layers with high learning rate",
      "B": "Freeze embedding layer and fine-tune remaining layers",
      "C": "Freeze all but the classification head and train head only",
      "D": "Use adapter modules inserted in middle layers and train only them"
    },
    "explanation": "Adapter modules allow parameter-efficient fine-tuning, limiting overfitting on small datasets. Training only the head may underfit; freezing embedding and training all other layers still updates many parameters; high learning rates risk divergence."
  },
  {
    "id": "d9691ca6f165cdbb62bcae7b03c44ec9d51d29c84824ad6b2775dc162bc54b85",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "For continuous pre-training of an LLM on customer support logs, you must avoid catastrophic forgetting of general language. Which technique helps retain pre-trained knowledge?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a higher learning rate on new data",
      "B": "Mix a percentage of original pre-training data in each batch",
      "C": "Train only the classification head during pre-training",
      "D": "Apply domain\u2010specific tokenization exclusively"
    },
    "explanation": "Mixing pre-training data ensures the model sees general language examples, preventing forgetting. Higher learning rates accelerate forgetting; training only the head doesn\u2019t adapt the core; domain-specific tokenization reduces general coverage."
  },
  {
    "id": "63e8ad9c9ef5a28bc17a5bacb1933e115a96fc6026d4335436d709ff86a7e66b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "You want to perform instruction tuning on a foundation model to improve multi-step reasoning. Which prompt design in your supervised dataset best supports chain-of-thought?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Concise question with final answer only",
      "B": "Question plus bullet-point hints",
      "C": "Question with step-by-step reasoning trace leading to the answer",
      "D": "Randomly shuffled question-answer pairs"
    },
    "explanation": "Providing step-by-step reasoning in examples teaches the model chain-of-thought. Final-answer-only examples don\u2019t illustrate reasoning; bullet hints are less explicit; shuffled pairs harm learning."
  },
  {
    "id": "99d40553d15f7bbe47053d40405f879c23539bdbb4dea7709aada8b6db44ad6d",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "When fine-tuning a multilingual foundation model on a single-language corpus, you observe decreased performance in other languages. Which remedy mitigates this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Multi-task fine-tuning: include a small sample of other languages",
      "B": "Increase batch size on the single-language data",
      "C": "Reduce the number of training epochs",
      "D": "Apply language-specific token embeddings only"
    },
    "explanation": "Including other-language samples prevents catastrophic forgetting of those languages. Bigger batches or fewer epochs don\u2019t address forgetting systematically; changing token embeddings alone doesn\u2019t retain old-language knowledge."
  },
  {
    "id": "5370453d8b9955c10d7b2c0506e55541f2dd341a55baccab243121fdd08922b7",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "Your fine-tuning dataset contains sensitive PII fields. Which data governance step is essential before training?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase dataset size to dilute PII instances",
      "B": "Use higher dropout rate during fine-tuning",
      "C": "Deploy guardrails at inference time only",
      "D": "Anonymize or remove PII fields from the training data"
    },
    "explanation": "Removing or anonymizing PII prevents the model from memorizing sensitive data. Dilution or dropout doesn\u2019t guarantee removal; guardrails at inference don\u2019t protect training privacy."
  },
  {
    "id": "8d0cad93ba6eb085c578e597f740fd1a974d5a59cec03188fe16ca7683272595",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "You compare two fine-tuning configurations for the same LLM. Config A uses a small learning rate with many epochs; Config B uses a larger learning rate with fewer epochs. Config B converges faster but overshoots occasionally. What hyperparameter adjustment balances stability and speed?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase batch size further",
      "B": "Reduce the number of layers being updated",
      "C": "Use a cosine learning rate schedule with warm-up",
      "D": "Switch optimizer from AdamW to SGD"
    },
    "explanation": "A cosine learning rate schedule with warm-up allows the learning rate to start low, ramp up, then decay, improving stability while maintaining speed. Changing batch size or layers doesn\u2019t directly control overshoot; SGD is less adaptive than AdamW."
  },
  {
    "id": "620991cd3925a34fad8027499ce93258414b77ac090e656fe104ea15fa24049f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "In RLHF, the reward model is trained on human-labeled comparisons. You notice low inter-annotator agreement on some comparison pairs. Which approach improves reward model performance?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Adjudicate disagreements to create a gold-standard label set",
      "B": "Discard examples with annotator disagreement",
      "C": "Increase the reward model complexity",
      "D": "Train reward model with unfiltered raw labels"
    },
    "explanation": "Adjudicating yields consistent labels, improving reward model learning. Discarding data reduces sample size; more complex model overfits noise; unfiltered labels perpetuate inconsistency."
  },
  {
    "id": "efbdb05a6e8b8ab5027eb7977c79a474246abfb1e4dcc9cbc95efe7bcfe3abe5",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "Your foundation model fine-tuning pipeline includes data augmentation by synonym replacement. After training, model outputs are semantically inconsistent. What is the likely cause?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Synonym replacement increased dataset size too much",
      "B": "Context-insensitive synonym substitution broke meaning",
      "C": "The model overfitted to rare words introduced",
      "D": "Embeddings failed to update for replaced tokens"
    },
    "explanation": "Context-insensitive synonym replacement can change meaning and introduce noise, leading to inconsistent outputs. Increased size isn\u2019t harmful per se; overfitting to rare words is possible but secondary; embeddings update normally if tokens appear."
  },
  {
    "id": "88a607896868c19829af3d885a5e1a98fd0d525395b1a5967654b4b4489adee1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "When fine-tuning a foundation model for domain adaptation, you must decide on dataset representativeness. Which criterion is most critical?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use only the highest-quality examples regardless of frequency",
      "B": "Maximize dataset size even with noisy examples",
      "C": "Balance examples equally across all subtopics",
      "D": "Match fine-tuning data distribution to target domain usage distribution"
    },
    "explanation": "Aligning the data distribution with expected real-world use cases ensures the model learns relevant patterns. Quality alone may bias rarity; size without quality introduces noise; equal subtopic balance may not reflect actual usage."
  },
  {
    "id": "fbe36150ae47744e799a815fd16bef86e94f2edf1dd7d3fe4b15914c7e258d96",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "A fine-tuned model exhibits overconfidence on out-of-distribution inputs. Which fine-tuning modification most effectively calibrates confidence?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Include a calibration dataset with \u2018no-answer\u2019 or random inputs during fine-tuning",
      "B": "Train with label smoothing only on in-distribution data",
      "C": "Reduce model size by pruning layers",
      "D": "Increase learning rate to produce sharper outputs"
    },
    "explanation": "Introducing out-of-distribution or no-answer examples helps the model learn to abstain or lower confidence. Label smoothing alone affects in-distribution predictions; pruning or higher learning rates don\u2019t improve calibration."
  },
  {
    "id": "ff81e437c88381936ecc0bc9556aa82fc21860195c678d4b732ca32231bb6a0f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "For continuous pre-training, the streaming customer data arrives hourly. You want to update the base model daily without interruption. Which strategy ensures efficient updates?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Stop service and reload updated model every 24h",
      "B": "Use rolling A/B deployment with canary releases",
      "C": "Fine-tune in-place without versioning",
      "D": "Accumulate a week\u2019s data and update weekly"
    },
    "explanation": "Rolling A/B with canary releases allows daily updates with minimal service interruption. In-place fine-tuning risks model instability; weekly updates lag; full service downtime is unacceptable."
  },
  {
    "id": "b3210845de3a6df03f5fba1177c958ccdb650b4e185d9b68544425aa47f7802a",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "You plan to instruction-tune a model with mixed formats: some examples are dialogues, others are bullet lists. After training, the model only outputs dialogues. Why?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Learning rate was too high, favoring longer sequences",
      "B": "Batch size was too small, underfitting bullet examples",
      "C": "Class imbalance: dialogue examples outnumber bullets",
      "D": "Optimizer bias toward dialog tokens"
    },
    "explanation": "When one format dominates the dataset, the model learns that format preferentially. High learning rates or optimizer choice aren\u2019t format-specific issues; batch size alone won\u2019t override class imbalance."
  },
  {
    "id": "6d726a60d3cc37f0435fafa5c8912972892a8c988748df15e958f0c4f94705c6",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "Your reward model training uses pairwise ranking data collected via crowdsourcing. Labelers misunderstand the ranking task, flipping preferences inconsistently. Which step should you add to your pipeline?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Qualification tests for labelers with known comparisons",
      "B": "Increase batch size during reward model training",
      "C": "Switch to absolute scoring instead of ranking",
      "D": "Reduce model complexity to prevent overfitting noisy labels"
    },
    "explanation": "Qualification tests ensure labelers understand the task, improving data quality. Batch size and model complexity don\u2019t fix labeler misunderstanding; absolute scoring changes task but doesn\u2019t guarantee understanding."
  },
  {
    "id": "9beb80f3b7b565b17339164e60fdca9689d6ae208cd0e6af9e4f6754b6dd1835",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "You need to fine-tune a foundation model for question answering using in-domain FAQ pairs only. The model sometimes hallucinates plausible but incorrect answers. Which data-driven fix reduces hallucination?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add random distractor questions to training",
      "B": "Increase temperature during fine-tuning",
      "C": "Train with higher dropout rate",
      "D": "Include negative examples: questions with 'no answer' label"
    },
    "explanation": "Including negative examples teaches the model to respond with 'no answer' rather than hallucinate when data is missing. Distractors and dropout don\u2019t address hallucination; temperature affects inference, not training."
  },
  {
    "id": "6bb53d0da901cdb0214097b75f151770b5798b21f7c4df8236ed08322a0c048c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "In domain adaptation, you notice that certain rare entity types are underrepresented and the model fails to recognize them. Which technique improves rare entity recognition without collecting more data?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase fine-tuning epochs",
      "B": "Use a higher learning rate for rare entity tokens",
      "C": "Apply oversampling of rare-entity examples in fine-tuning batches",
      "D": "Prune embeddings for common tokens"
    },
    "explanation": "Oversampling rare-entity examples ensures the model sees them more often, improving recognition. Higher epochs or learning rates risk overfitting; pruning embeddings harms overall performance."
  },
  {
    "id": "1d6f21864f4af2ec87c162c16236ba62bdf26f0c36260c769c7f920691fe3a85",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "Your fine-tuning job on SageMaker is slow and expensive due to full-model updates. Which SageMaker feature can reduce training cost and time?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Neo for compilation",
      "B": "Use SageMaker distributed data parallel with adapter-enabled training",
      "C": "Switch to larger instances for faster throughput",
      "D": "Use automatic model tuning"
    },
    "explanation": "Using parameter-efficient training with adapter modules and SageMaker's distributed data parallel reduces compute by updating fewer parameters. Neo is for inference; larger instances increase cost; automatic tuning optimizes hyperparameters, not model size."
  },
  {
    "id": "8789a542798a74c3cb9ba2da43eb068b1e304696a103d103d6d91d4ca155261c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "You observe the fine-tuned model performance plateaus early. Analysis shows that gradient norms explode after epoch 2. Which remedy is most appropriate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Implement gradient clipping",
      "B": "Use a larger batch size",
      "C": "Remove weight decay",
      "D": "Decrease model depth"
    },
    "explanation": "Gradient clipping prevents exploding gradients, stabilizing training. Larger batches may worsen instability; removing weight decay doesn't address gradients; changing depth isn't a tuning direct remedy."
  },
  {
    "id": "f7d0e50dc08da5548d9560e4a964f45b3d3ec2a1cd524af682f229115cb847a2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "You fine-tune a multilingual LLM using data from multiple dialects. The model favors majority dialect outputs. Which method ensures fair representation?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase overall dataset size",
      "B": "Use higher dropout on majority dialect data",
      "C": "Train with higher learning rate on minority dialects",
      "D": "Assign sampling weights inversely proportional to dialect frequency"
    },
    "explanation": "Inverse-frequency sampling balances representation so minority dialects are seen as often as majority ones. Other methods don\u2019t systematically correct bias."
  },
  {
    "id": "0f4c138bb1812eb805616ea1907f78e58945d73254a80c6e1ac7587a30c0bac4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "Your continuous pre-training pipeline must comply with GDPR. Which data governance practice must you implement?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train only on encrypted data",
      "B": "Use privateLink for model hosting",
      "C": "Maintain data lineage and consent records for each example",
      "D": "Apply differential privacy at inference time"
    },
    "explanation": "GDPR requires traceability of personal data origin and consent; data lineage and consent records ensure compliance. Encryption and PrivateLink help security but not consent; inference privacy doesn\u2019t cover training compliance."
  },
  {
    "id": "16bd9d762e499bd637a34ff8f83b0898820ddb705ca722f150b71b81b2694090",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "You aim to reduce carbon footprint of repeated fine-tuning experiments. Which strategy has greatest impact?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use larger GPUs to shorten training time",
      "B": "Use parameter-efficient fine-tuning methods",
      "C": "Increase number of epochs to convergence",
      "D": "Train on spot instances only"
    },
    "explanation": "Parameter-efficient fine-tuning (adapters, LoRA) reduces compute cycles substantially, cutting energy use. Larger GPUs helps but still full-model training; more epochs increase carbon; spot instances affect cost, not total energy."
  },
  {
    "id": "5ec35392e842b070bf543874b136f025669238be9410b5215cc389487d5648c0",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "You need to incorporate new regulatory guidelines annually into your foundation model. Which fine-tuning schedule best balances freshness and cost?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Semi-annual fine-tuning on guidelines plus a small weekly replay buffer of older data",
      "B": "Monthly full-data fine-tuning",
      "C": "Ad-hoc fine-tuning only when guidelines change",
      "D": "Continuous daily fine-tuning with only new guidelines"
    },
    "explanation": "Semi-annual comprehensive fine-tuning with weekly replay prevents forgetting and limits compute. Monthly full fine-tuning is costly; ad-hoc risks staleness; daily tuning is overkill and costly."
  },
  {
    "id": "b2d7e31626903bd6656a9c5baf4f7d8070baa0a2695d6942333af4e07342223c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "During supervised fine-tuning, your loss decreases but evaluation accuracy on domain tasks stagnates. Which hypothesis and fix are most plausible?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Data leakage\u2014shuffle training data",
      "B": "Optimizer misconfiguration\u2014switch from AdamW to SGD",
      "C": "Overfitting common patterns\u2014introduce early stopping based on validation",
      "D": "Model too small\u2014switch to larger foundation model"
    },
    "explanation": "Decreasing loss with stagnant accuracy indicates overfitting; early stopping on validation halts training before overfitting. Data leakage causes high eval scores; optimizer change alone doesn\u2019t address overfitting; larger model exacerbates issue."
  },
  {
    "id": "a4b5657f7eaf77bbee46fe65670dc5b9a44b87039d2ba7a1a35ec9abc4befe47",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "You want students to reproduce your fine-tuning results. Which practice ensures experiment reproducibility?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Publish final model artifacts only",
      "B": "Increase random seed variation",
      "C": "Use dynamic learning rate schedules",
      "D": "Log hyperparameters, seed values, code, and data versions in experiment tracking"
    },
    "explanation": "Comprehensive logging of hyperparameters, seeds, code, and data versions enables exact reproduction. Publishing only artifacts omits process; varying seeds reduces reproducibility; dynamic schedules must be recorded to be reproducible."
  },
  {
    "id": "37c68926bb3e76df9c485a8ece07520c8980e6d5184fc80c92d3c07792ad046d",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "What metric would you choose to evaluate the instruction-tuned model\u2019s step-by-step reasoning quality?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "BLEU score on final answer only",
      "B": "Perplexity on the prompt dataset",
      "C": "Fraction of steps where human evaluation rates reasoning correct",
      "D": "Token generation speed"
    },
    "explanation": "Human evaluation of reasoning steps captures quality of chain-of-thought. BLEU on final answer ignores reasoning; perplexity measures fluency; speed irrelevant to reasoning quality."
  },
  {
    "id": "4229b5fed3752742ebe736f24073b3b3d303a4a554874bef997e3d45016f3092",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "Your fine-tuning job on Amazon SageMaker fails due to out-of-memory errors on 8-GPU instances. Which solution reduces memory footprint without degrading performance?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable gradient checkpointing",
      "B": "Reduce batch size to 1",
      "C": "Disable mixed-precision training",
      "D": "Increase model parallelism chunks to more GPUs"
    },
    "explanation": "Gradient checkpointing trades compute for memory, allowing larger models to train without OOM. Batch size 1 slows convergence; disabling mixed precision increases memory; more GPUs adds cost and complexity but checkpointing is first fix."
  },
  {
    "id": "5e2316ae4723b1bf3c9cad1f31c10344b49ca0b2ab958f3ca09f44c4bfcf8f6e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "You must fine-tune a foundation model while ensuring fairness across demographic groups. Which data preparation tactic helps?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Oversample majority group examples",
      "B": "Use stratified sampling to balance demographic representation",
      "C": "Mask demographic tokens during training",
      "D": "Train separate models per demographic"
    },
    "explanation": "Stratified sampling balances training data across demographics. Oversampling majority worsens imbalance; masking tokens removes context; separate models increase maintenance and complicate fairness across groups."
  },
  {
    "id": "4871ef09022c543ccd40e25ac727b7f025f4d8234aad6212063b3a793d2867f7",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "In your RLHF pipeline, you need to efficiently generate rollouts for policy optimization. Which technique reduces computation without sacrificing policy quality?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use full-length trajectories for all episodes",
      "B": "Increase rollout batch size indefinitely",
      "C": "Train policy and value networks jointly with single head",
      "D": "Use off-policy sampling from a replay buffer with prioritized experiences"
    },
    "explanation": "Off-policy prioritized replay focuses on informative experiences, reducing the number of rollouts needed. Full-length trajectories are heavy; batch size increase has diminishing returns; joint training doesn\u2019t address rollout efficiency directly."
  },
  {
    "id": "83e06d8851e6dda137544f132b132cb37b1c33075c24be584c54b041a31e7cab",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "A legal firm uses a foundation model to summarize complex contracts. The firm requires summaries that preserve legal semantics rather than exact wording. Which automatic evaluation metric should be prioritized to measure semantic similarity?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ROUGE-L",
      "B": "BLEU",
      "C": "BERTScore",
      "D": "Perplexity"
    },
    "explanation": "BERTScore measures semantic similarity using contextual embeddings, making it more suited to capture meaning preservation than surface overlap metrics like ROUGE or BLEU."
  },
  {
    "id": "ef8233b227656eeb9e3c05bc51a3bafb9bcead0a3f0b3d6040a1bb1f6ae66604",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "A developer fine-tunes a code generation foundation model and wants to measure the percentage of generated functions that compile and pass unit tests. Which evaluation metric best captures this requirement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "BLEU",
      "B": "pass@k",
      "C": "Cyclomatic complexity",
      "D": "Perplexity"
    },
    "explanation": "pass@k directly measures the fraction of samples that pass testing under k attempts, reflecting functional correctness rather than surface similarity or code complexity."
  },
  {
    "id": "5e1eac71f2a30d4a053d92a3cbc40049593612b9d7f262d34e362c2ab5bbe53c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "You need to benchmark two multilingual translation foundation models across multiple language pairs. Which dataset and metric combination is most appropriate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "WMT and BLEU",
      "B": "GLUE and ROUGE",
      "C": "SQuAD and BERTScore",
      "D": "COCO and METEOR"
    },
    "explanation": "The WMT (Workshop on Machine Translation) dataset with BLEU is the industry standard for evaluating machine translation quality across language pairs."
  },
  {
    "id": "3f98118bebc38afcbaa57b1bdcccb94c67b6a5395049f9f028d7d9ea3265ec59",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "When evaluating paraphrased customer support responses, which metric accounts for synonym usage and paraphrase quality over exact word overlap?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "BLEU",
      "B": "ROUGE",
      "C": "Exact match",
      "D": "METEOR"
    },
    "explanation": "METEOR incorporates stemming and synonym matching, making it better for paraphrase evaluation than surface-overlap metrics like BLEU or ROUGE."
  },
  {
    "id": "66735096aea90a4feaae721dc28f7749f1d4910773602ff0a698ae50727456d6",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "A summarization model\u2019s outputs must be easy for non-expert readers. Which evaluation metric specifically measures readability?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ROUGE",
      "B": "Flesch-Kincaid score",
      "C": "BERTScore",
      "D": "Perplexity"
    },
    "explanation": "The Flesch-Kincaid score quantifies text readability based on sentence and word length, unlike ROUGE or BERTScore which measure overlap or semantics."
  },
  {
    "id": "6b5992f727d1cc9a847d54ecfbe162c95e7dd80a1ea390faee9be52ffa204460",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "A customer service chatbot built on a foundation model should actually resolve user inquiries. Which business metric aligns best with this objective?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "BLEU",
      "B": "Average turn length",
      "C": "Perplexity",
      "D": "Resolution rate"
    },
    "explanation": "Resolution rate directly measures the proportion of conversations that successfully resolve user issues, reflecting business value."
  },
  {
    "id": "a441522ce91b9881dd4ab2522f5be453a729e5a6a63d496d838b38a0bf9317bd",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "To detect hallucinations in a fact-based generation task, which evaluation approach is most effective?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Measure BLEU",
      "B": "Measure ROUGE",
      "C": "Use QAGS for factual consistency",
      "D": "Compute perplexity"
    },
    "explanation": "QAGS evaluates factual consistency by comparing generated claims against source documents, while BLEU/ROUGE/perplexity do not assess factual correctness."
  },
  {
    "id": "93b7c909c5543b2b669e64acff0f0956dbbd0f35941e528e4865d1887aa574e5",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "Given a limited evaluation budget but a need for an automatic metric that correlates well with human judgment on summarization quality, which metric should you choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "BLEU",
      "B": "BERTScore",
      "C": "ROUGE",
      "D": "Perplexity"
    },
    "explanation": "Studies show BERTScore correlates more closely with human judgments of semantic quality than BLEU or ROUGE for summarization."
  },
  {
    "id": "a9118bf216485022698d0e14f6fe9dffa0e3b41fe4d40e7badcadb6663f79364",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "In assessing the diversity of creative text generations from a foundation model, which metric helps quantify n-gram variation across outputs?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Self-BLEU",
      "B": "BLEU",
      "C": "ROUGE",
      "D": "BERTScore"
    },
    "explanation": "Self-BLEU measures diversity by computing BLEU of each output against other outputs; lower Self-BLEU indicates greater diversity."
  },
  {
    "id": "e025fa1bf02de0c75a66bdb5df10a6313da495dfab79fc0edc2f5b13d92a8005",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "A marketing team uses a foundation model to draft ad copy. To evaluate impact on business outcomes, which metric is the most direct indicator?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ROUGE",
      "B": "Perplexity",
      "C": "Click-through rate (CTR)",
      "D": "Accuracy"
    },
    "explanation": "CTR measures how often users click on ads, directly reflecting marketing effectiveness unlike text overlap or perplexity scores."
  },
  {
    "id": "960a133f51582ea02848b36223863d14e23f41967e668f796f2d07e1ac37ff6c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "When summarization outputs must not exceed 100 words consistently, which evaluation metric measures length compliance across the dataset?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ROUGE",
      "B": "BLEU",
      "C": "Temperature",
      "D": "Compression ratio"
    },
    "explanation": "Compression ratio (output length/reference length) and explicit length checks quantify adherence to word limits, unlike ROUGE or BLEU."
  },
  {
    "id": "a8e35d42d3393462e5df2c95c6393f4b5d9672a90849484a6decfa78a5ef89dc",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "To test whether an observed improvement in BLEU score between two model versions is statistically significant, which method should you apply?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Bootstrap resampling for confidence intervals",
      "B": "Paired t-test",
      "C": "ANOVA",
      "D": "Single-sample z-test"
    },
    "explanation": "Bootstrap resampling computes confidence intervals for BLEU differences without assuming normality, which is preferable for text metrics."
  },
  {
    "id": "5be79d1d733ffcccc124c02c6cd8dc190df871272afdccf42a0fee7f585d5a0a",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "For a retrieval-augmented generation (RAG) application, which pair of evaluation metrics best measure retrieval accuracy and subsequent generation quality?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "BLEU and perplexity",
      "B": "Recall@k and ROUGE",
      "C": "Accuracy and F1",
      "D": "Precision and recall"
    },
    "explanation": "Recall@k evaluates how often relevant documents are retrieved, and ROUGE assesses the quality of the generated text given those documents."
  },
  {
    "id": "5a1de594070036d0d85cab85bdfd8006e69d28b3664083e9e8c87e45c08a45f1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "Which evaluation strategy specifically quantifies the rate of hallucination occurrences in generated outputs?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Calculate BLEU",
      "B": "Compute ROUGE",
      "C": "Measure perplexity",
      "D": "Human annotation to label hallucinated statements"
    },
    "explanation": "Only human annotation can reliably identify and quantify hallucinations; automated overlap or likelihood metrics cannot detect factual errors."
  },
  {
    "id": "b8ed625cfa8a20240e9ea6d125802ba67f7a44cd50dc8e8e4aefad79dbec61ee",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "Which metric penalizes repetition in generated text by measuring the proportion of unique n-grams?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ROUGE",
      "B": "Perplexity",
      "C": "distinct-n",
      "D": "BLEU"
    },
    "explanation": "distinct-n computes the ratio of unique n-grams to total n-grams, penalizing repetitive outputs unlike ROUGE or BLEU."
  },
  {
    "id": "2c1307b97c16a54d1ff935bad73f757e62e72b18248b6c9be20a61d9166db5c6",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "Which benchmark dataset would you choose to evaluate a foundation model\u2019s question-answering performance on general topics?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SQuAD",
      "B": "GLUE",
      "C": "WMT",
      "D": "COCO"
    },
    "explanation": "SQuAD is designed for extractive question answering across diverse topics, while GLUE is for classification, WMT for translation, and COCO for images."
  },
  {
    "id": "b84530a5d8da8cd1be12be6b842f68d86c03ce0b3998b62fa342310d1a6589bd",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "To monitor drift in a deployed foundation model\u2019s outputs over time, which evaluation approach is most appropriate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Cross-validation with multiple folds",
      "B": "Periodic human review of sampled outputs",
      "C": "Single-run perplexity measurement",
      "D": "Monitoring parameter count"
    },
    "explanation": "Periodic human review identifies qualitative shifts or biases in outputs that automated metrics might miss, making it essential for drift detection."
  },
  {
    "id": "6baa01e10eb056782d7113fde62c9b571117b756c8ae219585016dd389d73b67",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "For evaluating translation at the character level to better handle morphologically rich languages, which metric should you use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "BLEU",
      "B": "ROUGE",
      "C": "FID",
      "D": "chrF"
    },
    "explanation": "chrF calculates F-scores over character n-grams, improving evaluation for languages with complex morphology compared to word-based metrics."
  },
  {
    "id": "9445abc1a0171d2318535b58d88631d996b1c2619eb6f7b7dd56b889dbba9b6b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "In cross-lingual summarization tasks, which automatic metric leverages multilingual embeddings to assess semantic similarity?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Perplexity",
      "B": "BLEU",
      "C": "BERTScore",
      "D": "METEOR"
    },
    "explanation": "Multilingual BERTScore uses contextual embeddings across languages for semantic evaluation, outperforming BLEU or METEOR on cross-lingual tasks."
  },
  {
    "id": "03e132f9feec9de6bee2de5ab7dc1d3d6dec71f760fdda076c14ad67f43ef56e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "Which evaluation procedure ensures that improvements in ROUGE score are due to true model gains and not sample variance?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Cross-validation with multiple folds",
      "B": "Single-run on a fixed test set",
      "C": "Increasing beam size during decoding",
      "D": "Lowering the generation temperature"
    },
    "explanation": "Cross-validation over multiple folds reduces variance by averaging performance across diverse data splits, validating genuine improvements."
  },
  {
    "id": "c58118f8c46ab3368295204f306542232c47d24a87132208fcd8fcaee121063d",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "When evaluating generative image models, which metric should you use to assess both fidelity and diversity of outputs?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "BLEU",
      "B": "Fr\u00e9chet Inception Distance (FID)",
      "C": "ROUGE",
      "D": "BERTScore"
    },
    "explanation": "FID measures the distance between feature distributions of real and generated images, capturing both diversity and quality."
  },
  {
    "id": "35e97b2d16fe8ed2a347b50f25bdcaf547a227011260c2c1566b497fe828fe33",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "A product team uses user engagement metrics to assess a chat model\u2019s improvements. Which combination best reflects user satisfaction?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "BLEU and ROUGE",
      "B": "Perplexity and BERTScore",
      "C": "Hit rate and recall",
      "D": "Net Promoter Score (NPS) and average session length"
    },
    "explanation": "NPS surveys gauge satisfaction and loyalty, while session length indicates engagement\u2014together they reflect real user experience improvements."
  },
  {
    "id": "5a07b268ab56a124502ab629998082d9f8e5a6ef3e9582719ba8a3ae7587a1a8",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "To evaluate whether a fine-tuned LLM answers domain-specific queries accurately, which evaluation dataset style should be used?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "GLUE",
      "B": "WMT",
      "C": "In-domain benchmark dataset with labeled answers",
      "D": "COCO"
    },
    "explanation": "An in-domain benchmark with labeled answers ensures evaluation reflects the specific content and difficulty of target queries."
  },
  {
    "id": "7aa9156981e372c4e636a25b23734af1504b2df5fd27c4145f97f0cb352aec89",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "Which automatic evaluation metric is least reliable for open-ended text generation tasks, often showing poor correlation with human judgment?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Perplexity",
      "B": "BLEU",
      "C": "Human evaluation",
      "D": "ROUGE"
    },
    "explanation": "Perplexity measures model confidence, not output quality, and often correlates weakly with human assessments of open-ended generations."
  },
  {
    "id": "d4a2ff164e3aed70e941f5fa862fe07c36ca3c3f1832d0e9d0654eafbfc96f7b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "Which metric can identify shifts in output token distribution between two model versions by measuring distributional divergence?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "BLEU",
      "B": "KL divergence of n-gram distributions",
      "C": "ROUGE",
      "D": "Perplexity"
    },
    "explanation": "KL divergence quantifies how one probability distribution diverges from another, making it suitable for detecting distribution shifts."
  },
  {
    "id": "5c7bb750a41f40d65dfd4a3f5049185d03dbd54531ff421702ee17382cba0f04",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "To evaluate factual consistency automatically, which specialized tool or metric should you use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Perplexity",
      "B": "BLEU",
      "C": "ROUGE",
      "D": "FactCC"
    },
    "explanation": "FactCC uses a classifier to detect factual errors in generated text, providing an automated measure of factual consistency."
  },
  {
    "id": "1816e1f6db724d4a5da247c7f81e0d5ed4c46e5f75c4f480089a266c4b022037",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "Which evaluation approach measures the trade-off between generation latency and text quality in a foundation model?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "BLEU alone",
      "B": "Log-likelihood",
      "C": "Plot BLEU against average response time",
      "D": "Perplexity"
    },
    "explanation": "Plotting BLEU versus response time reveals how quality changes with latency, enabling balanced performance optimization."
  },
  {
    "id": "a0160168463e5f6ca09bdb1fd8f3dcf71b8ca3d8ad4d072693a3690889cc1782",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "To evaluate a multilingual retrieval-augmented generation system, which combined metrics should be used for retrieval effectiveness and generated text quality?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "MRR and BERTScore",
      "B": "BLEU and Perplexity",
      "C": "FID and ROUGE",
      "D": "Accuracy and F1"
    },
    "explanation": "MRR measures retrieval ranking quality, and BERTScore evaluates semantic quality of the generated text in multiple languages."
  },
  {
    "id": "cbbb4461b309dd39f744e7a0381384db730f4c07b3480ec5f3b4ba9f6765f25d",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "Which evaluation technique reduces annotation bias in human assessments of generated outputs?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Rate outputs with known prompts visible",
      "B": "Use leaderboard scores only",
      "C": "Remove human evaluators",
      "D": "Blind evaluation of outputs without prompts"
    },
    "explanation": "Blind evaluation, where evaluators don\u2019t see prompts or model identity, minimizes bias toward known systems or prompts."
  },
  {
    "id": "f0512a329c1ce406b855fbf9e4a7029403ac96b3f555a9dbc032b53f944bfa54",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "When constructing a summarization benchmark for reliable performance evaluation, which dataset characteristic is most critical?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Minimal dataset size",
      "B": "Domain diversity in reference summaries",
      "C": "Single reference per example",
      "D": "Unlabeled references"
    },
    "explanation": "Domain diversity in references ensures the benchmark covers varied content, making evaluation results generalizable across use cases."
  },
  {
    "id": "b6f8596ed758f37220c4d4c791b55d39077816426bfc590216011fced1dfe3f0",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "A financial services company uses a SageMaker Clarify pre-training bias analysis and finds a high Statistical Parity Difference (SPD) for a protected group. Which next step best mitigates bias at the dataset level?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Apply class weighting during model training to penalize misclassifications for the protected group.",
      "B": "Inject synthetic samples of the protected group after model training to balance outputs.",
      "C": "Use stratified sampling or data augmentation to increase representation of the under-represented group before retraining.",
      "D": "Adjust the decision threshold post-inference to equalize positive rates across groups."
    },
    "explanation": "Mitigating SPD bias at the dataset level requires increasing representation via stratified sampling or augmentation prior to training. Other options address bias post-training or at inference."
  },
  {
    "id": "5f6f9e321eef67322d9e2b735c411abfdf7ca58f85ea0207d71456e42ffa1783",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "An e-commerce retailer is concerned about model overfitting and lack of robustness when training a computer vision model. Which combination of responsible AI practices most directly addresses variance risk?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Model Monitor to detect drift and apply differential privacy during training.",
      "B": "Implement RLHF and use Bedrock Guardrails to enforce safety constraints.",
      "C": "Apply data pruning and use a larger transformer foundation model for high accuracy.",
      "D": "Augment training images for diversity and use k-fold cross-validation to measure generalization."
    },
    "explanation": "Augmenting data increases diversity reducing variance, while k-fold cross-validation assesses generalization. Other options address drift, privacy, or performance but not variance."
  },
  {
    "id": "547539dfcfcf2a5ef85e6eb968618ba42fe02f978390d13ddccfcf494f5b6fd6",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "A legal team warns that a fine-tuned foundation model may produce outputs infringing third-party IP. Which responsible AI control best reduces IP infringement risk?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Implement content filtering using Bedrock Guardrails to block copyrighted text generation.",
      "B": "Use SageMaker Clarify to identify copyright in the training data.",
      "C": "Apply differential privacy during fine-tuning to obscure original data.",
      "D": "Deploy Model Monitor to detect IP patterns in inference logs."
    },
    "explanation": "Bedrock Guardrails enforce rules at generation time to block copyrighted outputs. Clarify and privacy techniques address bias/data privacy, not IP output control."
  },
  {
    "id": "b233d9a90dfa7cdc70e8f83fbeaabb3f4029be74df1f4afc14dcfeca3b4785e6",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "A model deployed on SageMaker shows a rising False Positive Rate (FPR) for a minority group over time. Which AWS capability and metric should the team use to detect this fairness drift?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Clarify post-training: monitor AUC separately for each group.",
      "B": "SageMaker Model Monitor: configure a bias drift job to track group-specific FPR over time.",
      "C": "Amazon A2I: route predictions for the minority group to human reviewers.",
      "D": "SageMaker Debugger: set up rule to detect increased loss for the minority group."
    },
    "explanation": "Model Monitor\u2019s bias drift jobs can track fairness metrics like group-specific FPR. Clarify is offline, A2I routes for human review but does not detect drift automatically, Debugger tracks training metrics."
  },
  {
    "id": "024f3ea8e8944e6a43dcd5ecdc731c7044d570ec8fb434dc9b55afffb9e0e3e9",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "An organization must document data lineage and responsible AI decisions across the ML pipeline. Which AWS service combination provides end-to-end lineage and audit capabilities?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Config to track resource changes and SageMaker Clarify for bias reports.",
      "B": "AWS CloudTrail for API activity and Amazon Macie for data classification.",
      "C": "SageMaker Model Monitor for drift and Amazon Inspector for vulnerabilities.",
      "D": "AWS Glue Data Catalog for metadata lineage and AWS CloudTrail for audit logs."
    },
    "explanation": "Glue Data Catalog captures dataset metadata and lineage; CloudTrail logs API calls and workflow steps for audits. Other options address security or bias reporting but not full lineage."
  },
  {
    "id": "c5611e158ee8605d344c941a18df46b8b86fe72910344d7f8031f62724e713d7",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "A healthcare provider needs to ensure its model does not discriminate by gender. They require transparency of feature impact. Which approach best delivers explainability and bias detection?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon A2I to have clinicians label misclassifications by gender.",
      "B": "Train with differential privacy to avoid encoding gender information.",
      "C": "Use SageMaker Clarify\u2019s SHAP explainability to measure gender\u2019s feature importance and bias metrics.",
      "D": "Configure Bedrock Guardrails to block gender-related outputs."
    },
    "explanation": "Clarify SHAP provides feature impact and bias metrics for sensitive attributes. A2I handles human review, privacy training obfuscates but not explains, Guardrails block outputs, not explain model internals."
  },
  {
    "id": "43f8c8d22f4c50e5110aff13de97d8c210a6406563806fd9b74632244b69cff0",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "A startup chooses between a large foundation model and a smaller custom model. To minimize environmental impact while meeting 95% of their accuracy target, which responsible AI strategy should they apply?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use the large foundation model with reduced precision (FP16) and no fine-tuning.",
      "B": "Benchmark both models for energy consumption per inference and select the smaller custom model if its accuracy meets the target.",
      "C": "Host the large model on GPU-powered instances with auto-scaling to reduce idle time.",
      "D": "Apply on-device inference to the foundation model to eliminate data center emissions."
    },
    "explanation": "Benchmarking energy per inference and choosing the smallest model that meets accuracy minimizes environmental impact. Precision and hosting strategies affect cost/performance but don't directly measure impact."
  },
  {
    "id": "a153954fcb1f7a0d040eca9ff080db2a74abb7c854bd1f49accb75289174de5e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "A retail company implements RAG with Bedrock for QA but fears hallucinations. Which responsible AI control at runtime best reduces hallucination risk?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable SageMaker Clarify\u2019s hallucination detection post-inference.",
      "B": "Use Amazon A2I to route uncertain answers for human verification.",
      "C": "Fine-tune the foundation model on curated domain data.",
      "D": "Implement Bedrock Guardrails with citation requirements to only respond with source-linked information."
    },
    "explanation": "Guardrails can enforce citations and block unsupported content, directly addressing hallucinations at runtime. Clarify does not detect hallucinations, A2I reviews but doesn\u2019t prevent, fine-tuning reduces but doesn\u2019t eliminate."
  },
  {
    "id": "87cc632081b767d1b5f57b5857c485e1a1d11826923884036d9a300f598ce01c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "A government agency needs to ensure ML model decisions are explainable to comply with regulations. Which AWS tool and practice combination supports this requirement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Clarify to generate Model Cards and SHAP explainability reports.",
      "B": "Leverage Amazon A2I to have domain experts annotate each decision.",
      "C": "Deploy foundation models via Bedrock with no customization for transparency.",
      "D": "Store all inference logs in Amazon S3 for manual auditing later."
    },
    "explanation": "Clarify Model Cards and SHAP reports provide structured model documentation and feature-level explainability. A2I reviews specific instances but not full model transparency; logs alone aren\u2019t sufficient."
  },
  {
    "id": "6bfd580753285150894e062b2dfa0856afde46623333da16f0be4ad8a39234e5",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "A fraud detection model exhibits high variance yet low bias. Which dataset characteristic adjustment most directly reduces variance?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Ensure the dataset is demographically representative by adding minority samples.",
      "B": "Increase overall dataset size by collecting more varied fraudulent/non-fraudulent examples.",
      "C": "Remove features highly correlated with the target variable to simplify the model.",
      "D": "Adjust the decision threshold to favor reducing false positives."
    },
    "explanation": "High variance is reduced by increasing dataset size and diversity. Adding minority samples addresses bias. Feature removal and threshold adjustments address complexity and operating point, not variance."
  },
  {
    "id": "92f9c0531fcfd2a4f6027d016280fd01bb6be9cfa6f7df938f47e0e2415c9280",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "During bias detection, a team discovers a proxy variable highly correlated with sensitive attribute. Which responsible AI action best addresses this issue?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Mask the sensitive attribute at inference to prevent discrimination.",
      "B": "Use RLHF to teach the model fairness constraints.",
      "C": "Remove or de-correlate the proxy variable from the training dataset.",
      "D": "Adjust class weights during training to counter the proxy effect."
    },
    "explanation": "Removing or de-correlating the proxy variable prevents the model from learning unintended sensitive correlations. Masking at inference doesn\u2019t retrain, RLHF isn\u2019t designed for bias removal, class weights address imbalance but not proxy variables."
  },
  {
    "id": "03c98be214362bf26a310ab2e9310bbc17cfb9b5f716b873fad0d3a76b885d55",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "A voice assistant model trained on English data shows poor performance and bias when used in a non-English region. Which dataset curation practice most responsibly addresses this outcome?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy an English foundation model with higher capacity GPUs to improve recognition.",
      "B": "Use Bedrock Guardrails to translate non-English inputs to English before inference.",
      "C": "Apply post-processing to normalize accents in the English transcripts.",
      "D": "Collect and include diverse voice data from the target region to retrain or fine-tune the model."
    },
    "explanation": "Collecting and including regional voice data ensures diversity and reduces bias. Other options patch translation or normalization without addressing dataset deficiency."
  },
  {
    "id": "6340ad8827ff06173711172a6e8c342094dc3111ea3e3cc2876d8d0bae5bab7c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "An NLP model fine-tuned on user reviews shows unexpected bias towards negative sentiment for a demographic. Which Clarify metric pair best identifies both bias and explainability issues?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Post-training bias analysis with Disparate Impact and SHAP feature importance for demographic tokens.",
      "B": "Pre-training bias analysis with Chi-squared test and LIME for local explanations.",
      "C": "Model monitor with data quality checks and concept drift detection.",
      "D": "A2I simulation to route negative sentiment for human audit and RLHF to adjust behavior."
    },
    "explanation": "Post-training bias analysis with Disparate Impact quantifies bias; SHAP explains feature contributions. Pre-training analysis doesn\u2019t reflect model; other options don\u2019t cover both bias and explainability."
  },
  {
    "id": "e9e69879331e1ea4a335ff16888812dff67a0fecba644b748a30c1cbe34c1b38",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "A social media platform wants to ensure its content recommendation model is robust and fair. Which multi-step approach correctly applies responsible AI practices?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy bedrock foundation model with no customization, then monitor user complaints.",
      "B": "Use diverse training data, perform Clarify bias analysis, then implement Model Monitor for drift and fairness metrics.",
      "C": "Apply RLHF to incorporate user feedback, then use A2I to human-review flagged recommendations.",
      "D": "Use Amazon Comprehend to detect hate speech and apply Bedrock Guardrails to block content."
    },
    "explanation": "Diverse data plus Clarify bias analysis and Model Monitor drift checks covers training data fairness, bias validation, and ongoing monitoring. Other options focus only on feedback loops or content filtering."
  },
  {
    "id": "236aa0c70722b60ebb2130728c7a51add003206b4756f4e2a22183cdf05214e1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "An energy company must choose between batch and real-time inference to reduce carbon footprint. Which responsible AI consideration should guide their choice?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Real-time inference ensures fairness by updating model with live data.",
      "B": "Batch inference simplifies explainability by aggregating predictions.",
      "C": "Batch inference on low-usage off-peak times reduces energy consumption compared to always-on real-time endpoints.",
      "D": "Real-time inference allows Guardrails to block hallucinations instantly."
    },
    "explanation": "Batch inference during off-peak reduces energy use, aligning with sustainability goals. Real-time addresses performance and safety but uses constant compute."
  },
  {
    "id": "4859eb40ed044ae86cb9887cfc56fb4a2ade4ce417e89608b344d70741b8a0d2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "A hiring platform\u2019s resume screening model shows high false negative rates for a protected group. Which fairness metric adjustment and mitigation step is appropriate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Optimize for equalized odds by balancing FPR across groups via post-processing calibration.",
      "B": "Target equal opportunity by adjusting threshold to equalize True Positive Rate (TPR) for the protected group.",
      "C": "Use DP-SGD during training to ensure differential privacy for all candidates.",
      "D": "Deploy Clarify\u2019s pre-training bias report and retrain on a larger dataset."
    },
    "explanation": "Equal opportunity focuses on equal TPR; adjusting thresholds post-training targets false negatives. Equalized odds balances both FPR and FNR; DP-SGD addresses privacy, not fairness; pre-training bias doesn\u2019t guarantee TPR parity."
  },
  {
    "id": "265ab2c85f4f3cf0347d75b6f18680baa722fc4d2b243edcc68f3d014f0ab922",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "A model serving medical diagnoses must be auditable for bias and quality. Which combination of AWS services automates bias detection and logging at inference time?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Clarify with server-side inference wrappers and CloudWatch Logs for real-time bias metrics.",
      "B": "Amazon A2I for human-in-the-loop and S3 event notifications for logs.",
      "C": "Model Monitor batch jobs scheduled hourly and AWS Config for configuration logs.",
      "D": "Bedrock Guardrails for diagnosis validation and CloudTrail for API logs."
    },
    "explanation": "Combining Clarify in inference wrappers emits bias metrics, and CloudWatch Logs captures them. Model Monitor process is offline. A2I is human review, not automatic detection. Guardrails validate content but not bias metrics."
  },
  {
    "id": "ef8c7df92097290dfddf9dc2fa1940be1b424e8efefe051d7582f389aaf81ce1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "A bank uses a decision tree model that is transparent but underperforms. Which trade-off should the team consider for responsible AI?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch to a black-box model and rely on post-hoc explanations to boost accuracy.",
      "B": "Remove explainability requirements to allow complex ensemble models.",
      "C": "Implement RLHF to retrain the decision tree on dynamic feedback.",
      "D": "Balance interpretability and performance by exploring a surrogate model or shallow ensemble with built-in explanations."
    },
    "explanation": "Surrogate or shallow ensemble allows slightly higher complexity with interpretability. Fully black-box loses transparency. Removing explainability breaks compliance. RLHF is for generative models, not decision trees."
  },
  {
    "id": "af3e56e9d30d3c35b7e1f54e7fca4d4d219d70de9bc9822509ccacd373cd66f7",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "A company trains a sentiment analysis model on customer feedback. They discover the model overfits on lengthier negative reviews. Which responsible AI technique addresses this bias?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use A2I to filter out long negative reviews during inference.",
      "B": "Implement Bedrock Guardrails to normalize output sentiment scores.",
      "C": "Apply length-based sampling or reweighting in the training set to balance review lengths.",
      "D": "Fine-tune the model further with more negative samples."
    },
    "explanation": "Balancing training samples by review length addresses overfitting to long texts. A2I and Guardrails act at inference; more negative samples reinforce, not fix length bias."
  },
  {
    "id": "d8ec7e80931233109bec9fbdea08d4594a72f50c8f648e3f72549e7a5f042468",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "A chatbot built with Bedrock must avoid generating unsafe content. Which responsible AI practice at the design stage is most effective?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Clarify to detect bias in the training corpus.",
      "B": "Ingest and label unsafe prompt examples, then configure Guardrails to block or redirect those scenarios.",
      "C": "Deploy Model Monitor to detect unsafe outputs after production.",
      "D": "Enable Amazon WAF to filter content returned by the chatbot."
    },
    "explanation": "Labeling unsafe prompts upfront and configuring Guardrails prevents unsafe content generation. Clarify addresses bias, Monitor is reactive, WAF filters at network level, not semantics."
  },
  {
    "id": "37a3d8df231c2043849d3817fbd75e70ec03096b9379c0c459acb33fb388752a",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "An AI image classification pipeline uses Amazon A2I. Which workflow change enhances responsible AI by reducing human bias in reviews?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provide diverse annotator pools and rotate assignments to minimize individual bias.",
      "B": "Only route low-confidence images for review without systematic sampling.",
      "C": "Use fixed guidelines without updating them to ensure consistency.",
      "D": "Increase the AI confidence threshold to reduce human interventions."
    },
    "explanation": "Diverse annotators and rotation reduce individual biases. Routing only low-confidence may miss systematic errors; fixed guidelines become outdated; raising threshold reduces oversight."
  },
  {
    "id": "10d327495a73d913be7de508c755f44dd5b08cdb040a926632428d444d7169ef",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "A transportation firm must ensure their ML model aligns with environmental sustainability guidelines. Which metric and AWS feature should they monitor?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Track energy consumption per batch inference via SageMaker Debugger and AWS CloudWatch metrics.",
      "B": "Monitor carbon emission estimates from Bedrock Guardrails.",
      "C": "Use Clarify to report environmental bias in the dataset.",
      "D": "Implement A2I to sample inferences for manual sustainability audits."
    },
    "explanation": "SageMaker Debugger profiles training/inference resource usage; CloudWatch aggregates metrics, enabling monitoring of energy consumption. Guardrails and Clarify don\u2019t provide sustainability metrics."
  },
  {
    "id": "285d57b1a33bd828186979f1adaa3eab893090daae3efcce350af2a5e8ae49ae",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "An AI system must comply with an accountability law requiring proof of non-discrimination. Which deliverable best demonstrates compliance?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Stored inference logs in S3 with anonymized user IDs.",
      "B": "A2I human review transcripts for flagged inferences.",
      "C": "Generated Model Card from SageMaker Clarify including bias metrics and mitigation steps.",
      "D": "Bedrock Guardrails configuration file showing blocked content rules."
    },
    "explanation": "A Model Card documents model details, bias metrics, and mitigation, serving as formal proof. Logs and reviews are raw data; Guardrails config shows rules but not measured bias."
  },
  {
    "id": "6bbabe651a742a9dac71d782263d75c60b933cb00929d1ed2933b49f3668fe3a",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "A retail AI solution applies oversampling of under-represented class, causing overfitting. Which strategy maintains dataset balance while reducing overfitting risk?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch to SMOTE to generate synthetic minority samples without real data.",
      "B": "Combine controlled undersampling of majority and slight oversampling of minority, followed by cross-validation.",
      "C": "Use AWS Glue DataBrew to mask minority class labels during training.",
      "D": "Increase model regularization hyperparameters until overfitting subsides."
    },
    "explanation": "Balanced undersampling and moderate oversampling reduces class imbalance and overfitting; SMOTE can introduce noise; masking labels corrupts data; regularization alone may not address imbalance."
  },
  {
    "id": "51b217a7f4a87c64b0b12215f3aae7a491e432ce23638de08f6b62f8ed7d2405",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "A company fine-tunes a foundation model on user data containing PII. Which responsible AI step ensures privacy compliance before fine-tuning?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Clarify to detect PII entities in training data and remove or anonymize them.",
      "B": "Enable Bedrock Guardrails to block PII during inference.",
      "C": "Configure Model Monitor to alert on PII patterns in logs.",
      "D": "Rely on Data Catalog policies to quarantine sensitive buckets."
    },
    "explanation": "Clarify PII detection can identify and remove/anonymize PII pre-training. Guardrails and Model Monitor act post-deployment; Data Catalog policies manage storage, not data content."
  },
  {
    "id": "c58830b29d3551f75440e7ab97ac1c6952c18bbbad52fc2821c4dbce5e67e5c7",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "A model shows underfitting due to overly strict fairness constraints during training. Which mitigation balances fairness with performance?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Remove all fairness constraints and retrain on raw data.",
      "B": "Increase penalty weight on fairness loss to reduce bias further.",
      "C": "Apply differential privacy to reduce model complexity.",
      "D": "Relax fairness constraint hyperparameter slightly and monitor performance vs bias trade-off."
    },
    "explanation": "Relaxing constraint hyperparameter allows model to trade a bit more bias for performance. Removing constraints violates fairness; increasing penalty worsens underfitting; privacy doesn\u2019t address fairness/performance trade-off."
  },
  {
    "id": "035aefa79e1864e9e3f927b7f9df1eca3707a0fc5f00c4ecb2d0855b510c01fd",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "A content moderation model must detect emerging hate speech terms. Which AWS service and practice support ongoing dataset curation?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Clarify analyzers to detect new terms in training data.",
      "B": "Implement Amazon Kendra on user reports to discover new terms, then retrain model periodically.",
      "C": "Configure Bedrock Guardrails to ban any unknown terms automatically.",
      "D": "Rely on Model Monitor drift detection to alert on lexical shifts."
    },
    "explanation": "Kendra can index user reports to surface new terms; human in the loop identifies them for dataset updates. Clarify doesn\u2019t detect emergent terms; Guardrails ban known rules; drift detection alerts but requires manual term discovery."
  },
  {
    "id": "d0125f7f199540c11acd1dbbb51a9ef21f92a18bff451b7da23367980c179863",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "Which combination of AWS services and tasks demonstrates end-to-end responsible AI for a fraud detection pipeline?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Glue Data Catalog for lineage, SageMaker Clarify for bias analysis, Model Monitor for drift, and CloudTrail for audit logs.",
      "B": "Amazon A2I for human review, Bedrock for model serving, WAF for security, and Cost Explorer for cost analytics.",
      "C": "AWS Config for configuration management, Amazon Comprehend for NLP, RDS for storage, and Inspector for vulnerability scanning.",
      "D": "SageMaker JumpStart for model templates, S3 for data storage, CloudFront for delivery, and Macie for data classification."
    },
    "explanation": "Glue Data Catalog tracks data lineage; Clarify performs bias analysis; Model Monitor checks drift; CloudTrail logs activities, covering responsible AI. Other combinations miss key responsibilities."
  },
  {
    "id": "447eeda9bc7b4c442f8b91debbd1ff3f93bc99e6b44744241997d36fa8750332",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "A model\u2019s predictions must be fair across subgroups; the team chooses Demographic Parity. Which operational step ensures this fairness definition is maintained?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Implement RLHF to shape model behavior according to parity.",
      "B": "Add a bias-aware post-processing component to adjust prediction probabilities to match subgroup rates.",
      "C": "Use differential privacy during training to equalize distributions.",
      "D": "Apply Bedrock Guardrails to filter outputs by subgroup outcomes."
    },
    "explanation": "Post-processing adjustment aligns subgroup positive rates for Demographic Parity. RLHF, privacy, or Guardrails do not directly enforce statistical parity at output distribution."
  },
  {
    "id": "031f92b6ae11e797463d3b590dc4fad4671f35286615797188a576c078ad7689",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "A biotech firm needs human review on high-risk inferences while minimizing bias in those reviews. Which Amazon A2I workflow configuration best achieves this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use private workforce with diverse backgrounds and stratify task assignments to avoid reviewer clustering.",
      "B": "Route all inferences above a confidence threshold to the same internal SME team.",
      "C": "Expose reviewer identities to each other to foster accountability.",
      "D": "Allow reviewers to self-select tasks for efficiency."
    },
    "explanation": "A private, diverse workforce with stratified assignment reduces bias in human review. Routing only high confidence misses risky cases; exposing identities can bias; self-selection leads to bias clustering."
  },
  {
    "id": "1920c4e956c0efda125aceadf2e00b5cb45f1ccd029236184a75731c37ac394f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "A loan approval model must demonstrate robustness against adversarial input perturbations. Which responsible AI technique helps achieve this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Clarify to detect adversarial examples in the training data.",
      "B": "Deploy the model via Bedrock with Guardrails against malicious inputs.",
      "C": "Include adversarial training samples in training set and retrain to increase robustness.",
      "D": "Use Model Monitor to block anomalous inference requests."
    },
    "explanation": "Adversarial training augments training data with perturbed inputs to improve robustness. Clarify flags bias, not adversarial; Guardrails guard content, not adversarial attacks; Monitor blocks but doesn\u2019t improve robustness."
  },
  {
    "id": "300d243468584a4ebe00583c8cd8eff5334d847e0ac21e07b52cc905c5f8286c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "A fintech startup needs an AI model to score loan applications with high interpretability for regulators. They require per-feature contribution analysis. Which model and tool combination best balances transparency and predictive performance?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deep neural network with LIME",
      "B": "Gradient Boosting Machine with SHAP values",
      "C": "Logistic regression with no post-hoc analysis",
      "D": "Transformer-based model with attention visualization"
    },
    "explanation": "GBMs offer strong performance and SHAP provides consistent, global and local interpretability, unlike attention or none."
  },
  {
    "id": "8da6764b38a70fb4cfed53afc443b2b0be953e5ffe0b38ecfa95a4f2f149db02",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "A healthcare system must explain individual patient risk predictions. They need an interactive dashboard of feature influence, counterfactuals, and confidence intervals. Which AWS service and feature set should you use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Clarify alone",
      "B": "Open source SHAP deployed on EC2",
      "C": "SageMaker Model Monitor with Clarify and Model Explainability",
      "D": "SageMaker Endpoints with CloudWatch metrics only"
    },
    "explanation": "Model Monitor plus Clarify supports real-time explainability and dashboards; Clarify alone lacks monitoring."
  },
  {
    "id": "f0ad87d9173c92d59e220bbfb6f9f5fbf2c24fbdfce91034bcfdb7d7dc5ca74e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "A retail chain uses an LLM for product recommendations but regulators demand transparency. Which approach best improves explainability with minimal performance loss?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Implement RAG with knowledge provenance and track token sources",
      "B": "Fine-tune the LLM on proprietary data without documentation",
      "C": "Switch entirely to a rule-based recommender",
      "D": "Use closed-book LLM inference with opaque prompt templates"
    },
    "explanation": "RAG captures source documents, improving transparency while preserving model strengths."
  },
  {
    "id": "fcf7bcd34edd53a60bb48c4a2209c351952c0850f0b741dcd784cca4f18d57cc",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "When presenting model decisions to non-technical stakeholders, which design principle ensures explanations are meaningful?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Display raw feature weights",
      "B": "Show full decision tree structure",
      "C": "Provide accuracy metrics only",
      "D": "Use counterfactual explanations in natural language"
    },
    "explanation": "Counterfactuals in NL align with human reasoning; raw weights or trees overwhelm non-experts."
  },
  {
    "id": "ddcce1725ea93b0e92bd1a6e3c29bf7d78718ceabef952c5747d4186c34b7093",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "A customer complaints model shows unexpected bias against a minority group. Which tool combination helps detect and mitigate this bias while maintaining explainability?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Clarify for bias detection and Fairness Metrics",
      "B": "SageMaker Autopilot for retraining without transparency",
      "C": "Deep neural network debugging with manual feature drop",
      "D": "LIME analysis without bias metrics"
    },
    "explanation": "Clarify\u2019s bias metrics and dashboards directly identify and quantify group disparities."
  },
  {
    "id": "22659f1154e26fc316a75f444a24b267c2651e4c8e4e7a466ca794d0b9ce44d0",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "An insurer needs an audit trail of model behavior and feature importances over time. Which AWS feature should you configure?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Store SageMaker Training jobs only",
      "B": "Enable SageMaker Model Monitor drift and Clarify Reports to S3",
      "C": "Log endpoint API calls in CloudTrail only",
      "D": "Use SageMaker Experiments without explainability modules"
    },
    "explanation": "Model Monitor with Clarify automatically logs drift and explanations to S3 for audits."
  },
  {
    "id": "edd121f305487467ab3e0bd8b113cf0cc721fbd21aea0c736439f9c475c7dca8",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "A supply chain optimization model is highly accurate but opaque. You need feature-level transparency and robust version control. Which workflow fits?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy as AWS Lambda with ad-hoc logging",
      "B": "Serve via SageMaker Endpoint and inspect output only",
      "C": "Use SageMaker Projects with Model Registry and Clarify explainability",
      "D": "Export model to S3 and analyze in external notebook"
    },
    "explanation": "SageMaker Projects plus Model Registry provides traceability and Clarify offers feature insights."
  },
  {
    "id": "372d3a7072ad0d63b88b35ff48168e137206f179e77c27e0c91645c274d23397",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "A marketing team wants to understand why language generation prompts produce certain outputs. You need an explainable approach without exposing model internals. What do you implement?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Visualize transformer attention heads",
      "B": "Extract embeddings for manual inspection",
      "C": "Run explainability only on input tokens",
      "D": "Generate attribution maps via integrated gradients API"
    },
    "explanation": "Integrated gradients gives token-level attribution without model internals exposure."
  },
  {
    "id": "8de7781dc175c4478c3935b0d061b0439e9360bce176bd9f25693677f42377fc",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "A credit scoring model uses a black-box ensemble. Regulators ask for global and local fairness metrics. Which pipeline achieves this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Ingest model predictions into SageMaker Clarify bias and explainability pipelines",
      "B": "Rebuild model as transparent GLM without metrics",
      "C": "Use CloudWatch metrics to track latency",
      "D": "Apply LIME locally on sample predictions only"
    },
    "explanation": "Clarify provides both fairness and explainability metrics globally and locally."
  },
  {
    "id": "4b8cdbf35a5ff66b58dbca45215df1012b9a868bdc08c77586ac153b2bdbe63c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "You must document model lineage, data sources, and explainability tests for compliance. Which combination offers a consolidated view?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CloudTrail logs plus ad-hoc Excel reports",
      "B": "SageMaker Model Registry with Model Cards and Clarify reports",
      "C": "Store docs in S3 without integration",
      "D": "Use AWS Config rules only"
    },
    "explanation": "Model Registry combined with Model Cards centralizes lineage, data, and explainability outputs."
  },
  {
    "id": "afc8ba6ad0950bf2d839db8057390124f8b8b8c2c9246442c6e090cdaf10e26b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "A fraud detection model must provide real-time, human-understandable alerts with explanation. Which integration achieves this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Publish raw scores to CloudWatch",
      "B": "Use SageMaker Asynchronous Inference without explainability",
      "C": "Embed Clarify real-time feature attributions into Lambda for alerting",
      "D": "Send model logits directly to SNS"
    },
    "explanation": "Clarify\u2019s real-time attributions can be passed via Lambda for contextual alerts."
  },
  {
    "id": "9ef12e08765f4339390c4f16cb8527d304a3b50ab28c94e956201082610e7624",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "An LLM-based chatbot must cite source documents to be explainable. Which AWS service and feature support this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Lex with sentiment analysis",
      "B": "SageMaker JumpStart with default prompts",
      "C": "Comprehend for entity extraction",
      "D": "Amazon Bedrock with Retrieval Augmented Generation"
    },
    "explanation": "Bedrock RAG includes provenance to cite source documents in responses."
  },
  {
    "id": "d296ef9b6bd7fb359add4c18ad1db374f77d50f264ca0337c62f9ebcc709ede7",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "To reduce hallucinations, you add guardrails but must maintain transparency. Which Bedrock feature do you enable?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AI Guardrails for response validation and logging",
      "B": "Auto-scaling with no logging",
      "C": "Fine-tuning without constraint definitions",
      "D": "Inline prompt encryption"
    },
    "explanation": "AI Guardrails enforce policies while producing logs for transparency."
  },
  {
    "id": "0b11b19ae6f6c71f6478cdbf5d99d7b2c3a2565e7882ff3b967abfaed90c5264",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "You need to compare performance vs interpretability trade-offs across models. Which practice follows human-centered design?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Automatically pick highest accuracy model",
      "B": "Use the most interpretable regardless of performance",
      "C": "Conduct user studies on prototype explanations and refine",
      "D": "Deploy all models in parallel without feedback"
    },
    "explanation": "User studies ensure explanations meet user needs and inform trade-offs."
  },
  {
    "id": "53f5c2ceaf25cb8803cc9389240935d9ad53f8fad70ba2cdbc537f9fcc60e9c1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "An AI ethics team requires documentation of model decision logic. Which artifact from SageMaker fulfills this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Training job CloudWatch logs",
      "B": "Model Card with transparency and test results",
      "C": "Endpoint invocation metrics",
      "D": "S3 bucket lifecycle policy"
    },
    "explanation": "Model Cards include decision logic, evaluation results, and transparency details."
  },
  {
    "id": "e254285079fb4037c30083f0722cfb0810abd35538346ba6486dbbdabfb54a3b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "A chatbot\u2019s responses vary nondeterministically, losing user trust. Which setting adjustment improves explainability consistency?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Lower temperature and set top-k sampling to 1",
      "B": "Increase top-p sampling",
      "C": "Add more context tokens only",
      "D": "Disable beam search"
    },
    "explanation": "Reducing temperature and sampling size makes outputs more consistent and explainable."
  },
  {
    "id": "f26060dbb5e02d93c98ba19f1062bbaf067f139a95784c70f031a14356398744",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "A manufacturing AI pipeline processes visual defects. Engineers need per-region heatmaps. Which technique and tool deliver this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "LIME on raw images",
      "B": "Manual annotation overlay",
      "C": "Confusion matrix in SageMaker Studio",
      "D": "SageMaker Clarify integrated with feature importance for images (SHAP)"
    },
    "explanation": "Clarify\u2019s SHAP-based image explainability produces region heatmaps integrated in Studio."
  },
  {
    "id": "a8dc6383fbb7a699ec916a363f922a49c5a0b888e1ddaf935cfed069f23613a4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "Your NLP classifier uses a transformer. You must expose decision rationale without revealing model internals. Which approach works?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Share model weights",
      "B": "Generate rationale tokens via chain-of-thought prompting",
      "C": "Distribute attention maps",
      "D": "Publish training data only"
    },
    "explanation": "Chain-of-thought prompts produce human-readable rationales without exposing weights."
  },
  {
    "id": "2f04cd5a1594ee7bc16257f53beace29e9b00602f1d081d50b35ef4edbb76508",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "A model\u2019s performance dips post-deployment. You suspect data drift and opaque decisions. How do you both monitor drift and maintain explainability?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable SageMaker Model Monitor data drift and periodic Clarify explainability reports",
      "B": "Rely on accuracy metrics in CloudWatch",
      "C": "Schedule manual reviews only",
      "D": "Use Lambda to retrain blindly"
    },
    "explanation": "Model Monitor tracks drift; Clarify periodic reports maintain up-to-date explainability."
  },
  {
    "id": "bf89cff5d9620d113d9c1673200e0424dfbc3a7d2b30705896b9b56546bb140a",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "An autonomous vehicle company needs to justify stopping decisions to regulators. Which combination delivers transparent logic?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use end-to-end CNN logs only",
      "B": "Deploy LLM for decision text",
      "C": "Implement rule-based safety layer with SHAP on perception model outputs",
      "D": "Record raw sensor input to S3"
    },
    "explanation": "A rule layer ensures deterministic stops; SHAP adds explainability to sensor model decisions."
  },
  {
    "id": "6eeff30c435a8857f43037678dc8dcfe5c980f18525be1c0bf986793bffa5dff",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "For a sensitive legal document summarizer, you must prove summaries\u2019 provenance. Which design ensures traceability?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Fine-tune LLM without logs",
      "B": "Use Lambdas to anonymize sources",
      "C": "Store only summaries in S3",
      "D": "Electronically tag and log source document sections via Bedrock RAG"
    },
    "explanation": "RAG stores document references alongside summaries for end-to-end provenance."
  },
  {
    "id": "151cd594baea7b029e3a4b0bd9a2a0fc6eca2a16c5485c194bc68cbf20ff079c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "Your team wants a transparent view of cluster-wide feature importances from a distributed model. Which AWS service feature helps?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CloudWatch distributed tracing",
      "B": "SageMaker Clarify with distributed explainability mode",
      "C": "Glue ETL transformation logs",
      "D": "EKS pod metrics"
    },
    "explanation": "Clarify\u2019s distributed explainability aggregates importances across shards."
  },
  {
    "id": "db2afe58d568b464e581d9ad27e7b5c1d1544c7f13c803f3741a580dd0057b86",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "A chatbot integrates multiple foundation models. You need a unified explainability layer without custom code. What do you choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Q for aggregated query provenance",
      "B": "Bedrock Agents without explainability",
      "C": "Lambda orchestration only",
      "D": "S3 auditing on each model"
    },
    "explanation": "Amazon Q provides a unified explainability interface for multi-model chat applications."
  },
  {
    "id": "1e5267406720591f29ac429e821145f0e67e05f2414a0da94831247482ab0ca9",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "To comply with GDPR\u2019s right to explanation, you must provide individuals model decision reasons. Which AWS feature most directly supports this?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "CloudTrail logs of API calls",
      "B": "CloudWatch metrics retention",
      "C": "SageMaker Clarify local explanations stored in Model Registry",
      "D": "S3 access logs"
    },
    "explanation": "Clarify local explanations stored in the Registry allow per-request rationale retrieval."
  },
  {
    "id": "61e87e6bb566d18fbf1c56605c27dd065503dff9c0a90f71c8e64158ac01a93c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "An energy company\u2019s demand forecast model is a black box. They need global interpretability and quantitative feature rankings. Which method and service combination should you apply?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run SHAP summary plots via SageMaker Clarify",
      "B": "Inspect first-layer weights in a DNN",
      "C": "Use LIME on a single prediction",
      "D": "Estimate feature correlation externally"
    },
    "explanation": "Clarify\u2019s SHAP summary provides global importance rankings across the dataset."
  },
  {
    "id": "bef89b0b63dbcfc4f1386694ec7da6e084d6c384042ec7acba60625fd64d6b3d",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "Your model uses transfer learning from a large foundation model. You need to show how fine-tuning changed decision boundaries. How do you document this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Store only final model weights",
      "B": "Produce before-and-after Clarify explainability reports",
      "C": "Log training loss only",
      "D": "Track only fine-tuning epochs"
    },
    "explanation": "Before/after Clarify reports reveal shifts in feature attributions due to tuning."
  },
  {
    "id": "0af623d06a435550a2e6888d7aaf20fa7943ed3f474f5402800bf4cf2a2a378f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "A real-time bidding system requires sub-second explanations for each bid. Which architecture meets this SLA and explainability requirement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker real-time endpoint with Clarify real-time attributions",
      "B": "Batch Clarify jobs on S3",
      "C": "Asynchronous inference only",
      "D": "Post-hoc analysis offline"
    },
    "explanation": "Real-time endpoints with Clarify provide low-latency attributions inline with predictions."
  },
  {
    "id": "605fb73ab06d51a42f399c1c8574618850087ccf8e59540cfcab5a895807f75b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "To enforce transparency, you restrict models to only open-source licenses. Which SageMaker feature helps manage license compliance and explainability?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "SageMaker Autopilot",
      "B": "SageMaker Studio notebooks",
      "C": "CloudTrail for notebooks",
      "D": "SageMaker Model Cards with license metadata"
    },
    "explanation": "Model Cards capture licensing details and explainability metadata for compliance audits."
  },
  {
    "id": "362db7b1ac9bd0bce9f6c4fbf5e66ee87b19e6f690e24c8c1291ea78d9dd19e4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "Your model serves multiple customer segments. You need segment-specific explanations without duplicating pipelines. Which design is most efficient?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy separate endpoints per segment",
      "B": "Use batch Clarify per segment offline",
      "C": "Use multi-tenant endpoint with dynamic Clarify context per request",
      "D": "Train individual models per segment"
    },
    "explanation": "A multi-tenant endpoint that passes segment context to Clarify yields dynamic explanations without duplication."
  },
  {
    "id": "e82155299e9d7ad9d81e16546293042b2268df0d98a6bea7285bb8315118b402",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "An AI ethics review board requests evidence of model robustness and interpretability under edge-case inputs. Which test suite do you implement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Generate adversarial examples and run Clarify on those samples",
      "B": "Review only training accuracy",
      "C": "Simulate random noise without explainability",
      "D": "Conduct user surveys only"
    },
    "explanation": "Testing adversarial inputs combined with Clarify shows robustness and explanation under edge cases."
  },
  {
    "id": "279c1d83e721e3b58ae48cefe5774cdc1607fe52ff6d15fe5e6c542569bbaec5",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.1",
    "stem": "A company deploys a SageMaker real-time inference endpoint in a VPC to serve sensitive PII. The security team requires that all network traffic between on-premises clients and the endpoint never traverse the public internet, and all traffic must be encrypted. Which combination of AWS features meets these requirements with the least operational overhead?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Direct Connect between on-premises and the VPC, and configure a SageMaker VPC endpoint in the VPC; enforce TLS in the endpoint configuration.",
      "B": "Use the public SageMaker endpoint over TLS and restrict access by IP address in the IAM policy.",
      "C": "Place a public Network Load Balancer in front of the endpoint and enforce TLS; restrict client IPs with security groups.",
      "D": "Deploy the endpoint in a private subnet with no internet gateway; use a NAT gateway for egress; require a client VPN."
    },
    "explanation": "A Direct Connect + VPC endpoint ensures traffic never leaves the AWS network. End-to-end TLS in the endpoint config provides encryption. Alternatives either traverse the internet or add unnecessary components."
  },
  {
    "id": "340b9152a5212c67f6ba36835ffd4675e5e528aac4f7de0f2c82c965ae8323bf",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.1",
    "stem": "An organization stores AI training data in an S3 bucket. They must detect accidental public exposure of sensitive data, including PII, as soon as possible. Which solution provides automated detection and alerting for public or compromised bucket objects?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable S3 Server Access Logging and analyze logs with a custom Lambda function.",
      "B": "Use AWS Config rule s3-bucket-public-read-prohibited to trigger SNS notifications.",
      "C": "Enable Amazon Macie to continuously monitor the bucket for public access and PII, and configure Macie alerts to Amazon Security Hub.",
      "D": "Use CloudTrail Data events for S3 and query with Athena daily for public object ACLs."
    },
    "explanation": "Macie continuously profiles S3 for sensitive data and public access. While AWS Config detects policy changes, it doesn\u2019t inspect object content; log analysis is slower and less accurate."
  },
  {
    "id": "fbe4045442a56efcc32838b5b560cd2cedd11e03f36f43d4f099648dc2f707e7",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.1",
    "stem": "A financial services firm uses a SageMaker feature store with high-volume streaming data. They must ensure data at rest is encrypted, and they need detailed audit logs of data access. Which KMS key configuration and logging combination meets these requirements?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use the AWS owned CMK for feature store and rely on CloudTrail management events.",
      "B": "Use a symmetric customer-managed CMK in KMS with default key usage and enable CloudTrail data events on the S3 bucket.",
      "C": "Use an asymmetric CMK for encryption and enable CloudTrail management events only.",
      "D": "Use a symmetric customer-managed CMK with key policies granting least privilege, enable KMS key rotation, and enable CloudTrail data events on S3 and KMS."
    },
    "explanation": "Customer-managed symmetric CMK provides control and rotation; data events on both S3 and KMS record object and decrypt/encrypt usage for audit. Asymmetric keys are not needed for SSE."
  },
  {
    "id": "8ceee870355789ea46e050053dbba2f3f1006cdb70da99c3be7a843cdfc3212b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.1",
    "stem": "A data pipeline ingests customer records into an S3 bucket before training. The security team mandates that data be encrypted in transit and that any malformed or malicious data be rejected before storage. Which combination of features enforces both requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable S3 default encryption (SSE-S3) and require HTTPS uploads.",
      "B": "Deploy an S3 VPC endpoint with a bucket policy requiring aws:SecureTransport=true and use a Lambda@Edge function to validate file format before writing to S3.",
      "C": "Use public S3 endpoint with HTTPS and configure AWS WAF to block API anomalies.",
      "D": "Use API Gateway to accept uploads over TLS and write directly to the S3 bucket."
    },
    "explanation": "An S3 VPC endpoint plus bucket policy disallows non-TLS uploads. A Lambda@Edge (via CloudFront) or Lambda trigger can validate content before storage. WAF doesn\u2019t validate payload format."
  },
  {
    "id": "b2dcd557d4bcf6718f8894eb30e94f49a1f3cf63c11ec08d791ac0c16126559b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.1",
    "stem": "Your team implements an ECR repository for Docker images used in AI workloads. They must automatically scan images for CVEs and receive prioritized remediation recommendations. Which AWS feature should they enable?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable AWS Config rule ecr-image-tag-mutability to require immutable tags.",
      "B": "Enable Amazon Inspector image scanning on ECR for scan-on-push and send findings to Security Hub.",
      "C": "Use ECR lifecycle policies to expire untagged images and rely on third-party scanners.",
      "D": "Configure GuardDuty to analyze ECR image traffic for anomalies."
    },
    "explanation": "Amazon Inspector\u2019s ECR image scanning provides CVE detection on push and integrates with Security Hub. Config rules manage mutability but don\u2019t scan for vulnerabilities."
  },
  {
    "id": "08911e2d715031597612f314aa581b3f7575be303913db9955abab4f0b5d122d",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.1",
    "stem": "A regulated healthcare application trains NLP models on medical records. The team must track data lineage and model changes for audit. Which AWS service combination provides automatic lineage capture of datasets, processing jobs, and model versions?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Experiments and the SageMaker Model Registry to record input datasets, training jobs, and model versions.",
      "B": "Deploy AWS Glue Data Catalog for lineage and use CloudWatch Logs for training job events.",
      "C": "Use AWS Config to track SageMaker resources and AWS X-Ray for job traces.",
      "D": "Implement AWS Step Functions for pipeline orchestration and log parameters to CloudTrail."
    },
    "explanation": "SageMaker Experiments captures lineage of datasets, training jobs and parameters. Model Registry tracks versioning. Glue and Config don\u2019t capture full ML pipeline lineage automatically."
  },
  {
    "id": "7419f90a3d8376ef67a068aad03ad21c7e7bf215807d99a17dd81bdd847e2e4e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.1",
    "stem": "A security audit discovered that your SageMaker notebook instances can download any internet asset because they are in a public subnet. The audit requires blocking internet access while still allowing package installations from approved AWS sources. Which VPC configuration meets these requirements with minimal changes?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Move notebooks to private subnets with no route to an Internet Gateway.",
      "B": "Keep public subnet but remove the Internet Gateway; traffic will fail.",
      "C": "Add a NAT Gateway in an isolated subnet and force all notebook traffic through it.",
      "D": "Move notebooks to private subnet with a VPC endpoint for Amazon S3 and Amazon ECR, and no Internet Gateway."
    },
    "explanation": "Private subnet plus S3 and ECR endpoints allows package installs from approved repos stored in S3/ECR while blocking all other internet egress without a NAT or IGW."
  },
  {
    "id": "95782f87552bee1a38fece460ce695252afcc8358faced06b0611ba20348f5c9",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.1",
    "stem": "Your AI application uses AWS Lambda functions to preprocess data stored in S3. The security team wants to ensure logs from these functions cannot be tampered with or deleted after 90 days. Which combination of services enforces log immutability?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure Lambda to write logs to a custom S3 bucket and enable versioning.",
      "B": "Route Lambda logs to CloudWatch Logs, set a retention period of 90 days, and rely on CloudTrail for API logs.",
      "C": "Route Lambda logs to CloudWatch Logs, export to an S3 bucket with S3 Object Lock in Compliance mode and a 90-day retention.",
      "D": "Send Lambda logs to Kinesis Data Stream and store to DynamoDB with TTL of 90 days."
    },
    "explanation": "CloudWatch Logs export to S3 with Object Lock in Compliance mode prevents deletion or modification for the retention period. Versioning alone is insufficient."
  },
  {
    "id": "9baad390be118f5f4ee4a12f67e3247934730d56694dfc71d710c2277e5e3666",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.1",
    "stem": "An organization builds a generative AI service that accepts free-form text inputs. To mitigate prompt injection risks, they want to enforce content filtering before passing inputs to the model. Which AWS components should they integrate?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon Comprehend to detect PII and block inputs containing sensitive data.",
      "B": "Implement AWS WAF with a custom rule to filter harmful patterns.",
      "C": "Use GuardDuty to detect anomalies in API calls and throttle requests.",
      "D": "Route inputs through an API Gateway custom authorizer Lambda function that sanitizes input and applies profanity/regex filters before forwarding to the model."
    },
    "explanation": "A custom authorizer Lambda on API Gateway can sanitize input against injection patterns. WAF is not NLP-aware; Comprehend detects PII but not injection."
  },
  {
    "id": "6a7e976aa4a5341dded51a5cf1d12e1cfd7ccbdd60fac6a51cc779a580dd265c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.1",
    "stem": "Your machine learning team uses an S3 bucket to store both raw and processed data. The security team requires separation of duties so that data scientists can read processed data but not raw data. Which IAM strategy enforces this least privilege model?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Apply a bucket policy to deny raw data prefix access for data scientist IAM group.",
      "B": "Create two IAM policies: one grants GetObject on processed-prefix only; assign to data scientists; the second attaches a Deny on raw-prefix for all users.",
      "C": "Use ACLs to grant Read access to processed prefix and remove Read from raw prefix.",
      "D": "Segment the bucket into two buckets: raw and processed, and grant permissions accordingly."
    },
    "explanation": "Prefix-level IAM policies enforce least privilege without creating separate buckets. ACLs are outdated. Splitting buckets adds operational overhead."
  },
  {
    "id": "2a0f578550576ce80a47670fd1ece04e012262c570d946a0373e07258bbc2438",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.1",
    "stem": "A data scientist must ensure that every step of the model training pipeline is logged and that logs cannot be retroactively modified. Which configuration satisfies this requirement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable CloudTrail data events for S3, SageMaker, and KMS; send CloudTrail logs to an S3 bucket with Object Lock in Compliance mode.",
      "B": "Use SageMaker Studio built-in logging and rely on CloudWatch Logs retention settings.",
      "C": "Configure CloudWatch Logs with a 90-day retention period and encrypt logs with an AWS owned CMK.",
      "D": "Export CloudWatch Logs to a Kinesis Data Firehose delivery stream to an Elasticsearch cluster."
    },
    "explanation": "CloudTrail data events record every API call; Object Lock ensures logs can\u2019t be altered. CloudWatch retention doesn\u2019t prevent log tampering."
  },
  {
    "id": "c239cc5ff6c55139ad0f81b74dfdbea71d79cb80ad2fb0e26f8774f8b18ef09f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.1",
    "stem": "Your inference service hosts a third-party model in a container from ECR. The security policy mandates vulnerability scans on the container image before deployment. Which automated workflow enforces this policy?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use CodePipeline to build the container image and trigger a Lambda that runs a third-party scanner.",
      "B": "Manually scan the image using Amazon Inspector CLI before pushing to ECR.",
      "C": "Enable Amazon Inspector\u2019s ECR image scanning on push, use ECR lifecycle policy to block unscanned images, and integrate findings with CodePipeline approval stage.",
      "D": "Use AWS Config custom rule to check for unscanned images daily."
    },
    "explanation": "Inspector on-push scanning plus lifecycle policy can prevent unscanned images from being deployed; integration into CodePipeline ensures gating."
  },
  {
    "id": "bee04ecbdcceedf9c7cf4e04500eb9204f0d3f8b1c764d892afd8b547b6392f4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.1",
    "stem": "An AI analytics team needs to share S3 training data with another AWS account securely, without exposing data publicly. They also need audit trails of every access. Which combination of features should they implement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use S3 bucket policy with AWSOrgPrincipal to allow the other account and enable S3 Access Logs.",
      "B": "Create an S3 Access Point with cross-account policy for the specific account and enable AWS CloudTrail data events for object reads.",
      "C": "Generate pre-signed URLs and distribute them; rely on S3 Server Access Logging.",
      "D": "Use S3 replication to the other account bucket; enable CloudWatch metrics."
    },
    "explanation": "S3 Access Points with cross-account policies scope access; CloudTrail data events record each read. Pre-signed URLs can leak; replication duplicates data."
  },
  {
    "id": "92202015fc02217354c197ea568073e98b184013cd3224c1a2d12ccc4692b422",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.1",
    "stem": "Your model training code pulls data from a private S3 bucket. To protect credentials, you want the SageMaker execution role to retrieve encrypted credentials at runtime. Which combination meets best practice for secrets management?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Store credentials as plaintext in S3 encrypted with SSE-S3; grant SageMaker access.",
      "B": "Hard-code credentials in the training container and restrict IAM policies.",
      "C": "Use AWS Systems Manager Parameter Store with SecureString and a Lambda to fetch at training start.",
      "D": "Store secrets in AWS Secrets Manager with automatic rotation; allow the SageMaker role to retrieve secrets via Secrets Manager API."
    },
    "explanation": "Secrets Manager provides encryption at rest, automatic rotation, and fine-grained access via IAM. Parameter Store also works but Secrets Manager is recommended for credentials."
  },
  {
    "id": "3b795978bd84a5a028449605e538e295f0822fd7213b3b9be8328b79aade27a0",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.1",
    "stem": "A security requirement mandates that all SageMaker model artifact S3 buckets must enforce encryption at rest and deny uploads without encryption. Which bucket policy accomplishes this?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "A Deny statement on s3:PutObject where s3:x-amz-server-side-encryption is null.",
      "B": "A Deny statement on s3:PutObject if aws:SecureTransport is false.",
      "C": "A Deny statement on s3:GetObject where the SSE algorithm is not aws:kms.",
      "D": "A Deny statement on s3:DeleteObject if the object is unencrypted."
    },
    "explanation": "Denying PutObject when s3:x-amz-server-side-encryption is missing enforces encryption at rest. SecureTransport relates to in-transit encryption."
  },
  {
    "id": "aaf14291c8b9d66d492b8ff2058ccbb0d31b40a57f0b9c82f3b94b9617208efb",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.1",
    "stem": "Your ingestion pipeline writes sensitive user data into an Amazon Kinesis Data Stream. The compliance team requires in-flight encryption and that only certain IAM roles can put data. How do you satisfy these requirements?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy a VPC endpoint for Kinesis and require HTTPS for producers.",
      "B": "Use client-side encryption in the producer application and attach an IAM policy to the stream resource.",
      "C": "Enable server-side encryption with a KMS CMK on the stream and configure an IAM resource policy on the stream to allow only specific IAM roles to PutRecord.",
      "D": "Configure a Kinesis Data Firehose delivery stream with encryption enabled."
    },
    "explanation": "Kinesis SSE with CMK secures data in transit and at rest; an IAM resource policy restricts which roles can put records. Firehose is for delivery, not streaming ingestion."
  },
  {
    "id": "1cacd6d9cdc1e3908f715156522f15545f23e5652c3ee4b2b808fcd76f902242",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.1",
    "stem": "A data engineer uses AWS Glue to preprocess training data. The security team demands that no Glue job can write to production buckets unless it\u2019s explicitly approved. Which control mechanism enforces this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use IAM tags on Glue jobs and restrict S3 writes based on tags.",
      "B": "Implement an AWS Config rule that audits Glue job role permissions and triggers a remediation Lambda to remove unauthorized access.",
      "C": "Use Glue job bookmarks to prevent accidental overwrites.",
      "D": "Enable CloudTrail logs for Glue and manually review permissions weekly."
    },
    "explanation": "An AWS Config rule can automatically detect and remediate Glue job roles with unauthorized S3 permissions. Tag-based restrictions aren\u2019t enforced by Glue role assignment."
  },
  {
    "id": "76934e540a5fdb4768a7e219b4c0192669c1eb2b1ce72d273e19d08e3c2b716d",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.1",
    "stem": "Your AI platform uses AWS Secrets Manager to store database credentials. The security team wants automatic detection of secrets exfiltration by unauthorized API calls. Which service and feature combination should you use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable CloudTrail management events and alert on unauthorized GetSecretValue API calls.",
      "B": "Use AWS Config to detect changes to Secrets Manager resources.",
      "C": "Use AWS WAF to block suspicious API calls to Secrets Manager endpoints.",
      "D": "Enable CloudTrail data events for Secrets Manager and integrate with Amazon GuardDuty to alert on anomalous API activity."
    },
    "explanation": "CloudTrail data events capture fine-grained Secrets Manager calls; GuardDuty analyzes logs for anomalies. Config and WAF don\u2019t detect unauthorized API usage."
  },
  {
    "id": "917ab4eb831a31f02a05979d3dfaf4e8b2ef4b96c961ce7d82ad74fa9abffc51",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.1",
    "stem": "To comply with data sovereignty, training jobs must run in a specific Availability Zone and use only encryption keys stored in that AZ. How do you configure this in SageMaker?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create a symmetric CMK in the desired region and specify it in the training job; use an AZ-specific subnet.",
      "B": "Launch training in a VPC with subnets in the required AZ and use a customer-managed CMK with an \"aws:sourceVpce\" condition restricting usage to that VPC endpoint.",
      "C": "Use an AWS owned CMK and specify the AZ in the training job config.",
      "D": "Deploy training on spot instances in the AZ and use SSE-S3 for data encryption."
    },
    "explanation": "Restricting the CMK via a condition on the VPC endpoint ensures the key can only be used within the specific AZ\u2019s VPC. AWS owned keys cannot be restricted."
  },
  {
    "id": "c86ccf8a6df966aa502665d9a768f050fc9b4c88d8506d816aa1603d68af42a7",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.1",
    "stem": "Your inference pipeline uses an Amazon API Gateway front end to invoke SageMaker endpoints. The security team wants to ensure that only calls signed with SigV4 and containing a valid JWT from Amazon Cognito can invoke the endpoint. How do you enforce this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure API Gateway with AWS_IAM authorization and a Cognito authorizer; require SigV4 signed requests and a valid JWT.",
      "B": "Use a Lambda authorizer that validates SigV4 signatures and JWTs.",
      "C": "Enable AWS WAF with rules to verify JWT and AWS SigV4 in the HTTP headers.",
      "D": "Place a CloudFront distribution in front of API Gateway with origin access control."
    },
    "explanation": "API Gateway\u2019s built-in AWS_IAM auth requires SigV4; a Cognito authorizer enforces JWT. Combining both enforces both constraints."
  },
  {
    "id": "f27f2afa08b2cd1e4a8a1f375f16f21a0ea0fa34f30d8f100d544a8f5a179897",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.1",
    "stem": "A threat model identifies that SageMaker Notebook instance metadata endpoint could expose IAM credentials. You must prevent notebooks from calling instance metadata. How can you enforce this without code changes?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Disable the metadata service on the instance via user data script.",
      "B": "Apply an IAM policy denying ec2:DescribeInstanceMetadata for the notebook role.",
      "C": "Use an IAM instance profile with ec2:MetadataHttpTokens=required and enforce IMDSv2 only via the launch configuration.",
      "D": "Deploy the notebook in a Fargate container that has no metadata endpoint."
    },
    "explanation": "Requiring IMDSv2 with enforced tokens prevents unauthorized metadata access. IAM policies cannot restrict metadata access directly."
  },
  {
    "id": "1c19e971524ec3c3e4759111d17a671b6ffd566b209053a59e694e08e0448bd3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.1",
    "stem": "Your regression model writes output to a DynamoDB table. The security team mandates encrypted data in transit, encrypted at rest with a customer-managed key, and real-time audit of table operations. Which configuration satisfies all requirements?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable DynamoDB encryption at rest with AWS owned CMK and configure TLS in the SDK; use CloudWatch metrics.",
      "B": "Use a DynamoDB global table for cross-region, enable SSE-KMS with a CMK, and export to CloudTrail.",
      "C": "Configure DynamoDB with SSE-C and require HTTPS; enable DynamoDB Streams for auditing.",
      "D": "Enable DynamoDB SSE-KMS with a customer-managed CMK, enforce HTTPS in IAM policy by aws:SecureTransport, and enable CloudTrail data events for the table."
    },
    "explanation": "A CMK gives at-rest control; SecureTransport enforces TLS; CloudTrail data events audit all PutItem/GetItem calls in real time."
  },
  {
    "id": "c3d2936bf40d636a3ea802d9bec25e770d67414a13493217e11d180ab0347680",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.1",
    "stem": "A security audit flagged that S3 objects used by your models are sometimes decrypted on client side without proper access checks. You want to ensure that only authorized services can decrypt data and that every decryption is logged. Which strategy fulfills this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure default S3 bucket encryption with SSE-S3 and rely on CloudTrail management events.",
      "B": "Use SSE-KMS with a customer-managed CMK that has a key policy granting decryption only to specific IAM roles and enable CloudTrail data events for KMS.",
      "C": "Implement client-side encryption in application code using a symmetric key stored in Secrets Manager.",
      "D": "Use SSE-C and supply a passphrase at runtime via an API call."
    },
    "explanation": "SSE-KMS with CMK policies restrict decryption; CloudTrail data events record decrypt operations. SSE-S3 cannot limit decryption or log usage."
  },
  {
    "id": "8693b2805022301faafbbfdb1a1fb23b471f6d5eb4db707cd13655d5eda32676",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.1",
    "stem": "Your batch inference jobs run on EMR clusters accessing S3 training data. The security policy requires min-entropy checks on data inputs to detect tampering. Which AWS feature or service helps implement this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable S3 Object Lock with checksum validation (SHA-256) and use AbortIncompleteMultipartUpload.",
      "B": "Use Amazon Macie to scan data for integrity violations.",
      "C": "Configure EMR bootstrap actions to compute and compare checksums manually.",
      "D": "Use AWS Artifact to validate data source compliance."
    },
    "explanation": "S3 supports checksum validation to detect data corruption/tampering. Object Lock enforces immutability but checksum ensures integrity. Macie doesn\u2019t check integrity."
  },
  {
    "id": "c6a5ffc0a9365dba3da85a7517086735e209e4f15e669808d6629c415997c954",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.1",
    "stem": "When deploying a multi-model SageMaker endpoint, you want to prevent unauthorized model copies or downloads from the endpoint container. Which configuration provides this protection?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy the endpoint in a private subnet with no outbound internet.",
      "B": "Use a VPC endpoint to restrict container ECR pull traffic.",
      "C": "Disable IAM role for the endpoint and use access keys instead.",
      "D": "Use an endpoint execution role with Allow pulls from ECR repository only, no S3 or SSM permissions."
    },
    "explanation": "Restricting the execution role to only ECR pull permissions prevents the container from accessing S3 or other services to copy or download model artifacts."
  },
  {
    "id": "404db3795697c66df93d68598306b8deff7a98105ff0b3b12855a8b157c8cbaf",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.1",
    "stem": "Your AI application uses an Amazon DynamoDB table to store model metadata. The security team requires that table backups must be encrypted with a separate key from the primary table encryption. How do you configure this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable point-in-time recovery; it uses same table CMK by default.",
      "B": "Use on-demand backups with a separate KMS key specified for backup encryption.",
      "C": "Export the table to S3 and apply SSE-C encryption.",
      "D": "Use continuous backups to another DynamoDB table encrypted with its own CMK."
    },
    "explanation": "On-demand backups allow specifying a different CMK for backup encryption. PITR uses the table key and cannot be changed."
  },
  {
    "id": "20a6355a33e254716c966a1a9f4c4815313b4f60a5ec2a50d3962e2935fd0455",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.1",
    "stem": "Your inference function is vulnerable to malicious payloads sent via JSON content. You want to reject any requests containing executable code patterns before they reach the Lambda runtime. Which service or feature is best suited to enforce this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS WAF with SQL injection ruleset.",
      "B": "CloudFront with a managed ruleset for cross-site scripting.",
      "C": "API Gateway request validation with a model schema that rejects payloads not matching the JSON schema.",
      "D": "Lambda layers that sanitize input at runtime."
    },
    "explanation": "API Gateway request validation enforces JSON schema and rejects invalid or unexpected fields before reaching Lambda, reducing risk of code injection."
  },
  {
    "id": "8a17b96739feb1475b9ed573b2b8e876de8cd61e852319f2e7dc943e98280851",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.1",
    "stem": "A large dataset of training images is stored in an S3 bucket. The security mandate requires that any object tagged \"PII\" be encrypted with a dedicated CMK, and other objects with a different CMK. Which mechanism enforces encryption by tag at upload time?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use an S3 bucket policy with separate Deny statements blocking PutObject when s3:RequestObjectTag/PII=true and sse-kms-key-id != CMK-PII, and another for !PII tag.",
      "B": "Use S3 default encryption with an encryption key that rotates based on object tags.",
      "C": "Rely on client-side tagging logic in the ingestion application.",
      "D": "Use S3 Lifecycle rules to re-encrypt mismatched objects after upload."
    },
    "explanation": "Bucket policy conditions on request-tagging and SSE-KMS key ID enforce correct CMK per tag at put time. Lifecycle re-encrypts after-the-fact, not preventive."
  },
  {
    "id": "4af2fd43cf7d9d8301a7826fdd0a5802e412255184810dbdee6c136dbc5ff444",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.1",
    "stem": "Your SageMaker training jobs pull data from an RDS database inside a private subnet. The training role requires credentials but you must avoid long-lived credentials. Which solution follows AWS best practices?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Store DB credentials in SSM Parameter Store and grant SageMaker role access.",
      "B": "Hard-code temporary credentials in the training script.",
      "C": "Use AWS Secrets Manager with rotation disabled.",
      "D": "Use AWS Secrets Manager with rotation enabled and grant the SageMaker execution role GetSecretValue permission."
    },
    "explanation": "Secrets Manager with rotation and least-privilege IAM access follows best practices. Parameter Store can work but Secrets Manager is recommended for rotating database credentials."
  },
  {
    "id": "95192efcf4db5039b6eb4e53621630388c83ab7bccf5665505ad75b2095060b6",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.1",
    "stem": "A security review requires that all Lambda functions in your AI pipeline must run with the minimal set of privileges. You want to enforce that any new functions cannot have wildcard actions in their execution role. How can you automatically detect violations?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use GuardDuty to monitor Lambda execution logs for unauthorized API calls.",
      "B": "Enable an AWS Config managed rule IAM_ROLE_NO_STATEMENT_WITH_WILDCARD and scope it to Lambda execution role ARNs.",
      "C": "Implement CloudWatch alarms on IAM policy changes.",
      "D": "Use AWS CloudTrail to audit IAM PutRolePolicy events daily."
    },
    "explanation": "AWS Config rule IAM_ROLE_NO_STATEMENT_WITH_WILDCARD automatically flags any IAM role policy containing wildcard actions. GuardDuty and alarms are not specific to role definitions."
  },
  {
    "id": "59a96d599a98a121eb7f3e0805f973e3235f68082c37daf37e87330370921fd7",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.1",
    "stem": "Your inference application logs traffic to an S3 bucket. You must ensure logs are encrypted in transit and at rest, and cannot be modified after writing. Which combination of S3 features satisfies these requirements?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable SSE-S3 and enforce https via bucket policy.",
      "B": "Use SSE-KMS with a CMK and default encryption, and rely on S3 versioning.",
      "C": "Enable SSE-KMS with a CMK, enforce aws:SecureTransport=true, enable Object Lock in Governance mode.",
      "D": "Use client-side encryption before upload and enable MFA delete."
    },
    "explanation": "SSE-KMS + SecureTransport ensures encryption. Object Lock in Governance mode prevents modification but still allows authorized changes if needed. Versioning alone doesn\u2019t prevent modifications."
  },
  {
    "id": "904a7c605172d498ba627e828d396d0ef3e705a9ac468a2404e392d394ecf5c9",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.1",
    "stem": "A compliance requirement states that every S3 object used for training must be scanned for malware before training jobs read the data. Which architecture best satisfies this requirement with minimal latency impact?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon Macie for malware scanning and store only approved objects in a separate prefix for training jobs.",
      "B": "Deploy AWS Lambda on PutObject S3 event to call GuardDuty for malware analysis synchronously.",
      "C": "Build a custom EC2-based scanner that pulls from S3 and pushes to the training prefix.",
      "D": "Use ECR image scanning for data objects in S3."
    },
    "explanation": "Macie malware protection can scan on upload and tag or move clean objects to a different prefix used by training. Lambda or custom scanners add latency and operational overhead."
  },
  {
    "id": "82d1e96f8240dea86a6bf39c60faedade1940e6d8a7d8e8c6ea5a523b053429e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.1",
    "stem": "Your SageMaker inference endpoint uses HTTPS. The security team wants to rotate the TLS certificate every 90 days without downtime. Which solution achieves this with minimal changes?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy the endpoint behind an Application Load Balancer with ACM-managed certificates and rotate certificates periodically.",
      "B": "Store certificates in S3 and update the endpoint config with CLI on rotation.",
      "C": "Use CloudFront with self-managed certificate and swap distributions.",
      "D": "Use API Gateway (REST) integration with SageMaker and enable custom domain with ACM-managed certificate, auto-renewed."
    },
    "explanation": "API Gateway custom domain with ACM auto-issues and renews certificates. No downtime occurs and no manual rotation is needed. ALB could work but adds complexity."
  },
  {
    "id": "1a9c2b1778436cf0f05d00fcd9f267ef80dd1179a4267b6acb6df13a317e4c32",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.2",
    "stem": "A regulated financial services firm deploys an AI model on Amazon SageMaker to process sensitive loan applications. To demonstrate compliance with ISO 27001 and SOC 2, they must provide evidence of model invocation logs, resource configuration drift, and change history across the AI environment. Which AWS service should they use to aggregate and automate the evidence collection for audits?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS CloudTrail",
      "B": "AWS Config",
      "C": "AWS Audit Manager",
      "D": "AWS Artifact"
    },
    "explanation": "AWS Audit Manager automates collection of evidence (from CloudTrail, Config, etc.) and maps it to frameworks like ISO 27001 and SOC 2. CloudTrail and Config alone store logs and configurations but don\u2019t automate audit evidence collection. Artifact provides compliance documents, not operational evidence."
  },
  {
    "id": "caaa6d9a1e05d7c6a2742421c3cd9ad39a52fe1123ff2ab5d644215ddbb54a53",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.2",
    "stem": "A healthcare startup uses AI to categorize patient imaging data. They must implement data retention policies to comply with a 7-year medical record retention law and ensure no deletions before that period. Which combination of features enforces this requirement at the storage layer?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon S3 Lifecycle rule transitioning objects to Glacier after 7 years",
      "B": "Amazon S3 Object Lock in Compliance mode with a 7-year retention period",
      "C": "S3 Versioning enabled with manual deletion controls",
      "D": "AWS Backup vault locks for S3 with a 7-year lock period"
    },
    "explanation": "S3 Object Lock in Compliance mode enforces retention periods (immutable) and prevents deletion before expiry. Lifecycle rules alone cannot prevent deletion. Versioning doesn\u2019t enforce retention, and AWS Backup doesn\u2019t support S3 object lock at source."
  },
  {
    "id": "09de1d5e5773313976e4e2e2354f01f8ce74a845c42ee7a3c9357e897a98070c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.2",
    "stem": "An AI-driven supply chain analytics platform must maintain an auditable history of all configuration changes in its AWS infrastructure for five years. They need near real-time alerts for noncompliant changes. Which service combination meets these requirements?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS CloudTrail with S3 storage and Athena queries",
      "B": "AWS Config with monthly compliance reports",
      "C": "AWS CloudWatch Events with Lambda remediation",
      "D": "AWS Config with advanced resource configuration recording and AWS Config Rules SNS notifications"
    },
    "explanation": "AWS Config continuously records configuration changes, stores history for compliance periods, and Config Rules can trigger SNS for near real-time alerts on noncompliance. CloudTrail logs events but doesn\u2019t provide compliance alerts out-of-the-box. Athena and monthly reports aren\u2019t real-time."
  },
  {
    "id": "6a7b69281b42319f6f349b6287e1cd4f66242230d5d8781188698954d549b4e6",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.2",
    "stem": "A multinational AI company must comply with data sovereignty laws requiring that model training data and logs for EU-based customers never leave EU regions. Which approach enforces this at the account level?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Organizations Service Control Policy to restrict resource creation to EU regions",
      "B": "Enable AWS Config Multi-Region recording only for EU regions",
      "C": "Set S3 bucket policies to deny non-EU region replication",
      "D": "Use IAM permissions to limit access based on AWS region tag"
    },
    "explanation": "A Service Control Policy (SCP) can block any creation of resources outside specified regions across accounts. Config recording and bucket policies address logging and replication but don\u2019t prevent resource creation globally. IAM region tags can be bypassed if misconfigured."
  },
  {
    "id": "9728b114c57ff81defea8f47f9b56afa1251352cd88443cd7291fc15a9e55893",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.2",
    "stem": "An AI solution processes customer PII and must comply with GDPR\u2019s right-to-be-forgotten. They need a documented process to delete PII across data stores and ensure deletions are auditable. Which AWS service helps orchestrate and report on the deletion workflow?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Step Functions with DynamoDB Streams",
      "B": "AWS Systems Manager Change Manager with runbooks and approval workflows",
      "C": "AWS Config with remediation functions",
      "D": "Amazon EventBridge scheduled rules triggering Lambdas"
    },
    "explanation": "Systems Manager Change Manager runbooks can orchestrate multi-step workflows with approvals, track actions, and generate audit reports. Config remediation focuses on resource compliance, Step Functions and EventBridge don\u2019t provide built-in approval tracking or audit documentation."
  },
  {
    "id": "cd2404f33bb165653b8ca155f635ba8d49e7fd465d68fedcad5ade4a749745d5",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.2",
    "stem": "A gaming company uses a foundation model via Amazon Bedrock. They need to ensure all prompts and responses are logged and retained for auditing, while minimizing administrative overhead. Which feature should they enable?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Bedrock Fine-tuning with S3 logging",
      "B": "Enable CloudTrail Data Events for Bedrock",
      "C": "Use Bedrock API Gateway with CloudWatch logs",
      "D": "Enable Amazon Bedrock Activity Logging to Amazon S3"
    },
    "explanation": "Amazon Bedrock Activity Logging delivers prompts and model responses directly to S3 for auditing. CloudTrail doesn\u2019t natively capture Bedrock payloads. API Gateway with CloudWatch adds complexity and may not log responses."
  },
  {
    "id": "7279aac29e42a7ffd547e29829d9a75849210f17ede1aa7e862565fbeb3e842d",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.2",
    "stem": "An AI enterprise requires an automated quarterly review of its governance policies and evidence collection for internal auditors. Which AWS service can schedule and manage these recurring assessments?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Config conformance packs",
      "B": "AWS CloudTrail Lake queries",
      "C": "AWS Audit Manager assessment frameworks with scheduled assessments",
      "D": "AWS Trusted Advisor scheduled reports"
    },
    "explanation": "Audit Manager can schedule assessments against compliance frameworks on a quarterly basis, automating evidence collection and reporting. Config conformance packs check compliance configurations but don\u2019t schedule full audit reports. CloudTrail Lake and Trusted Advisor don\u2019t provide compliance frameworks or scheduled audit deliverables."
  },
  {
    "id": "158c0e30b52754d7ada9c149748fce6fdf6c9024429e013740613a18bef2dd94",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.2",
    "stem": "A biotech firm must ensure that cryptographic keys used by its AI workloads comply with rotation policies mandated by NIST 800-57. They also need to record each rotation event for audits. Which configuration satisfies both requirements?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS KMS automatic key rotation enabled and AWS CloudTrail logging of KMS key usage events",
      "B": "Manual KMS key rotation every 90 days and SNS notifications",
      "C": "Store keys in AWS Secrets Manager with rotation schedule and CloudWatch Logs",
      "D": "Use KMS Customer-Managed Keys without rotation and document rotations manually"
    },
    "explanation": "KMS automatic key rotation meets NIST\u2019s rotation requirement. CloudTrail logs each key usage and rotation event for audits. Secrets Manager is for secrets, not KMS keys. Manual rotation and manual documentation are prone to error."
  },
  {
    "id": "06b9ee9489ccf4a49feaca0048cbc53e7aac77198553d2c09b72306067abe691",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.2",
    "stem": "A government agency requires proof that all data classification labels applied to AI training datasets are consistently enforced at access time and logged for audit. Which AWS feature combination accomplishes this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Macie classification jobs with CloudWatch Logs",
      "B": "AWS Lake Formation tag-based access control with CloudTrail logging",
      "C": "S3 bucket policies using object tags with Config rules",
      "D": "AWS Glue Data Catalog tags with Athena access logs"
    },
    "explanation": "Lake Formation tag-based access control enforces classification labels at query/run time and CloudTrail logs access attempts and policy evaluations. Macie only classifies data, not enforce. S3 policies and Glue tags don\u2019t provide unified enforcement with audit logging across services."
  },
  {
    "id": "da4691a024d355946bdf463cb204d1ddc725c6bc0f4ad605cf473bfa009c0697",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.2",
    "stem": "An AI SaaS provider must comply with SOC 2 Type II. They want automated continuous monitoring of configuration compliance against best practices and to generate evidence for auditor review. Which service should they deploy?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Trusted Advisor checks via API",
      "B": "Amazon Inspector scans",
      "C": "CloudWatch Alarms",
      "D": "AWS Config conformance packs integrated with AWS Audit Manager"
    },
    "explanation": "Config conformance packs continuously evaluate compliance against rules and can feed evidence into Audit Manager. Trusted Advisor provides best practice checks but is not integrated with audit evidence. Inspector scans vulnerabilities, CloudWatch alarms aren\u2019t compliance focused."
  },
  {
    "id": "6c48ce0aff5a27d03b850a05e7b461187ac38bbec05be3e2267485ac7a71e22c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.2",
    "stem": "A retail AI application must maintain governance documentation for all third-party AI model licensing across its organization. They need to store model license documents, track approval status, and make them available for compliance reviews. Which AWS service combination is most appropriate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Artifact for storing compliance documents and AWS Service Catalog for approval workflows",
      "B": "AWS License Manager for AI model licenses and AWS Config for tracking",
      "C": "Amazon S3 with Inventory and AWS IAM for access control",
      "D": "AWS Secrets Manager for document storage and AWS Systems Manager Parameter Store"
    },
    "explanation": "AWS Artifact provides storage and retrieval of compliance/licensing documents, while Service Catalog can manage approval workflows for model usage. License Manager is for software licenses, not documents. S3/inventory doesn\u2019t provide workflows. Secrets Manager and Parameter Store aren\u2019t suited for documents."
  },
  {
    "id": "9a2adb3a38a6fc1d6077bbba06b0df709688b1b7a7844bdf4eb718542fde45b0",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.2",
    "stem": "A media company must ensure that all AI model inference requests are encrypted in transit and that they can prove encryption usage to auditors. What combination of AWS features provides proof of TLS usage and endpoint encryption enforcement?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker endpoint with IAM authentication enforced",
      "B": "Use VPC endpoints and CloudWatch Logs",
      "C": "Enable HTTPS-only on Amazon API Gateway private integration for SageMaker and log TLS handshake data in CloudTrail",
      "D": "Configure SageMaker endpoint security configurations with AWS WAF"
    },
    "explanation": "Enforcing HTTPS-only on API Gateway ensures TLS. CloudTrail logs the API calls including protocol used, providing evidence of TLS usage. IAM authentication alone doesn\u2019t prove TLS. VPC endpoints encrypt at network layer but don\u2019t log TLS. WAF is for web filtering."
  },
  {
    "id": "2e8f4b546497612bfe939314a6909dd50296b118f52146fcac97717d6e402f6a",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.2",
    "stem": "An AI-driven compliance tool ingests financial documents and must adhere to data residency policies. Audit reviewers need to confirm where data is stored and processed. Which AWS artifact should the compliance team download and review?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Config compliance pack",
      "B": "AWS CloudTrail event history report",
      "C": "AWS Audit Manager evidence report",
      "D": "AWS Artifact Service Organization Control (SOC) reports and ISO certifications"
    },
    "explanation": "AWS Artifact provides official compliance reports (SOC, ISO) that document AWS\u2019s data center locations and controls. Config and CloudTrail give operational logs but not official compliance attestations. Audit Manager evidence is operational, not formal certification."
  },
  {
    "id": "4505c2923cf31c8c8665bdf170c13d02ce7f73c57a76ff5202b5a5b42965f14f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.2",
    "stem": "A global AI service must enforce that all customer data is tagged with its classification level and that any untagged resources trigger an alert and are remediated automatically. What should they implement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "S3 bucket tagging policies with AWS Config Aggregator",
      "B": "AWS Config Rules to detect missing resource tags and Automated Remediation via AWS Systems Manager",
      "C": "IAM permission boundary on tagging operations and CloudWatch Events",
      "D": "AWS Organizations tag policies with CloudTrail triggers"
    },
    "explanation": "Config Rules can detect untagged resources and trigger automated remediation through Systems Manager Automation runbooks. Tag policies enforce tagging at creation but don\u2019t alert or remediate existing untagged resources. IAM boundaries limit tagging but don\u2019t enforce remediation. Organizations tag policies only define tag keys/values."
  },
  {
    "id": "b3f877ce79fa1bc54cf56b9385a312fc9547be884c355771bc23276c72400357",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.2",
    "stem": "A logistics company automates AI workflows across multiple AWS accounts. They need a centralized view of compliance posture, policy drift, and remediation status for AI workloads. Which architecture best meets this requirement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Organizations with a delegated admin account for AWS Config Aggregator and cross-account Config Rules",
      "B": "Enable CloudTrail multi-account trails to a central S3 bucket",
      "C": "Deploy AWS Audit Manager in each account and aggregate reports manually",
      "D": "Use AWS Security Hub with guardrails in each account"
    },
    "explanation": "Config Aggregator centralized in a delegated admin account aggregates compliance data and cross-account Config Rules enforce policies. CloudTrail centralization logs events but doesn\u2019t enforce compliance. Audit Manager manual aggregation is error-prone. Security Hub focuses on security findings, not general compliance rules."
  },
  {
    "id": "c886900db7648a6a324bd15b750f730ecdaeca7e217c5b33eda167bb60799b7b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.2",
    "stem": "A company\u2019s AI models handle regulated PII and require an immutable audit trail of all data access events and API calls for five years. Which setup ensures immutable storage and forensic-grade audit logs?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CloudTrail logs to encrypted S3 with lifecycle prevent deletion",
      "B": "CloudWatch Logs encrypted and archived monthly",
      "C": "CloudTrail multi-region trails with S3 Object Lock in Compliance mode and AWS Config recording",
      "D": "Store logs in DynamoDB with point-in-time recovery"
    },
    "explanation": "CloudTrail multi-region provides comprehensive logs; S3 Object Lock Compliance mode ensures immutability for 5 years; Config adds resource change history. CloudWatch Logs and DynamoDB don\u2019t guarantee immutability at forensic standard."
  },
  {
    "id": "e1d4d38b6a993476bad01a2fcbb613fb461fbce4a9fdb483cdb3c237441c2e93",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.2",
    "stem": "An AI startup must prove to auditors that its development and deployment processes follow its internal governance framework. They want to automatically capture process adherence events (e.g., model promotion, dataset approval) and store them with cryptographic integrity. Which solution provides this capability?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS CodePipeline with CloudWatch Logs",
      "B": "AWS EventBridge custom event bus with CloudTrail integration and S3 Object Lock",
      "C": "AWS Step Functions with DynamoDB Streams",
      "D": "AWS Config custom rule with SNS notifications"
    },
    "explanation": "Custom events on EventBridge can record governance events; CloudTrail integration logs them; storing in S3 with Object Lock preserves cryptographic integrity. CodePipeline logs aren\u2019t structured for governance events. Step Functions/DynamoDB don\u2019t ensure immutability."
  },
  {
    "id": "ee2aae797a8a0c3feff8c57c9438be2c1a07471cff296631a26f4b02493c77a3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.2",
    "stem": "A financial AI application needs to enforce multi-factor authentication (MFA) for any user executing model deployment actions. Auditors request logs of MFA usage per action. Which configuration meets these requirements?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Require MFA in IAM policies and use CloudWatch Logs for user activity",
      "B": "Enable AWS SSO with MFA and review SSO audit logs",
      "C": "Use AWS Organizations SCP to enforce MFA and check AWS Config",
      "D": "Enforce MFA via IAM policy conditions and audit CloudTrail logs for sts:AssumeRole MFAAuthenticated field"
    },
    "explanation": "IAM policy conditions can require MFAAuthenticated:true. CloudTrail logs sts:AssumeRole events including MFAAuthenticated flag, providing audit trail. SSO logs may not map to IAM actions, and SCP cannot enforce MFA at action level."
  },
  {
    "id": "ba6905fdcdeb86dfaa10d44739c2bad10cf50f96595a035a29805cc0fb3143e4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.2",
    "stem": "A healthcare AI solution must ensure that model explainability artifacts (e.g., SHAP values) are retained for at least three years and accessible for audits. They also need to track who accessed these artifacts. Which approach satisfies both requirements?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Store artifacts in S3 with access logging enabled and Object Lock with 3-year retention",
      "B": "Publish artifacts to AWS Glue Data Catalog with 3-year lifecycle",
      "C": "Save artifacts in DynamoDB with PITR and CloudWatch Logs",
      "D": "Use AWS RDS with automated backups and IAM auditing"
    },
    "explanation": "S3 with Object Lock in Compliance mode ensures retention. S3 access logging records who accessed artifacts. Glue Catalog doesn\u2019t store artifacts; DynamoDB PITR and CloudWatch Logs don\u2019t provide immutability or direct access logs at file level; RDS backups aren\u2019t suitable for artifact storage."
  },
  {
    "id": "e6c9bc828d63ab335e06bb33643d88a2d0b2b41571c97a55bf4adaedaac67b46",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.2",
    "stem": "A global AI service provider must comply with multiple regulatory frameworks (GDPR, HIPAA, PCI DSS) and present a unified compliance dashboard. Which AWS service can consolidate evidence from multiple sources and provide framework-based status?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Security Hub",
      "B": "AWS Config dashboard",
      "C": "AWS Audit Manager",
      "D": "AWS Well-Architected Tool"
    },
    "explanation": "Audit Manager maps evidence from CloudTrail, Config, IAM, etc., to multiple compliance frameworks and provides a unified dashboard. Security Hub focuses on security findings, Config dashboards show resource compliance, and Well-Architected Tool is for architecture reviews."
  },
  {
    "id": "ae197ede218dedcce84c18b689c24296daed220b6e2335b42bad269e1a9e19bc",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.2",
    "stem": "An AI platform processes credit card transactions and must adhere to PCI DSS data retention and encryption-at-rest requirements. They need to demonstrate that all EBS volumes hosting model artifacts are encrypted and snapshots retained for 1 year. Which configuration provides auditors with automated proof?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable EBS encryption by default and use SSM Automation for snapshot retention",
      "B": "Use AWS Config managed rule ebs-volume-in-use-encrypted and aws-config SnapshotRetention custom rule",
      "C": "Manually tag encrypted volumes and generate quarterly reports",
      "D": "Use AWS Backup vault lock with a 1-year lock period"
    },
    "explanation": "Config managed rule checks encryption on EBS volumes; a custom Config rule can enforce and report snapshot retention. This automates evidence. Default encryption doesn\u2019t prove snapshot retention; manual tagging and reporting is not automated; AWS Backup supports snapshots but not CSI-level reporting in Config."
  },
  {
    "id": "c627e1fee3f0a76b4a7ea8d8fdee142494ca446410e44120a6d08fba82274a59",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.2",
    "stem": "A pharmaceutical AI workflow uses Amazon ECS with Fargate. Auditors require logs of container image provenance and compliance validation results before deployment. Which combination ensures traceability and compliance checks?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon ECR image scanning with AWS Config rule ecr-image-scanning-required",
      "B": "Enable CloudTrail on ECS and tag images",
      "C": "Run Amazon Inspector on Fargate tasks at runtime",
      "D": "Use AWS Service Catalog to deploy ECS services"
    },
    "explanation": "ECR image scanning finds vulnerabilities. Config rule ensures all images are scanned before use. CloudTrail logs ECS API calls but doesn\u2019t enforce scanning; Inspector runtime doesn\u2019t ensure pre-deployment checks; Service Catalog handles provisioning but not scanning enforcement."
  },
  {
    "id": "b17c645408b8d939c562ae12355a0d45d8ed57e04f6f995ce70103a9939f9e4e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.2",
    "stem": "An AI-driven fraud detection service must implement least-privilege IAM policies for model invocation and ensure policies are reviewed every 90 days. Which AWS feature helps automate policy compliance checks and certification workflows?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "IAM Access Analyzer with monthly reports",
      "B": "AWS IAM Access Analyzer for policy findings and AWS Systems Manager Patch Manager calendar for review reminders",
      "C": "AWS Artifact for policy certifications",
      "D": "AWS Config with IAM policy evaluation rules"
    },
    "explanation": "IAM Access Analyzer identifies overly permissive policies. Combined with Systems Manager maintenance windows or Patch Manager calendar, it can schedule certification reminders. Config can evaluate policies but doesn\u2019t manage certification workflows. Artifact is for documents."
  },
  {
    "id": "42dba768967f67a9b8ec8613b5a7dc10ac1a0eb25e5a275d59308946a9b3e589",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.2",
    "stem": "A global healthcare company uses AI to process patient data. They need to enforce that all AI-related API calls originate from within their VPC and that any violations are captured for audit. Which configuration meets this need?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS WAF to block external calls and CloudWatch alarms",
      "B": "Require TLS mutual authentication and log to CloudTrail",
      "C": "Deploy VPC Interface endpoints for AI services and enable CloudTrail for AWS PrivateLink logs",
      "D": "Use Security Groups to restrict IPs and S3 bucket policies"
    },
    "explanation": "VPC interface endpoints (PrivateLink) ensure API calls stay within VPC; CloudTrail logs PrivateLink calls for audit. WAF is for HTTP endpoints, mutual TLS doesn\u2019t enforce VPC origin, security groups and bucket policies don\u2019t cover all AI service APIs."
  },
  {
    "id": "761e1300f04de04061b1c1d844fdedece20e915b9de590431d0fe793a149d8dc",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.2",
    "stem": "A data analytics firm must prove to regulators that their AWS AI environment is configured according to the company\u2019s custom governance standards. They want continuous evaluation and weekly compliance reporting. Which AWS service should they leverage?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Trusted Advisor scheduled checks",
      "B": "Amazon Macie with scheduled classification",
      "C": "CloudTrail Lake queries",
      "D": "AWS Config with custom conformance packs and delivery channels"
    },
    "explanation": "Config custom conformance packs can encode the company\u2019s governance standards and AWS Config delivery channels export weekly compliance reports. Trusted Advisor and Macie focus on specific checks, CloudTrail Lake is for ad-hoc analysis."
  },
  {
    "id": "a95c93a16a40c8d4eefb104d6d2316851dd3d8f48d8f20a44f489e640cbb2bbe",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.2",
    "stem": "An AI solution must be accredited under the EU AI Act, requiring documentation of governance processes and risk assessments. The company wants to version-control governance artifacts, track approval status, and retrieve an audit trail. Which combination of AWS services meets this requirement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS CodeCommit for artifacts, AWS Config for tracking",
      "B": "AWS CodeCommit with signed commits, AWS CodePipeline approvals, and AWS Artifact for framework mapping",
      "C": "Amazon S3 with Object Lock and AWS Step Functions",
      "D": "AWS Systems Manager Parameter Store and AWS CloudTrail"
    },
    "explanation": "CodeCommit signed commits version artifacts; CodePipeline approval actions track governance process; Artifact provides regulatory framework documentation. Config tracks resources not governance docs; S3/Object Lock store artifacts but lack workflow; Parameter Store unsuitable for documents."
  },
  {
    "id": "b8d44d8326a4ae4cff914f7966cb64d176497ba5193aa329a4b16b8e8f681758",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.2",
    "stem": "A fintech company\u2019s risk committee requires that any change to AI-model hyperparameters in production must be approved and logged. They also want to recover the previous approved hyperparameters if needed. What solution provides this capability?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Model Registry with approval workflows and version tracking",
      "B": "Store hyperparameters in AWS Secrets Manager with version labels",
      "C": "Log hyperparameter changes via CloudWatch and manual approval",
      "D": "Maintain hyperparameters in DynamoDB with PITR"
    },
    "explanation": "Model Registry supports model versions, approval statuses, and metadata including hyperparameters. It allows rollback to prior versions. Secrets Manager isn\u2019t designed for hyperparameter metadata and approvals. CloudWatch logs are not structured for governance; DynamoDB PITR captures data but lacks approval workflows."
  },
  {
    "id": "264fb8306f385d395b73da38d9d18cc1c3312b5f371ab76e5e18f8718f25e24f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.2",
    "stem": "An AI company must implement data residency enforcement for model inference logs, ensuring logs for APAC customers are stored only in APAC regions. Which mechanism enforces this and prevents misconfiguration?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CloudTrail multi-region trails with S3 replication restrictions",
      "B": "Use IAM conditions to restrict bucket ARNs by region",
      "C": "AWS Organizations SCP denying log delivery outside specified APAC regions",
      "D": "AWS Config rule to detect cross-region log buckets"
    },
    "explanation": "An SCP can block any S3 log delivery to buckets outside allowed regions at organization level. Config rules detect misconfig but don\u2019t prevent creation. IAM conditions can be bypassed if incorrect policy applied. CloudTrail replication restrictions aren\u2019t a feature."
  },
  {
    "id": "6b7f1d25ea005d0dcb395a880df26a0a465a084c8f520a446c79aa7fd8d7aebc",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.2",
    "stem": "A government contractor\u2019s AI system must maintain an immutable record of data provenance, model training parameters, and inference results, accessible for audits but not deletable. Which storage configuration satisfies these needs?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon RDS with encryption and backups",
      "B": "DynamoDB with point-in-time recovery",
      "C": "EFS with lifecycle policies",
      "D": "S3 with Object Lock in Compliance mode and versioning enabled"
    },
    "explanation": "S3 Object Lock Compliance mode ensures immutability. Versioning preserves object history. RDS backups and DynamoDB PITR can be changed or deleted, EFS doesn\u2019t offer immutability."
  },
  {
    "id": "7d90a0e6c8fe61ea87030e5eb56158e05ce2359d864a4c10944705d567f4fb07",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.2",
    "stem": "An AI-driven recommendation service must demonstrate to auditors that all API keys for third-party data sources are rotated every 90 days and usage is logged. Which combination of AWS services enforces rotation and tracks usage?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Secrets Manager automatic rotation and CloudTrail data events for Secrets Manager",
      "B": "IAM Access Keys for service accounts and CloudWatch Logs",
      "C": "AWS KMS key rotation and AWS Config rule",
      "D": "AWS SSO for credential management and CloudTrail"
    },
    "explanation": "Secrets Manager supports automatic secret rotation and CloudTrail Data Events capture every GetSecretValue call for usage logging. IAM Access Keys and KMS key rotation don\u2019t cover API keys, and SSO doesn\u2019t rotate secrets automatically."
  },
  {
    "id": "a9c6df5c741da873b397a8c8941f5559502363c8964b8cb9b5cd9fcf75885708",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.2",
    "stem": "A multinational enterprise needs to certify quarterly that its AI environment complies with its internal security policy and generate a report for each region. Which service simplifies cross-region compliance evidence collection and report generation?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Security Hub with custom insights",
      "B": "AWS Audit Manager with multiple regional assessments",
      "C": "AWS Trusted Advisor consolidated view",
      "D": "AWS Config dashboard exports"
    },
    "explanation": "Audit Manager supports running assessments per region and consolidates evidence into reports. Security Hub insights focus on security findings, not policy compliance frameworks. Trusted Advisor and Config dashboards lack formal reporting against internal policies."
  },
  {
    "id": "2ee07bc0e152262036fe24fd12eb169cdf5bc723817ce64ec410133569ffbc5a",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "5.2",
    "stem": "A financial AI platform must enforce that no S3 bucket associated with model artifacts is publicly exposed. They want automated remediation when a violation occurs and documentation of the remediation action for auditors. What implementation meets these needs?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Config rule s3-bucket-public-read-prohibited with remediation via Systems Manager Automation runbook and Config remediation history",
      "B": "S3 Block Public Access and CloudWatch alarm",
      "C": "Bucket policies to deny public access and SNS notifications",
      "D": "IAM SCP to prevent public ACL creation and manual ticketing"
    },
    "explanation": "Config managed rule detects public buckets; remediation runbooks can automatically fix and Config records remediation history for audit. Block Public Access prevents exposure but doesn\u2019t document an audit trail. Bucket policies and SCPs prevent but lack remediation tracking."
  },
  {
    "id": "5c97557a7b8d41c13529307713d704e9d4ce497f5b077f5ac2fc93ff31ffdd50",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A company implements a rule-based tax calculation engine that applies predefined rules to compute tax liabilities and never adapts from past transactions. According to standard AI/ML definitions, this solution is classified as:",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "An AI system but not a machine learning system",
      "B": "A machine learning system but not an AI system",
      "C": "A deep learning system",
      "D": "Neither an AI nor a machine learning system"
    },
    "explanation": "Rule-based systems fall under AI broadly but do not learn from data, so they are not considered ML."
  },
  {
    "id": "6fcbc845bc047bfa23af5bba3094f259e026482f8584698a33dcb03c7c2bd4c2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "An online recommendation service updates user suggestions instantly as users browse items. Which inferencing type best describes this system?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Batch inferencing",
      "B": "Real-time inferencing",
      "C": "Offline inferencing",
      "D": "Mini-batch inferencing"
    },
    "explanation": "Instant updates require real-time inferencing, not periodic batch processing."
  },
  {
    "id": "c6be6468efcb8f416d48c7b82a4b44b0a87ff9c084a0ce8955f9c5c0dd42db98",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A facial recognition model shows lower accuracy on certain demographic groups compared to others. To address this, the practitioner must improve which concept?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Bias",
      "B": "Fit",
      "C": "Fairness",
      "D": "Generalization"
    },
    "explanation": "Fairness ensures consistent performance across groups; bias refers to error sources but fairness is the goal."
  },
  {
    "id": "0783fda7fa2010dcf09221370ba89bcf2f7b765977a1863b7747e03d4661b236",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A model achieves 98% accuracy on training data but only 60% on validation data. This pattern indicates:",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Underfitting",
      "B": "Overfitting",
      "C": "Good fit",
      "D": "Data bias"
    },
    "explanation": "High training and low validation accuracy show the model memorized training data (overfitting)."
  },
  {
    "id": "116ec7524754f50bef64dd34b6410bd4edd6a4f838fef6f56eac10b19bafa3d0",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A retailer has transactional logs without labels and wants to segment customers by purchase behavior. Which learning approach is most appropriate?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Supervised learning",
      "B": "Unsupervised learning",
      "C": "Reinforcement learning",
      "D": "Deep learning"
    },
    "explanation": "Clustering segments without labels requires unsupervised learning."
  },
  {
    "id": "d48a98cc9b0968a7cc56046b8cd251cfed68cd1ebd48765259662c3141ff6ea3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "Which data type best describes sensor readings collected every second from an industrial machine?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Unstructured data",
      "B": "Tabular static data",
      "C": "Time-series data",
      "D": "Categorical data"
    },
    "explanation": "Sequential timestamped readings are time-series data."
  },
  {
    "id": "45bdba1c829358cfbf396b0d8b4e77e456a9e1e6bccd3880854d927cd6361a59",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A startup wants to index and search transcripts of customer support calls. Which AI domain should they leverage?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Computer vision",
      "B": "Natural language processing",
      "C": "Reinforcement learning",
      "D": "Generative AI"
    },
    "explanation": "Processing and searching text transcripts is an NLP use case."
  },
  {
    "id": "6314b086e605e719290a0b835cdf9d84430294a7e876b1933887969779cce9cf",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "In machine learning, the iterative process that adjusts weights during model training is called the \u201calgorithm,\u201d whereas the learned weights themselves constitute the \u201c\u201d:",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Model",
      "B": "Feature",
      "C": "Dataset",
      "D": "Payload"
    },
    "explanation": "The algorithm is the procedure; the model is the learned representation (weights)."
  },
  {
    "id": "82fca61a67bc5d1015b337c22adf590ef7d6079e70120fc46bc1e03df540f2b7",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "Which of the following algorithms is NOT considered a deep learning method?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Convolutional neural network",
      "B": "Recurrent neural network",
      "C": "Random forest",
      "D": "Transformer"
    },
    "explanation": "Random forest is an ensemble of decision trees, not a neural network."
  },
  {
    "id": "cb68d51cf56ea74792a9ff067cd3916ce70f5067d230181be14b1346e2a9d3ed",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A monthly invoice processing pipeline processes transactions at month end. What type of inferencing does it use?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Real-time inferencing",
      "B": "Online inferencing",
      "C": "Batch inferencing",
      "D": "Stream inferencing"
    },
    "explanation": "Processing data at set intervals is batch inferencing."
  },
  {
    "id": "d1ae6836cf587de995a80ad3a0cf1952b2e8a682ef5db6094ed349d620bcda14",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A model with multiple hidden layers that learns hierarchical feature representations is best described as:",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Machine learning model",
      "B": "Deep learning model",
      "C": "Reinforcement learning model",
      "D": "Statistical model"
    },
    "explanation": "Multiple hidden layers characterize a deep learning model."
  },
  {
    "id": "d3a328816f19e9cee02e24fbddb553b8a8f8ea5bb17129d5c15b285d1a65ffbe",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A robotic arm learns to sort objects by trial and error with a reward for correct sorting. This exemplifies:",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Supervised learning",
      "B": "Unsupervised learning",
      "C": "Reinforcement learning",
      "D": "Transfer learning"
    },
    "explanation": "Learning via rewards/punishments is reinforcement learning."
  },
  {
    "id": "eeb75b2dc912493f1975ae6890115992f02b69b16dde2cf160244106aac36a49",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "When a model\u2019s training and validation errors both decrease to a stable low value, this indicates:",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Underfitting",
      "B": "Overfitting",
      "C": "Optimal fit",
      "D": "Data leakage"
    },
    "explanation": "Low and stable errors on both sets suggest a good balance (optimal fit)."
  },
  {
    "id": "1b6eef57871ef0947276a044ebf07b2c6c9a63359c9254446d5541c94a4b5989",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A marketing team uses a pre-trained model with billions of parameters on text corpora to generate summaries. This model type is known as:",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Small language model",
      "B": "Large Language Model (LLM)",
      "C": "Rule-based model",
      "D": "Reinforcement model"
    },
    "explanation": "Models with billions of parameters trained on text are Large Language Models."
  },
  {
    "id": "22b0f9def460ffbf7f412f296ee510a003d9929604f743227a8fad3d935ae942",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "Which concept maps textual tokens to dense numerical vectors for downstream processing?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Chunking",
      "B": "Tokenization",
      "C": "Embedding",
      "D": "Sampling"
    },
    "explanation": "Embeddings convert tokens into continuous vector representations."
  },
  {
    "id": "b9bb67fb7a141bae9daf5eee0d158175b4aee7079f0ea93f2bb0442c70da0295",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "Image files used in computer vision tasks are best characterized as which data type?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Structured data",
      "B": "Unstructured data",
      "C": "Tabular data",
      "D": "Categorical data"
    },
    "explanation": "Image pixels do not follow fixed schema\u2014hence unstructured."
  },
  {
    "id": "90ed2415de998e77e0453d901dc0969367401b3d50733a2c4a58f2b52f1d839c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "Feeding new data into a trained model to generate predictions is called:",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Training",
      "B": "Inferencing",
      "C": "Preprocessing",
      "D": "Tuning"
    },
    "explanation": "Predicting with a trained model is inferencing."
  },
  {
    "id": "a37a0ba0e89e5fcb8c78b60d856a5f2eb8349d756e1ebcff44d50caabd52ecc5",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "In linear regression, the linear equation y=mx+b with learned m and b represents the:",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Model",
      "B": "Algorithm",
      "C": "Feature",
      "D": "Weight"
    },
    "explanation": "The equation with learned parameters is the model; algorithm is how it\u2019s learned."
  },
  {
    "id": "23c7837e6282bb7a6d6a8abe93961164da5d6789b59b17e2f90704da39f27c9f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A telecom company wants to group customers based on usage patterns without prior labels. Which technique and learning type apply?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Supervised classification",
      "B": "Unsupervised clustering",
      "C": "Supervised regression",
      "D": "Reinforcement Q-learning"
    },
    "explanation": "Grouping without labels uses unsupervised clustering."
  },
  {
    "id": "2c0904dc98ca29149bd211af408caf0630020dd9b34b21e43e58892e7a206216",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A delivery drone optimizes its navigation policy by trial and error with reward for timely delivery. Which learning type and business objective does this exemplify?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Supervised learning for prediction",
      "B": "Unsupervised learning for segmentation",
      "C": "Reinforcement learning for decision optimization",
      "D": "Deep learning for feature extraction"
    },
    "explanation": "Using rewards to improve decisions is reinforcement learning for decision optimization."
  },
  {
    "id": "5c13cd388cc1c2f0b683ac746cf4dc0fbcfe98872c3ca604611d9d3d684f26b9",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "An autonomous vehicle collects LIDAR point clouds in real time but processes them hourly for route refinement. Identify the data type and inferencing mode.",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Time-series & real-time",
      "B": "Unstructured & batch",
      "C": "Structured & streaming",
      "D": "Tabular & real-time"
    },
    "explanation": "Point clouds are unstructured; hourly processing is batch inferencing."
  },
  {
    "id": "590fbd25640b43f02e95bb6334b077203aaefcee210ea6e5f321e43dbca93a87",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "DeepMind\u2019s AlphaGo learns to win Go by playing games against itself and receiving rewards. Which learning paradigm is this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Supervised learning",
      "B": "Unsupervised learning",
      "C": "Reinforcement learning",
      "D": "Transfer learning"
    },
    "explanation": "Self-play with rewards is a reinforcement learning approach."
  },
  {
    "id": "36907c8a7401e682a11e8a5cb2e6be953c7a8a9ac0a0f949180c89eca8765ed6",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "In a sentiment analysis pipeline, converting word indices into dense vectors is called:",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Tokenization",
      "B": "Embedding",
      "C": "Chunking",
      "D": "Sampling"
    },
    "explanation": "Embedding maps discrete indices to continuous vectors."
  },
  {
    "id": "755a1cb26fe16e46589278e8eff789ec928a3503122f4d4dbd2cfaa026ad33e1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A bank predicts the likelihood of loan default as a probability. Which ML task is this?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Classification",
      "B": "Regression",
      "C": "Clustering",
      "D": "Dimensionality reduction"
    },
    "explanation": "Predicting discrete outcome probabilities is classification."
  },
  {
    "id": "dfb63928624ad476028f287bed7276d026b695b38600c5352b29d79836ecb1a1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "An email spam filter learns from labeled spam and non-spam examples. Identify the data labeling and learning type combination.",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Labeled data & supervised learning",
      "B": "Unlabeled data & unsupervised learning",
      "C": "Labeled data & reinforcement learning",
      "D": "Unstructured data & supervised learning"
    },
    "explanation": "Spam filters use labeled examples for supervised learning."
  },
  {
    "id": "0a58c54e4decae0cebd849649ec52c91d882805dd6705cac956bce5714d6b3bd",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "An autoencoder learns normal transaction patterns without labels to detect anomalies. This technique falls under:",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Unsupervised learning",
      "B": "Supervised learning",
      "C": "Reinforcement learning",
      "D": "Transfer learning"
    },
    "explanation": "Autoencoders train on unlabeled data to learn patterns\u2014unsupervised learning."
  },
  {
    "id": "ed02144bd445e025c47a6bfebefa1bfc4aaef8dddd47eb2364c1b8d2b54afdc2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A fraud detection system predicts fraudulent transactions instantly using a trained supervised model. Which combination applies?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Unsupervised learning & batch inferencing",
      "B": "Supervised learning & batch inferencing",
      "C": "Supervised learning & real-time inferencing",
      "D": "Reinforcement learning & real-time inferencing"
    },
    "explanation": "Predicting fraud instantly with a trained supervised model is real-time inferencing."
  },
  {
    "id": "70b46ef83292859320bd130409d22e95d87d1dee9893fdeeaa23c1cdc0243230",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "A customer segmentation model groups users on the fly into clusters for personalized offers. Which combination applies?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Supervised learning & real-time",
      "B": "Unsupervised learning & real-time",
      "C": "Supervised learning & batch",
      "D": "Reinforcement learning & batch"
    },
    "explanation": "On-the-fly grouping without labels is unsupervised real-time inferencing."
  },
  {
    "id": "f09d3d40f977800f38f5d21c70b17d4a9f229984264d61612d6d9b962b7218fa",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "Gradient descent, used to minimize loss during training, is an example of a:",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Model",
      "B": "Algorithm",
      "C": "Feature",
      "D": "Hyperparameter"
    },
    "explanation": "Gradient descent is the algorithm that updates model parameters."
  },
  {
    "id": "f7f5a4718ba2712e69dd1fbdd7e1cd9699229d6569fdafac806ab7eb15c0a508",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.1",
    "stem": "In a text-to-speech system, converting written language into spoken audio falls under which AI subdomain?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Computer vision",
      "B": "Natural language processing",
      "C": "Reinforcement learning",
      "D": "Anomaly detection"
    },
    "explanation": "Text-to-speech processes language, an NLP task."
  },
  {
    "id": "623ff3d0bb7527e884f7e0a325b1dbcd1d77eda26bc394f36177ff26a8d80dc0",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A financial institution must flag potentially fraudulent credit card transactions in real time. Which ML technique is most appropriate to minimize false negatives while allowing probabilistic scoring?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Logistic regression with threshold tuning",
      "B": "K-means clustering with centroid distance",
      "C": "Principal component analysis for dimensionality reduction",
      "D": "Linear regression predicting fraud score"
    },
    "explanation": "Fraud detection is a supervised, binary classification problem. Logistic regression provides probabilistic outputs and can be threshold-tuned to balance false positives/negatives. Clustering and PCA are unsupervised and linear regression predicts continuous values, not classes."
  },
  {
    "id": "009fc08e4be8be2a9b7deb06ebf81822bcfdd1785a68a49d847a7d381de0210a",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "An e-commerce retailer wants to segment customers for targeted marketing using only purchase history without labels. Which ML technique should they apply?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Hierarchical or k-means clustering",
      "B": "Decision tree classification",
      "C": "Logistic regression",
      "D": "Reinforcement learning"
    },
    "explanation": "Customer segmentation without labels is an unsupervised clustering problem. Classification and regression require labeled outcomes; reinforcement learning is for action policies, not segmentation."
  },
  {
    "id": "ea283bf7757e5feddde18b45ec0760f1726e66a5d481b984ca3cc1b5386f375a",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A subscription service wants to predict if a customer will churn at the end of their billing cycle. Which ML approach best matches this requirement?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Time-series regression forecasting",
      "B": "Supervised classification",
      "C": "Unsupervised clustering",
      "D": "Reinforcement learning"
    },
    "explanation": "Churn prediction is a binary outcome (churn/no churn), so supervised classification is appropriate. Forecasting is for continuous numeric predictions, clustering is unsupervised, and reinforcement learning addresses sequential decision making."
  },
  {
    "id": "60585be2fff5daf7a459cb71142efec83f320b4df148ffb7fd0f44dfb83594c6",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A retail chain needs to forecast next quarter\u2019s sales volume at each store using historical daily sales data. Which ML technique is most suitable?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Time-series forecasting",
      "B": "Binary classification",
      "C": "K-means clustering",
      "D": "Anomaly detection"
    },
    "explanation": "Sales forecasting over time is a time-series regression problem. Classification and clustering are not designed for predicting continuous values over time; anomaly detection identifies outliers, not future values."
  },
  {
    "id": "540dfb335482a04ef325ee815070f6a8217eed8fa0082ff699b8edd90f346a0a",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A travel website wants to analyze free-form customer reviews to determine overall sentiment. Which AWS managed service should they choose?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Comprehend",
      "B": "Amazon Translate",
      "C": "Amazon Lex",
      "D": "Amazon Polly"
    },
    "explanation": "Amazon Comprehend provides NLP capabilities including sentiment analysis. Translate is for language translation, Lex is for chatbots, and Polly is for text-to-speech."
  },
  {
    "id": "493479b88712ba964807b95f2fe9b39bf214aa43b11ebb21bd702b6a7be0600c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A call center needs to generate text transcripts from live voice calls for compliance auditing. Which AWS service do you recommend?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Transcribe",
      "B": "Amazon Comprehend",
      "C": "Amazon Lex",
      "D": "Amazon Polly"
    },
    "explanation": "Amazon Transcribe converts speech to text. Comprehend analyzes text, Lex builds chatbots, and Polly generates speech from text."
  },
  {
    "id": "ecd2e8d5d9797973c06eb58f168b82e2ff8687de096ac6e9812cce2d23abb26d",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A company wants to build an IVR system that understands customer intent and routes calls accordingly. Which service is the best fit?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Lex",
      "B": "Amazon Polly",
      "C": "Amazon Transcribe",
      "D": "Amazon Comprehend"
    },
    "explanation": "Amazon Lex provides intent recognition and dialog management ideal for IVR. Polly is TTS, Transcribe is speech-to-text, and Comprehend is NLP analysis not optimized for conversational flows."
  },
  {
    "id": "06ccff03573473dcb1deef5d6ce3644edca606ee64156efbbf8f786cbfcceba3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "An operations team wants to generate spoken alarms from textual alerts. Which AWS service fulfills this without custom model training?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Polly",
      "B": "Amazon Lex",
      "C": "Amazon Comprehend",
      "D": "Amazon Transcribe"
    },
    "explanation": "Amazon Polly provides high-quality text-to-speech directly. Lex is for chatbots, Comprehend is text analysis, and Transcribe is speech recognition."
  },
  {
    "id": "fe408cb68c08b6fe4b7c499b250c99b6f7da7f62284409058792875db7b2fc9c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A global marketing team needs to localize ad copy from English to Spanish at scale. Which service will deliver accurate, context-aware translations?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Translate",
      "B": "Amazon Comprehend",
      "C": "Amazon Polly",
      "D": "Amazon Transcribe"
    },
    "explanation": "Amazon Translate provides neural machine translation with context awareness. Comprehend analyzes sentiment and entities, Polly is TTS, and Transcribe is STT."
  },
  {
    "id": "f728a71a5950c6cb2c37475f6fc770b712b9f5db1b5017e93c8c3adb16d3db90",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A logistics provider wants to automatically extract line-item details from scanned invoices. Which AWS service is most appropriate?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Textract",
      "B": "Amazon Comprehend",
      "C": "Amazon Rekognition",
      "D": "Amazon Translate"
    },
    "explanation": "Amazon Textract extracts structured text (forms, tables) from documents. Comprehend analyzes unstructured text, Rekognition analyzes images, and Translate handles translation."
  },
  {
    "id": "e77c96d1d385adf3e379fb080367b60a82335b442a6780b405479c286842cfe1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A social media platform needs to tag objects in user-uploaded photos for content discovery. Which AWS service should be used?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Rekognition",
      "B": "Amazon Textract",
      "C": "Amazon Comprehend",
      "D": "Amazon Translate"
    },
    "explanation": "Rekognition provides object and scene detection in images. Textract is OCR, Comprehend is text NLP, and Translate is for language translation."
  },
  {
    "id": "78dc775aea7a01251c2fd36d22d7332f97fbad949be65442f6f7406cf883d6b2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "An R&D team wants to discover underlying topics in a large corpus of unlabeled news articles. Which ML approach is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Unsupervised topic modeling or clustering",
      "B": "Supervised classification",
      "C": "Reinforcement learning",
      "D": "Linear regression"
    },
    "explanation": "Topic discovery in unlabeled data requires unsupervised methods like clustering or topic modeling. Supervised and regression require labels; reinforcement learning is for sequential decision-making."
  },
  {
    "id": "a3ce56b74015406ffd544b81f28ef45afa958f8937ca8bb9c99857da84c93861",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A small bakery with stable weekly orders wants to plan production without high tooling costs. Which solution is most appropriate?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Rule-based heuristic forecasting",
      "B": "Time-series ML forecasting",
      "C": "Unsupervised clustering",
      "D": "Supervised classification"
    },
    "explanation": "Low variance and predictable demand can be handled with simple heuristics. ML forecasting adds unnecessary cost and complexity; clustering and classification don\u2019t predict numerical demand."
  },
  {
    "id": "2f402a5f3be0b0e1fda83287eb822960c93ca9a8f9aad4c09812ab65b118597f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A news aggregator must group articles in real time as they arrive without prior labels. Which AWS service or technique is optimal?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Kinesis + custom clustering in SageMaker",
      "B": "Amazon Comprehend sentiment analysis",
      "C": "Amazon Translate real-time translation",
      "D": "Amazon DetectTopic API"
    },
    "explanation": "Real-time grouping requires streaming + unsupervised clustering in SageMaker. Comprehend Topic Modeling is batch, no native DetectTopic API exists, and Translate only translates."
  },
  {
    "id": "7639fa19e704ee9232213b6556a34014663a78c11377b7948f4ba8ccc9a6d8f5",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A startup with only 100 users wants to recommend content. Labels are sparse. Which initial approach maximizes ROI?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Rule-based content filtering",
      "B": "Deep collaborative filtering",
      "C": "Supervised classification",
      "D": "Reinforcement learning"
    },
    "explanation": "With sparse data and few users, simple rule-based filtering yields ROI. Complex models require more data and expertise and may not outperform basic heuristics initially."
  },
  {
    "id": "55661066b46c4399c4bbefaaab76f10d7ca58c4f160dc21c5b50a6c98a65369a",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A credit card company needs to identify outlier spending behaviors without labeled fraud examples. Which technique fits best?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "K-means clustering",
      "B": "Isolation Forest anomaly detection",
      "C": "Logistic regression",
      "D": "Decision tree classification"
    },
    "explanation": "Isolation Forest is specifically designed for unsupervised anomaly detection. Clustering may group anomalies but is less direct; classification requires labels."
  },
  {
    "id": "5f4ebc65e3e40a5bbd1ba7bf697845637072d1b862b788f8152d9167f97fdacf",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A legal firm needs to extract entities (names, dates, citations) from contracts. Which AWS service combination is ideal for structured entity extraction?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Textract for form/key-value extraction + Amazon Comprehend for entity recognition",
      "B": "Amazon Transcribe + Amazon Translate",
      "C": "Amazon Rekognition + Amazon Polly",
      "D": "Amazon Lex + Amazon Comprehend"
    },
    "explanation": "Textract extracts structured text and key-value pairs; Comprehend then identifies entity types. Other combinations don\u2019t handle both OCR and NLP entity extraction."
  },
  {
    "id": "bf8245a018009abef9d3bd08f217fac8133022f536e0d733926e73dfade9a0e8",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A marketing team wants to detect keywords and sentiment in social media streams in near real time. Which architecture is best?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Kinesis Data Streams \u2192 Lambda \u2192 Comprehend Real-Time APIs",
      "B": "S3 batch upload \u2192 Comprehend Batch APIs",
      "C": "Lex chatbot \u2192 Comprehend",
      "D": "Transcribe streaming \u2192 Translate"
    },
    "explanation": "Kinesis + Lambda + Comprehend Real-Time enables streaming analysis. Batch APIs introduce latency; Lex is chat-focused; Translate is irrelevant."
  },
  {
    "id": "32d0f2a6e05bae892e8533c798d5f884812da8244b620f48d5b086198975f529",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A telecom wants to forecast network traffic spikes to provision capacity. Which ML solution suits this business case?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Time-series forecasting (ARIMA or deep learning)",
      "B": "Binary classification",
      "C": "Unsupervised clustering",
      "D": "Reinforcement learning"
    },
    "explanation": "Traffic spike prediction is a time-series forecasting problem. Classification predicts categories, clustering groups data, RL learns policies\u2014not forecasting continuous values."
  },
  {
    "id": "770724e21fcb97a03e416fd92d80c4b3570ca8ee672d4d46ce8cd8faf349299f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A publisher needs to translate large volumes of articles to multiple languages while preserving idiomatic expressions. Which AWS service best fits?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Translate with custom terminology",
      "B": "Amazon Comprehend sentiment analysis",
      "C": "Amazon Textract OCR",
      "D": "Amazon Polly TTS"
    },
    "explanation": "Translate with custom terminology supports idioms and domain-specific terms. Comprehend analyzes text, Textract extracts text, Polly synthesizes speech."
  },
  {
    "id": "40f86bb6a485a42aea267900074d443edd1456dfcdc01a91d37c05a416acb10b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A media company wants to summarize long-form video transcripts into bullet-point highlights. Which combination meets this need?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Transcribe \u2192 Comprehend Summarization",
      "B": "Translate \u2192 Polly",
      "C": "Lex \u2192 Comprehend Entities",
      "D": "Rekognition \u2192 Textract"
    },
    "explanation": "Transcribe produces text from video; Comprehend Summarization extracts bullet points. Other combos don\u2019t provide summarization."
  },
  {
    "id": "ebfd5eb3d1f80aff2b7694020d782d622c0bee32823a76e997b26830f4a04f30",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A law enforcement agency seeks to group seized documents by topic without pre-labeling. Which AWS ML feature supports this directly?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Comprehend Topic Modeling",
      "B": "Amazon Translate",
      "C": "Amazon Rekognition",
      "D": "Amazon Polly"
    },
    "explanation": "Comprehend Topic Modeling groups documents by topic unsupervised. Translate, Rekognition, and Polly serve other use cases."
  },
  {
    "id": "aff71f74bdcde1b33c556a1c8e60179d367b92f22251f63b68856d25f50d247d",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A retail chain has historical sales and external factors; they need to fine-tune a model to predict sales uplift from promotions. Which AWS service covers end-to-end without deep ML coding?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon SageMaker Autopilot",
      "B": "Amazon Personalize",
      "C": "Amazon Forecast",
      "D": "Amazon Comprehend"
    },
    "explanation": "SageMaker Autopilot automates training and tuning with custom data. Forecast is specialized for time-series only, Personalize is for recommendations, and Comprehend is NLP."
  },
  {
    "id": "965b76f2457fdf817945d426f0e25b2448dcc7616f65147467d4d18483ca93fa",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A fraud team wants an anomaly detection service tuned for metrics like transaction volume spikes without building models. Which AWS service is ideal?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon SageMaker",
      "B": "Amazon Comprehend",
      "C": "Amazon Lookout for Metrics",
      "D": "Amazon Kinesis"
    },
    "explanation": "Lookout for Metrics is designed for ML-based anomaly detection on time-series metrics. SageMaker requires model building; Comprehend is NLP; Kinesis is streaming infrastructure."
  },
  {
    "id": "9ee47bde8b877a024e4fa93d3c68797a8af5e3b0362987896d9efe5856552073",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A content platform must recommend articles based on user reading history but has no labeled preferences. Which AWS service accelerates this?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Personalize",
      "B": "Amazon SageMaker Ground Truth",
      "C": "Amazon Comprehend",
      "D": "Amazon Kendra"
    },
    "explanation": "Amazon Personalize provides collaborative filtering and content-based recommendations without requiring deep ML expertise. Ground Truth labels data, Comprehend is NLP, and Kendra is enterprise search."
  },
  {
    "id": "1c0cbefca99c6b09524d0fd2a78a6b2236ed3ee9c4fbb366d0f527c89d65390e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A hospital wants to classify radiology images into normal versus abnormal categories. There\u2019s sufficient labeled data but no DL expertise. What is the fastest path?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Build a CNN in SageMaker training jobs",
      "B": "Use SageMaker JumpStart prebuilt image classification pipeline",
      "C": "Use Rekognition",
      "D": "Use Amazon Comprehend"
    },
    "explanation": "SageMaker JumpStart offers prebuilt image classification pipelines. Rekognition doesn\u2019t support custom medical imaging; Comprehend is NLP."
  },
  {
    "id": "00d84e9bba22cc72ad50288c01c07d690f782aa6e8ce5a8fd79866551ee5bf22",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A compliance team needs to automatically detect PII (names, SSNs) in stored S3 objects. Which service fulfills this requirement?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Macie",
      "B": "AWS Config",
      "C": "Amazon GuardDuty",
      "D": "Amazon Macie"
    },
    "explanation": "Amazon Macie uses ML to detect PII in S3. Config monitors resources, GuardDuty detects threats, so only Macie fits."
  },
  {
    "id": "33962c89e2fbe8deba448fc5d3bf21ae31abb05688aa94d55d7a15266cf3879c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A streaming platform wants to cluster users based on viewing patterns for A/B testing but has no outcome labels. Which ML paradigm applies?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Supervised classification",
      "B": "Reinforcement learning",
      "C": "Unsupervised clustering",
      "D": "Regression"
    },
    "explanation": "Clustering viewing patterns without labels is unsupervised. Classification and regression need labeled targets; reinforcement learning requires a reward signal."
  },
  {
    "id": "064b9aa21ec1419149f18712e6d73b4844be19253e485e05665672dbe4b2cff4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A retailer must decide whether to use ML or rule-based automation for order fraud checks. Which factor indicates ML is NOT appropriate?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Large historical dataset available",
      "B": "Patterns frequently change",
      "C": "High risk of loss on misclassification",
      "D": "Business rules are stable and simple"
    },
    "explanation": "Stable and well-defined rules don\u2019t justify ML complexity. ML is appropriate when data is large, patterns evolve, and high accuracy is needed."
  },
  {
    "id": "8591400943cc2af003f88aa468095c17c53dc953b75536de9c7a6bc30731f7b3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "An automotive manufacturer needs to predict remaining useful life of machinery components (continuous output). Which ML technique fits?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Binary classification",
      "B": "Regression modeling",
      "C": "Clustering",
      "D": "Topic modeling"
    },
    "explanation": "Remaining useful life is a continuous numeric prediction, so regression is appropriate. Classification outputs discrete labels; clustering groups data; topic modeling handles text."
  },
  {
    "id": "8d4bcfc2f846f0c4537dda577fa54490554c2a7f2f2f917dc6a6e40300bd7292",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A media company needs to automatically generate metadata tags (keywords) for video frames. Which service solves both image analysis and metadata extraction without custom training?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Rekognition Video",
      "B": "Amazon Comprehend",
      "C": "Amazon Translate",
      "D": "Amazon Textract"
    },
    "explanation": "Rekognition Video provides frame-level object detection and tagging. Comprehend is NLP, Translate translates text, Textract extracts text from images/documents."
  },
  {
    "id": "2eb63534523cb7b68be2874754f6e80488e30d2e15e4e88c9cf8b8930aa9e512",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A news aggregator wants to deliver personalized article feeds by learning user preferences over time. Which AWS service should be used to avoid building custom ML pipelines?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Personalize",
      "B": "Amazon SageMaker Ground Truth",
      "C": "Amazon Comprehend Topics",
      "D": "Amazon Forecast"
    },
    "explanation": "Amazon Personalize offers managed recommendation models. Ground Truth is labeling, Comprehend Topics is unsupervised grouping, Forecast is time-series."
  },
  {
    "id": "e02b116580f68569cad8a8f5d33f4f7186166671950fa67063751b719a14d87d",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A vendor needs to translate medical device manuals, preserving domain-specific terminology. Which feature of Amazon Translate helps achieve this?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Custom parallel data",
      "B": "Custom terminology glossary",
      "C": "Neural Text to Speech",
      "D": "Active Custom Translation"
    },
    "explanation": "Custom terminology glossaries ensure consistent translation of terms. Parallel data can improve quality, but glossaries specifically lock term translations. Neural TTS is Polly; active custom translation isn\u2019t a separate feature."
  },
  {
    "id": "a099a9f915e7fecac3aba079d193793e69546408a7beeceb58753db52508f0f1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A support center wants to detect emerging topics in customer feedback over time without manual labeling. Which AWS managed feature supports this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Translate",
      "B": "Amazon Comprehend Sentiment API",
      "C": "Amazon Comprehend Topics",
      "D": "Amazon Lex"
    },
    "explanation": "Comprehend Topics performs unsupervised topic modeling to find themes. Sentiment API only labels sentiment; Translate and Lex serve other purposes."
  },
  {
    "id": "a919137837953946a38d4fbd8991cee5acc973d9bdaeea9a946a9be4a5cb85f9",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "For an AI solution that must provide sub-second inference on simple numeric input, which deployment method in AWS SageMaker is most cost-effective?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Batch transform",
      "B": "Serverless inference",
      "C": "Training job",
      "D": "Real-time endpoint with minimal provisioned instances"
    },
    "explanation": "Sub-second inference requires a real-time endpoint. Minimal instance count reduces cost; serverless has higher cold-start latency; batch transform is asynchronous; training jobs don\u2019t serve inferences."
  },
  {
    "id": "4c144b0a0a941915ebe54d18c4b8d4153696938a9022cac5c2e8b18766080ec1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.2",
    "stem": "A multi-language support team needs to extract key phrases and then translate them. Which service sequence is correct?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Comprehend key-phrases \u2192 Translate",
      "B": "Translate \u2192 Comprehend sentiment",
      "C": "Transcribe \u2192 Polly",
      "D": "Rekognition \u2192 Comprehend"
    },
    "explanation": "You first extract text features (key phrases) with Comprehend, then translate them with Translate. Other sequences do not fulfill both steps."
  },
  {
    "id": "7371fae0730b5a89c292a3987725b657fb6bf9956b239b3ea9cb55648ddc4637",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A retail analytics team needs to perform exploratory data analysis and feature engineering on customer transaction data before model training. They want a visual, low-code tool that integrates with Amazon S3 and writes features directly to a centralized store for reuse. Which AWS service combination should they choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Glue DataBrew for EDA and Amazon SageMaker Feature Store",
      "B": "Amazon SageMaker Data Wrangler for EDA and Amazon SageMaker Feature Store",
      "C": "AWS Glue for EDA and Amazon S3 for feature storage",
      "D": "Amazon Athena for EDA and Amazon DynamoDB for feature storage"
    },
    "explanation": "Data Wrangler provides visual EDA and preprocessing and natively writes to SageMaker Feature Store, enabling feature reuse."
  },
  {
    "id": "57a0a5c1b9366189e109fc6f736fac0e67e7beb079c94f7ac70298d72b901f64",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "An online gaming company wants to monitor model performance post-deployment to detect data drift and bias. Which combination of AWS services meets both requirements with minimal custom code?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon CloudWatch metrics and Amazon QuickSight dashboards",
      "B": "AWS Lambda functions and Amazon SNS alerts",
      "C": "Amazon SageMaker Model Monitor and Amazon SageMaker Clarify",
      "D": "AWS Config rules and AWS CloudTrail logs"
    },
    "explanation": "Model Monitor captures data drift, Clarify monitors bias. Together they automate monitoring with minimal code."
  },
  {
    "id": "059585985c6a00c0c8c7cb0abbef25df84309bd0c062c8fb689a82f229636014",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A financial services firm needs hyperparameter tuning at scale for their credit-risk classification model. Which SageMaker feature should they use, and why?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Automatic Model Tuning, because it launches parallel training jobs to search hyperparameter space",
      "B": "SageMaker Neo, because it compiles models for edge deployment",
      "C": "SageMaker Debugger, because it visualizes gradients during training",
      "D": "SageMaker Edge Manager, because it manages endpoints at the edge"
    },
    "explanation": "Automatic Model Tuning runs parallel training with different hyperparameter sets to optimize a specified metric."
  },
  {
    "id": "33247ad02944d61d67384253cddbba1983d25cbae388cb1afde54cbb71324fb1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A start-up wants to serve a deep learning inference endpoint with unpredictable traffic patterns and minimize cost. Which production method should they choose?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy on Amazon EC2 behind an Application Load Balancer",
      "B": "Use SageMaker batch transform with scheduled jobs",
      "C": "Host a self-managed Kubernetes cluster with auto-scaling",
      "D": "Deploy a SageMaker serverless inference endpoint"
    },
    "explanation": "Serverless inference automatically scales to zero and up on demand, minimizing cost under unpredictable traffic."
  },
  {
    "id": "8d5d46df9fb8262b86d325d70f75f1de43625d12e195ece1ec4c1fd24d120c9d",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "During model evaluation, the data scientist notices class imbalance. They need a metric that accounts for precision and recall. Which metric should they choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Accuracy",
      "B": "F1 score",
      "C": "Mean squared error",
      "D": "Log loss"
    },
    "explanation": "F1 score combines precision and recall, making it suitable for imbalanced classes."
  },
  {
    "id": "63785ff6c0b9c67abf976081b2ff982df03426f39c576eeff1295cd12b10d9f7",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A marketing team wants reproducible experiments and lineage tracking for each model iteration. Which SageMaker capability should they leverage?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Model Monitor",
      "B": "SageMaker Clarify",
      "C": "SageMaker Experiments",
      "D": "SageMaker Edge Manager"
    },
    "explanation": "SageMaker Experiments tracks parameters, metrics, artifacts, and lineage of experiments for reproducibility."
  },
  {
    "id": "bf455b4e21f019d244dcac66df27b9fd72f5f6b6f5cac50f3735384bd0e5a8f4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A hospital develops a medical imaging classifier. Regulations require ad-hoc manual review of flagged cases. Which combination helps automate inference and human review?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker batch transform and Amazon CloudWatch Alarms",
      "B": "SageMaker real-time endpoint and AWS Lambda",
      "C": "Amazon Augmented AI (A2I) with SageMaker real-time endpoint",
      "D": "SageMaker Neo compiled model and AWS Step Functions"
    },
    "explanation": "A2I routing config on SageMaker endpoint sends uncertain predictions for human review automatically."
  },
  {
    "id": "5f1739aa142f505373677caac67bf8304a7d0a4b5f19a4379eb0b5c0c1de96be",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "An e-commerce firm wants to compare performance of a custom model against a pre-trained model from JumpStart without managing training infrastructure. Which approach meets this requirement?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Download JumpStart model, fine-tune locally, compare metrics",
      "B": "Use SageMaker JumpStart to deploy both models to endpoints and run A/B tests",
      "C": "Use Amazon EC2 instances for both models, deploy batch transforms",
      "D": "Use AWS Lambda for custom code and AWS Glue for pre-trained inference"
    },
    "explanation": "JumpStart can deploy both pre-trained and custom models to endpoints; A/B testing through Invocation and metrics comparison."
  },
  {
    "id": "c5221967465b510d18130b40f4b82c9bb9cf0c3fd506e9a16125acb058ee41c9",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A credit card company wants to store features for online real-time fraud detection and offline batch risk scoring. Which storage option should they choose?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon S3 for both online and offline",
      "B": "Amazon DynamoDB for both",
      "C": "Amazon Aurora for both",
      "D": "SageMaker Feature Store with online and offline stores"
    },
    "explanation": "Feature Store provides both low-latency online and batch offline stores with consistent feature data."
  },
  {
    "id": "3dd795eef4bb33f7caa5643ef1c673253590d6c7fb77991396e4e1217f15db05",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A manufacturing company needs to orchestrate an ML pipeline that includes data processing, model training, tuning, deployment, and monitoring in a repeatable way. Which service is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon SageMaker Pipelines",
      "B": "AWS Step Functions without SageMaker integrations",
      "C": "AWS Data Pipeline",
      "D": "AWS Batch"
    },
    "explanation": "SageMaker Pipelines is designed to orchestrate ML workflows end-to-end with native SageMaker steps."
  },
  {
    "id": "1c43eff08b1cd19d8b8a9aa45cde8d104eedec0054053aa2d3917a6e298ca87b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "After deployment, a model\u2019s prediction latency spikes during peak hours. Which SageMaker feature helps maintain consistent latencies?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model Monitor data capture",
      "B": "Automatic endpoint scaling with target utilization",
      "C": "Clarify bias detection",
      "D": "Feature Store online cache"
    },
    "explanation": "Automatic scaling on SageMaker endpoints adjusts instance count based on utilization to maintain latency."
  },
  {
    "id": "83e64c9f438b962ced13ea9c057520b75cc5ffb8067eaa6b7977c4e192476fc3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A data science team must compare multiple model versions\u2019 performance on a benchmark dataset and register the best version for production. Which workflow accomplishes this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Glue jobs to evaluate models and update S3",
      "B": "Evaluate in Jupyter notebooks, manually tag models",
      "C": "Use SageMaker Experiments for tracking, register in SageMaker Model Registry",
      "D": "Deploy all versions to endpoints and pick highest throughput"
    },
    "explanation": "Experiments track runs; Model Registry stores approved model versions for production."
  },
  {
    "id": "1da792752e5d49b5bc30b1ebfd0ac4c75e88728795d037a8dbea3ab7220ed780",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A company collects streaming sensor data and wants to preprocess in real time before scoring. Which SageMaker component should they use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Data Wrangler",
      "B": "Glue ETL",
      "C": "Athena",
      "D": "SageMaker Processing with a streaming endpoint"
    },
    "explanation": "SageMaker Processing allows custom preprocessing code in real-time via a model endpoint or processing container integrated with streaming sources."
  },
  {
    "id": "441b64545fd73c9c2d1002b95ff41e262062a12f89c16d8dae991cfc354d8a0a",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "Which metric is most appropriate for selecting a regression model that balances prediction error and penalizes large errors in a housing price prediction task?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "R\u00b2 (coefficient of determination)",
      "B": "Root Mean Squared Error (RMSE)",
      "C": "Mean Absolute Percentage Error (MAPE)",
      "D": "Accuracy"
    },
    "explanation": "RMSE penalizes larger errors more heavily, suitable when large deviations are especially undesirable."
  },
  {
    "id": "5e1ea25d94b3975da547a8a23747654cd4a6d91949fcff8670277c9d1c1fc8fd",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A team needs to retrain their fraud detection model when input data distribution changes significantly. Which process should they implement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Model Monitor to detect drift and trigger SageMaker Pipelines retraining step",
      "B": "Daily manual retraining based on calendar schedule",
      "C": "Use CloudWatch to trigger a Lambda every hour for retraining",
      "D": "Rebuild model only when accuracy falls below 99%"
    },
    "explanation": "Model Monitor detects statistical drift and can trigger Pipelines to automate retraining upon drift detection."
  },
  {
    "id": "df4fe82492f29e5285e960da2934e5b20865859d6095c2aad39149e8d4ed7907",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A model deployed with multi-model endpoints in SageMaker needs version control and staged rollout. Which pattern supports this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use separate endpoints per model and DNS routing",
      "B": "Use batch transform for staging",
      "C": "Use SageMaker Model Registry with endpoint variant weights",
      "D": "Use AWS Lambda alias shifting"
    },
    "explanation": "Model Registry supports deployment as endpoint variants with weight shifting for staged rollouts."
  },
  {
    "id": "7f0ab06a54830f1b8fde8fcaec37cce2e1f4e8125076ffbf2af4a8ca79935300",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A recommendation engine team wants to evaluate two ranking algorithms offline without affecting production. Which SageMaker feature helps conduct such evaluation at scale?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Real-time inference endpoint with traffic split",
      "B": "SageMaker Batch Transform on held-out data",
      "C": "Ground Truth labeling jobs",
      "D": "Athena on S3 logs"
    },
    "explanation": "Batch Transform can process large volumes of data offline to compare algorithm outputs on the same dataset."
  },
  {
    "id": "538077f595fb50f794c720a78f620323bc4683370fd3c98a60522c382d0817bc",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A life-sciences company needs transparent audit trails for all data and model changes. Which combination provides lineage tracking and immutable storage?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Experiments for lineage and Amazon S3 Object Lock for immutability",
      "B": "AWS CloudTrail and AWS Config",
      "C": "AWS Glue Data Catalog and AWS Backup",
      "D": "Amazon Macie and AWS KMS"
    },
    "explanation": "Experiments tracks ML lineage; S3 Object Lock ensures immutable storage of artifacts."
  },
  {
    "id": "c9f345a13939cfb683a95c22b905fc3eecffb798477aaf2ea2eb0f5754be21a5",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "For a use case requiring nightly batch scoring and daily model refresh, which deployment architecture is most cost-effective and operationally simple?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Real-time endpoint always on",
      "B": "Multi-model endpoint with constant instances",
      "C": "Self-hosted EC2 cluster with auto-scaling",
      "D": "Batch Transform for scoring and Pipelines for scheduled retraining"
    },
    "explanation": "Batch Transform supports scheduled batch scoring; Pipelines can orchestrate daily retraining, reducing always-on cost."
  },
  {
    "id": "cff3b003abc61bf1344caf6896cf2c69c8b2c655fb14e9958edc7bb33bfe6faf",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "Which SageMaker feature helps ensure model reproducibility by capturing Docker environment, code, data input location, and hyperparameters automatically?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Clarify",
      "B": "SageMaker Experiments",
      "C": "SageMaker Model Monitor",
      "D": "SageMaker Edge Manager"
    },
    "explanation": "Experiments automatically records environment, inputs, parameters, and outputs to reproduce runs exactly."
  },
  {
    "id": "072e0a19613f5d057138b5836425a1bb5e7c7c25ebbada2e34c79fdeb65e86a7",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A team is evaluating open-source versus custom-trained models for NLP. They need a service that supports both and provides unified deployment. Which AWS service should they choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Comprehend",
      "B": "Amazon Translate",
      "C": "Amazon SageMaker endpoints",
      "D": "Amazon Lex"
    },
    "explanation": "SageMaker endpoints can host custom and prebuilt open-source models for unified deployment."
  },
  {
    "id": "dfc0c5e6081b4d11ec4c249f5c51718ee445f895abeaba4f72b0b2b8a77b921e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "After deploying a computer vision model to an endpoint, the data science team needs to detect bias toward certain image attributes. Which tool and integration should they use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use CloudWatch to track accuracy per attribute",
      "B": "Use Batch Transform and Athena queries",
      "C": "Use QuickSight dashboards on logs",
      "D": "Integrate SageMaker Clarify with endpoint data capture"
    },
    "explanation": "Clarify can analyze captured prediction data from endpoints to assess bias across attributes."
  },
  {
    "id": "f592b0260fd89088328718ab9d2e3802fc58180c1c0c39a32f5beaf554c0f616",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A media company wants to compare training jobs across different compute instance types to optimize cost-performance trade-off. Which SageMaker feature helps automate this comparison?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Automatic Model Tuning with resource search",
      "B": "SageMaker Edge Manager",
      "C": "SageMaker Debugger",
      "D": "SageMaker Neo"
    },
    "explanation": "Automatic Model Tuning can search over hyperparameters and resource types to optimize performance metric."
  },
  {
    "id": "3ba43ad712eaf01487b4ee31bea507d6b564bf2f4bd92c6b4f8426fd6c713260",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A healthcare provider must ensure their model pipeline is compliant and records all configuration changes. Which service combination supports compliance and auditing for both data and model steps?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Model Monitor and GuardDuty",
      "B": "AWS CloudTrail with SageMaker Pipeline logging",
      "C": "AWS Config and Amazon Inspector",
      "D": "Amazon Macie and AWS Key Management Service"
    },
    "explanation": "CloudTrail records API calls for pipelines and configurations, ensuring audit trails; pipelines log each step."
  },
  {
    "id": "efdcc651280e400212aa169509dcba2a4c7f779753f06250ee5cef226086c3da",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "Which training approach allows you to use pre-built AWS container images with custom code, track experiments, and easily transition to deployment?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Training jobs with pre-built containers and Experiments",
      "B": "Use EC2 with Docker installed manually and notebooks",
      "C": "Use AWS Batch",
      "D": "Package code for AWS Lambda training"
    },
    "explanation": "SageMaker Training with AWS images lets you supply entry point scripts, integrates with Experiments, and deploys to endpoints."
  },
  {
    "id": "c481bb9890b7519d2b02048225518b02fe3ad50a73b3cc5507376093789d3b1b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A telecom operator wants near real-time feature updates for churn prediction. Which architecture best supports low-latency feature ingestion and model inference?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Glue ETL feeding S3 to batch transform",
      "B": "Athena queries feeding endpoint",
      "C": "Lambda writing to DynamoDB invoked by batch job",
      "D": "Kinesis Data Streams -> Lambda -> Feature Store online -> SageMaker endpoint"
    },
    "explanation": "Kinesis and Lambda capture streaming data, store in Feature Store\u2019s online store, then low-latency SageMaker endpoint for inference."
  },
  {
    "id": "06c8623ba9ea5b7eb234b4ee37e3278b8ef1c3d9b4cfd26cc727a91bf3e60e90",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "A logistics firm wants automated model retraining whenever model accuracy falls below 85% on live data. Which pattern achieves this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Integrate Model Monitor alerts with EventBridge to trigger Pipelines retraining",
      "B": "Manually check CloudWatch and run Pipeline",
      "C": "Use Lambda on a fixed schedule",
      "D": "Use CloudTrail to capture retraining calls"
    },
    "explanation": "Model Monitor can stream metrics; EventBridge rule triggers retraining Pipeline when accuracy metric crosses threshold."
  },
  {
    "id": "0f645f31de4f9c135eb026c188c2f44d87b1458478be94a12bf8c5bbe936c113",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "During feature engineering, a data scientist wants to quickly prototype SQL-based transformations on S3 data without spinning up servers. Which service should they use?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Glue Studio notebooks",
      "B": "Amazon EMR cluster",
      "C": "Amazon Athena with SageMaker Data Wrangler integration",
      "D": "Amazon Redshift"
    },
    "explanation": "Athena can run serverless SQL on S3; Data Wrangler integrates with Athena queries for rapid prototyping."
  },
  {
    "id": "8d68c01dc460bb0a82490427428ad4c5cd4df92a61891b1aedab61d84b67cb3e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "Which key MLOps principle is demonstrated by using SageMaker Model Registry with approval workflows before production deployment?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Scalability",
      "B": "Governance and reproducibility",
      "C": "Experimentation",
      "D": "Edge deployment"
    },
    "explanation": "Model Registry approval workflows enforce governance and ensure reproducibility and auditability before deployment."
  },
  {
    "id": "92d9fa9051beee2154c9d5687d56062cb102d5fe77b1a7665b024b846af8a483",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "1.3",
    "stem": "Which combination of SageMaker services supports the full ML lifecycle from data collection through monitoring, while minimizing custom infrastructure?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Data Wrangler, Training jobs, Pipelines, Model Monitor",
      "B": "EC2, Lambda, DynamoDB, CloudWatch",
      "C": "Glue, Batch, S3, Athena",
      "D": "EMR, Kinesis, Redshift, QuickSight"
    },
    "explanation": "Data Wrangler handles EDA, Training jobs train models, Pipelines orchestrate, and Model Monitor provides post-deployment monitoring."
  },
  {
    "id": "c4e57ea867b4d6e305f74504f7bc4f88c1c13fa40363537fe1f55644880e5667",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "A data science team is building a document retrieval system using embeddings. They initially split documents into fixed 512-token chunks with no overlap, resulting in poor semantic coherence at chunk boundaries. Which chunking strategy best improves retrieval quality while controlling embedding count?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Keep 512-token fixed chunks and add a 50-token overlap between consecutive chunks.",
      "B": "Use sentence-boundary chunking, grouping up to 512 tokens per chunk without splitting sentences.",
      "C": "Reduce chunk size to 256 tokens with no overlap to avoid splitting concepts.",
      "D": "Increase chunk size to 800 tokens with no overlap to cover full paragraphs."
    },
    "explanation": "Sentence-boundary chunking preserves semantic units and limits embedding count, improving retrieval without unnecessary overlap."
  },
  {
    "id": "b50269a871868b09cc852e37a896f34df06c96175ed02516117c34e36e357ac2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "You need to store embeddings for a multi-language knowledge base. Which metric best captures semantic similarity across languages in a shared vector space?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Euclidean distance, since it measures absolute difference between vectors.",
      "B": "Manhattan distance, because it sums per-dimension differences.",
      "C": "Cosine similarity, as it normalizes for vector magnitude and focuses on orientation.",
      "D": "Hamming distance, due to binary representation of multilingual tokens."
    },
    "explanation": "Cosine similarity normalizes vector lengths and highlights semantic orientation, critical for cross-language embeddings."
  },
  {
    "id": "70d84056489ca9e60909ff1a05a5aa435f82574174d4e831293cb22de76c5644",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "A generative AI model produces repetitive output when asked to summarize varied documents. Which embedding-based retrieval technique can mitigate this by enriching prompts with relevant context?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use larger chunks so more context is in each prompt.",
      "B": "Apply k-means clustering on embeddings to select representative chunks.",
      "C": "Use Euclidean-based nearest neighbor retrieval for variety.",
      "D": "Implement Retrieval Augmented Generation (RAG), retrieving top-k embeddings to include in the prompt."
    },
    "explanation": "RAG enriches the prompt with diverse, relevant context retrieved via embeddings, reducing repetition."
  },
  {
    "id": "986436812cb3340b8010730654e7ca2f875f8fc8f8fb39e066083ccf1a208fae",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "When fine-tuning a foundation model on domain text, you observe that rare domain tokens are tokenized into multiple subwords, harming learning. Which tokenization change addresses this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a custom byte-pair encoding tokenizer including domain vocabulary.",
      "B": "Switch to character-level tokenization to avoid subwords.",
      "C": "Increase context window size so subwords span longer sequences.",
      "D": "Use word-level tokenization to treat each word as a token."
    },
    "explanation": "Custom BPE with domain terms as tokens ensures rare domain words are single tokens, improving model learning."
  },
  {
    "id": "c1f01507cc843d035abc5341c5fc899796661d2516bd23a0bacb9b17894f2785",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "A team is comparing transformer and diffusion models for image generation. They need rapid, deterministic output for UI previews. Which model type is most appropriate?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Transformer-based autoregressive diffusion model for faster, one-shot inference.",
      "B": "Standard diffusion model, despite its iterative denoising steps.",
      "C": "Variational autoencoder, even though it often blurs details.",
      "D": "GAN, because it\u2019s non-iterative but suffers from mode collapse."
    },
    "explanation": "Autoregressive transformer models provide fast, deterministic generation suitable for previews; standard diffusion is iterative."
  },
  {
    "id": "b92030087d3246725d1668d1eae01e5e124cee0dce5ef6d330b349b0a4e8b154",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "During prompt engineering you observe model hallucinations when generating code. Which embedding-based approach helps verify correctness?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature to introduce randomness and explore alternatives.",
      "B": "Use larger context windows to feed entire codebase at once.",
      "C": "Retrieve relevant code snippets via embeddings and include them as examples in the prompt.",
      "D": "Switch to pure instruction-tuned model without retrieval."
    },
    "explanation": "Retrieving similar code via embeddings grounds the model and reduces hallucinations by providing factual examples."
  },
  {
    "id": "b92ce73a60471231342a5789cb05752914e80b5328002aff6a4a4f0d4f23ed92",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "You must index embeddings for a 10 B-token text corpus. Which approximate nearest neighbor index balances query latency and memory footprint?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Brute-force linear scan with GPU acceleration.",
      "B": "KD-Tree index, which degrades in high dimensions.",
      "C": "Exact VP-Tree for guaranteed accuracy.",
      "D": "HNSW (Hierarchical Navigable Small World) graph index for fast, memory-efficient search."
    },
    "explanation": "HNSW offers a good trade-off of performance and resource use for high-dimensional embeddings."
  },
  {
    "id": "72f76aab6e39701181251937f78d20b9fec0dcb7242d29b7593872c629a8bd9b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "In a multi-modal foundation model, how are image pixels and text tokens aligned in the embedding space?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Image CNN features appended to token IDs directly.",
      "B": "Project both image and text through modality-specific encoders into a shared vector space.",
      "C": "Interleave pixel values and token embeddings in a single sequence.",
      "D": "Convert images to text descriptions only and embed as text."
    },
    "explanation": "Modality-specific encoders map different inputs into a unified embedding space for multi-modal reasoning."
  },
  {
    "id": "c0676a8105a904d654dc3f902364cc9e7c4b07dc762b7df00ac1ca0ad00ef7fa",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "A foundation model uses 16-bit floating point embeddings, but memory constraints prompt reducing to 8-bit. What is the primary risk?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increased overfitting due to lower precision.",
      "B": "Longer inference latency from dequantization overhead.",
      "C": "Reduced embedding fidelity, harming retrieval accuracy.",
      "D": "Incompatibility with cosine similarity calculations."
    },
    "explanation": "Quantizing to 8-bit lowers vector precision, which may degrade semantic similarity estimates."
  },
  {
    "id": "bd9d8ee3922a723bba13b0d1ae4edbba96b1b3f5401683cfe9c96e4fc6c9c54f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "Which prompt technique helps a generative model better follow multi-step instructions requiring reasoning?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Single-shot prompting with all instructions in one block.",
      "B": "Zero-shot prompting with no examples.",
      "C": "Chain-of-thought prompting, listing reasoning steps internally.",
      "D": "Few-shot chain-of-thought prompting, demonstrating multi-step reasoning with examples."
    },
    "explanation": "Few-shot chain-of-thought shows the model how to reason step-by-step, improving multi-step task performance."
  },
  {
    "id": "d11335e9a508e36b3316e80c78b07ae5b5fc47e31be2887325fdcf0d93b85320",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "Your diffusion model generates artifacts when denoising high-frequency image regions. Which adjustment reduces this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase the number of diffusion timesteps to smooth transitions.",
      "B": "Reduce batch size to focus on each image.",
      "C": "Use a higher learning rate during fine-tuning.",
      "D": "Switch to a smaller context window."
    },
    "explanation": "More timesteps allow finer-grained denoising, reducing artifacts in detailed areas."
  },
  {
    "id": "dc476865d0ba492183139695b2f6b3a2c64acbfe08d5e40d3f7492ade17ee179",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "A team wants to pre-train a transformer model on text and images jointly. Which architecture component is essential for consistent position tracking?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Separate attention heads for each modality without sharing.",
      "B": "Shared positional embeddings so both text tokens and image patches have coherent position encoding.",
      "C": "Sinusoidal positional encodings only for text.",
      "D": "Relative positional encodings only for images."
    },
    "explanation": "Shared positional embeddings ensure the model understands spatial and sequential relationships across modalities."
  },
  {
    "id": "ceae7b5334f783c11f3898f361f95ce334456fa15f334b662f44208d63aadbb4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "During foundation model selection for a customer Q&A chatbot, which factor is least critical?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model\u2019s tokenization scheme aligns with domain vocabulary.",
      "B": "Embedding dimension matches existing vector store.",
      "C": "Inference latency meets SLA.",
      "D": "Model\u2019s pre-training data license region."
    },
    "explanation": "While licensing matters generally, regional license restrictions typically don\u2019t affect inference suitability directly."
  },
  {
    "id": "8639e9f922d6ddf7492b22585f0bd5e143eb315bc5d902dde8e6156578976d0a",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "When using embeddings for semantic search, why might you prefer dense vector indexes over inverted term indexes?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Dense indexes capture semantics beyond exact keyword matches.",
      "B": "Inverted indexes are faster at high-dimensional similarity search.",
      "C": "Dense indexes require exact token overlap.",
      "D": "Inverted indexes scale linearly with vector dimension."
    },
    "explanation": "Dense vector indexes use embeddings to capture meaning similarity rather than exact term matching."
  },
  {
    "id": "823a41c1345b9dfce3be7988523738da91b457f5ff4d4425cf263bfab71c5c62",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "A model fine-tuned with instruction prompts fails on unseen instructions. What tokenizer issue could cause this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Tokenizer uses byte-level encoding, splitting known words.",
      "B": "Tokenizer vocabulary size is too large.",
      "C": "New instruction tokens map to unknown embeddings causing OOV splitting.",
      "D": "Tokenizer adds unwanted BOS/EOS tokens only."
    },
    "explanation": "Unknown instruction tokens are split into multiple subwords, hurting the model\u2019s understanding of the new instructions."
  },
  {
    "id": "7e6eafe220f66baadd9dad711e6d9d3e7d2e8ad8eab5fd42233888e349829be1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "For long-form text generation, which transformer attention mechanism reduces quadratic complexity?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Full self-attention with learned masks.",
      "B": "Sparse attention, attending only to local and selected global tokens.",
      "C": "Relative positional attention only.",
      "D": "Cross-attention without self-attention."
    },
    "explanation": "Sparse attention limits token interactions, reducing complexity while preserving key global context."
  },
  {
    "id": "09b3438526dd584c9b2ddb856298856779d129cde54685f72aff07cd1e6eeb81",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "In a foundation model lifecycle, which phase directly addresses embedding drift over time?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Pre-training from scratch on general corpus.",
      "B": "Initial fine-tuning on task data.",
      "C": "Continuous pre-training or periodic re-embedding with fresh data.",
      "D": "One-time batch embedding generation."
    },
    "explanation": "Continuous pre-training or re-embedding on new data counteracts embedding drift as data evolves."
  },
  {
    "id": "1e9d59e3f5f6f3ade46e3352c225460f8d669c60a4f4a6f3c56eccec2fb95d80",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "A generative audio model uses diffusion. To speed up inference while preserving fidelity, which strategy helps?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use knowledge distillation to train a smaller model replicating diffusion outputs.",
      "B": "Increase diffusion timesteps and lower scheduling rate.",
      "C": "Use pure autoregressive sampling instead of diffusion.",
      "D": "Reduce model depth without any distillation."
    },
    "explanation": "Distillation transfers knowledge to a smaller network that approximates diffusion outputs faster."
  },
  {
    "id": "1be437b060e7503b0fd4e28fd5eef7b31fd604ae8cd0e3f220675295fbfc830c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "Which foundational concept differentiates transformer embeddings from RNN hidden states?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Transformers use convolution over tokens instead of recurrence.",
      "B": "Transformers have no positional encodings.",
      "C": "RNN states are parallelizable, transformers aren\u2019t.",
      "D": "Transformer embeddings derive from global self-attention, not sequential hidden states."
    },
    "explanation": "Self-attention lets transformers compute embeddings considering all tokens simultaneously, unlike sequential RNN states."
  },
  {
    "id": "72da00f5e681debe4dd1c14556833a9715bbac39393bf8aef93a4240cda64eac",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "Your diffusion-based image model hallucinates text in generated images. Which training adjustment reduces this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Reduce diffusion timesteps for coarser denoising.",
      "B": "Include more varied real images without text overlays in pre-training data.",
      "C": "Increase learning rate during fine-tuning on textless images.",
      "D": "Switch to autoregressive transformer model."
    },
    "explanation": "Excluding text-overlaid images during pre-training prevents the model from learning to generate unwanted text."
  },
  {
    "id": "7b8a826836ca8bde5523bf5fa830eebdc441a5979729eeae55ed31d0e5c3ff90",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "In embedding-based clustering, which technique improves grouping of semantically similar documents?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Use raw TF-IDF vectors instead of embeddings.",
      "B": "Apply PCA retaining only the top principal component.",
      "C": "Normalize embeddings to unit length before clustering.",
      "D": "Cluster on unnormalized Euclidean distances."
    },
    "explanation": "Unit-normalizing embeddings ensures clustering uses angular relationships, reflecting semantic similarity."
  },
  {
    "id": "91a83b2a187d96a8bb08fee5a57a55e9e4d14423a4c9fa41ac3ff6cd4e9921cb",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "A text-to-image pipeline uses CLIP embeddings for retrieval. Which aspect of CLIP embeddings is most critical for this task?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CLIP\u2019s large vocabulary size.",
      "B": "Joint text-image embedding alignment for cross-modal similarity.",
      "C": "CLIP\u2019s text generation capability.",
      "D": "Its autoregressive decoding mechanism."
    },
    "explanation": "CLIP embeddings are designed so text and image reside in the same vector space, enabling retrieval."
  },
  {
    "id": "af56bfc70065b3011a470b4e9ff77b27390a535b97030bd6ca43c32d82db8b26",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "To compress large embedding indexes for cost efficiency, which quantization method trades off minimal accuracy loss?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Product quantization splitting vectors into subspaces.",
      "B": "Binarization of entire embedding to 1 bit.",
      "C": "Scalar quantization to nearest integer.",
      "D": "Full 8-bit uniform quantization without subspace partitioning."
    },
    "explanation": "Product quantization reduces storage with limited accuracy loss by vector subspace quantization."
  },
  {
    "id": "4322ff7c3752ffb6217a7843e8dbb00c7bff4865ae74a7b277af7144a726e33e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "Your foundation model uses sinusoidal positional encodings but underperforms on very long sequences. Which encoding modification addresses this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase the frequency of sinusoidal embeddings.",
      "B": "Remove positional encodings altogether.",
      "C": "Switch to rotary positional embeddings supporting extrapolation.",
      "D": "Use absolute position embeddings learned only during pre-training."
    },
    "explanation": "Rotary embeddings generalize better to unseen sequence lengths, improving long-sequence performance."
  },
  {
    "id": "40bf35f7277b544240196985f802fb3646503c855494426aca856afae867808b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "When designing prompts for image generation with diffusion models, what does a negative prompt typically achieve?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Specifies the desired style explicitly.",
      "B": "Increases sampling steps for clarity.",
      "C": "Embeds an example image in the prompt.",
      "D": "Tells the model which elements to avoid generating."
    },
    "explanation": "Negative prompts instruct diffusion models on concepts or artifacts to omit during generation."
  },
  {
    "id": "3e38f3ab3a2d6fed858fe04ad780f9a8d5de723e256a4096ade0ee4c207a9e5c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "In a retrieval-augmented LLM application, you notice redundant context in multiple retrieved passages. Which embedding approach reduces redundancy?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase embedding dimension to separate topics.",
      "B": "Use Maximal Marginal Relevance to re-rank and diversify retrieved embeddings.",
      "C": "Switch to Euclidean similarity to penalize similar vectors.",
      "D": "Apply k-means clustering on the prompt itself."
    },
    "explanation": "MMR re-ranks retrieval to maximize relevance while reducing redundancy among selected passages."
  },
  {
    "id": "2d91178faf56e9a3a1e6584afb34e942012f1028df95dc9b5cf053babad6b1d7",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "Which tokenization approach best handles morphologically rich languages in foundation models?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SentencePiece byte-pair encoding capturing subword units.",
      "B": "Word-level tokenization splitting on whitespace.",
      "C": "Character-level tokenization ignoring subwords.",
      "D": "Rule-based stemming before tokenization."
    },
    "explanation": "Byte-pair encoding adapts to morphological variation by learning subword units reflecting language structure."
  },
  {
    "id": "c9003c35662425abab603a57f401a6b6c425bc6fb4a3b29d9fcc697f146cf2e1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "During fine-tuning, you observe overfitting on a small domain dataset. Which embedding-related step can mitigate this?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Reduce embedding dimension to lower model capacity.",
      "B": "Switch to a larger pre-trained foundation model.",
      "C": "Freeze lower transformer layers including initial embedding layer.",
      "D": "Increase learning rate for the embedding layer."
    },
    "explanation": "Freezing early layers prevents overfitting by limiting parameter updates to higher-level layers."
  },
  {
    "id": "c9771ac03bbe0f690cc361d5e7a9c62adf6fb113c691ba360a6ba16307bfc837",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "Your multi-modal foundation model\u2019s cross-modal retrieval suffers from hubness (few vectors nearest to many). Which technique alleviates hubness?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Decrease embedding dimension to 128.",
      "B": "Normalize only image embeddings, not text.",
      "C": "Use Euclidean distance instead of cosine.",
      "D": "Apply Local Scaling to adjust neighbor distances by local density."
    },
    "explanation": "Local Scaling adjusts similarity based on neighborhood density and reduces hubness."
  },
  {
    "id": "6e4a453809b76943614d65c857da2bf9c48efdbb85f570fdfc54fb631c15a1e2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.1",
    "stem": "When deploying a diffusion model behind an API, prompt sizes vary. How can chunking token sequences for the denoiser improve throughput?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use fixed 1024-token chunks regardless of prompt length.",
      "B": "Batch similar token lengths together to minimize padding and GPU waste.",
      "C": "Always pad to the max context length to standardize batches.",
      "D": "Split on sentence boundaries without re-batching."
    },
    "explanation": "Batching similar lengths reduces padding overhead, improving GPU utilization and throughput."
  },
  {
    "id": "898f4a1caf1c20169fb921e2dd8745f225596b2edb1f0371e4a64ff66698bd7f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A financial services firm plans to deploy a generative AI assistant to draft client communications that must be consistent and fully auditable. Which decoding strategy and parameter setting best ensures deterministic outputs for audit trails?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use top-p sampling with p=0.9",
      "B": "Apply temperature=0.7 with nucleus sampling",
      "C": "Use greedy decoding or temperature=0.0",
      "D": "Use beam search with high diversity penalty"
    },
    "explanation": "Greedy decoding (or temperature=0.0) produces deterministic outputs, critical for auditability. Sampling methods introduce nondeterminism."
  },
  {
    "id": "6f3d8f2ba9d4d12192d2de9d495af561101533c90e019b66f0187ded53910674",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A startup needs to generate personalized product descriptions at high throughput while controlling per-token costs. Which pricing model and AWS service offers the best trade-off?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon SageMaker endpoint using large custom model charged by instance-hour",
      "B": "Amazon Bedrock with token-based Foundation Model API",
      "C": "SageMaker JumpStart small model with on-demand EC2 billing",
      "D": "Amazon Lex usage-based utterance pricing"
    },
    "explanation": "Bedrock token-based billing aligns cost to usage and supports high throughput, minimizing idle instance costs."
  },
  {
    "id": "8e160151a81f1c1de529ef772bd372f66447f8c7379727d9d502592fa79713dd",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A media company uses a foundation model for video caption generation but sees occasional hallucinated events. Which mitigation approach most directly reduces hallucinations?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature to encourage diversity",
      "B": "Use a larger foundation model",
      "C": "Switch to zero-shot prompting",
      "D": "Implement retrieval-augmented generation with vetted transcript store"
    },
    "explanation": "Retrieval-augmented generation grounds outputs in verified transcripts, reducing hallucination."
  },
  {
    "id": "c74a0502805dae8603c9c6ad32b0e859a596a2d6a15bd49a3c8e1c814d369003",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "An e-commerce platform measures conversion lift after integrating generative recommendations. Which metric best isolates model impact on average order value (AOV)?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Relative increase in AOV of users exposed vs. control group",
      "B": "Total number of generated recommendations",
      "C": "Model perplexity on recommendation prompts",
      "D": "Percentage of successful API calls"
    },
    "explanation": "Comparing AOV between exposed and control isolates the model\u2019s impact on order value."
  },
  {
    "id": "dd0ddf066fbfa2a8aba9f80162f3096fbde567c4e92584665fd38a4c18e5c129",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A legal tech firm must ensure generated contract clauses are legally accurate. Which foundation model setting reduces factual inaccuracy?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use higher temperature",
      "B": "Use low temperature with domain-specific fine-tuning",
      "C": "Use larger context window without fine-tuning",
      "D": "Use few-shot prompting with random examples"
    },
    "explanation": "Fine-tuning on domain-specific legal text plus low temperature reduces inaccuracy."
  },
  {
    "id": "6b8b4f77136b4207f051edb77ef1d51d6d331dc2b3be73ac00751e4c0b78a79c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A healthcare chatbot must comply with privacy regulations and avoid hallucinations. Which deployment choice best balances compliance and performance?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Public foundation model hosted on AWS GovCloud",
      "B": "High-capacity public API with encryption in transit",
      "C": "Bedrock API with multi-tenant model",
      "D": "Deploy a private Bedrock foundation model in a VPC endpoint"
    },
    "explanation": "Private Bedrock in VPC ensures data residency, encryption, and reduces risk of data leakage."
  },
  {
    "id": "c31a5306ca036ad3e89cd1f4604840019447fbf98a4187a6bc1713fe11ecd453",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A retailer notices that their generative AI product descriptions sometimes omit critical warranty details. What\u2019s the most effective prompt engineering approach to address this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase model temperature for creativity",
      "B": "Switch to a larger foundation model",
      "C": "Add explicit instruction and examples highlighting warranty details (few-shot)",
      "D": "Use an external summarization service"
    },
    "explanation": "Few-shot prompting with explicit examples ensures the model includes required details."
  },
  {
    "id": "1733934664e6d97a118f9d3b5b548b7838f3e9e106e2012c2dde4e385fb4e2fd",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A SaaS provider wants to measure API latency cost trade-offs for real-time generative responses. Which strategy yields lowest average latency per dollar?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Scale up to a larger GPU instance in SageMaker",
      "B": "Select a smaller Bedrock foundation model with sufficient quality",
      "C": "Use high-throughput batching on a large SageMaker endpoint",
      "D": "Implement multi-region replication of a large model"
    },
    "explanation": "Smaller Bedrock models reduce latency and cost per inference, balancing quality and performance."
  },
  {
    "id": "e57b986bd5c45dace3aaef8ecc27dcba14ecb257e93e2b48bd8a33b935e9182b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A marketing team must generate social media posts quickly but with brand consistency. Which generative AI advantage addresses this need?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model interpretability",
      "B": "Cross-domain performance",
      "C": "Non-determinism",
      "D": "Adaptability and responsiveness via prompt templates"
    },
    "explanation": "Prompt templates leverage adaptability/responsiveness to rapidly produce consistent brand voice."
  },
  {
    "id": "fbdbd1f1b57d358311a7596bbc18fe48b6ae359e4ba8704d10410ba47cc97885",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A data scientist is comparing performance of two foundation models on cross-domain tasks. Which metric best captures relative performance?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Average F1 score across domain-specific benchmark datasets",
      "B": "Total number of parameters",
      "C": "Training compute FLOPs",
      "D": "Context window size"
    },
    "explanation": "F1 across multiple benchmarks measures cross-domain performance, capturing precision and recall."
  },
  {
    "id": "309b0e2c77741b394eef80eaa3ff2ac6ce35575275d34a29c93e967a596908c3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A business requires the simplest possible integration of generative AI without managing infrastructure or model customization. Which AWS offering meets this?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon SageMaker custom endpoint",
      "B": "Amazon Bedrock managed API",
      "C": "SageMaker JumpStart custom container",
      "D": "Self-hosted open-source model on EC2"
    },
    "explanation": "Bedrock managed API abstracts infrastructure and customization, offering simplicity."
  },
  {
    "id": "1488c8f1b9f4ba9af9e2931448b5c7e3d38fb1bf46ecaf8bdf354da3feb2cac2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A global company must comply with data residency laws when generating customer insights. Which approach ensures compliance?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use multi-region Bedrock endpoints without restrictions",
      "B": "Encrypt data in transit only",
      "C": "Deploy region-specific Bedrock foundation models with VPC lockdown",
      "D": "Use SageMaker JumpStart across regions"
    },
    "explanation": "Region-specific deployment with VPC lockdown ensures data stays within required jurisdictions."
  },
  {
    "id": "de3a5e68ffa21fbaa4147e6fb8a1626147b961567609254ca9f3b9689aabe410",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A team measures ARPU uplift after integrating generative chat. Which analysis isolates generative AI\u2019s contribution to ARPU?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A/B test difference in ARPU between chat-enabled and baseline cohorts",
      "B": "Total API token usage",
      "C": "Average session length",
      "D": "Number of chat messages generated"
    },
    "explanation": "A/B testing ARPU directly measures generative chat\u2019s impact on revenue per user."
  },
  {
    "id": "5153c6efdf4803120e6950c74319241804b39a37a8913ee7d9bb55d846e6d381",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "An enterprise struggles with hallucinations in summary generation of compliance documents. Which model choice most reduces hallucinations?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase prompt context length",
      "B": "Use zero-shot summarization on large model",
      "C": "Switch to text-diffusion model",
      "D": "Implement RAG over a curated compliance document store"
    },
    "explanation": "RAG uses verified source documents, preventing unsupported hallucinated content."
  },
  {
    "id": "c2602cc76a3c71d281c0304b69258f7b26bb315765662d71fff8a817a1b42076",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A company must choose between fine-tuning a foundation model or employing few-shot prompting. They have limited labeled data (<100 examples). Which approach is preferable?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Fine-tune model with limited examples",
      "B": "Use few-shot in-context learning via prompt templates",
      "C": "Pre-train model on unrelated large dataset",
      "D": "Deploy zero-shot without examples"
    },
    "explanation": "Few-shot prompting is more effective than fine-tuning when labeled data is scarce."
  },
  {
    "id": "7c1b8756156003fc149ba00db1f4a9722db1a441e6f2956e5df2a8dd90a4b934",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A manufacturer wants to generate maintenance logs from sensor data summaries. They require high explainability for audits. Which model attribute should they prioritize?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Large multimodal diffusion model",
      "B": "High temperature setting",
      "C": "Transparent architecture with model card and provenance tracking",
      "D": "Black-box large LLM with accuracy metrics"
    },
    "explanation": "Transparent model with documented provenance supports explainability required for audits."
  },
  {
    "id": "02948c96cf7a03fa3b1d399fdcbc893f160f2a209e58ea5409f1065740a0bedd",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A retailer must generate dynamic promotional images. They care most about generation speed over fine detail. Which generative model type is best?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Transformer-based text-to-image",
      "B": "Diffusion model optimized for low sampling steps (e.g., high-speed pipeline)",
      "C": "GAN with extensive fine-tuning",
      "D": "VAE-based high-fidelity model"
    },
    "explanation": "Diffusion models with fewer sampling steps trade quality for speed, aligning with requirements."
  },
  {
    "id": "c08df5f82faeea7813cacecba4ab777341e5b972849d1da3ec99dc51d46f57a8",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A gaming company observes inconsistent NPC dialogues. They need more coherent responses. Which prompt technique improves coherence?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase max tokens limit",
      "B": "Chain-of-thought prompting with structured steps",
      "C": "Use random negative prompts",
      "D": "Switch to unsupervised learning"
    },
    "explanation": "Chain-of-thought prompting guides the model through structured reasoning, improving coherence."
  },
  {
    "id": "371f3d242d919c85dc74d0eb00939944bb6e95754c2f3d723b59877fcccd3605",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "An organization tracking customer lifetime value wants generative personalization. Which performance metric best evaluates business impact?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model perplexity on user prompts",
      "B": "Token generation latency",
      "C": "Number of personalized messages",
      "D": "Incremental CLV uplift compared to control"
    },
    "explanation": "Incremental CLV uplift directly measures personalization\u2019s effect on long-term customer value."
  },
  {
    "id": "adfb7f0936021212301da875c8398689e266c12d464cacc7f74d1554347dbfa6",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A compliance team must prevent inadvertent exposure of regulated content in generative outputs. Which technique best addresses this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Implement guardrails with Bedrock safety modules",
      "B": "Increase model temperature",
      "C": "Use larger context window",
      "D": "Switch to zero-shot prompting"
    },
    "explanation": "Bedrock guardrails allow policy-based filters to block sensitive content, ensuring compliance."
  },
  {
    "id": "f74a78797cb781d71a2f928137e640114f34b5da8702de407913149715652ae2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A financial application requires consistent terminology across generated reports. Which model setting prevents terminology drift?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable stochastic beam search",
      "B": "Apply high top-k sampling",
      "C": "Use low temperature with glossary injection in prompt",
      "D": "Increase response length"
    },
    "explanation": "Low temperature reduces randomness and injecting a glossary anchors terminology."
  },
  {
    "id": "1f3a4b0bdd3e6cc0d541e9c656c282d196d85700de1391c203851442013c586b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "An AI team compares API cost effectiveness for low-volume high-value document generation. Which AWS billing dimension should they optimize?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Instance-hour in SageMaker",
      "B": "Token count in Bedrock",
      "C": "Number of API calls in Lex",
      "D": "Storage bytes in S3"
    },
    "explanation": "Token count billing in Bedrock maps directly to document length and value, optimizing cost."
  },
  {
    "id": "cc2b9d0f3e88f6115df9392da6e789f59ccc10bbfd495c13b5e0fc948d5a094e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A pharma company is concerned about model interpretability for clinical summaries. Which limitation of foundation models is most relevant?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Token-based pricing unpredictability",
      "B": "High throughput latency",
      "C": "Multimodal data handling",
      "D": "Black-box nature limiting transparency"
    },
    "explanation": "Foundation models often lack interpretability, a key concern in regulated clinical contexts."
  },
  {
    "id": "c35a3ff3483a0e6149b98a3b4b0e6c2f5844ab03933452d59bce5f44e3f2358e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A customer wants to balance creativity and factual accuracy in a marketing copy generator. Which temperature setting is optimal?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Moderate temperature (~0.5)",
      "B": "High temperature (~1.0)",
      "C": "Zero temperature",
      "D": "Top-p=0.2 only"
    },
    "explanation": "A moderate temperature balances creativity and control, mitigating hallucinations while allowing variation."
  },
  {
    "id": "6710b2b7a0f60a3be626b0e77443a5305afb5502b7e7bca08e6bb9d3ec98430b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A global brand uses generative AI translations. They need consistent style across languages. Which foundation model feature supports this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Larger parameter count",
      "B": "Zero-shot capabilities",
      "C": "Multilingual fine-tuning on brand corpus",
      "D": "Higher decoding temperature"
    },
    "explanation": "Fine-tuning on a multilingual brand corpus ensures consistent style across languages."
  },
  {
    "id": "7b65ddcae16aac6fde2371c897acef5fb7739d472fb6042d662d7830291c910f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A customer embedding service sees decline in similarity accuracy across evolving knowledge base. Which approach restores cross-domain performance?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase embedding vector size",
      "B": "Recompute embeddings with incremental index using RAG pipeline",
      "C": "Switch to random sampling metrics",
      "D": "Apply high temperature during encoding"
    },
    "explanation": "Recomputing embeddings and using RAG ensures embeddings reflect updated knowledge, restoring accuracy."
  },
  {
    "id": "6914de31d2c7eacfbc7f6df587bbf5483989cba1ed153eaa462679e9fed8938e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A brand wants to minimize latency for chat responses during peak hours. Which scaling strategy is most cost-effective?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provision large SageMaker endpoint 24/7",
      "B": "Increase VPC throughput",
      "C": "Use Bedrock serverless with auto-scaling",
      "D": "Deploy on single EC2 spot instance"
    },
    "explanation": "Bedrock serverless auto-scales with demand, minimizing idle cost and ensuring low latency."
  },
  {
    "id": "2dd53f3e6a6814661886aa00b92365a89c03494b7c7b5d6e3c916a04e34da935",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "An organization tracks generative AI performance across domains. Which composite metric provides holistic evaluation?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Weighted score combining BLEU, ROUGE, and domain GDP impact",
      "B": "Average token count per response",
      "C": "Only perplexity on validation set",
      "D": "Percentage of API errors"
    },
    "explanation": "A composite metric blending NLP scores and business impact yields holistic performance evaluation."
  },
  {
    "id": "5a1e81c7a428d79563f9cf83594b22ea2381bd3d84cca6a75578edad879313ee",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A consulting firm must enforce data governance in generative pipelines. Which AWS feature helps trace data lineage in Bedrock?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Model Monitor",
      "B": "AWS Config rules",
      "C": "AWS Macie data classification",
      "D": "Bedrock model cards with provenance metadata"
    },
    "explanation": "Model cards in Bedrock include provenance metadata and lineage, supporting governance."
  },
  {
    "id": "be422b4a7bdfeb4b335d759e43e4898e8e352fc40a1c0654bcc14e0171c98333",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.2",
    "stem": "A team needs to estimate throughput cost for batch image generation via diffusion models. Which factor most affects cost estimate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Number of prompt parameters",
      "B": "Number of sampling steps per image",
      "C": "Vocabulary size",
      "D": "Context window length"
    },
    "explanation": "Sampling steps determine computation per image, directly impacting cost in diffusion pipelines."
  },
  {
    "id": "0fc4b9ad832a599edd848f2da244bcec9a3cf9ba786113c3e9cdcaea022f2122",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A regulated healthcare provider needs to deploy a generative AI service that ensures all inference logs and API calls are captured in AWS CloudTrail for audit. They must minimize operational overhead by not managing servers. Which AWS service meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon SageMaker JumpStart endpoint behind AWS API Gateway",
      "B": "Amazon Bedrock",
      "C": "Amazon Q",
      "D": "Amazon Comprehend"
    },
    "explanation": "Bedrock is a fully managed service with built-in CloudTrail logging for all API calls and no server management. SageMaker JumpStart requires you to manage endpoints. Amazon Q is for document Q&A but less mature for full audit and isn\u2019t explicitly CloudTrail-integrated. Comprehend isn\u2019t a foundation model service."
  },
  {
    "id": "27b4d0406e4c49e633719be69e20d859797ce930ba76f92f90db1ceafc5f5ba4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A startup wants to generate marketing copy using a pre-trained LLM. They anticipate low monthly usage but occasional spikes. They need to minimize costs when idle. Which AWS offering is most cost-effective?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Bedrock token-based pricing",
      "B": "SageMaker JumpStart with on-demand EC2 instances",
      "C": "Self-hosted LLM on an always-on EC2 cluster",
      "D": "Amazon Q provisioned capacity"
    },
    "explanation": "Bedrock\u2019s token-based pricing means you pay only when you invoke the model, minimizing idle costs. JumpStart incurs instance-hour charges whenever the endpoint is active, even if idle. Self-hosting means always-on cluster costs. Amazon Q\u2019s provisioned capacity must be paid for even when idle."
  },
  {
    "id": "a6676f7ff12ea3c2ffb263932ac3dd4b88e54225151a05df7f1ca7be223df25f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A telecom company needs a generative AI service within their VPC for legal compliance. They want to fine-tune a foundation model with internal data without exposing data outside the VPC. Which AWS service and deployment pattern should they choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock public endpoint with KMS encryption",
      "B": "Amazon Q with private link",
      "C": "SageMaker JumpStart in VPC with private subnets",
      "D": "Self-managed open-source model in EKS"
    },
    "explanation": "SageMaker JumpStart can be deployed in your VPC private subnets, keeping data in-VPC and isolating model training. Bedrock endpoints are managed outside your VPC. Amazon Q does not support private VPC deployments. Self-managing requires substantial ops overhead."
  },
  {
    "id": "281d3057a06393be2652b97b591416e048e8e3f1058cfe93e569c7f566b773b1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A media company wants to generate realistic images using a diffusion foundation model but lacks GPU expertise. They also want auto-scaling for unpredictable workloads. Which AWS feature combination should they use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run Stable Diffusion on EC2 GPU instances with Auto Scaling",
      "B": "Deploy Stable Diffusion in EKS with spot instances",
      "C": "Use SageMaker JumpStart customized Stable Diffusion endpoint",
      "D": "Use Amazon Bedrock\u2019s Stable Diffusion model with built-in autoscaling"
    },
    "explanation": "Bedrock offers pre-trained Stable Diffusion with managed autoscaling and no GPU management. JumpStart requires you to set up instance fleets and autoscaling policies. EC2/EKS requires manual GPU and scaling management."
  },
  {
    "id": "4c4ece0f0cdc3d1449355c8b22e0b21db8159afc04bc7021ea17c618236c5c27",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A legal firm needs an AI assistant that uses private documents stored in S3. They require retrieval-augmented generation. Which AWS service integration best supports RAG out-of-the-box?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Q with Kendra indexing",
      "B": "Amazon Bedrock with AWS Kendra retrieval plugin",
      "C": "SageMaker JumpStart with DocumentDB",
      "D": "AWS Comprehend with ElasticSearch"
    },
    "explanation": "Bedrock integrates natively with Kendra for retrieval-augmented generation. Amazon Q focuses on QA but requires separate retrieval setup. SageMaker JumpStart and Comprehend need custom orchestration for RAG."
  },
  {
    "id": "e0423a2291a93d432ca333993ea246c88e8e997a718c22ae3ffaa3477ee30b70",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "An enterprise wants to serve low-latency multi-turn chat responses to millions of users. They need a high-throughput conversational foundation model. Which AWS service and instance type should they choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock with provisioned capacity",
      "B": "SageMaker JumpStart on g4dn.xlarge",
      "C": "Amazon Q with on-demand throughput",
      "D": "Amazon Comprehend with batch processing"
    },
    "explanation": "Bedrock provisioned capacity delivers guaranteed throughput and low latency at scale. JumpStart g4dn.xlarge may not scale to millions or guarantee throughput. Amazon Q is for enterprise Q&A with lower throughput SLAs. Comprehend isn\u2019t conversational."
  },
  {
    "id": "1861d38c5011a3f24775618d808162533ccdc83201dcca7636c7204ed9a0a3a6",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A gaming company wants to prototype code generation for game logic quickly and with minimal configuration. They do not need production-grade SLAs. Which AWS technology is best to start?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Bedrock with RLHF customization",
      "B": "SageMaker JumpStart custom training",
      "C": "Amazon CodeWhisperer preview through Amazon Bedrock",
      "D": "Amazon Q code generation feature"
    },
    "explanation": "CodeWhisperer is accessible through Bedrock with minimal setup for code generation. It is ideal for prototyping. SageMaker JumpStart requires endpoint setup. Bedrock RLHF needs customization. Amazon Q doesn\u2019t focus on code."
  },
  {
    "id": "0ff02776027c48ed5e651b26ca7b0f72263636c30b2a75f1c5895ec7928262f6",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A retailer must ensure data sovereignty by keeping inference data within the EU. They need a managed generative service compliant with GDPR and EU data residency. Which AWS service configuration meets this need?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock in us-east-1 with EU region replication",
      "B": "Amazon Bedrock in eu-west-1 with customer-managed KMS",
      "C": "SageMaker JumpStart in us-west-2 with VPC endpoints",
      "D": "Amazon Q global endpoint with VPC private link"
    },
    "explanation": "Bedrock deployed in eu-west-1 ensures data residency in the EU. Coupled with a customer-managed KMS key in the same region, it meets GDPR. Replication from us-east-1 violates sovereignty. JumpStart in us-west-2 is outside EU. Amazon Q global endpoint may process data outside the EU."
  },
  {
    "id": "99966f9597891776855c830232f85062608c42fc240ce291d26236042dff742e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A startup evaluating AWS generative AI options wants to minimize time-to-market and avoid model fine-tuning. They also require SLA-backed availability. Which AWS service should they select?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock",
      "B": "SageMaker JumpStart zero-shot models",
      "C": "Self-hosted LLaMA on EC2",
      "D": "Amazon Q with custom indexing"
    },
    "explanation": "Bedrock offers fully managed foundation models, zero configuration, and SLA-backed availability. JumpStart endpoints have no formal SLAs. Self-hosting and Q both require additional maintenance or retrieval setups."
  },
  {
    "id": "776573f55b35c32969da2ecefbc042415095db405455f03a8c4948ec5d8c9db8",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A firm must process sensitive legal text with a foundation model and ensure encryption in transit and at rest, plus private networking. They prefer pay-per-use. Which AWS solution fits all criteria?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock public endpoint with TLS",
      "B": "SageMaker JumpStart on spot instances without VPC",
      "C": "SageMaker JumpStart endpoint in VPC with KMS encryption",
      "D": "Amazon Q via internet with customer key"
    },
    "explanation": "SageMaker JumpStart in a VPC with customer-managed KMS ensures private networking and encryption at rest and transit. Bedrock endpoints are public and you cannot deploy them in VPC. Amazon Q does not support private VPC deployments."
  },
  {
    "id": "2a4b574b60d899d504003d59896830b3c59ac24c6fc7c95660cea4157d5d9577",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A media startup expects to generate thousands of images per hour and needs predictable monthly billing. Which AWS generative AI service and pricing model should they choose?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock token-based pricing",
      "B": "Amazon Q request-based billing",
      "C": "SageMaker JumpStart on-demand instances",
      "D": "SageMaker JumpStart on reserved GPU instances"
    },
    "explanation": "Reserving GPU instances for SageMaker JumpStart gives predictable instance-hour billing for high-volume image generation. Bedrock\u2019s token-based pricing can vary unpredictably. Amazon Q\u2019s billing is also usage-based. JumpStart on-demand has variable costs."
  },
  {
    "id": "fc41df951bf99dcd0ad2c2eaaadcbc51ee92169f404e7f5d8fccc0a298d2b756",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A logistics company wants to experiment with multiple foundation models (text, image, code) rapidly without provisioning infrastructure. They also need to manage access via IAM. Which AWS service meets these criteria?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon SageMaker Ground Truth",
      "B": "Amazon Bedrock with IAM policies",
      "C": "SageMaker JumpStart with KMS roles",
      "D": "Amazon Q with Cognito authentication"
    },
    "explanation": "Bedrock offers multiple model modalities through a single console/API, IAM integration for access control, and no infra provisioning. JumpStart focuses on SageMaker models only. Ground Truth is data labeling. Amazon Q is limited to QA."
  },
  {
    "id": "35e34feadda3a3456dfda79f33d43a2f3e9d8a347bb995b2985e01d8b560cfed",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "An organization needs to generate long-form text (>5,000 tokens) reliably. Which service supports extended output lengths natively?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock with a model supporting long context windows",
      "B": "Amazon Q with streaming pagination",
      "C": "SageMaker JumpStart with multi-chunk stitching",
      "D": "Amazon Comprehend for long documents"
    },
    "explanation": "Bedrock offers foundation models with extended context windows for single-stream long outputs. Q is for QA, not generative long-form. JumpStart requires stitching and manual orchestration. Comprehend is analytics, not generation."
  },
  {
    "id": "ef9f94a654c93eb62d22b6585449f4f953e4a0d348d9825fe2d5df8fb6a4f6bd",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A research team needs to prototype embedding generation and storage with minimal setup. They want an AWS service that handles embeddings and index management. Which service is most appropriate?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Q",
      "B": "Amazon Comprehend",
      "C": "Amazon Bedrock embeddings with Kendra integration",
      "D": "SageMaker JumpStart with DocumentDB"
    },
    "explanation": "Bedrock provides an embeddings API and can integrate with Kendra for index management. Amazon Q and Comprehend don\u2019t offer native embedding/integration. JumpStart requires you to set up database and indexing."
  },
  {
    "id": "518b730c0bee9469a88f33056ee6c587e202591539708208b1e6c6b2d8b76c1c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A global enterprise requires a generative AI solution that complies with SOC 2 and ISO 27001 without custom certification. Which AWS service provides this out-of-the-box?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock",
      "B": "SageMaker JumpStart",
      "C": "Self-hosted Hugging Face on EC2",
      "D": "Amazon Q"
    },
    "explanation": "Bedrock is SOC 2 and ISO 27001 compliant by AWS\u2019s managed service. JumpStart endpoints inherit SageMaker compliance but require configuration. Self-hosting requires you to certify. Amazon Q\u2019s compliance coverage is narrower."
  },
  {
    "id": "afc07aabc4b0f5cdf5203cab663ee388b71f7f7a7bb627e85dd12576104000e6",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A developer needs to quickly test an open-source foundation model for style transfer in a sandbox environment with minimal AWS service limits. Which option is fastest to set up?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Deploy model on EKS with SageMaker Serving containers",
      "B": "Use SageMaker JumpStart one-click deployment",
      "C": "Walk through Bedrock console to add third-party model",
      "D": "Set up Amazon Q custom connector"
    },
    "explanation": "JumpStart offers one-click deployment of many open-source models into a test endpoint. Bedrock doesn\u2019t allow arbitrary third-party sandbox models. EKS setup is slower. Amazon Q isn\u2019t for custom style transfer."
  },
  {
    "id": "349d471786ee691e8c7697727b152ab910c548541ba40d6100f919683efc3858",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "An automotive company wants to leverage a proprietary foundation model from AWS Marketplace while maintaining complete customization control. They need to bring-your-own-license (BYOL). Which service supports this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock with custom model marketplace",
      "B": "Amazon Q with enterprise connector",
      "C": "SageMaker JumpStart BYOL container",
      "D": "Amazon Comprehend custom classification"
    },
    "explanation": "SageMaker JumpStart supports BYOL containers for proprietary models, giving full customization. Bedrock marketplace models are managed by AWS and don\u2019t support BYOL. Q and Comprehend do not support BYOL models."
  },
  {
    "id": "c55df7efe62a10ff3e0e9fe5ae18a542db0b3c88a44112737ff07934b9bc6b08",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A company requires the lowest possible inference latency in milliseconds for high-frequency trading advisory. They need direct GPU control. Which AWS solution is most appropriate?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Amazon Bedrock provisioned capacity",
      "B": "Amazon Q synchronous endpoint",
      "C": "SageMaker JumpStart on Fargate",
      "D": "Self-hosted GPU cluster on EC2 G5 instances"
    },
    "explanation": "Self-hosted GPU cluster on EC2 G5 gives you control over GPU allocation and network optimizations for sub-10ms latency. Bedrock and Q have added network hops. JumpStart on Fargate doesn\u2019t support GPUs."
  },
  {
    "id": "5d1d3076c694c9d0ecd9a65e7c1490f2f487f07f6545580641c33c603d6c5642",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A biotech research group needs to fine-tune a protein-structure foundation model with hundreds of GPU instances managed automatically, and then serve the model. Which AWS offering should they use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock automatic fine-tuning",
      "B": "SageMaker JumpStart fine-tuning job and endpoint",
      "C": "Amazon Q custom model service",
      "D": "AWS Lambda with EFS storing weights"
    },
    "explanation": "SageMaker JumpStart supports distributed fine-tuning on GPU clusters and hosting via managed endpoints. Bedrock does not allow user fine-tuning. Amazon Q is QA only. Lambda cannot handle large model weights."
  },
  {
    "id": "acc528f9ff6b3796457a734d275da3011f0ea4cc31543504555dae2a0c1d681e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A fintech startup needs a generative AI chatbot secured by IAM and integrated with DynamoDB for context retrieval. They want minimal orchestration. Which AWS service combination is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock with custom DynamoDB retrieval code",
      "B": "Amazon Q with Lambda polling DynamoDB",
      "C": "SageMaker JumpStart with Kendra",
      "D": "Amazon Lex with QLDB"
    },
    "explanation": "Bedrock allows direct API calls from Lambda or code that integrates DynamoDB for RAG, with IAM-based security, offering minimal orchestration. Q does retrieval but requires additional Lambda orchestration. JumpStart and Lex combinations are heavier."
  },
  {
    "id": "f66c7e18faa4364e63c2669fb08aae6342e32252ab4defc7fdbc4596262fd036",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A gaming studio needs burst capacity for character voice synthesis using Amazon Polly-style generative audio. They must avoid over-provisioning GPUs. Which service is preferred?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Self-hosted Tacotron on EC2 Spot Instances",
      "B": "SageMaker JumpStart audio model",
      "C": "Amazon Bedrock\u2019s text-to-speech foundation model",
      "D": "Amazon Polly standard voices"
    },
    "explanation": "Bedrock\u2019s audio foundation models offer burstable pay-per-use without GPU provisioning. Polly is not a foundation model with customizable voices. JumpStart audio models still require GPU instances. Self-hosting implies GPU management."
  },
  {
    "id": "3e0f1910018b7a4c38d682543af1ddc9df0a819319492d3baf0064245022e82a",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A research lab wants to benchmark multiple foundation models for summarization on identical hardware to isolate model performance. Which AWS approach is most appropriate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon Bedrock comparison APIs",
      "B": "Use SageMaker JumpStart on a fixed EC2 instance type",
      "C": "Use Amazon Q with differencing parameters",
      "D": "Use AWS Lambda with multi-model endpoints"
    },
    "explanation": "JumpStart lets you deploy multiple models on the same EC2 instance type, ensuring hardware consistency during benchmarking. Bedrock runs on hidden infra. Q doesn\u2019t allow model selection. Lambda can\u2019t guarantee fixed hardware."
  },
  {
    "id": "d15c37109b059b16fddc87e0203e1f28311d9383b2e1080711d99842d8f910d7",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A company must ensure prompt inputs and model outputs are redactable for GDPR \u2019right to be forgotten\u2019. They want a managed solution with data lifecycle controls. Which AWS service best fits?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock with customer-managed S3 logs and lifecycle policies",
      "B": "SageMaker JumpStart with default CloudWatch logs",
      "C": "Amazon Q auditing mode",
      "D": "Amazon Comprehend with Data Lifecycle Manager"
    },
    "explanation": "Bedrock stores logs in S3 under your account where you can apply lifecycle and deletion policies. JumpStart stores logs in CloudWatch with limited control. Q\u2019s audit logs aren\u2019t easily exported. Comprehend isn\u2019t generative."
  },
  {
    "id": "faf7a85d58bd8e9ed22443e56baeee023825cc7fe0a4a97559b1097efe74ed32",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A government agency requires FedRAMP High compliance for generative AI use. They need a model service with the highest security boundary. Which service qualifies?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Q",
      "B": "SageMaker JumpStart",
      "C": "Amazon Comprehend",
      "D": "Amazon Bedrock in an AWS GovCloud region"
    },
    "explanation": "Bedrock in AWS GovCloud has FedRAMP High boundary. Q and Comprehend are not offered in GovCloud with FedRAMP High. JumpStart on regular SageMaker may not meet FedRAMP High."
  },
  {
    "id": "213f42f7b7fcec687b041c3b91eaceb8a4eacf89ffb584ab9fd6aacb7a52cab5",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A marketing team wants to rapidly A/B test two different foundation models for tone consistency. They want to avoid endpoint cold-start latency affecting results. Which AWS service configuration should they choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock token calls to different models",
      "B": "SageMaker JumpStart provisioned endpoints for both models",
      "C": "Amazon Q with session stickiness",
      "D": "Amazon Lambda multiplexing Bedrock endpoints"
    },
    "explanation": "Provisioned SageMaker JumpStart endpoints keep models warm for consistent low latency. Bedrock cold starts can vary per request. Q isn\u2019t designed for multi-model A/B testing. Lambda multiplexing adds unpredictable jitter."
  },
  {
    "id": "866a2678ebd0b307f3132420b578d7aa8d252baa7717c95ecbe1474a4b75a3d2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "Your startup wants to reduce inference cost by caching frequent prompts and responses. Which AWS service supports built-in caching at the model inference layer?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock with CloudFront**",
      "B": "Amazon Q with DynamoDB cache",
      "C": "SageMaker JumpStart with accelerator inference accelerator",
      "D": "Amazon Comprehend with batch cache"
    },
    "explanation": "SageMaker JumpStart inference accelerators (Elastic Inference) allow caching intermediate layers and reduce compute costs. Bedrock doesn\u2019t offer built-in caching. Amazon Q requires building your own cache. Comprehend batch is offline only."
  },
  {
    "id": "62e39af9b7cd5b270f07625667cf6f7fba2fc3abbcfb945f1971fc3ad1a2fdf6",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A telecom wants to run simultaneous experiments on fine-tuning strategies (RLHF vs. instruction tuning) across different foundation models. They need automated experiment tracking and reproducibility. Which AWS feature should they employ?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Bedrock version control",
      "B": "SageMaker JumpStart experiment trials",
      "C": "AWS CodeCommit with Bedrock",
      "D": "Amazon Q audit logs"
    },
    "explanation": "SageMaker JumpStart supports experiment trials and tracking for fine-tuning jobs, enabling reproducibility. Bedrock does not offer built-in experiment tracking. CodeCommit is source control, and Q audit logs don\u2019t manage experiments."
  },
  {
    "id": "018a43d060f67cb50f7d15714b8bf6bdbf8ec7d53386832429199148635cbf37",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "An e-commerce platform wants high-availability inference across two regions for a foundation model without data replication. Which AWS generative AI service supports cross-region endpoints?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock multi-region endpoints",
      "B": "SageMaker JumpStart VPC peering endpoints",
      "C": "Amazon Q regional fallback",
      "D": "Amazon Comprehend global endpoint"
    },
    "explanation": "Bedrock supports multi-region endpoints for high availability without manual replication. JumpStart requires deploying separate endpoints per region. Q and Comprehend are region-specific without multi-region abstraction."
  },
  {
    "id": "a576fac6483c71b02a037e8299f75a6111fd15fbaf799592c5c62bd615fd78bc",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "2.3",
    "stem": "A fintech company needs to fine-tune a LLM under strict PCI DSS controls. They want built-in data encryption, audit, and key rotation. Which AWS service should they pick?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock with default settings",
      "B": "Amazon Q with PCI integration",
      "C": "SageMaker JumpStart in private subnet with customer-managed KMS",
      "D": "Amazon Comprehend custom classification"
    },
    "explanation": "SageMaker JumpStart in a private subnet offers VPC isolation, customer-managed KMS keys for encryption with rotation, and CloudTrail audit logs. Bedrock defaults don\u2019t guarantee VPC isolation or key rotation. Q and Comprehend aren't designed for fine-tuning."
  },
  {
    "id": "611537f13dcb390ddd4ecdb8a6c406883a25ebcefccecac4b5d469cb572a5c74",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A fintech startup needs to embed a foundation model into its fraud detection pipeline. They require sub-200 ms real-time inference, support for English and Spanish, moderate customization for domain-specific terminology, and predictable cost. Which pre-trained model selection best meets these criteria?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A large, high-capacity English-only model offering the best accuracy but 500 ms latency.",
      "B": "A tiny model (<1 B parameters) with 50 ms latency supporting only English and no customization.",
      "C": "A mid-sized multilingual model (5 B parameters) with documented <200 ms real-time latency and in-context domain adaptation support.",
      "D": "A large multilingual model (50 B parameters) optimized for batch inference but no SLA on latency."
    },
    "explanation": "Option C balances latency, language coverage, and in-context customization within cost constraints; other options sacrifice a required dimension."
  },
  {
    "id": "9747fac24c7aee52ad3164da7f564ea7759883db6ec45f6d4af6bf517a61e872",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "An e-commerce platform must generate personalized product descriptions on demand. They expect bursts of 1 K requests/sec. Which selection criterion most directly impacts their ability to scale with bursts?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model parameter count, since larger models always scale better.",
      "B": "Throughput SLA (requests/sec) and horizontal scaling support.",
      "C": "Provisioned I/O length per request.",
      "D": "Quality benchmark on small datasets."
    },
    "explanation": "Throughput SLA and horizontal scaling directly determine burst handling; other factors don\u2019t guarantee throughput at scale."
  },
  {
    "id": "c9f3ad592ccc69dea4ee50ff8d4abc30d2e27bf69ec86035cd75dc1a22679c07",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A multilingual chatbot requires support for 20 languages, low cost per token, and minimal per-inference customization. Which model attribute should be prioritized?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Highest parameter size for maximum quality.",
      "B": "Lowest latency single-language model.",
      "C": "Batch inference throughput.",
      "D": "Multi-language coverage with token-based pricing and endpoint customization."
    },
    "explanation": "Multi-language coverage and token-based pricing align with cost and language requirements; others focus on irrelevant characteristics."
  },
  {
    "id": "e97b75b9f1d14ad77608bf4e07c86b1f39d6ec1b780a21d9b317540db2cd0989",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A research team needs high accuracy on technical language and is willing to accept 1 s latency. They will fine-tune heavily. Which selection criterion matters most?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model openness and fine-tuning support (e.g., foundation model with instruction-tuning API).",
      "B": "Lowest latency endpoints.",
      "C": "Multimodal support.",
      "D": "Minimal model size to reduce cost."
    },
    "explanation": "Fine-tuning support is critical for customizing technical language; latency and multimodal needs are secondary."
  },
  {
    "id": "3abb51f7950a3681c724204f38bb381b839ba3c0ae062a47fa35b1d514b0a74d",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A mobile app must run offline without connectivity, so they need an on-device foundation model. Which selection criterion rules out server-hosted models?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High token throughput requirements.",
      "B": "On-device footprint size and local inference capability.",
      "C": "Batch vs real-time inference.",
      "D": "Language coverage across cloud endpoints."
    },
    "explanation": "On-device footprint size and local inference determine offline suitability; other criteria are irrelevant to offline use."
  },
  {
    "id": "aebd7e4d326c53817b200ad564401c33854f540ec08e5e240175067eaeebcaf8",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "Users report inconsistent creativity in a marketing slogan generator. Which inference parameter adjustment will systematically increase output variability?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature to allow more randomness.",
      "B": "Decrease max output length to limit content.",
      "C": "Lower repetition penalty to allow more rote repetition.",
      "D": "Reduce context window to truncate input."
    },
    "explanation": "Higher temperature yields more varied outputs; other parameters do not directly affect creativity randomness."
  },
  {
    "id": "b4902378929a5d0f5a511e35d1abf9d99213f0695429f62581bf492b24e95af2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A document summarizer cuts off mid-sentence. Which parameter change prevents truncation?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature.",
      "B": "Lower top-p sampling.",
      "C": "Increase max output token length.",
      "D": "Decrease model batch size."
    },
    "explanation": "Raising max output token length allows longer summaries; other parameters don\u2019t extend outputs."
  },
  {
    "id": "06a904f4cf5c4c3469fd2d7207511eaa5d846d048a0472516666c482d64d22e7",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A chatbot sometimes repeats phrases verbatim. Which inference parameter reduces repetition?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature.",
      "B": "Increase top-k sampling.",
      "C": "Increase max input length.",
      "D": "Apply or raise repetition penalty."
    },
    "explanation": "Repetition penalty discourages repeated tokens; other settings control randomness or length."
  },
  {
    "id": "8d390e309e1065eb87973dcf2d8e3132c9567ee40718c2f78b156e2a23f40730",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A code generator rarely includes required imports. Which change to inference parameters is most likely to address this?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Lower temperature for more deterministic output.",
      "B": "Increase top-p to sample more tokens.",
      "C": "Increase max input length to include import instructions in prompt.",
      "D": "Increase repetition penalty."
    },
    "explanation": "A longer prompt context ensures the model sees import instructions; temperature and top-p aren\u2019t effective here."
  },
  {
    "id": "6fdf5c232a54a36f1c346269d09248b77708cb79fb20dece0011001f473743ba",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "An LLM-based assistant hallucinates facts when answering. Which parameter tweak minimizes factual errors?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Lower temperature to produce more conservative responses.",
      "B": "Increase max output length.",
      "C": "Raise top-k to sample more tokens.",
      "D": "Decrease context window."
    },
    "explanation": "Lowering temperature makes outputs more deterministic and less prone to hallucination; other changes don\u2019t reduce hallucinations."
  },
  {
    "id": "aa4b4642bc0f5670f8008b51e138efd00103b92e2571cc07bc382c7a756e7f90",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A financial advisory app uses RAG to answer queries from compliance documents. Which component is essential for a RAG implementation?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Continuous in-context learning without external data.",
      "B": "An embedding store and a retrieval mechanism to fetch relevant passages.",
      "C": "Fine-tuning the base model on compliance text.",
      "D": "High temperature to diversify retrieved content."
    },
    "explanation": "RAG requires embedding storage plus retrieval to augment prompts; fine-tuning isn\u2019t RAG and temperature is irrelevant to retrieval."
  },
  {
    "id": "04cb6fde7359130f2cff5bf315396738d377f663aa4a4282f6b29d38d47d76d7",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "An enterprise uses Salesforce documents to answer sales queries via RAG. Which AWS service can host the knowledge base for low-cost retrieval?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Aurora PostgreSQL.",
      "B": "Amazon Neptune with Gremlin queries.",
      "C": "Amazon OpenSearch for vector search on document embeddings.",
      "D": "Amazon S3 with no indexing."
    },
    "explanation": "OpenSearch supports vector search with scalable indexing; S3 alone cannot index embeddings; other databases are less optimized."
  },
  {
    "id": "f923e719d16650d9b72447e45e18e6e96a269b0f02a7c47a20ca62f0084a9744",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A law firm needs to update its RAG corpus daily with new case law, ensure fast retrieval, and minimize operational overhead. Which architecture best fits?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run Neptunedb with nightly data imports and manual indexing.",
      "B": "Use RDS Aurora with full-text search and custom Lambda indexing.",
      "C": "Store JSON in S3 and scan files per query.",
      "D": "Use Amazon OpenSearch Service with automated ingestion pipelines and managed vector indices."
    },
    "explanation": "Managed OpenSearch with automation balances speed, freshness, and low overhead; other options require heavy management or are slow."
  },
  {
    "id": "fec16180aea2499845093b34ab08138aeb5110a72aaba1845cbd2735e8eae235",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A startup wants RAG only for urgent queries (5%). For the rest, direct LLM inference is cheaper. How should they architect cost-efficiently?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Always perform RAG retrieval on every query.",
      "B": "Branch logic: detect urgent queries, use RAG only when needed, otherwise direct inference.",
      "C": "Fine-tune model to internalize everything, avoiding RAG.",
      "D": "Pre-embed all possible queries offline and store in S3."
    },
    "explanation": "Conditional branching uses RAG only when necessary, reducing retrieval costs; options A and C increase costs, D is impractical."
  },
  {
    "id": "55aa8b5e7cb31d840dcdfc31a6a2851947807db1868a17624de4db10c158cb81",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "To implement RAG, a team must vectorize documents. Which AWS feature automates embedding generation?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon SageMaker JumpStart embedding pipelines.",
      "B": "Bedrock custom fine-tuning.",
      "C": "Lambda with batch translation.",
      "D": "AWS Glue DataBrew."
    },
    "explanation": "SageMaker JumpStart provides embedding pipelines; Bedrock fine-tuning and Glue are unrelated to embedding generation."
  },
  {
    "id": "f5f4ef08f005d8d6ba640c7d20a2decc4d444e94e82cc85d26e87df830703239",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A healthcare app requires HIPAA-eligible embedding storage with sub-second vector search. Which AWS service meets both needs?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon RDS for PostgreSQL with pgvector extension.",
      "B": "Amazon DocumentDB undocumented vector plugin.",
      "C": "Amazon Neptune with HaVQL endpoints.",
      "D": "Amazon OpenSearch Service HIPAA-eligible domain with vector search."
    },
    "explanation": "OpenSearch supports vector search and is HIPAA-eligible; others lack managed support or eligibility."
  },
  {
    "id": "7f0be8e3db33b616c341467caab5f1433394d47a84f7e469ac77f7e4980e6854",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "At 1 M documents, vector searches slow below SLA. Which storage change offers lowest maintenance while improving performance?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch to Neptune with custom indexing.",
      "B": "Scale OpenSearch cluster with dedicated ML nodes for vector search.",
      "C": "Migrate to RDS Aurora serverless.",
      "D": "Store embeddings in S3 and perform Lambda scans."
    },
    "explanation": "Scaling OpenSearch with ML nodes optimizes vector speed with minimal management; others add complexity or are too slow."
  },
  {
    "id": "b9f8bf7da3dec1e9a1b09525166e993f5a068820e61020dc68d0080152274c63",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A startup considers Aurora PostgreSQL vs OpenSearch for embeddings. They need cost-effective writes for infrequent updates and fast reads. Which choice?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Aurora PostgreSQL serverless with pgvector for low update cost and indexed reads.",
      "B": "OpenSearch with daily bulk reindexing.",
      "C": "Neptune cluster for graph queries.",
      "D": "DocumentDB with manual sharding."
    },
    "explanation": "Aurora serverless minimizes cost on infrequent writes while supporting indexed reads; OpenSearch reindexing is heavier."
  },
  {
    "id": "7f4fa418b0941dfaaafac720568efaaa41c2ae6c1ec924ad4af197e1b4300c1c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A media company fine-tunes a foundation model for style consistency vs using in-context learning. Which cost factor most impacts this trade-off?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Data storage cost for training datasets.",
      "B": "Latency of inference endpoints.",
      "C": "Temperature parameter tuning.",
      "D": "Compute and storage costs of fine-tuning vs pay-per-token RAG or in-context calls."
    },
    "explanation": "Fine-tuning incurs upfront compute/storage costs, whereas in-context RAG is pay-per-token; that cost trade-off drives decision."
  },
  {
    "id": "fe4d375adbcd591021503cb61825efc033759ac0045c485511f0594d47630026",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A company must decide between full model fine-tuning vs few-shot prompts. They expect high query volume and low per-request cost. Which is likely cheaper over time?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Full fine-tuning, since customized models always cost less.",
      "B": "Few-shot prompting, due to lower maintenance and pay-per-token pricing at scale.",
      "C": "Pre-training from scratch.",
      "D": "Building custom retrieval engines to avoid LLM calls."
    },
    "explanation": "Few-shot prompting avoids fine-tuning overhead and scales with token-based pricing; other options are impractical or expensive."
  },
  {
    "id": "2922948ded2db4b363d579468bc9a9f3b972c51abf9b27394c2dca1692c33009",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "For sporadic complex queries, a company debates pre-training a new model vs RAG. What cost factor favors RAG?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Data labeling costs.",
      "B": "Model parameter count.",
      "C": "High upfront pre-training compute versus per-call retrieval fees.",
      "D": "Inference latency."
    },
    "explanation": "Pre-training requires massive upfront compute; RAG incurs per-call retrieval fees, making it cheaper for sporadic use."
  },
  {
    "id": "8ad59281faa5d6cc767ea149e465ed1a8f89055812007995dce499ed77852534",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A biotech firm needs multiple specialized models and wonders if fine-tuning each is cost-effective vs a single foundation model with RAG. Which scenario favors RAG?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "When domain corpora are small and finely segmented, making many fine-tunings expensive.",
      "B": "When data volumes justify building separate huge models.",
      "C": "When inference latency must be under 50 ms.",
      "D": "When on-device deployment is required."
    },
    "explanation": "Small segmented domains make many fine-tunings costly, so using RAG with one model is more cost-effective."
  },
  {
    "id": "826be4b70570794ad0b00870ca77893fb3e821cf345525cd4a34808aa55391ed",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A multi-step travel booking agent needs to search availability, book flights, and confirm hotels in sequence. Which AWS feature enables orchestrating these tasks?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Bedrock prompt chaining via a single prompt.",
      "B": "RAG retrieval with multi-document queries.",
      "C": "Custom Lambda orchestration without LLM.",
      "D": "Bedrock Agents to execute step-by-step actions autonomously."
    },
    "explanation": "Bedrock Agents coordinate multi-step tasks via LLM-driven actions; prompt chaining alone isn\u2019t sufficient orchestration."
  },
  {
    "id": "3a7b7a2cae79dcdbf9a5f0ac58b8d06230bdb588f11724a0e6252cdd52e45154",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A supply chain assistant must call external APIs, ingest responses, and produce a final report. Which approach reduces custom code?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Fine-tune a foundation model to embed API logic.",
      "B": "Use RAG to index API docs.",
      "C": "Deploy a Bedrock Agent with connector plug-ins to APIs.",
      "D": "Use SageMaker Pipelines orchestrating Lambdas."
    },
    "explanation": "Bedrock Agents support connectors to external APIs with minimal code; other methods require more custom development."
  },
  {
    "id": "fbd302dcc8942caa113960404c6f020f90977e341fe5e5edd257b34d327cc2e8",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "An HR chatbot must verify identities, fetch records, and schedule interviews. Which architecture supports LLM reasoning plus API calls?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Direct model inference with embedded API keys.",
      "B": "Use a Bedrock Agent that invokes AWS Lambda functions for each step.",
      "C": "RAG with HR document embeddings only.",
      "D": "Fine-tune the model on HR policies."
    },
    "explanation": "Agents calling Lambdas allow secure API interactions and stepwise logic; RAG and fine-tuning alone can\u2019t orchestrate actions."
  },
  {
    "id": "275da7cdf4abf711f22f1bd62cd22ef28514025697426e5ba9eff971b6376010",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "To troubleshoot a failed Agent workflow, which Bedrock feature helps inspect each step's LLM decision?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Agent execution logs and step traces in Bedrock console.",
      "B": "SageMaker Clarify bias reports.",
      "C": "Model Monitor drift plots.",
      "D": "CloudWatch S3 access logs."
    },
    "explanation": "Bedrock Agents provide execution logs/traces of each decision step; other services monitor different aspects."
  },
  {
    "id": "59f545adc5b9906b19c07f88ff13e8fc1e2d2254777add6edf9881936578348a",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.1",
    "stem": "A sales bot Agent must maintain context across ten user turns. Which Agent configuration ensures context retention?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "High temperature setting.",
      "B": "Low top-p value.",
      "C": "RAG knowledge base only.",
      "D": "Stateful Agent session with conversational memory enabled."
    },
    "explanation": "Stateful sessions with memory retain multi-turn context; inference parameters and RAG alone don\u2019t preserve session state."
  },
  {
    "id": "d884faad29a3d3e0acf8cafc7232b2ae1137fa146558c6e8360aa18f6c8aaf6c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "You are designing a prompt to guide a large language model to perform multi-step mathematical reasoning before answering. Which technique will most reliably produce a correct solution?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Embed the full solution in the prompt as context and ask the model to repeat it.",
      "B": "Use a single-shot prompt with the question and a request for the answer only.",
      "C": "Use a chain-of-thought prompt that explicitly asks the model to \"explain your reasoning step by step\" before giving the final answer.",
      "D": "Provide multiple unrelated examples and then ask for the solution to the target problem."
    },
    "explanation": "Chain-of-thought prompts explicitly break down reasoning steps, improving multi-step problem solving over single-shot or unrelated examples."
  },
  {
    "id": "1f9bae631a15439a69e35de095952c699753a97cb9985f17a15c8fdcb580a861",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "A developer wants the model to avoid sensitive topics without explicit deletion. Which prompt engineering technique should they apply?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature to reduce determinism.",
      "B": "Include a negative prompt section listing prohibited topics.",
      "C": "Use chain-of-thought to deter the model from broaching sensitive content.",
      "D": "Provide only a few-shot prompt of acceptable topics."
    },
    "explanation": "Negative prompts explicitly instruct the model to avoid specified content, more reliable than few-shot or chain-of-thought for exclusion."
  },
  {
    "id": "93571a7074e66e052062dd3849f38053dd9e209bc56e98695e7d844c1c739608",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "You need to design a prompt template for a financial query chatbot that extracts entities. Which practice ensures consistent outputs?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Leave input schema flexible and let the model infer structure.",
      "B": "Encourage the model to ask follow-up questions when uncertain.",
      "C": "Use a zero-shot prompt with only instructions and no examples.",
      "D": "Define a structured template with labeled slots and provide two few-shot examples mapping questions to slot values."
    },
    "explanation": "A template with labeled slots and few-shot examples yields consistent structured outputs, reducing variability."
  },
  {
    "id": "3dec1363bfe9e3935fabd350c94d1d6594064d618c0858288148365cd288fd67",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "A prompt engineer notices that the model hallucinates project deadlines. What prompt modification could reduce this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add explicit \"I do not know\" fallback instructions for unknown dates.",
      "B": "Increase temperature to add variability.",
      "C": "Remove instructions about default responses.",
      "D": "Use random negative examples to confuse the model."
    },
    "explanation": "Providing an explicit fallback instruction reduces hallucinations by telling the model how to respond when uncertain."
  },
  {
    "id": "0fc6826537bc279fbddfe8de53a63d7c4b943829d77e681fe4311468048b3ae5",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "Your application needs consistent summaries of legislative text. To limit length without losing key points, which parameter and prompt style should you use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High temperature and a zero-shot prompt.",
      "B": "Lower temperature with a summarization template including a max_length instruction.",
      "C": "Few-shot examples of summaries of unrelated texts.",
      "D": "Chain-of-thought prompting with long context."
    },
    "explanation": "A lower temperature and explicit max_length instruction control verbosity while preserving important content."
  },
  {
    "id": "28d93e1e324a46bdf288fcd8e479df1d57f40154201bf52c83f39677240b88f8",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "A customer support bot must avoid revealing system internals. Which engineering approach secures the prompt?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use high temperature to generalize responses.",
      "B": "Provide examples of system\u2010internal responses to alter them.",
      "C": "Implement guardrails by embedding instructions that explicitly forbid mentioning internal details.",
      "D": "Allow the model to ask itself follow-up questions."
    },
    "explanation": "Embedding guardrail instructions to explicitly forbid certain content is the standard way to enforce boundaries."
  },
  {
    "id": "6ecbe92c17f7a7ef79cfa44d0fecd8a41e76ddebb75a790fc8bc0f8202c8dc6d",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "You must translate text but avoid literal translations of idioms. Which prompt best achieves this?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "\"Translate this text to French.\"",
      "B": "Provide a few idiom translations as examples then translate.",
      "C": "Use chain-of-thought to explain how to translate.",
      "D": "Use a prompt with instructions: \"Translate preserving meaning\u2014not literal word-for-word\u2014especially for idioms.\""
    },
    "explanation": "Explicit instructions about preserving meaning over literal translation guide the model away from word-for-word renders."
  },
  {
    "id": "e6d7a7f21f734feed61d078538c7114b2ef950955d821de7b173bd2ee5e24fcf",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "A security audit reveals prompt-injection vulnerabilities. What prompt design change can mitigate prompt hijacking?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a system-level instruction layer separate from user input.",
      "B": "Chain multiple user inputs to confuse attackers.",
      "C": "Increase maximum token length.",
      "D": "Use few-shot examples of malicious inputs."
    },
    "explanation": "Separating system prompts from user prompts prevents user-supplied text from overriding core instructions."
  },
  {
    "id": "e55278c1106d3ea04e98c80dec25749923380f0223a12df6b221c6a38706013b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "When designing prompts for few-shot learning on a classification task, you notice class imbalance. Which tactic can improve performance?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature to diversify outputs.",
      "B": "Provide more zero-shot instructions.",
      "C": "Include proportional examples for underrepresented classes in the few-shot examples.",
      "D": "Remove examples for overrepresented classes."
    },
    "explanation": "Balancing the few-shot examples ensures the model sees adequate samples of each class, improving bias."
  },
  {
    "id": "b898ceff5b71c765b73eb83e3ab5c06f7139f2975f1b69476aaa373ec0481812",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "Your prompt includes context, instruction, and examples. The model ignores the instruction. What likely caused this?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Temperature is set too low.",
      "B": "Your chain-of-thought prompt is missing.",
      "C": "You used negative prompts incorrectly.",
      "D": "The examples contradict the instructions, causing the model to follow examples instead."
    },
    "explanation": "Models prioritize exemplars over abstract instructions; contradictory examples override instructions."
  },
  {
    "id": "abfbbbc37d202fb86cfe47463d3cf424ff97a95046551988463fa95f55bb1d76",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "To improve factual accuracy on Q&A, you plan to retrieve documents and include them in the prompt. What technique is this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Zero-shot prompting",
      "B": "Retrieval Augmented Generation (RAG)",
      "C": "Chain-of-thought prompting",
      "D": "Negative prompting"
    },
    "explanation": "RAG supplies external knowledge to the prompt to improve factual grounding."
  },
  {
    "id": "9c32232bb27256d0c4f59149953dc8fd9e8ab15b0005c8342e6259bca9e3e6f2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "A prompt engineer wants to generate SQL queries from natural language. Which style yields the most reliable queries?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Few-shot prompt with paired NL questions and SQL statements.",
      "B": "Zero-shot ask \"Convert to SQL\" without examples.",
      "C": "Chain-of-thought to show stepwise table scanning.",
      "D": "A negative prompt listing forbidden SQL functions."
    },
    "explanation": "Few-shot paired examples guide the model to learn the NL-to-SQL mapping effectively."
  },
  {
    "id": "73c668724499b0b88b24fea7e2b1662bba9e7b9c2ed6844d8c4b6d86d5fb6aea",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "You notice that adding more few-shot examples degrades performance. What is the most likely cause?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Temperature is too low.",
      "B": "Instruction length is too short.",
      "C": "Context window exceeded, causing truncation of important examples.",
      "D": "Negative prompts are overridden."
    },
    "explanation": "Exceeding the context window truncates earlier examples, harming performance."
  },
  {
    "id": "18cfa561b934351e31416e6365fbf5dc9be9bc7c3991741e6a8cc65d8a399cd1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "For a creative brainstorming assistant, you need diverse ideas. Which prompt setting supports this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Low top_p and low temperature.",
      "B": "Use zero-shot instructions only.",
      "C": "Provide few-shot focused examples.",
      "D": "High temperature and high top_p with an open-ended instruction template."
    },
    "explanation": "High temperature and top_p increase diversity, suited for brainstorming tasks."
  },
  {
    "id": "53b1143a4235b10bc676c517188fa672c1192ebea01e2c9ce7156211d14f7b90",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "Your prompt includes sensitive user data. To prevent leakage, which practice should you employ?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Increase temperature so the model forgets specifics.",
      "B": "Redact PII before including any user data in the prompt.",
      "C": "Use a negative prompt to exclude PII.",
      "D": "Use chain-of-thought to obscure the data."
    },
    "explanation": "Redacting PII is the reliable approach to prevent sensitive data from appearing in responses."
  },
  {
    "id": "bf5395c350d77db0f625a5e7867e92d95e0c60ed5da654501405f8126046520d",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "You are using negative prompts to block disallowed content, but they sometimes fail. What additional measure can improve enforcement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Combine negative prompts with model-level content filters.",
      "B": "Lower the temperature to zero.",
      "C": "Use more few-shot allowed examples.",
      "D": "Add chain-of-thought steps rejecting the content."
    },
    "explanation": "Pairing negative prompts with content filters catches any disallowed outputs that slip through."
  },
  {
    "id": "40c29b1819cdbf50d3bf8465f6096a2f4d972d3c5c1533d2a58c98c457347a56",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "A prompt leverages placeholders like {{user_name}} and {{date}}. What benefit does this template provide?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Reduces hallucinations.",
      "B": "Enables chain-of-thought prompting.",
      "C": "Supports dynamic injection of variables while keeping prompt structure consistent.",
      "D": "Increases model temperature."
    },
    "explanation": "Templates with placeholders allow dynamic content while preserving prompt consistency and structure."
  },
  {
    "id": "09eca14001a978d81ed2e4a42d1c675b9d0b2233ea1fb1bf568eda6d151464e4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "Your prompt for summarization often includes irrelevant details. Which prompt adjustment will focus the model?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Add more few-shot examples of long summaries.",
      "B": "Include an instruction: \"Focus only on the main points and exclude minor details.\"",
      "C": "Increase temperature to reduce determinism.",
      "D": "Use chain-of-thought to list every detail then summarize."
    },
    "explanation": "Explicit instructions to exclude minor details refocus the model on main points."
  },
  {
    "id": "268cabe1f6856433f5f92bc4920693b90e188e2765e9255d59b0212dffaf06d3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "In a translation prompt, the model sometimes outputs back-translated English. How do you correct this?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Add few-shot examples of translations back to English.",
      "B": "Remove context from the prompt.",
      "C": "Use higher top_p.",
      "D": "Clarify: \"Translate from English to Spanish only, do not output in English.\""
    },
    "explanation": "Explicitly specifying source and target languages in instructions prevents unintended English output."
  },
  {
    "id": "4af9d6b0f7a030c99599b584237173257efc27be50544efd812114b89ec256d4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "You want the model to follow a strict JSON output schema. Which practice ensures compliance?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provide a JSON schema in the prompt and ask \"Output strictly in this JSON format.\"",
      "B": "Use chain-of-thought to generate JSON step by step.",
      "C": "Ask for the output in bullet points then convert.",
      "D": "Use negative prompts to block non-JSON text."
    },
    "explanation": "Including the exact JSON schema and instructing strict adherence drives the model to produce valid JSON."
  },
  {
    "id": "4f1407f15f6f5a938f0417b9072acf3e5e8c430b78ecdb1fe766b7df91b5589f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "A prompt engineer tests with boundary questions and finds inconsistent behavior. What best practice can improve prompt robustness?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature to handle edge cases.",
      "B": "Use zero-shot with generic instructions.",
      "C": "Include random noise in examples.",
      "D": "Iteratively test and refine prompts with boundary and adversarial cases."
    },
    "explanation": "Iterative testing with adversarial prompts helps identify weaknesses and refine prompt reliability."
  },
  {
    "id": "2e2e23dfbf71d6c3671a07d5a63c4e03a7be8b9df099b460ec022b94cfdad89d",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "Your prompt includes an example that is semantically similar to the target but uses different phrasing. This causes errors. How do you fix it?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Remove the example altogether.",
      "B": "Rewrite examples to use consistent phrasing and terminology.",
      "C": "Add negative prompts blocking the old phrasing.",
      "D": "Increase max_tokens to include more context."
    },
    "explanation": "Consistent phrasing ensures the model learns the correct mapping between example and task."
  },
  {
    "id": "2e1755204e7e63d34f9c136195fc0b9ff7297079cdf37c2ac1192231644e890f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "When optimizing prompts for latency, you notice longer prompts slow response. What trade-off can you adjust?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Increase temperature to speed up decoding.",
      "B": "Add more few-shot examples.",
      "C": "Shorten the prompt by trimming non-essential context and rely on zero-shot instructions.",
      "D": "Add chain-of-thought to guide response quickly."
    },
    "explanation": "Trimming prompt context and using concise instructions reduces token count and lowers latency."
  },
  {
    "id": "96e69fa711103e035e59eb190d1e419b24d9d4167795e8cdf6f1eb8510334cf0",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "A prompt accidentally reveals internal model behavior. How can you prevent this?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Add instructions: \"Do not mention internal reasoning or hidden policies.\"",
      "B": "Increase temperature to obfuscate.",
      "C": "Use chain-of-thought explicitly.",
      "D": "Include examples of internal behaviors to guide them."
    },
    "explanation": "Explicitly forbidding the mention of internal processes prevents model exposition."
  },
  {
    "id": "af39d18bd527cb82431470c57efe9b023379143f80d0c44483dbdf5f75409318",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "Your prompt works well at low volume but degrades when many users query concurrently. What is a prompt engineering mitigation?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add more examples per prompt.",
      "B": "Standardize prompts via a shared template and cache frequent prompt structures.",
      "C": "Use higher temperature to distribute load.",
      "D": "Switch to chain-of-thought prompting."
    },
    "explanation": "Standardizing and caching prompt templates reduces construction overhead and variability under load."
  },
  {
    "id": "18c7b9ccdac41fe060c22b11b348bb823526180a422ebed4baa7ed8a9f1997c9",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "You want the model to ask clarifying questions if the input is ambiguous. Which prompt style supports this behavior?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Zero-shot with final answer only.",
      "B": "Few-shot examples showing direct answers.",
      "C": "Few-shot examples where the model asks follow-up questions before answering.",
      "D": "Chain-of-thought showing internal ambiguity resolution."
    },
    "explanation": "Few-shot examples demonstrating clarifying questions teach the model to adopt that behavior."
  },
  {
    "id": "e516dcac9cf73bcf91fc4bc9240337e2a7aec485672a3fe78c4e76d1779f7d68",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "During prompt iteration you try random synonyms in instructions and see erratic results. What is the root issue?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Instruction too specific.",
      "B": "Temperature too low.",
      "C": "Examples are missing.",
      "D": "Model is sensitive to wording; maintain consistent terminology for stability."
    },
    "explanation": "Models can be highly sensitive to instruction wording; consistent terminology yields more stable outputs."
  },
  {
    "id": "030152443fa136f8b68dda0cf38ef058cfb6a95794fceca29f9d72f5c0d2eea6",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "A prompt engineer applies chain-of-thought for sentiment analysis but sees no benefit. Why?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Chain-of-thought only works with few-shot prompts.",
      "B": "Sentiment analysis is single-step; chain-of-thought adds unnecessary cost without accuracy gain.",
      "C": "Temperature was set too high.",
      "D": "They forgot negative prompts."
    },
    "explanation": "Chain-of-thought aids multi-step reasoning; single-shot tasks like sentiment analysis don't benefit significantly."
  },
  {
    "id": "e1e7c64189be0f75690567116e84ed425dbcb4bd4e3049fa8cf32348b22c4b1f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "To prevent prompt leakage in logs, which practice should you follow?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Log full raw prompts for auditing.",
      "B": "Use chain-of-thought to split prompts.",
      "C": "Mask or hash sensitive portions of prompts before logging.",
      "D": "Set temperature to zero."
    },
    "explanation": "Masking or hashing sensitive data in logs protects privacy while preserving auditability."
  },
  {
    "id": "6ebf5075cdcdf981512af98bfa951706e76770bb7cbf8e2e7f3c6daba349b7ab",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "Your prompt uses multi-shot examples but the order influences correctness. How do you mitigate ordering bias?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Randomize the order of few-shot examples during prompt construction.",
      "B": "Increase temperature.",
      "C": "Use negative prompts blocking first examples.",
      "D": "Switch to chain-of-thought."
    },
    "explanation": "Randomizing example order avoids accidental bias toward earlier examples."
  },
  {
    "id": "023eb3cc8b52bd2b0a6b862d7efec60b87ea564a477a87ef397878e800b00de4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.2",
    "stem": "You need to test a prompt\u2019s vulnerability to jailbreaking. Which step is most effective?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature to encourage rule breaking.",
      "B": "Use few-shot allowed content examples.",
      "C": "Add negative prompts blocking \u201cjailbreak\u201d.",
      "D": "Design adversarial inputs that attempt to override instructions and test responses."
    },
    "explanation": "Adversarial testing reveals if malicious inputs can override system instructions, identifying jailbreak risks."
  },
  {
    "id": "e45b840f71fb220985bf3aafd4914fc46157b9e1fcf7af526891c288cbf893c7",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "A financial services firm needs a foundation model to understand domain\u2010specific jargon from its proprietary 10 GB corpus. They have limited human\u2010labelled data but extensive raw text. Which customization method should they apply to best adapt the foundation model to their domain?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Instruction tuning using supervised examples",
      "B": "Reinforcement learning with human feedback (RLHF)",
      "C": "Domain adaptation via continuous pre\u2010training on raw text",
      "D": "Transfer learning with a small labelled dataset only"
    },
    "explanation": "Domain adaptation (continued pre\u2010training) uses raw domain text without heavy labels to adapt model."
  },
  {
    "id": "ceac063f956779d0a41d4038c204c35b7a59e322d94180c2617484a4e8c2f85b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "A startup wants to fine\u2010tune a public LLM for summarization of legal contracts but cannot afford extensive compute. Which low\u2010resource fine\u2010tuning approach balances cost and performance?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Full parameter fine\u2010tuning on all layers",
      "B": "Instruction tuning with a prompt example dataset",
      "C": "Reinforcement learning with human feedback",
      "D": "Domain adaptation with large unlabelled data"
    },
    "explanation": "Instruction tuning with curated prompts uses less compute than full fine\u2010tuning and improves summarization."
  },
  {
    "id": "8edaa36754d8ea7ac562449c7cb19c59b2a80fae64acadff3e53081384e4e1f5",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "A retail company notices its fine\u2010tuned foundation model performance degrading monthly due to changing product catalogs. Which process addresses this issue?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "One\u2010time fine\u2010tuning on initial data",
      "B": "Periodic instruction tuning only when issues appear",
      "C": "Revert to the base foundation model",
      "D": "Continuous pre\u2010training with recent catalog data"
    },
    "explanation": "Continuous pre\u2010training on fresh data combats concept drift."
  },
  {
    "id": "292f8c8f2fa6302b45eb5a0db0bc47315d23ccf773b337955f6d5da52ff72cb7",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "When preparing data for fine\u2010tuning a foundation model with RLHF, which step is MOST critical to ensure high\u2010quality reward signals?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Curating diverse human preference comparisons",
      "B": "Balancing classes in the raw unlabeled corpus",
      "C": "Normalizing text token lengths",
      "D": "Applying data augmentation to unlabelled data"
    },
    "explanation": "RLHF needs human\u2010annotated preference data for reward model training."
  },
  {
    "id": "00539a617060bdc0a26dc5426627b22508691fc67bb0cbcb128e85ac3fedc3ca",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "A life\u2010sciences company wants to restrict a foundation model\u2019s clinical advice to approved guidelines. Which fine\u2010tuning method best encodes these constraints?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Instruction tuning with guideline\u2010based prompts",
      "B": "Domain adaptation on general clinical text",
      "C": "Supervised transfer learning on labeled Q&A pairs",
      "D": "Reinforcement learning with patient feedback"
    },
    "explanation": "Instruction tuning can encode guideline style and constraints directly."
  },
  {
    "id": "b46061fe847ca88fbe5d1d2ddaf0cb07a318e12cb7ecbc1717fcbe3860893460",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "You have a foundation model fine\u2010tuned on 50k labeled examples. You want further improvement on a subcategory representing only 500 examples. Which approach is optimal?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Retrain entire model on combined 50.5k dataset",
      "B": "Perform targeted fine\u2010tuning (transfer learning) on 500 examples",
      "C": "Use RLHF with broad general feedback",
      "D": "Continuous pre\u2010training on unlabeled data"
    },
    "explanation": "Targeted transfer learning on few examples adapts specialized subcategory."
  },
  {
    "id": "1447ed8b0d9d54d4f1804e6c8f2a41f448ac99d235b576d2fc9dc25d1af84248",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "A conversational agent generates unsafe suggestions occasionally. You have developer time but limited data. Which customization pipeline addresses safety concerns?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Domain adaptation on more unlabeled conversation logs",
      "B": "Instruction tuning with longer prompts",
      "C": "Transfer learning on generic safe corpora",
      "D": "RLHF incorporating human safety feedback"
    },
    "explanation": "RLHF with human\u2010defined safe/unsafe labels tunes reward for safety."
  },
  {
    "id": "805cbe8b65a118003818b353429b421e0df883b46c9bb5be0f8ca697f357c097",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "During fine\u2010tuning a foundation model on image captioning, performance stalls. You suspect data issues. What should you evaluate FIRST?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model hyperparameters like learning rate",
      "B": "Compute instance type",
      "C": "Representativeness and labeling consistency of captioned images",
      "D": "Batch size and tokenization schema"
    },
    "explanation": "Data quality and representativeness are first to inspect when fine\u2010tuning stalls."
  },
  {
    "id": "23c3de686c650eeec312e9fa850c3cbdf68aa1975bee4aa75045947a3ad31508",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "An LLM trained for medical advice must comply with data\u2010privacy laws. Which data preparation practice is MOST important before fine\u2010tuning?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Converting text to lower\u2010case tokens",
      "B": "Anonymizing and removing personal identifiers",
      "C": "Segmenting documents by sentence length",
      "D": "Balancing positive/negative sentiment"
    },
    "explanation": "Removing personally identifiable information is key for compliance."
  },
  {
    "id": "2c712060cf5281bf0908ffd3c3804ca98d44793d7c888e4236097afc1a48ae88",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "A customer support chatbot was instruction tuned but underperforms on technical queries. Which next step will most effectively improve domain accuracy?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase prompt length",
      "B": "Switch to RLHF",
      "C": "Domain adaptation with technical support transcripts",
      "D": "Fine\u2010tune on general instruction datasets"
    },
    "explanation": "Continued pre\u2010training on domain transcripts boosts technical knowledge."
  },
  {
    "id": "aa5c75922fe4130395317f9af70512b77bd463791fb99e7d3cb5f562ee9cf0c3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "Which characteristic distinguishes instruction tuning from domain adaptation?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Instruction tuning uses paired input\u2010output examples",
      "B": "Domain adaptation requires human feedback loops",
      "C": "Instruction tuning uses raw unlabeled text",
      "D": "Domain adaptation modifies reward functions"
    },
    "explanation": "Instruction tuning relies on (instruction, response) supervised pairs."
  },
  {
    "id": "311434ce267adc64085780d53a9fab833054548c5345311c845120a088c35b63",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "A retailer fine\u2010tuned a model on product descriptions but wants to reduce hallucinations. Which addition to the fine\u2010tuning pipeline helps?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Increase batch size",
      "B": "Swap tokenizer for larger vocabulary",
      "C": "Add more unlabeled data",
      "D": "Incorporate RLHF with human\u2010rated truthfulness"
    },
    "explanation": "RLHF can penalize hallucinations via human feedback."
  },
  {
    "id": "d24038d38aa0aa6305ee4e3288eaf38d75cec6d6729bfc96a44ef195bb71f871",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "You must measure the impact of fine\u2010tuning on model size and latency. Which evaluation metric combination is most relevant?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Perplexity and BLEU score",
      "B": "Model parameter count and inference time",
      "C": "ROUGE and BERTScore",
      "D": "Training loss and validation accuracy"
    },
    "explanation": "Parameter count and latency directly measure size and inference performance."
  },
  {
    "id": "17bb373100ac792f674710a6d57317161a00bbe0de8d89c053346558bbc16807",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "A LLM fine\u2010tuning job on AWS Bedrock fails due to OOM errors. Which action best mitigates the issue?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase learning rate",
      "B": "Use larger batch size",
      "C": "Enable parameter-efficient fine\u2010tuning (PEFT)",
      "D": "Switch to full model fine\u2010tuning"
    },
    "explanation": "PEFT reduces memory use by tuning fewer parameters."
  },
  {
    "id": "108ead884c40ff99147c6125889b814bd77b8ed68f420f5c3adb74433717e5d0",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "For training a specialized legal model you need detailed citations. Which fine\u2010tuning method supports injecting citations into outputs?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Instruction tuning with citation\u2010annotated examples",
      "B": "Domain adaptation on legal text only",
      "C": "Transfer learning on generic corpora",
      "D": "RLHF without examples"
    },
    "explanation": "Instruction tuning with explicit examples teaches citation patterns."
  },
  {
    "id": "1781d73c275eeecf88969b7c92a4fffae97c5dea79813312c0bf02258630a2aa",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "A media company wants to adapt an LLM to generate poetry in a niche style. They have 1k curated poems. Which approach?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Domain adaptation on large unlabeled text",
      "B": "Instruction tuning (few\u2010shot) with provided poems",
      "C": "RLHF with reader preferences",
      "D": "Continuous pre\u2010training on general poetry"
    },
    "explanation": "Instruction tuning on curated examples captures niche style effectively."
  },
  {
    "id": "dbaafddb384003501e3d70e9ccdf421b4ee7be283dd32c54f694c47bda157ef3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "Which risk is MOST associated with a foundation model undergoing continuous pre\u2010training on uncurated data?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Longer inference latency",
      "B": "Insufficient parameter updates",
      "C": "Introduction of bias and drift",
      "D": "Loss of base\u2010model capabilities"
    },
    "explanation": "Uncurated data can introduce unwanted biases and concept drift."
  },
  {
    "id": "191bc9a98011f8d970f17eb71b94ad25caea176f00812265621f7d5a87a35089",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "After instruction tuning, a model still fails a key business requirement 30% of the time. What is the MOST direct way to further optimize output quality?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase dataset size by adding raw text",
      "B": "Switch foundation model to a larger variant",
      "C": "Decrease training epochs",
      "D": "Apply RLHF with quality\u2010weighted feedback"
    },
    "explanation": "RLHF optimizes according to human\u2010rated quality metrics."
  },
  {
    "id": "906f942d32a8540335b1a57536ae6ce7886e7bc1e75ccbd0a42215fa8bec7b6b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "A foundation model fine\u2010tuning on SageMaker took days. You need faster iterations. Which practice speeds up experiments?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use full\u2010precision (FP32) training",
      "B": "Employ parameter\u2010efficient fine\u2010tuning methods",
      "C": "Remove early stopping",
      "D": "Increase dataset size"
    },
    "explanation": "Parameter\u2010efficient methods reduce compute and speed up tuning."
  },
  {
    "id": "abeeacfa3fc74a8591b6afcdcc45c0ac22acbdbda751b1919687978e5f4dd67a",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "Which dataset split is MOST appropriate for preventing data leakage in fine\u2010tuning evaluation?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "80% train, 20% train",
      "B": "90% train, 10% test (no validation)",
      "C": "Train, validation, test with disjoint IDs",
      "D": "Train on combined train/validation and test on same"
    },
    "explanation": "Disjoint splits avoid leakage and ensure true evaluation."
  },
  {
    "id": "788e1cf1f95c83a48805c1f1832db35429deeb071ccbcba34cb07abf1b430b92",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "A sensitive language model requires high transparency. Which customization step provides the best audit trail?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Record data lineage and fine\u2010tuning parameters",
      "B": "Use only instruction tuning",
      "C": "Skip continuous pre\u2010training",
      "D": "Rely solely on prompt engineering"
    },
    "explanation": "Versioning data and parameters ensures auditability."
  },
  {
    "id": "7a1fcbe591f18b9eee831ce1ceeb36dd481f61f0081aa1d6213ef1fa4c4e1afa",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "A model fine\u2010tuned via RLHF exhibits mode collapse. Which corrective action is most effective?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase batch size",
      "B": "Add more uncurated data",
      "C": "Diversify human feedback and reward coverage",
      "D": "Reduce feedback frequency"
    },
    "explanation": "Broadening feedback signals prevents collapse to narrow outputs."
  },
  {
    "id": "16aac0a18a5bafac5dd0af7875ed384c5b76f072782c8b8015d5783ec2a9fcf3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "Your team wants to reuse a fine\u2010tuning pipeline for multiple domains. Which architectural choice ensures modularity?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Embed all data in a single preprocessing script",
      "B": "Parameterize data pipelines per domain",
      "C": "Hard\u2010code model names",
      "D": "Use one monolithic training job"
    },
    "explanation": "Parameterization allows easy swapping of domain data."
  },
  {
    "id": "6c9114e9b77adeba95dca43f4d0b448d20e0e003525d610733af72eddb6267b0",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "Which fine\u2010tuning strategy best minimizes catastrophic forgetting of base knowledge?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Combined training on base and new data",
      "B": "Full fine\u2010tuning only on new data",
      "C": "Domain adaptation on new data alone",
      "D": "Instruction tuning after base training"
    },
    "explanation": "Mixing base and domain data retains original capabilities."
  },
  {
    "id": "ebda780871803a240c1691bb9e340dbacdda2d2527495c87871be77f9d6bbff0",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "You have limited human feedback for RLHF. Which technique improves the reward model training?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Remove low\u2010variance feedback",
      "B": "Train on raw unlabelled data",
      "C": "Increase model size",
      "D": "Apply data augmentation on feedback pairs"
    },
    "explanation": "Augmenting few feedback pairs increases diversity for reward training."
  },
  {
    "id": "c013c2f0d15f3d0af8d03e82f5ffbd251f08eed7059dfdc04381c0337e4e403a",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "A compliance audit demands record of each fine\u2010tuning run. Which AWS feature ensures full traceability?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Hyperparameter Tuning jobs",
      "B": "SageMaker Experiments and model registry",
      "C": "AWS CloudTrail only",
      "D": "SNS notifications"
    },
    "explanation": "SageMaker Experiments tracks run metadata and model versions."
  },
  {
    "id": "6899530c8cc90f4845d74c4766e002fcc3b51bd46de83b0931ba4ab903793fbc",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "A large multimodal foundation model is adapted to classify medical images. Which fine\u2010tuning method uses both image and text pairs?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Instruction tuning on text only",
      "B": "RLHF with clinician feedback",
      "C": "Multimodal transfer learning with paired datasets",
      "D": "Continuous pre\u2010training on image tags"
    },
    "explanation": "Multimodal transfer learning adapts model on both modalities."
  },
  {
    "id": "d05a30836acb090508731cdc059c98a879d9457d978703081eb20d9cf0d5d236",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "Your fine\u2010tuning budget is fixed. You need to decide dataset size vs label quality trade\u2010off. Which guideline is MOST appropriate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Prefer smaller high\u2010quality labelled data over larger noisy data",
      "B": "Always maximize dataset size",
      "C": "Label all data regardless of noise",
      "D": "Use only unlabelled data"
    },
    "explanation": "High\u2010quality labels have more impact than noisy large data."
  },
  {
    "id": "ede25274505f7d56e1243b1597097af6b7304784af7bb9597c1586944709c709",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.3",
    "stem": "During continuous pre\u2010training, model overfits to recent data. Which addition to pipeline mitigates this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase training epochs",
      "B": "Include regularization with replay of older data",
      "C": "Remove older data entirely",
      "D": "Decrease batch size"
    },
    "explanation": "Regular replay of old data prevents overfitting to new data."
  },
  {
    "id": "ec46185964bdf440b0a3940b88a83c2b267734ee85be92e8c49c4531b5f9bd82",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "A retail company fine-tunes a foundation model to summarize customer reviews. They notice that ROUGE scores are high, but human evaluators rate summaries as irrelevant. Which action MOST effectively diagnoses the issue?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase the beam width during inference to improve relevance.",
      "B": "Switch from ROUGE to BLEU to better capture summary quality.",
      "C": "Apply a larger prompt context window before generation.",
      "D": "Perform a human evaluation focused on summary faithfulness and compare against ROUGE components."
    },
    "explanation": "Comparing human faithfulness ratings with ROUGE sub-scores (e.g., ROUGE-L for longest common subsequence) reveals where automatic metrics diverge from human judgment, diagnosing the misalignment."
  },
  {
    "id": "30b8a77f0d81469a1c4beeef139566c8d84b93833454eccae72c3b4161e34798",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "An enterprise uses a foundation model for code generation. They benchmark on a public dataset and report BLEU-4 of 45. After deploying to production, user satisfaction drops. Which evaluation step could have predicted this drop?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Measuring perplexity on the public dataset.",
      "B": "Tracking model loss during fine-tuning.",
      "C": "Conducting scenario-based human evaluation with real developer prompts.",
      "D": "Comparing BLEU scores across different code languages."
    },
    "explanation": "Real-world developer prompts differ from benchmark data; scenario-based human evaluation reveals usability issues before deployment."
  },
  {
    "id": "6df44423afb3392e5aa57e4d8377d495574149fa1d18f94fb385885d633a2b01",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "A team uses BERTScore to evaluate translation quality, but it favors verbose outputs. What complementary metric should they include to penalize verbosity?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SARI",
      "B": "Length-penalized BLEU (BLEU with brevity penalty)",
      "C": "ROUGE recall",
      "D": "METEOR without synonym matching"
    },
    "explanation": "BLEU\u2019s brevity penalty discourages overly long translations, complementing BERTScore which does not penalize verbosity."
  },
  {
    "id": "c9e14d2b074e79d9910debcd739222ed8028d57c61c78a415738d2834b9c4be6",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "A chatbot powered by a foundation model shows high automated metric scores but low conversion rates. Which evaluation metric addresses this gap?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Online A/B testing measuring end-to-end conversion",
      "B": "Token-level accuracy on a held-out dataset",
      "C": "Self-BLEU to measure diversity",
      "D": "Embedding cosine similarity against training data"
    },
    "explanation": "Online A/B testing with conversion as the key metric ties model performance directly to business objectives, unlike offline metrics."
  },
  {
    "id": "a8212a1cb794e2cca693bcf68d2b8bc2db0f43a63b3957def9cb403a1420b707",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "During evaluation of a summarization foundation model, diversity of outputs is a priority. Which metric combination BEST captures both quality and diversity?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ROUGE-L and BLEU-4",
      "B": "BLEU-4 and perplexity",
      "C": "BERTScore and sequence length variance",
      "D": "ROUGE for quality and Self-BLEU for diversity"
    },
    "explanation": "ROUGE assesses quality against references, while Self-BLEU measures diversity by evaluating overlap among generated summaries."
  },
  {
    "id": "8e2be27e48ec41a80fc2412fe7492957ec029db0dd2aaee0487b2e88ec2d7808",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "A finance company fine-tunes a model to generate risk analysis reports. They must ensure factual accuracy. Which evaluation approach is MOST appropriate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Compute ROUGE-F1 against sample reports.",
      "B": "Use a domain expert human evaluation with a fact-checking rubric.",
      "C": "Measure perplexity on a financial corpus.",
      "D": "Evaluate BLEU against generic report templates."
    },
    "explanation": "Only domain experts can reliably assess factual accuracy in risk reports; automated metrics cannot detect subtle errors."
  },
  {
    "id": "5864b0ab5e7e4ab66e51a4711220e043d0bed157235e6b0ba73552c0e72b6e59",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "A team compares two foundation models for dialogue generation using BLEU score. The model with higher BLEU yields more generic responses. What evaluation change addresses this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase training data size.",
      "B": "Switch to ROUGE-L metric.",
      "C": "Include human evaluation of response informativeness.",
      "D": "Add length penalty to BLEU."
    },
    "explanation": "Human evaluation of informativeness identifies generic but high-BLEU outputs, guiding selection of the model that better satisfies user needs."
  },
  {
    "id": "1a41ac98f6f3d2924414ae273ebbc037d24cb53cd9342443497b15b35e4e39ba",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "A model yields high BERTScore but low user engagement. Which additional metric or process should be introduced?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Task completion rate in an interactive user study",
      "B": "Perplexity measured on training data",
      "C": "ROUGE recall",
      "D": "BLEU-1 to check unigram overlap"
    },
    "explanation": "Task completion rate in actual interactions ties performance to user engagement, uncovering issues automated scores miss."
  },
  {
    "id": "d9a9414b7e98a7af137aad3fdf68725bb35f9125c143fc1fd8f0db2d6414ec8c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "An LLM-based summarization service is evaluated using ROUGE. Reports show high recall but low precision. What does this imply and how can it be improved?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Summaries are too short; increase summary length.",
      "B": "Summaries include too much irrelevant content; implement extractive filtering.",
      "C": "Models are underfitting; increase training epochs.",
      "D": "ROUGE is unreliable; switch to BLEU."
    },
    "explanation": "High recall/low precision indicates summaries cover all reference content but add noise; extractive filtering removes irrelevant parts, improving precision."
  },
  {
    "id": "e39d9f433b1fa77428806a1ba09192587fafbd15c356e942368673e65b83f557",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "A model\u2019s BLEU score is stable but user satisfaction drops after fine-tuning. Which evaluation should have been run pre-deployment?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Compute higher n-gram BLEU (BLEU-5)",
      "B": "Measure BLEU on a larger validation set",
      "C": "Conduct blind human evaluations comparing pre- and post-fine-tuning outputs",
      "D": "Check perplexity on fine-tuning data"
    },
    "explanation": "Blind human evaluations directly compare outputs to detect declines in quality that BLEU alone may not reveal."
  },
  {
    "id": "5dcf81117c7ccefba3f492114038f5e5653eddf4233c0b972c26a2ed2eacae57",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "When evaluating a generation model across multiple languages, which metric combination balances adequacy and fluency?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "BLEU for adequacy and perplexity for fluency",
      "B": "ROUGE for fluency and BERTScore for adequacy",
      "C": "Self-BLEU for consistency and BLEU for adequacy",
      "D": "METEOR for adequacy and language-specific perplexity for fluency"
    },
    "explanation": "METEOR accounts for synonyms and alignments indicating adequacy, while perplexity per language measures fluency in that language."
  },
  {
    "id": "c6c4bf893128448e35e825458c7f60489a06463829071152f41054a7d838a163",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "A company uses a benchmark dataset to evaluate a foundation model but suspects data leakage. Which approach BEST uncovers leakage?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Compute overlap of n-grams between training and test sets.",
      "B": "Re-run evaluation with a different metric.",
      "C": "Increase the test set size.",
      "D": "Shuffle the test data."
    },
    "explanation": "N-gram overlap analysis reveals if test examples are too similar to training data, indicating potential leakage."
  },
  {
    "id": "b621ae9ecbaf96cf7e4204f38caf9fa6a041c9c29cf5f4c2163aff55e3b7dd64",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "During evaluation, a summary model\u2019s ROUGE scores vary significantly across topics. What evaluation strategy addresses this variability?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use BLEU instead of ROUGE.",
      "B": "Segment evaluation by topic and report per-topic metrics.",
      "C": "Aggregate scores across all topics only.",
      "D": "Increase model size to reduce variability."
    },
    "explanation": "Per-topic metrics identify domains where performance lags, guiding targeted improvements."
  },
  {
    "id": "6d7120fd78ceead77d9b4980eafd05f8606d491dde0427992f6bfaa15a9c08f3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "A model generates financial recommendations. Business requires explanations. Which evaluation method assures explanation quality?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Compute BERTScore on explanation tokens.",
      "B": "Use perplexity to assess explanation fluency.",
      "C": "Conduct human evaluation with a rubric for explanation completeness and correctness.",
      "D": "Measure explanation length."
    },
    "explanation": "Only human evaluators using a rubric can reliably assess the substance and correctness of model-generated explanations."
  },
  {
    "id": "f0be34444460845d38816d20094ba50c26763ca6a470a0bb626da20f106aa56b",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "A foundation model returns biased generation for certain demographics. Which evaluation process best uncovers this bias?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Check perplexity across demographics.",
      "B": "Compute aggregate BLEU scores across the dataset.",
      "C": "Use Self-BLEU for diversity.",
      "D": "Perform targeted human evaluation on demographic-specific prompts."
    },
    "explanation": "Targeted human evaluation reveals bias in outputs across demographic groups, which automated metrics may not detect."
  },
  {
    "id": "2b65a802273e37c656a632bd2046ab7f6543169acbabd66d990f43eb812e36df",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "You need to evaluate grounding in RAG-enhanced generation. Which approach measures ground-truth consistency?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Compute ROUGE against retrieval documents only.",
      "B": "Use factuality metrics or human fact-checking against source documents.",
      "C": "Measure Self-BLEU against previous answers.",
      "D": "Evaluate token perplexity conditional on retrieved context."
    },
    "explanation": "Factuality metrics or human fact-checking directly assess whether generated content is consistent with retrieved source documents."
  },
  {
    "id": "13ab1739d90067fd4a189fcf4c21a55491162968a0f4ae5f6f0da3c3e50f59c8",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "A document QA system for legal files reports high EM (Exact Match) but low user trust. What evaluation should be added?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Human evaluation of answer justification coherence.",
      "B": "Compute BLEU against answer keys.",
      "C": "Increase EM to higher n-grams.",
      "D": "Measure Self-BLEU for consistency."
    },
    "explanation": "Human assessment of justification coherence evaluates trustworthiness beyond exact matching of answers."
  },
  {
    "id": "f0266b27576aa1efc4d79dea126b8e5f17b493d718845944eb572caf21c8aadd",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "An AI agent orchestrating multiple foundation models must be evaluated holistically. Which method captures end-to-end task success?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Compute average BLEU per subtask.",
      "B": "Use combined ROUGE-BLEU macro score.",
      "C": "Define and measure a multi-step task completion metric in simulation.",
      "D": "Measure cumulative perplexity."
    },
    "explanation": "A task completion metric that tracks successful execution of all steps in simulation evaluates orchestration performance end to end."
  },
  {
    "id": "3a04b6dc140557e1a9bf7b5ec6a94eb54c13ed7e4c0b961d248f6025149f326d",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "During offline evaluation of a model for news article generation, automated metrics correlate poorly with editorial ratings. What is the root cause?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Test set too small.",
      "B": "Metrics (ROUGE/BLEU) emphasize n-gram overlap rather than style and coherence.",
      "C": "Training data imbalance.",
      "D": "Inference temperature too high."
    },
    "explanation": "ROUGE/BLEU focus on overlap, failing to capture narrative style and coherence valued by editors."
  },
  {
    "id": "27424ec230102ebdb45b762e5112342cf404f0bf84962b59182407ec22df5315",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "A cross-domain foundation model shows varied performance on different benchmarks. How can you fairly compare its overall performance?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Average raw scores across benchmarks.",
      "B": "Use the highest score from any benchmark.",
      "C": "Evaluate on only one benchmark domain.",
      "D": "Normalize scores within each benchmark and compute a weighted aggregate."
    },
    "explanation": "Normalization accounts for differing score ranges across benchmarks, allowing fair aggregated comparison."
  },
  {
    "id": "67167e8ba0606bf6e8321d9ff9c7a0d6fbe5b7cb40c6f781c4080a4ebaa9f2f6",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "You want to automate ongoing evaluation of a deployed LLM summary service. Which pipeline component is essential?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Scheduled sampling of production summaries with periodic human or crowdsource review.",
      "B": "Real-time BLEU computation on user-provided inputs.",
      "C": "Automatic ROUGE computation against live user feedback.",
      "D": "Self-BLEU to measure production diversity."
    },
    "explanation": "Scheduled human review of sampled production outputs is necessary for reliable ongoing evaluation, as automated metrics alone are insufficient."
  },
  {
    "id": "ba7d1cc46dedd622a8eb6c346dd80afa516b0713f9dcaa2ed091a0631e228325",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "A team assesses hallucinations by measuring overlap between output and source documents. Which automated metric can help?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ROUGE-L recall",
      "B": "QAEval (question-answering based factuality)",
      "C": "Self-BLEU",
      "D": "Perplexity"
    },
    "explanation": "QAEval uses QA over source documents to check factual consistency, detecting hallucinations automatically."
  },
  {
    "id": "51baf2ca8e2665e37cd2acdf1939dcda9f6bf294cbe22d0a3d8ae5ea65b94e8a",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "An LLM generates marketing copy. Which evaluation metric aligns best with expected business KPIs?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Click-through rate measured via A/B testing.",
      "B": "BLEU against a reference ad copy.",
      "C": "ROUGE-F1 against training examples.",
      "D": "Perplexity on marketing dataset."
    },
    "explanation": "Click-through rate directly measures the business goal of marketing copy performance, aligning evaluation with KPIs."
  },
  {
    "id": "d3e8d7101f8d4c117031ac0743b363b46b2a4303e3e700f724a5c32b363e4581",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "When evaluating multilingual summarization, the team must ensure language fairness. Which evaluation strategy ensures no language is underrepresented?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Compute aggregate ROUGE across all languages.",
      "B": "Use English reference translations only.",
      "C": "Maintain per-language test sets with equal sample sizes and report metrics separately.",
      "D": "Use a single English-centric metric for all languages."
    },
    "explanation": "Equal per-language test sets and separate reporting detect performance disparities, promoting fairness."
  },
  {
    "id": "3dd3770a535438bb06de7bd0a0720f9fb71692b203b393c4bce8dbe6322de5a2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "A foundation model exhibits high metric scores on public benchmarks but fails on proprietary data. What evaluation practice could have prevented this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Include proprietary hold-out data in the evaluation benchmark.",
      "B": "Increase public benchmark size.",
      "C": "Compute Self-BLEU on public benchmarks.",
      "D": "Measure perplexity on public data."
    },
    "explanation": "Including proprietary data in evaluation ensures the model is tested on domain-specific content, preventing overreliance on public benchmarks."
  },
  {
    "id": "19cf0d2780f41aa1887e37a0648188275a1c1bcd906fce4303e25108ba118ccc",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "Your team measures BLEU-4 for a poetry generator but suspects it penalizes creativity. Which metric or strategy addresses this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use BLEU-2 instead.",
      "B": "Increase reference set size to many poems.",
      "C": "Measure perplexity instead of BLEU.",
      "D": "Adopt human evaluation of creativity and stylistic quality."
    },
    "explanation": "Human evaluation is required for subjective qualities like creativity, which BLEU cannot capture adequately."
  },
  {
    "id": "470bf00d307b7f8ee71b6a58c1c677fd027537b3c79d4057bcb85a9b61582777",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "You must evaluate a model\u2019s responsiveness to user instructions in few-shot prompts. Which evaluation method verifies instruction compliance?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Measure n-gram recall against instruction text.",
      "B": "Human evaluation scoring compliance and correctness of responses.",
      "C": "Compute ROUGE-LF against instruction examples.",
      "D": "Use perplexity conditioned on instruction tokens."
    },
    "explanation": "Human evaluation directly assesses whether model outputs follow instructions and are correct, which automated metrics cannot reliably do."
  },
  {
    "id": "b082fd718f60dd434ea4ad62f82f1477729aad40c4fe1f08a215287117363d6d",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "A generative model\u2019s performance is evaluated with a single reference per input. What is a key risk and how to mitigate it?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Overfitting to reference; reduce training epochs.",
      "B": "Underestimating recall; increase beam size.",
      "C": "Lower metric reliability due to limited references; use multiple diverse references or human evaluation.",
      "D": "Inflated BLEU scores; apply brevity penalty."
    },
    "explanation": "Single references limit metric reliability; adding multiple references or human evaluation improves reliability and fairness."
  },
  {
    "id": "ed888c8f520c89480c9cd199d90788d657293e0ca200e67d7be8e1dcf0de1224",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "An LLM-based search assistant\u2019s relevance is evaluated by Precision@5 but gives poor recall. Which complementary metric provides a fuller view?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Recall@5",
      "B": "ROUGE-L",
      "C": "BLEU-1",
      "D": "Perplexity"
    },
    "explanation": "Recall@5 measures the proportion of relevant documents retrieved, complementing Precision@5 for balanced evaluation."
  },
  {
    "id": "dafa6107d571b41114070b79de16730ae75bdc2bc267e840c90897d161ec8581",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "3.4",
    "stem": "A team logs user interactions to evaluate a deployed chatbot. Which observed metric best indicates dialogue coherence?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Average response length",
      "B": "Turn-taking frequency",
      "C": "Conversation success rate (user achieves goal without mid-dialog corrections)",
      "D": "BLEU score against template responses"
    },
    "explanation": "Conversation success rate reflects coherence and goal achievement in dialogue, which automated metrics do not capture."
  },
  {
    "id": "a6567b6e259150a459fc27007027a8afd2e70b5fae3de9b1e530b8707b5a22f6",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "A financial institution uses an ML model to screen loan applications and discovers that applicants from a specific demographic group are disproportionately rejected. Which responsible AI principle is most directly violated?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Fairness \u2013 the model is treating demographic groups unequally.",
      "B": "Transparency \u2013 the model\u2019s decision logic is hidden.",
      "C": "Robustness \u2013 the model cannot handle noisy data.",
      "D": "Safety \u2013 the model is causing harm to users."
    },
    "explanation": "Disparate treatment of a demographic group indicates a fairness violation, as responsible AI demands equal outcomes across groups."
  },
  {
    "id": "5baea73c1e7a6244a6f7a895fae5d9a5c0011ead6a8ebd7efe540ac7c3f517db",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "An HR recruiter deploys a resume\u2010screening model that filters out all non\u2010English resumes, thereby excluding qualified multilingual candidates. Which responsible AI feature is lacking?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Inclusivity \u2013 the model fails to consider diverse candidate backgrounds.",
      "B": "Veracity \u2013 the model is providing inaccurate screening.",
      "C": "Robustness \u2013 the model cannot process different document formats.",
      "D": "Safety \u2013 the model is exposing sensitive information."
    },
    "explanation": "Excluding non\u2010English resumes indicates a lack of inclusivity, as a responsible AI system should serve diverse users."
  },
  {
    "id": "c6505827a310ac40b54930f18929401f896282c302783a872f53d70efd1472ae",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "A self\u2010driving car\u2019s perception model performs well in clear weather but fails in foggy conditions. Which responsible AI characteristic should be improved?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Fairness \u2013 ensure equal performance across users.",
      "B": "Transparency \u2013 explain the model\u2019s failure mode.",
      "C": "Robustness \u2013 maintain performance under varied conditions.",
      "D": "Veracity \u2013 avoid generating false sensor readings."
    },
    "explanation": "Robustness refers to consistent performance under different real\u2010world conditions; failure in fog indicates a robustness issue."
  },
  {
    "id": "f7cd7347b3377c16114ff2e4642e9fd658e386fbbaca9e3bc93ee687a5ff9dd8",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "A chatbot powered by a foundation model confidently fabricates statistics in its responses. Which responsible AI principle is compromised?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Safety \u2013 the model must avoid harmful outputs.",
      "B": "Fairness \u2013 the model must treat user queries equally.",
      "C": "Robustness \u2013 the model must handle adversarial prompts.",
      "D": "Veracity \u2013 the model\u2019s outputs must be truthful and accurate."
    },
    "explanation": "Veracity is about truthfulness and accuracy; hallucinated statistics violate veracity."
  },
  {
    "id": "adc1157cf684128417d132ee78572881b48949abde42690e792d8c06b436a42c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "A medical advice AI system suggests a treatment that poses health risks to users. Which responsible AI attribute has been neglected?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Fairness \u2013 equal recommendation quality.",
      "B": "Safety \u2013 avoiding harm to end users.",
      "C": "Inclusivity \u2013 serving diverse patient groups.",
      "D": "Veracity \u2013 providing only factual information."
    },
    "explanation": "Safety focuses on preventing harm; recommending risky treatments shows a safety lapse."
  },
  {
    "id": "2e619b9b63dc12d77f1becbce1d8cd90bfdfa4deb051afdedcdf63925069ede6",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "Your team needs to enforce content policies (e.g., block hate speech) and prevent hallucinations in responses from a foundation model. Which AWS feature should you use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS WAF \u2013 protects web applications only.",
      "B": "Amazon SageMaker Model Monitor \u2013 monitors metrics post\u2010deployment.",
      "C": "Amazon Bedrock Guardrails \u2013 enforces input/output policies in real time.",
      "D": "Amazon Macie \u2013 discovers and protects sensitive data in S3."
    },
    "explanation": "Bedrock Guardrails allows you to define input/output policies to block disallowed content and reduce hallucinations in generative AI."
  },
  {
    "id": "a307ee7150cb0f9b19ce9ca4a1104345e8fa49713c0a3cc4c8a0e6791e16400e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "To trigger an alert whenever a Bedrock Guardrails policy is violated, which AWS service should you integrate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon CloudWatch Events \u2013 can detect and alert on Guardrails events.",
      "B": "AWS Config \u2013 tracks configuration changes, not real\u2010time guardrail events.",
      "C": "AWS CodePipeline \u2013 orchestrates CI/CD, not monitoring.",
      "D": "Amazon GuardDuty \u2013 detects threats, not policy violations in LLM responses."
    },
    "explanation": "CloudWatch Events can be configured to catch Bedrock Guardrails violations and send alerts or trigger workflows."
  },
  {
    "id": "b98e36a6a667537b681b3daf167125a2860a4a50a255bec5a8c4eaedbe2192dd",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "You want to validate user input against allowed schemas before sending it to a foundation model and reject malformed requests. Which component of Bedrock Guardrails addresses this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Output guard \u2013 sanitizes model responses.",
      "B": "Input guard \u2013 enforces schema/patterns before invocation.",
      "C": "Model selector \u2013 chooses which model to use.",
      "D": "Fallback mechanism \u2013 handles service outages."
    },
    "explanation": "Input guards are designed to validate and reject unauthorized or malformed inputs before invoking the model."
  },
  {
    "id": "db2797eefe7d002ff6d04f0805990fffabdbf190193cec6255ca716f7527e5ed",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "You need to automatically mask profanity in generative responses without modifying your application code. Which Bedrock Guardrails feature do you configure?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Input guard \u2013 stops profanity at input stage.",
      "B": "IAM policy \u2013 controls access, not content.",
      "C": "Model selector \u2013 picks a sanitized model.",
      "D": "Output guard \u2013 transforms or filters responses to remove profanity."
    },
    "explanation": "Output guards can apply transformations or filters to model-generated text, such as masking profanity."
  },
  {
    "id": "483130ab7e79c2b4a83521e9ee00db39d61593ed7d576d0fc2d6d1867e3e4e14",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "To audit all calls made to your Bedrock foundation models and any guardrail violations for compliance, which AWS service provides an immutable record?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon CloudWatch Logs \u2013 useful but not immutable.",
      "B": "AWS Config \u2013 captures resource configs, not API calls.",
      "C": "AWS CloudTrail \u2013 records API calls and guardrail events immutably.",
      "D": "Amazon X-Ray \u2013 traces requests but not policy violations."
    },
    "explanation": "CloudTrail records all API calls, including Bedrock invocations and guardrail events, for auditing."
  },
  {
    "id": "0c771f087db974c06e68a85d1c704c50e6b4fb1c4646394eca26b60c78135179",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "A company must choose between a large foundation model and a parameter\u2010efficient adapter layer approach to minimize carbon emissions. Which choice best meets sustainability goals?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use the largest available model for higher accuracy.",
      "B": "Implement parameter-efficient fine-tuning with a smaller base model.",
      "C": "Train the model longer to improve performance.",
      "D": "Host models in on-premises data centers to control cooling."
    },
    "explanation": "Parameter-efficient fine-tuning of a smaller base model substantially reduces compute and hence carbon footprint compared to large full-model training."
  },
  {
    "id": "445062d29145365898b006114a04289af31d813a3fcd88b7e043c1fce19c7536",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "To reduce the energy consumption of inference in production, which technique should you apply?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Quantize the model to lower-precision (e.g., INT8) operations.",
      "B": "Execute inference in real time only.",
      "C": "Increase batch size to 1 for latency improvement.",
      "D": "Fine-tune the model with more data."
    },
    "explanation": "Quantization reduces compute and memory requirements, lowering energy consumption during inference."
  },
  {
    "id": "28ba6e33f8bc2806906b775b4185415669a861d64b4f36ac9f88701ce3850515",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "A team is deciding between AWS Graviton-based instances and GPU instances for large-scale model training. Which choice improves energy efficiency?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Graviton-based instances for CPU-optimized, energy-efficient training.",
      "B": "Choose the highest-end GPU for maximum throughput.",
      "C": "Run training on on-premises hardware to reduce cloud usage.",
      "D": "Use burstable instances to save cost only."
    },
    "explanation": "Arm-based Graviton processors deliver better performance per watt for many ML workloads, reducing energy usage."
  },
  {
    "id": "29fe85a77348dbc793ab3bbce8fb99c023b1591efa2e36fe53364ce55ba496f1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "Scheduling batch inference jobs during off-peak hours to take advantage of a lower PUE (power usage effectiveness) is an example of which sustainability practice?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model quantization \u2013 reduces model size.",
      "B": "Workload scheduling \u2013 aligns compute with grid efficiency.",
      "C": "Using renewable energy certificates \u2013 purchases carbon offsets.",
      "D": "Data pruning \u2013 reduces dataset size."
    },
    "explanation": "Scheduling workloads when the data center\u2019s PUE is lower minimizes overall energy overhead, a key sustainability tactic."
  },
  {
    "id": "9321989fa30f901458838043675fa87d797dc300fa9580d3a6862a584591930f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "A business wants to minimize its AI carbon footprint by selecting an AWS region. Which factor should guide their choice?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Lowest network latency for user traffic only.",
      "B": "Region with all available instance types.",
      "C": "Region with deepest discount pricing.",
      "D": "Region with higher renewable energy usage on the local grid."
    },
    "explanation": "Regions powered by cleaner energy sources reduce the carbon footprint of compute workloads."
  },
  {
    "id": "a18d8433d38651d488194b500a1aa8702b1ba87d434591bec83402b73e668600",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "An ML team fine\u2010tunes a foundation model using unlicensed internet text, ignoring copyright constraints. Which legal risk are they exposed to?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Intellectual property infringement \u2013 unauthorized use of copyrighted content.",
      "B": "Model drift \u2013 performance degradation over time.",
      "C": "Data poisoning \u2013 malicious training inputs.",
      "D": "Overfitting \u2013 too specific to training data."
    },
    "explanation": "Using copyrighted text without permission risks IP infringement and potential legal action."
  },
  {
    "id": "732c5845ec7b0f5718368bf0f9373f8b824c182da95d6413224e02a90fb918ba",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "A chatbot trained on scraped forum posts inadvertently reveals personal user data. Which legal/compliance risk does this represent?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Hallucinations \u2013 fabricating facts.",
      "B": "IP infringement \u2013 misusing copyrighted content.",
      "C": "Privacy violation \u2013 exposing personal identifiable information.",
      "D": "Overfitting \u2013 echoing training examples."
    },
    "explanation": "Revealing PII from training data breaches privacy regulations and violates data protection laws."
  },
  {
    "id": "e5013217ba36b970c475006a4f7142052e1f52120d9154ecd60e7f7375e257e0",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "A lender\u2019s credit-scoring model shows statistically significant bias against a protected group, undermining consumer trust and inviting regulatory scrutiny. Which risk category best describes this issue?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Safety risk \u2013 potential physical harm.",
      "B": "Customer trust loss \u2013 reputational and compliance risk.",
      "C": "Sustainability risk \u2013 environmental impact.",
      "D": "Latency risk \u2013 slow response times."
    },
    "explanation": "Biased outcomes erode customer trust and invite legal/regulatory consequences, a customer trust risk."
  },
  {
    "id": "4d935d23005efc54c76ad8d96e77867d10b32fcb937b4281f68fe633b66a8ec9",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "An insurance chatbot confidently gives incorrect policy advice that leads to financial harm. Which specific risk from responsible AI does this example illustrate?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Data poisoning \u2013 corrupted training data.",
      "B": "Class imbalance \u2013 skewed training labels.",
      "C": "Drift \u2013 changing data distributions.",
      "D": "Hallucinations \u2013 inaccurate outputs presented as fact."
    },
    "explanation": "Confidently incorrect advice from a generative model is a hallucination, risking legal liabilities."
  },
  {
    "id": "5e13a1f9ef8d84291d2e01565625591a3935ccdda7ba05c1ad4540e28108d293",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "A healthcare application\u2019s language model suggests off-label drug uses without evidence, exposing the company to legal liability. Which risk type is this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Bias risk \u2013 unfair treatment across populations.",
      "B": "Sustainability risk \u2013 high energy usage.",
      "C": "End\u2010user risk \u2013 harm from incorrect guidance.",
      "D": "Veracity risk \u2013 lack of model explainability."
    },
    "explanation": "Incorrect medical guidance poses direct harm to end users, an end\u2010user risk under responsible AI."
  },
  {
    "id": "1d9114d48b26233974905f6bb4c0962f789a43fc5ef84cea95a5b548ac23ee04",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "A computer vision dataset for facial recognition lacks images of darker skin tones, resulting in higher error rates for certain groups. Which dataset characteristic is missing?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Curation \u2013 removing low-quality images.",
      "B": "Diversity \u2013 including varied skin tones.",
      "C": "Balanced classes \u2013 equal count for each class.",
      "D": "Inclusivity \u2013 serving all age groups."
    },
    "explanation": "Lack of varied skin tones indicates poor diversity, leading to biased performance."
  },
  {
    "id": "59429a3ec498b275086cced17b594588df4fa5605a07d28b36876e9686fa96ec",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "Your image classification dataset contains duplicates, mislabeled entries, and missing annotations. Which dataset characteristic issue does this describe?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Inclusivity \u2013 unsuitable for demographic fairness.",
      "B": "Diversity \u2013 missing variation of classes.",
      "C": "Curation \u2013 poor data quality and organization.",
      "D": "Balance \u2013 unequal class representation."
    },
    "explanation": "Duplication and mislabels reflect poor data curation practices, harming model quality."
  },
  {
    "id": "b86ecc137754d6158ee461aee4b73b3f00b78dc133ca62124b19521b523d5870",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "A binary classification dataset has 95% of examples in one class and 5% in the other. What dataset property should you address?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Inclusivity \u2013 demographic representation.",
      "B": "Curation \u2013 data cleanliness.",
      "C": "Diversity \u2013 feature variation.",
      "D": "Class imbalance \u2013 skewed target distribution."
    },
    "explanation": "A 95/5 split indicates severe class imbalance, which can bias model predictions toward the majority class."
  },
  {
    "id": "ce69f6d0ce8bf582e3e36febb7eb3d0a796ca705932088d90fb1090b07986787",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "A global voice assistant is trained exclusively on North American English accents. Which dataset characteristic is lacking?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Inclusivity \u2013 support for diverse accents and dialects.",
      "B": "Balance \u2013 equal number of male/female voices.",
      "C": "Curation \u2013 removal of background noise.",
      "D": "Veracity \u2013 truthful responses."
    },
    "explanation": "Excluding non\u2013North American accents shows lack of inclusivity, leading to poor user experience globally."
  },
  {
    "id": "cdee1ac0de9e8c57fafeb99e30f80bc81a2944bc0df22475562637b849a973db",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "Before training, you resample your dataset so each class reflects real\u2010world population proportions. Which property does this ensure?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Diversity \u2013 varied feature space coverage.",
      "B": "Inclusivity \u2013 demographic fairness.",
      "C": "Balance \u2013 representative class distribution.",
      "D": "Sustainability \u2013 efficient data usage."
    },
    "explanation": "Resampling to real\u2010world proportions addresses class balance, ensuring representative distribution."
  },
  {
    "id": "bc5ebef197e5f5d37740e73ac56f7b797cacd95498b188a5e399d8402d87e73f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "Your model achieves 99% accuracy on training data but only 60% on validation data. Which bias/variance issue does this indicate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High bias \u2013 underfitting the data.",
      "B": "High variance \u2013 overfitting to the training set.",
      "C": "Data drift \u2013 changing data distribution over time.",
      "D": "Concept drift \u2013 evolving relationship between features and labels."
    },
    "explanation": "A large gap between train and validation accuracy indicates high variance (overfitting)."
  },
  {
    "id": "ef661991603cf8f94ee834df176eb6d2093d640a7d0bfe54bd342db7b9b5a40a",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "To detect demographic bias in model predictions post\u2010deployment, which AWS service should you use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon SageMaker Clarify \u2013 analyzes bias and feature attributions.",
      "B": "Amazon SageMaker Model Monitor \u2013 monitors data drift only.",
      "C": "Amazon A2I \u2013 human-review workflow for low-confidence cases.",
      "D": "AWS Config \u2013 monitors resource configurations."
    },
    "explanation": "SageMaker Clarify provides bias detection and feature importance analysis across defined groups."
  },
  {
    "id": "406a57d0ef268168b0cfad1e03ab3c6ee6a396032ce5553b93e70784f1ff77a4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "You want to continuously track fairness metrics (e.g., false-positive rates across groups) in production. Which AWS tool is best?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Clarify \u2013 batch analysis only.",
      "B": "SageMaker Model Monitor \u2013 continuous real\u2010time monitoring of metrics.",
      "C": "Amazon A2I \u2013 human review only.",
      "D": "AWS CloudTrail \u2013 API audit logs, not metrics."
    },
    "explanation": "Model Monitor can track custom metrics, including fairness, in real time."
  },
  {
    "id": "687cd6b51cff1e918259641616f5776335602fd751a9d6ad3020b20ebf7a7e2f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "To route low-confidence or high-risk predictions for human review, which AWS service should you integrate?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "SageMaker Clarify \u2013 bias detection but no routing.",
      "B": "SageMaker Model Monitor \u2013 metrics tracking only.",
      "C": "Amazon Augmented AI (A2I) \u2013 orchestrates human reviews.",
      "D": "Amazon Rekognition \u2013 only for image analysis."
    },
    "explanation": "Amazon A2I allows you to define human-review workflows for model outputs based on confidence or policy criteria."
  },
  {
    "id": "faf7a75287834da2b8c43f687a77c564e46a619253bab66491ce8a2ad0301870",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.1",
    "stem": "After running SageMaker Clarify, you observe that a feature\u2019s importance differs significantly across demographic groups. Which concept does this reveal?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Data drift \u2013 change in feature distributions over time.",
      "B": "Overfitting \u2013 model too complex.",
      "C": "Variance \u2013 model instability across datasets.",
      "D": "Disparate impact \u2013 unequal model behavior across groups."
    },
    "explanation": "Differing feature importance across groups indicates disparate impact, a form of bias requiring mitigation."
  },
  {
    "id": "e831b2e65c3f97266ca93568a51eaf539bb59291e1a31f013596b1370692b94f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "A healthcare startup must deploy a model to predict patient readmission risk. Regulators require clear rationale for each prediction. Which model choice best balances accuracy and explainability?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A deep ensemble of neural networks with gradient-based saliency maps",
      "B": "A random forest with thousands of trees and SHAP explanations",
      "C": "A shallow decision tree pruned for clarity with feature importance annotations",
      "D": "A support vector machine with RBF kernel and LIME explanations"
    },
    "explanation": "A pruned decision tree yields inherently transparent rules, satisfying regulators with minimal complexity, whereas complex ensembles or SVMs remain opaque despite post hoc methods."
  },
  {
    "id": "640c37c977a45a40e710a89de19cdf50e5848a0224af047b5c3b383a7f448bfc",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "An e-commerce company uses SageMaker Model Cards for several models. A risk audit requires assessment of data lineage and bias evaluation. Which section of the Model Card provides this information?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Quantitative analysis section detailing performance metrics",
      "B": "Training data section documenting sources, preprocessing, and bias checks",
      "C": "Usage recommendations section explaining inference patterns",
      "D": "Hyperparameter section listing tuning configurations"
    },
    "explanation": "The training data section of a Model Card records data provenance and any bias analyses, which is needed for risk and lineage audits."
  },
  {
    "id": "27d5d94b623591ed16780658beb9bd8de607a958e71c9e077ac5b43a4b482287",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "A finance firm must choose a pre-trained NLP model under an open-source license that allows modification and internal transparency. Which license attribute ensures the firm can inspect and modify the model code?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Proprietary commercial license with source code escrow",
      "B": "Non-commercial Creative Commons license",
      "C": "End-user license agreement without vendor source access",
      "D": "OSI-approved permissive license (e.g., Apache 2.0)"
    },
    "explanation": "An OSI-approved permissive license like Apache 2.0 grants rights to view, modify, and redistribute source code, meeting transparency needs."
  },
  {
    "id": "09a601c4cd6beb001c4e37dcdac492c117dd9ac20659da59b85c97752f37181f",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "A user interface team requires explanations in layman\u2019s terms for model predictions. Which human-centered design principle should guide the explanation content?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Contextualizing model output in domain-specific language familiar to end users",
      "B": "Providing raw feature importance scores without interpretation",
      "C": "Including advanced mathematical derivations for transparency",
      "D": "Displaying performance metrics like precision and recall"
    },
    "explanation": "Contextualizing predictions using common domain terminology aligns explanations with users\u2019 mental models, improving comprehension."
  },
  {
    "id": "83a757454b54da3fa60f6f08717a5bc565089efc36b149250f4fa396ebe36ba4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "A model\u2019s Model Card indicates high accuracy but warns of demographic bias in certain groups. Which step is most appropriate before deployment?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Ignore the warning because accuracy is sufficient",
      "B": "Retrain or tune preprocessing to balance representation before deploying",
      "C": "Deploy and monitor only high-confidence predictions",
      "D": "Apply ensemble averaging to mask bias"
    },
    "explanation": "Addressing bias through data balancing or preprocessing is required to mitigate demographic disparities rather than ignoring or masking them."
  },
  {
    "id": "7e6a1db111ffbcaaf961d58e1a7f36c8f7ddc619345b6b83b0b3a5af2aad57cb",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "A bank must explain credit decisions to customers. Which model explanation technique provides individual-level counterfactual examples?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Partial dependence plots",
      "B": "Global SHAP summary plots",
      "C": "LIME local feature attribution",
      "D": "Counterfactual explanation generation"
    },
    "explanation": "Counterfactual explanations show how input changes alter decisions at the individual level, supporting regulatory right-to-explanation."
  },
  {
    "id": "0df6087ba6ec3e4dc052edf069e901e1d88fc82d48c3e6e16ed125f00046c34e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "An operations team debates using a black-box model with high throughput vs. an interpretable white-box model with lower throughput. What trade-off should they document?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Interpretability vs. performance in latency-critical applications",
      "B": "Accuracy vs. model complexity when performance metrics are equal",
      "C": "Cost vs. regulatory compliance if throughput varies",
      "D": "Scalability vs. maintenance overhead"
    },
    "explanation": "They need to weigh interpretability against latency/performance, especially if auditability is required in real time."
  },
  {
    "id": "c893843eeb321739eef8d76c33ba334d104cd28bc79fd1254645a86a86e18730",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "Which AWS tool can be used to generate global and local feature attributions to improve model transparency?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Model Monitor",
      "B": "SageMaker Clarify",
      "C": "SageMaker Autopilot",
      "D": "Amazon A2I"
    },
    "explanation": "SageMaker Clarify provides built-in methods for global and local feature attribution, aiding transparency analyses."
  },
  {
    "id": "37ae470f5af878b1ec6cd960a63289be97a5b1373fb75198d18059256e6c11e8",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "A text\u2010classification model gives reasons like \u201cword occurrence frequency\u201d to end users. Which explanation method does this represent?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Intrinsic explanations using model-internal weights",
      "B": "Post hoc example\u2010based explanations",
      "C": "Surrogate decision tree explanations",
      "D": "Counterfactual explanation"
    },
    "explanation": "Describing predictions based on internal token weight frequencies is an intrinsic explanation, reflecting model internals."
  },
  {
    "id": "f5ed2c719ee92cc278212e0c60c792b89f6142b1620f9c9e3108361c41e02aa4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "A team uses SHAP values but end users find the output confusing. How can they improve human-centered explainability?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch to raw weight visualizations",
      "B": "Provide detailed mathematical formulas alongside plots",
      "C": "Translate SHAP values into natural language summaries",
      "D": "Expose full data distributions for each feature"
    },
    "explanation": "Summarizing SHAP insights in natural language makes explanations accessible, aligning with human-centered design."
  },
  {
    "id": "b98a784ef33f6df622b5c254c97b7e28649f6bf9d0aee48d1541c49b19f5b93c",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "A self-driving car project needs transparent vision models. Which architecture offers the best explainability without drastic performance loss?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deep convolutional network without modifications",
      "B": "Shallow convolutional network combined with attention maps",
      "C": "Ensemble of deep generative adversarial networks",
      "D": "Transformer-based vision model with no interpretability features"
    },
    "explanation": "Adding attention maps to a shallow CNN yields insight into areas driving predictions while maintaining acceptable performance."
  },
  {
    "id": "b171070ce1f8792052f0834bfb39f33d350ef860118490966629ab2574245e01",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "A PwC auditor requests a summary of model limitations before deployment. Which part of the Model Card should be updated?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Quantitative results",
      "B": "Training details",
      "C": "Usage recommendations",
      "D": "Limitations and caveats section"
    },
    "explanation": "The limitations and caveats section explicitly documents known weaknesses for audit transparency."
  },
  {
    "id": "03628cf6f702cf12a32f46dbb74cef44a4e9c2e173b3f3f01c873dc57b76559d",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "Your company plans to fine-tune a foundation model but needs to maintain explainability. What approach minimizes opacity?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Fine-tune all layers with large datasets",
      "B": "Use prompt tuning rather than full model fine-tuning",
      "C": "Increase model depth post fine-tuning",
      "D": "Apply knowledge distillation without retaining student model"
    },
    "explanation": "Prompt tuning customizes model behavior without altering internal weights, preserving transparency of the base model."
  },
  {
    "id": "287f626ba78d8e2add7a8d711cd17d2c21a21e59c1145f100ba7001afdf8bfd1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "A credit model must log all decisions and rationales. Which AWS service can automate capturing predictions and explanation metadata?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Model Monitor with Clarify integration",
      "B": "SageMaker Ground Truth",
      "C": "Amazon CloudWatch Application Insights",
      "D": "AWS Config"
    },
    "explanation": "Model Monitor with Clarify captures prediction inputs, outputs, and feature attributions for logging and review."
  },
  {
    "id": "fc0bb60d0f3a5ce0a702b83c8753c898c7abc9370a76527e59b632b3d6254dda",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "To comply with GDPR\u2019s right-to-explanation, which design practice is essential?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Encrypt all decision logs",
      "B": "Use only cloud-managed proprietary models",
      "C": "Provide counterfactual or feature\u2010based explanations",
      "D": "Store model artifacts indefinitely"
    },
    "explanation": "GDPR requires providing explanations of automated decisions, which is met by counterfactual or feature\u2010based explanations."
  },
  {
    "id": "21620c52843cc95928c0822ba4f7d433772c0e0be7bf46d4afedfca837a69993",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "A chat assistant uses a black-box large language model. Users report inconsistent reasoning. Which strategy improves transparency?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase model temperature",
      "B": "Add more user feedback loops",
      "C": "Limit response length",
      "D": "Use a retrieval augmented generation pipeline with provenance citations"
    },
    "explanation": "RAG with provenance citations shows source documents, making model outputs more transparent and verifiable."
  },
  {
    "id": "2b97fc10988ae67f19c25ac30c51a69f846d9d0c14c23e35ea5485a2137e1704",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "Which explanation method is model-agnostic and builds a local surrogate to explain individual predictions?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SHAP TreeExplainer",
      "B": "LIME",
      "C": "Integrated Gradients",
      "D": "Attention heatmaps"
    },
    "explanation": "LIME fits a simple interpretable model locally around each prediction, regardless of the underlying model type."
  },
  {
    "id": "2e90dcb7fb86ac2e31fcd46fc413db102239a6d0f82c246e717dacce8e22806d",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "Your product team insists on maximal model performance and opposes transparency features that slow inference. How do you address this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Propose a hybrid system: high\u2010performance model behind the scenes with an interpretable surrogate for user explanations",
      "B": "Disable logging to speed up inference",
      "C": "Switch entirely to an interpretable model and accept accuracy loss",
      "D": "Use asynchronous explanation requests after inference"
    },
    "explanation": "A surrogate for explanations preserves throughput while offering users transparent reasoning separate from core inference."
  },
  {
    "id": "51b85adcaa719fe6b34dbc6bfa29b26ffc2e7a47ff32cf9680c4df05927982c0",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "Which is a risk of choosing a highly transparent model in a security-sensitive application?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Regulatory non-compliance",
      "B": "Excessive latency",
      "C": "Easier maintenance",
      "D": "Model inversion attacks exposing training data"
    },
    "explanation": "Transparent models with exposed structures may reveal sensitive training data patterns, risking inversion attacks."
  },
  {
    "id": "dad00d81eb879a09526afd81e1e8af204b467612883375a69bb0ff35a2ab6f48",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "During user testing, explanations based only on model confidence scores are unhelpful. Which alternative best aids user understanding?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provide raw probability distributions",
      "B": "Show training dataset size",
      "C": "Offer top contributing features per prediction",
      "D": "List all hyperparameters"
    },
    "explanation": "Highlighting key features driving each decision gives users actionable insights beyond mere confidence levels."
  },
  {
    "id": "71b452119c5974da18cc49af37f463479ab2a64b33848f251a75289ad0bdd1d7",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "A marketing team wants to know if a text classification model considers banned words. Which transparency tool reveals word\u2010level influences?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "SageMaker Model Monitor",
      "B": "SageMaker Clarify word\u2010attribution feature",
      "C": "Model Card quantitative metrics",
      "D": "Feature Store audit logs"
    },
    "explanation": "SageMaker Clarify\u2019s word-attribution identifies text tokens\u2019 importance in predictions, revealing banned word influence."
  },
  {
    "id": "ec88ced46c1a279ea35d68177e234b2b451b53ffc572c9948dc3533c27776853",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "Your team uses a black-box ensemble. A stakeholder demands global explanations. Which technique provides global surrogate model insights?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a decision tree surrogate on the ensemble\u2019s inputs and outputs",
      "B": "Generate local LIME explanations and average them",
      "C": "Use integrated gradients",
      "D": "Deploy attention mechanisms"
    },
    "explanation": "A global surrogate decision tree approximates the ensemble\u2019s decision surface, offering interpretable rules."
  },
  {
    "id": "8abf71fa92b9796afe91dd29d4c10e3a89abf97c7c4458ea76274b734857fa3d",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "A supply chain model\u2019s Model Card lacks information on feature engineering. Why is this a transparency concern?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Performance metrics might be inflated",
      "B": "Cost estimates could be inaccurate",
      "C": "Stakeholders cannot trace how raw inputs become features, hindering interpretability",
      "D": "Model deployment will fail"
    },
    "explanation": "Without feature engineering details, users can\u2019t understand how inputs map to features, reducing model explainability."
  },
  {
    "id": "54a7c7108d98e4a0f48d59e247fd8e5ba37b78c47896cb0d03bea353ee405f99",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "Which step best mitigates explanation risks when using third-party foundation models?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Trust vendor documentation blindly",
      "B": "Skip bias assessments to save time",
      "C": "Only use models with closed-source weights",
      "D": "Perform independent evaluations and review open-source Model Cards"
    },
    "explanation": "Independent tests and reviewing Model Cards ensures third-party models meet transparency and bias requirements."
  },
  {
    "id": "7e3b60a0f9a8bb7934f3a2299bd4bbd699f4543b3fbd56a4be1b00daa4216a91",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "A logistics company needs to explain anomaly detection alerts in real time. Which design is most suitable?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use an interpretable isolation forest with path length visualizations",
      "B": "Deploy a deep autoencoder and log latent activations",
      "C": "Train a GAN and inspect discriminator features",
      "D": "Apply black-box clustering with post hoc clustering explanations"
    },
    "explanation": "An interpretable isolation forest allows visualization of how anomalies are isolated, offering real\u2010time clarity."
  },
  {
    "id": "ac6a0c7d1476612eba788f3f3311053071a05a39db7acaa77d19bb901f77a71e",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "Which documentation practice supports transparency throughout the model lifecycle?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Only logging final model metrics",
      "B": "Maintaining versioned Model Cards with data and design decisions",
      "C": "Archiving raw code without metadata",
      "D": "Keeping hyperparameter settings private"
    },
    "explanation": "Versioned Model Cards record evolving data sources, design rationale, and performance, ensuring transparency over time."
  },
  {
    "id": "b0b8e34ab5bc38cacb6c7eef08114d905e32845f3961d4ec21385dc558dc0942",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "Which explanation approach is least prone to misleading interpretations for correlated features?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Individual conditional expectation plots",
      "B": "Permutation feature importance",
      "C": "SHAP dependence plots with interaction effects",
      "D": "Linear model coefficients"
    },
    "explanation": "SHAP dependence plots with interaction terms separate correlated feature effects, reducing misleading attributions."
  },
  {
    "id": "8f102842a7877b63a7da7c8de72fd501a413a19fda8aa18b7a069eaf26ad1cef",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "taskStatement": "4.2",
    "stem": "A customer disputes a decision and wants a human review. Which AWS service facilitates human-in-the-loop explainability?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Model Monitor",
      "B": "AWS Config",
      "C": "SageMaker Clarify",
      "D": "Amazon Augmented AI (A2I)"
    },
    "explanation": "Amazon A2I routes flagged predictions to human reviewers, providing an interface for explanation and correction."
  },
  {
    "id": "7213f1d803244b1f3f70c8b84597f5c18c7c6987c974434c62e95b35e97a4418",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A company is planning to migrate its overburdened on-premises infrastructure to the AWS Cloud. They want to ensure minimal downtime, optimized costs, and high availability across multiple regions. Which AWS Cloud benefits should they prioritize to achieve these goals, and what strategies should they implement to maximize these benefits? Select the most comprehensive set of benefits and strategies.",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Speed of deployment, Local data storage, manual scaling; Use single region deployment, and static provisioning.",
      "B": "Cost savings from reduced hardware investments, Extended data lifecycle management, manual failover; Rely on on-premises backups and manual scaling.",
      "C": "Global reach and rapid deployment, High availability, Elasticity, Cost-efficiency; Use multi-region deployment, Auto Scaling, and Infrastructure as Code (IaC).",
      "D": "Enhanced security, Fixed capacity planning, On-demand resource allocation; Implement rigid capacity planning and local data centers."
    },
    "explanation": "The most effective strategies involve leveraging AWS Cloud's global infrastructure for rapid and widespread deployment, implementing auto scaling for elasticity, and using IaC to automate deployments and updates. These strategies maximize the benefits of speed, cost efficiency, high availability, and global accessibility, aligning with the company's migration and business continuity goals."
  },
  {
    "id": "906b03ccfb4fda73b0e9bb3e6a3b5b15cdcaaa0124aba8790331f175d845bb31",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.1",
    "stem": "A startup is developing a web application with unpredictable traffic patterns. They want to deploy their application on AWS using a method that supports rapid provisioning, flexibility, and automation, while minimizing manual effort. Which deployment and operation method should they choose, and what are the key considerations to ensure scalable and resilient deployment?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Manual deployment via AWS Management Console; manually modify resources as needed for scaling.",
      "B": "Using Infrastructure as Code (IaC) with CloudFormation; deploy stacks once and update manually.",
      "C": "Deploying directly via AWS CLI commands; script deployments without version control.",
      "D": "Utilizing Infrastructure as Code (IaC) with tools like AWS CloudFormation or Terraform; automate deployment, updates, and scaling with version-controlled templates and pipelines."
    },
    "explanation": "IaC tools like CloudFormation or Terraform enable automation, repeatability, and version control, which are ideal for a startup with variable traffic. This approach supports rapid, automated provisioning and scaling, reduces manual errors, and enhances resilience and maintainability."
  },
  {
    "id": "8633678f6edd0e8d046c21509c69a5bbe2cea1bc7bf861a5ef7f9d616d3b5dc6",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "A media company needs to run a large-scale transcoding application that requires high scalability, flexible resource management, and cost optimization. They also want to minimize operational overhead. Which AWS compute service best fits these requirements, and what are the key features that support their workload?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Lambda; serverless execution, automatic scaling, pay-per-use model.",
      "B": "Amazon EC2; dedicated server management, manual scaling, custom AMIs.",
      "C": "Amazon ECS; container orchestration, cluster management, integration with EC2 or Fargate.",
      "D": "AWS Batch; batch job scheduling, managed compute environment, dynamic resource provisioning."
    },
    "explanation": "AWS Lambda offers serverless computing, automatically scales with workload, and charges based on actual runtime, making it highly suitable for batch transcoding tasks with unpredictable load patterns. It minimizes operational overhead, providing a highly scalable and cost-effective solution."
  },
  {
    "id": "31e5150f83ce3ac74c70ad1fe80bda8889613d1140f1362f9261859ea53a3359",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.2",
    "stem": "A company plans to migrate its legacy application to AWS and aims to design a solution that maximizes fault tolerance, scalability, and operational excellence. Considering the AWS Well-Architected Framework pillars, which combination of design principles best aligns with these goals? \n\nA) Choose a single Availability Zone for cost savings, implement manual failover procedures, and tightly couple services to reduce latency.\nB) Design for high availability across multiple Availability Zones, automate deployment and recovery processes, and decouple services for resilience.\nC) Focus on minimizing operational overhead by using a monolithic architecture, leverage manual scaling, and maintain tight integrations between components.\nD) Optimize for performance by allocating dedicated resources, avoid automation to reduce complexity, and keep services tightly integrated for speed.",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Choose a single Availability Zone for cost savings, implement manual failover procedures, and tightly couple services to reduce latency.",
      "B": "Design for high availability across multiple Availability Zones, automate deployment and recovery processes, and decouple services for resilience.",
      "C": "Focus on minimizing operational overhead by using a monolithic architecture, leverage manual scaling, and maintain tight integrations between components.",
      "D": "Optimize for performance by allocating dedicated resources, avoid automation to reduce complexity, and keep services tightly integrated for speed."
    },
    "explanation": "Option B aligns with the AWS Well-Architected Framework pillars by emphasizing high availability, automation, and decoupling of services, which collectively enhance fault tolerance, scalability, and operational excellence. Choices A, C, and D either compromise resilience, increase operational overhead, or reduce flexibility, making them less suitable for a robust, scalable cloud architecture."
  },
  {
    "id": "d65c3af6ac318356322d368e0891234903cde79a523d4af441002e145758c30c",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.2",
    "stem": "In designing a new cloud-native application, an architect is considering the use of managed services and microservices architecture to ensure resilience and agility. Which key design principles should the architect prioritize according to the AWS Well-Architected Framework?\n\nA) Monolith deployment, manual scaling, and tight service coupling.\nB) Serverless computing, decoupled components, and automated deployment pipelines.\nC) Fixed infrastructure, manual patching, and tightly integrated services.\nD) Large, monolithic databases, vertical scaling, and low automation levels.",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Monolith deployment, manual scaling, and tight service coupling.",
      "B": "Serverless computing, decoupled components, and automated deployment pipelines.",
      "C": "Fixed infrastructure, manual patching, and tightly integrated services.",
      "D": "Large, monolithic databases, vertical scaling, and low automation levels."
    },
    "explanation": "Option B advocates for serverless architectures, decoupled services, and automation, which are core principles of a modern, resilient, and scalable system as per AWS best practices. The other options reflect traditional or outdated practices that reduce agility and fault tolerance."
  },
  {
    "id": "81f046b5d01f8c7d579a4b6092f8b54525b4b182545cd120d3ca1b08f4853b7c",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.2",
    "stem": "An organization is transitioning to a multi-region deployment model to improve disaster recovery and latency. They want to ensure their architecture adheres to AWS's high availability principles. Which approach best exemplifies this goal?\n\nA) Deploy all resources in a single region using the same Availability Zone.\nB) Use multiple regions with independent, replicated resources and automate failover mechanisms.\nC) Centralize resources in one Region to simplify management, and manually replicate data to disaster recovery sites.\nD) Deploy resources in different regions but avoid data replication to reduce complexity, relying on backups for disaster recovery.",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy all resources in a single region using the same Availability Zone.",
      "B": "Use multiple regions with independent, replicated resources and automate failover mechanisms.",
      "C": "Centralize resources in one Region to simplify management, and manually replicate data to disaster recovery sites.",
      "D": "Deploy resources in different regions but avoid data replication to reduce complexity, relying on backups for disaster recovery."
    },
    "explanation": "Option B employs multi-region deployment with replication and automated failover, which aligns with high availability and disaster resilience principles in AWS architecture. The other options lack sufficient redundancy or automation to ensure robust high availability."
  },
  {
    "id": "057d8ffc3a7c8493e2f689e51b9334f49df1ecaea475f771490e48942e2539e3",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A global retail startup needs to launch identical test, staging, and production environments in three AWS Regions with minimal manual effort and ensure consistency across regions. Which AWS capability best addresses this requirement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Manually replicate resources in each Region using the AWS Management Console",
      "B": "Use AWS CloudFormation StackSets with a common template across the three Regions",
      "C": "Deploy AWS Elastic Beanstalk applications separately in each Region",
      "D": "Use Amazon Machine Images (AMIs) to copy EC2 instances to each Region"
    },
    "explanation": "CloudFormation StackSets automates deployment of identical stacks across multiple Regions, ensuring consistency and minimal manual effort."
  },
  {
    "id": "53f2e2f56b85154b67dd316b5c269180fbc14e79e57559a505a4edc64c30801a",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.2",
    "stem": "A media streaming service must deliver live video to viewers worldwide with minimal latency. Which combination of AWS global infrastructure features and services provides both global reach and high availability?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy EC2 instances in one Region and use an Application Load Balancer",
      "B": "Use AWS Wavelength in all major cities with CloudFront",
      "C": "Use Amazon CloudFront with edge locations and deploy origin servers in multiple AWS Regions",
      "D": "Deploy AWS Outposts in each data center and use Route 53 geolocation routing"
    },
    "explanation": "CloudFront edge locations cache content close to viewers, and origin servers in multiple Regions ensure availability if one Region fails."
  },
  {
    "id": "4791f1b6d167b78440e1756c3c58699bd34a4905adcbb7cfaf21d48c59f29fe7",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A financial analytics firm expects unpredictable spikes in compute usage during market events. They want to pay only for what they use and to scale automatically without manual intervention. Which AWS benefit and service combination fulfills these requirements?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Elasticity using Amazon EC2 Auto Scaling",
      "B": "High availability using Multi-AZ RDS",
      "C": "Agility using AWS CloudFormation",
      "D": "Global reach using Route 53 latency-based routing"
    },
    "explanation": "Elasticity with EC2 Auto Scaling automatically adds or removes instances to match usage, ensuring they pay only for the capacity they consume."
  },
  {
    "id": "af3797baa29f241fe347f5b7653fca1bbcda03f424fe99eefe8a54673952b049",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "During a global product launch, a company needs DNS-based traffic routing that directs users to the lowest-latency Region and fails over if that Region becomes unavailable. Which AWS service and routing policy accomplish this with minimal configuration?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Global Accelerator with geoproximity routing",
      "B": "Amazon CloudFront with origin failover",
      "C": "AWS WAF with regional ACLs",
      "D": "Amazon Route 53 latency-based routing with active-active endpoints in multiple Regions"
    },
    "explanation": "Route 53 latency-based routing directs users to the Region with the lowest latency and supports health checks for failover."
  },
  {
    "id": "f63dae5dcdf5d23deb3c43a2edef0f946a3072c9b1c035314243a4ea119b0b42",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "A robotics research team must spin up dozens of GPU-backed EC2 instances in under 10 minutes for machine learning training, then tear them down when finished. Which AWS advantage and feature align best with this need?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High availability via Multi-AZ deployments",
      "B": "Speed of deployment via Amazon EC2 and Elastic GPU",
      "C": "Global reach via AWS Outposts",
      "D": "Economies of scale via Reserved Instances"
    },
    "explanation": "EC2\u2019s rapid provisioning (minutes) and on-demand Elastic GPUs demonstrate agility and speed of deployment, enabling quick spin-up and tear-down."
  },
  {
    "id": "69525e7af28475b2b058679759cdcfd887df4031895cff2ed4fffe5153f2af30",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.2",
    "stem": "An enterprise with offices in Europe, Asia, and North America wants a single DNS record that automatically routes traffic based on user location for compliance and performance. Which AWS global infrastructure feature supports this?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon CloudFront with geo restriction",
      "B": "Amazon Route 53 geolocation routing",
      "C": "AWS Global Accelerator with endpoint groups",
      "D": "AWS Direct Connect global network"
    },
    "explanation": "Route 53 geolocation routing sends user requests to endpoints based on their geographic location for both compliance and reduced latency."
  },
  {
    "id": "28e880cc3e28ce81042b577eef63ad6db001e0dc8d66cdd9d34cfe8c1d23f5e3",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A biotech startup must maintain a highly available data processing pipeline across two Regions. They require automatic failover if one Region loses connectivity. Which architecture best meets these requirements?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy all resources in one Region and back up data to another Region",
      "B": "Use Amazon S3 cross\u2010Region replication and process data in a single Region",
      "C": "Deploy active-active compute and database clusters in two Regions with Route 53 health checks for failover",
      "D": "Use AWS Backup to replicate snapshots across Regions and manually promote"
    },
    "explanation": "Active-active deployment across two Regions with Route 53 health checks ensures continuous availability and automatic failover."
  },
  {
    "id": "aaabec59042f4a0e48ff6c140fcc9c61ab5d0b1979776c14bbcbb771a2d09584",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A software team needs to prototype a new application architecture in hours, iterate rapidly, and discard it without incurring ongoing cost. Which AWS characteristic and service combination is most aligned?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Global reach using AWS Direct Connect",
      "B": "High availability using RDS Multi-AZ",
      "C": "Elasticity using EC2 Reserved Instances",
      "D": "Agility using AWS CloudFormation and on-demand pricing"
    },
    "explanation": "Agility with CloudFormation allows rapid prototyping and cleanup, and on-demand pricing prevents long-term costs for disposable environments."
  },
  {
    "id": "865765e77662d89d85ad4c8e7c6bc8d1a4fe0a245999d42e9c61ed9087fd7135",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.7",
    "stem": "A gaming company wants to minimize latency for voice chat between players in South America, Europe, and Asia-Pacific. Which AWS infrastructure feature combination should they adopt?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon CloudFront global edge network and AWS Global Accelerator",
      "B": "Multi-Region Outposts deployments",
      "C": "AWS Storage Gateway in each continent",
      "D": "Amazon RDS Multi-AZ with cross-Region read replicas"
    },
    "explanation": "CloudFront edge network reduces latency for user traffic, and Global Accelerator optimizes path selection across the AWS global network for real-time voice chat."
  },
  {
    "id": "e67ce7a0a6a5b2ecae8e7b516522a86178ab9e926e88e6609eb9611bb3b1f5a9",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A retail analytics platform must scale out automatically to handle unpredictable holiday traffic surges within seconds. Which two AWS benefits are primarily being utilized?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Elasticity and agility",
      "B": "High availability and economies of scale",
      "C": "Global reach and cost savings",
      "D": "Security and compliance"
    },
    "explanation": "Automatic scaling in response to demand demonstrates elasticity, and rapid deployment/configuration changes demonstrate agility."
  },
  {
    "id": "d00bafe75715da9c19407a11c30540f3fb20e2fbcbd3f5a9113efb6cd748f678",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A multinational corporation needs to ensure its application is served from multiple physical locations near users to meet data residency and performance requirements. Which AWS feature best satisfies both needs?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy Amazon WorkSpaces in each country",
      "B": "Use multiple AWS Regions with Amazon Route 53 geoproximity routing",
      "C": "Leverage AWS Outposts with per-country deployments",
      "D": "Use a single Region with AWS CloudFront geofencing"
    },
    "explanation": "Multiple Regions meet data residency, and Route 53 geoproximity routing directs traffic to the nearest regional endpoint for performance."
  },
  {
    "id": "9d59432ee625e0e2dec9399e6349ad43a06d3fc5ff5231bc7280d630898fc4b9",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.7",
    "stem": "A data analytics startup wants to process real-time IoT data streams in multiple geographic areas without managing servers. Which AWS service and benefit combination should they choose?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon EC2 Auto Scaling for IoT applications",
      "B": "AWS IoT Greengrass on on-premises gateways",
      "C": "AWS Lambda@Edge with CloudFront for serverless global processing",
      "D": "AWS Snowball Edge devices"
    },
    "explanation": "Lambda@Edge allows serverless code execution at CloudFront edge locations, enabling real-time processing close to data sources without server provisioning."
  },
  {
    "id": "a201c414e8da994e247d037a8263110b3bd9ac36abe874ca6908607e9ef146ca",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "An organization subject to strict regulatory requirements must keep data within specific countries yet provide a single application interface. Which AWS global infrastructure feature supports compliance and agility?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Amazon CloudFront with geo restriction",
      "B": "AWS Snowball for data transfer",
      "C": "AWS Direct Connect with public VIF",
      "D": "Deploy Regions in required countries and use Route 53 geolocation routing with CloudFormation Templates for deployment automation"
    },
    "explanation": "Deploying in specific Regions meets data residency, geolocation routing enforces compliance at DNS level, and templates automate deployment (agility)."
  },
  {
    "id": "a07ab8bea7559610ecf836256844f32d30a647567b7c8a7e0615d4f130cb4303",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "An online gaming company wants to ensure that all user actions and administrative changes are recorded and centrally viewable to support incident investigations and forensics. Which Well-Architected design principle best addresses this need?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable traceability",
      "B": "Apply security at all layers",
      "C": "Perform operations as code",
      "D": "Experiment more often"
    },
    "explanation": "The security pillar\u2019s Enable traceability principle ensures that all actions and changes are logged and monitored centrally for audit and forensic purposes. The other options address different concerns."
  },
  {
    "id": "042a8e8155ce9506fed33c540b752e286dcbcfe00de990acda7f5861a7e7fa26",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A fintech startup needs to deploy daily updates with minimal risk and quick rollback in production. Which Well-Architected principle should they follow to meet this requirement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Stop spending money on undifferentiated heavy lifting",
      "B": "Make frequent, small, reversible changes",
      "C": "Test recovery procedures",
      "D": "Democratize advanced technologies"
    },
    "explanation": "Making small, reversible changes reduces blast radius and accelerates rollback, an operational excellence principle. The other choices do not directly address deployment risk."
  },
  {
    "id": "be33a627902fb2cfec0e79c038131e08e67167cb9946240776671fa4d6892610",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A social media platform anticipates unpredictable viral events and wants to automatically distribute traffic across multiple compute resources to maintain service availability. Which Well-Architected principle supports this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Scale horizontally to increase aggregate system availability",
      "B": "Use serverless architectures",
      "C": "Adopt a consumption model",
      "D": "Learn from all operational failures"
    },
    "explanation": "Scaling horizontally (adding more instances) is a reliability principle that increases aggregate availability. The other options address different aspects of the framework."
  },
  {
    "id": "df992f32f4375566919f1a3694553270448e815ee35aeda2fdc8e8f4308c2870",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "An online retailer aims to avoid manual capacity forecasting and over-provisioning by allowing AWS services to provision resources automatically based on demand. Which principle are they applying?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Stop guessing capacity",
      "B": "Go global in minutes",
      "C": "Use managed and application-level services to reduce cost of ownership",
      "D": "Keep people away from data"
    },
    "explanation": "Stop guessing capacity is a reliability principle that removes the need for manual forecast and over-provisioning. The other choices relate to different pillars."
  },
  {
    "id": "eab5b16770a38045d6a95bb3aac3f49e4081ccfab06e3396a81d9e1b8aa385f2",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A photo-sharing application wants to minimize upfront investments and pay only for storage and compute when they actually use them. Which design principle should guide their architecture?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Adopt a consumption model",
      "B": "Automatically recover from failure",
      "C": "Prepare for security events",
      "D": "Use automated security best practices"
    },
    "explanation": "Adopt a consumption model is a cost optimization principle encouraging pay-per-use. The other options address reliability or security concerns."
  },
  {
    "id": "87b8e45790d190d159023bc6075d280c8d74499ebe85bf830c845494ca37c113",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A company building real-time ML inference workloads systematically tests different EC2 instance types, measuring CPU, GPU, and network utilization to optimize cost and throughput. Which Well-Architected principle are they applying?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Consider mechanical sympathy",
      "B": "Stop guessing capacity",
      "C": "Use serverless architectures",
      "D": "Annotate documentation"
    },
    "explanation": "Consider mechanical sympathy is a performance efficiency principle that encourages understanding hardware characteristics to optimize performance. The others address different pillars."
  },
  {
    "id": "0c8d222e3dc3dccec70732eb901d75dcea34c4fed3c52f9ca98d989e9348f934",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A biotech firm processes sporadic genomic data analysis using AWS Lambda functions and S3 to avoid provisioning servers for intermittent workloads. Which Well-Architected principle is illustrated?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use serverless architectures",
      "B": "Scale horizontally to increase aggregate system availability",
      "C": "Adopt a consumption model",
      "D": "Perform operations as code"
    },
    "explanation": "Use serverless architectures is a performance efficiency principle that allows workloads to run without managing servers. The other options relate to reliability, cost, and operations."
  },
  {
    "id": "e52c6b63df039aa41176a79dcefa86b7c3b9bbfb050ac52e1b474991666359fa",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.3",
    "stem": "A security team wants to ensure that only approved API calls invoke critical functions. They implement least-privilege IAM policies and centralized identity management. Which security design principle does this represent?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Implement a strong identity foundation",
      "B": "Keep people away from data",
      "C": "Experiment more often",
      "D": "Democratize advanced technologies"
    },
    "explanation": "Implement a strong identity foundation is a security principle that emphasizes least-privilege access and centralized identity. The other options are unrelated to access control."
  },
  {
    "id": "9a81146bdfb79013fc9518debcb21cff0f96013b50a0e2d2adb3331bcc89b35b",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "An operations team uses AWS CloudFormation and version control for all infrastructure changes, enabling peer review and rollback. Which operational excellence principle is demonstrated?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Perform operations as code",
      "B": "Manage change in automation",
      "C": "Use serverless architectures",
      "D": "Analyze and attribute expenditure"
    },
    "explanation": "Perform operations as code is an operational excellence principle promoting IaC and version control. The others address reliability, performance, or cost."
  },
  {
    "id": "89b7aad4b8fd5cccb554e9d515f82c067d5e852d5981bddbdf758467750f2a5b",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A healthcare application must ensure that patient data is encrypted both when stored and when transmitted. Which Well-Architected principle covers this requirement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Protect data in transit and at rest",
      "B": "Keep people away from data",
      "C": "Annotate documentation",
      "D": "Scale horizontally to increase aggregate system availability"
    },
    "explanation": "Protect data in transit and at rest is a security principle that mandates encryption. The other choices do not directly mandate encryption."
  },
  {
    "id": "0c33ec4bb3ea6427cff9fb7e45e2c2a3788d26744af3fe53e0afc3da808e0492",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "To reduce operational toil, a tech company automates security patching and vulnerability scanning across all accounts using AWS Systems Manager. Which design principle is addressed?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Apply security at all layers",
      "B": "Automate security best practices",
      "C": "Manage change in automation",
      "D": "Adopt a consumption model"
    },
    "explanation": "Automate security best practices is a security principle that recommends embedding security automation. The other options are less specific to automated security."
  },
  {
    "id": "409fec684cf7170b0554558293921be9201237252b22b55133c33c17f37badf2",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A mobile game developer selects managed DynamoDB and Lambda to minimize self-management of servers. Which Well-Architected principle is this?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Stop spending money on undifferentiated heavy lifting",
      "B": "Use serverless architectures",
      "C": "Perform operations as code",
      "D": "Scale horizontally to increase aggregate system availability"
    },
    "explanation": "Stop spending money on undifferentiated heavy lifting is a cost optimization principle advocating managed services. Although serverless is used, the focus is on reducing undifferentiated work."
  },
  {
    "id": "72d7e5d826649e5dc1795d19f8371475e63987ec8b853f15dd437c731a7f9991",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "A data science team provides self-service access to GPU instances and ML algorithms without central IT approval. Which Well-Architected principle is this?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Democratize advanced technologies",
      "B": "Go global in minutes",
      "C": "Adopt a consumption model",
      "D": "Enable traceability"
    },
    "explanation": "Democratize advanced technologies is a performance efficiency principle that encourages self-service use of advanced tools. The others address global reach, cost, or security."
  },
  {
    "id": "8a9647c7b67634c73a9d6038c45420a045d9256ad401b154122a4effcaf325c4",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A global e-commerce company routes traffic to the nearest AWS edge location using Amazon CloudFront to serve static content. Which Well-Architected principle is best exemplified?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Go global in minutes",
      "B": "Enable traceability",
      "C": "Adopt a consumption model",
      "D": "Prepare for security events"
    },
    "explanation": "Go global in minutes is a performance efficiency principle that leverages the AWS global network to reduce latency. The other options relate to security or cost."
  },
  {
    "id": "3c18bf165d01c1a3d1e1037968fa1b1e64c347af8eded49eb5d19797fac43a7a",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A SaaS vendor implements feature flags to deploy new features to a subset of users, monitors key metrics, and rolls back instantly if issues are detected. Which Well-Architected principle does this best represent?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Experiment more often",
      "B": "Test recovery procedures",
      "C": "Perform operations as code",
      "D": "Protect data in transit and at rest"
    },
    "explanation": "Experiment more often is a performance efficiency principle encouraging controlled, metric-driven feature rollouts. The other options do not capture the iterative experimentation aspect."
  },
  {
    "id": "53c1075ad06da5779b3416a5d7bbbf68a67b145962943a938e7e4b0d138529b0",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A large enterprise plans to migrate a multi-tier web application with a small tolerance for downtime, but significant on-premises database volume. The application code cannot be modified. Which migration approach and AWS services combination meets the requirement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Rehost of application servers with AWS Application Migration Service and continuous replication of the database with AWS Database Migration Service",
      "B": "Refactor the application into microservices on AWS Lambda and migrate the database via AWS Snowball",
      "C": "Replatform the application to Elastic Beanstalk and perform a one-time database import using AWS DataSync",
      "D": "Repurchase a SaaS equivalent and use AWS Snowcone to transfer incremental data"
    },
    "explanation": "Rehost plus DMS continuous replication supports minimal code change and low downtime. Refactoring to Lambda requires code changes. Elastic Beanstalk replatform still can\u2019t guarantee low downtime with a one-time import. Repurchase doesn\u2019t address application hosting and Snowcone is too small for bulk on-prem data."
  },
  {
    "id": "9c4a85e04577328868fa30b5c4b4aceabfd36f3824a1e830d17324e565eb4d35",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "An organization has no formal cloud strategy and struggles with change management. According to the AWS Cloud Adoption Framework (CAF), which perspective should be addressed first to define roles, responsibilities, and incentives for migration?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Governance",
      "B": "Security",
      "C": "People",
      "D": "Platform"
    },
    "explanation": "The People perspective focuses on organizational roles, skills, and change management. Governance covers policies and compliance. Security covers risk. Platform covers technology architecture. Only People directly handles human change strategies."
  },
  {
    "id": "9683559ba3758ec94f63ea8d967bc3cafdd7404be872ca3759a32ec6c21e9e7a",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "A company must migrate an on-premises Oracle database to AWS with near-zero downtime and preserve existing Oracle licenses. Which migration strategy and licensing model should they choose?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "Rehost the database on EC2 under Bring-Your-Own-License (BYOL) using AWS Application Migration Service",
      "B": "Replatform to Amazon RDS for Oracle with BYOL and use AWS Database Migration Service for continuous replication",
      "C": "Refactor to Amazon Aurora with license-included and perform bulk import via Snowball",
      "D": "Repurchase as Amazon RDS for PostgreSQL and use AWS SCT and DMS"
    },
    "explanation": "Replatforming to RDS Oracle with BYOL and DMS continuous replication preserves licenses and minimizes downtime. Rehosting on EC2 also supports BYOL but lacks managed patching and high availability. Refactoring to Aurora changes engine, losing licenses. Repurchasing as PostgreSQL requires heavy refactoring."
  },
  {
    "id": "c17ed2b74de017a019c7e8574a8244faf4dcb32dbe9831008af3a0a63ea159ab",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "During migration planning, you discover that several applications share tightly coupled on-premises file storage. To minimize refactoring, which migration approach and AWS service should you adopt?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Replatform to Amazon EFS and migrate data with AWS DataSync",
      "B": "Refactor to Lambda with data in Amazon S3 via AWS Transfer Family",
      "C": "Rehost on EC2 with local EBS volumes using AWS Application Migration Service",
      "D": "Rehost on EC2 with shared Amazon FSx for Windows File Server using AWS DataSync"
    },
    "explanation": "Rehosting on EC2 with FSx and DataSync preserves shared file semantics with minimal code change. EFS might not support Windows SMB. Lambda requires code refactor. EBS volumes on EC2 aren\u2019t shared."
  },
  {
    "id": "156afae7ff85366dcca8305b60e483436b0256e007beaae8df7b5a7d567c3e10",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "Which CAF perspective should be consulted to design the governance rules for account structure, landing zones, and budgets when migrating multiple business units?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Governance",
      "B": "Operations",
      "C": "Security",
      "D": "Business"
    },
    "explanation": "Governance perspective covers policies, compliance, budgets, and account structure. Operations focuses on run operations. Security addresses data protection. Business is not a CAF perspective."
  },
  {
    "id": "4eff80557aa512ce9a08e4ced59e0645faf972dbdd84f817b0ccb2a35edccf9e",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.8",
    "stem": "A team wants to minimize the effort of rehosting 200 Windows servers. Which AWS resource helps automate discovery, grouping, and tracking progress across servers during migration?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Application Migration Service replication agents",
      "B": "Amazon CloudWatch agent",
      "C": "AWS Migration Hub",
      "D": "AWS Systems Manager Session Manager"
    },
    "explanation": "Migration Hub provides a central dashboard for discovery, grouping, and tracking migrations across multiple services. Application Migration Service handles server replication but lacks broad tracking. CloudWatch agent collects metrics. Session Manager is for shell access."
  },
  {
    "id": "e28d0a1e9d134ca7d13969e403ec47bb57608adf223015fe335d32fb65340598",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A business-critical legacy application requires full operating system compatibility on AWS and must be migrated in phases. Which migration tool and strategy combination is most appropriate?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Rehost using AWS Application Migration Service with block-level continuous replication",
      "B": "Replatform to AWS Elastic Beanstalk with AWS SMS",
      "C": "Refactor into containers using Amazon ECS and AWS DMS",
      "D": "Repurchase SaaS and import data by AWS Snowball"
    },
    "explanation": "Application Migration Service supports phased, block-level replication and exact OS compatibility. Elastic Beanstalk replatform still changes environment. SMS is deprecated. Containers require refactoring. Repurchasing SaaS doesn\u2019t host the OS."
  },
  {
    "id": "df0687dfa707b99937a9b62a254c7a217503c6c7a1b47f02c6cd51104bf1c640",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "Which migration strategy most reduces long-term operational overhead by shifting to a managed service model?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Rehost",
      "B": "Repurchase",
      "C": "Refactor",
      "D": "Retain"
    },
    "explanation": "Refactoring into managed services (e.g., Lambda, RDS) reduces operational overhead. Repurchasing SaaS shifts overhead but may not cover custom workloads. Rehost keeps existing management needs. Retain keeps on-premises."
  },
  {
    "id": "8e848b0b00dd7fb4287e6d8fe7d6b9bca6d5b6370073e764e85c67dd821b5054",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A company has petabytes of archival data with low retrieval frequency. Which migration strategy and resource optimize cost and retrieval requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Replatform to Amazon S3 Standard-IA via AWS DataSync",
      "B": "Rehost to Amazon S3 Glacier Deep Archive using AWS Snowball Edge for bulk transfer",
      "C": "Refactor access layer to serve data from Amazon EFS Infrequent Access",
      "D": "Repurchase third-party archive and import via DMS"
    },
    "explanation": "Glacier Deep Archive paired with Snowball Edge is ideal for large infrequent data with cost optimization. Standard-IA is more expensive. EFS infrequent access is block storage, expensive for petabytes. Repurchasing third party is unnecessary."
  },
  {
    "id": "9d19f71de1319e6266cdb69caef4ab58555e00846db688b6da2900c674eef5ef",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "Your migration team is undecided on the default AWS account structure. Which CAF perspective and tool combination helps govern multi-account design, guardrails, and blueprints?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Governance perspective with AWS Control Tower",
      "B": "Security perspective with AWS IAM Identity Center",
      "C": "Operations perspective with AWS CloudTrail",
      "D": "Platform perspective with AWS CloudFormation"
    },
    "explanation": "Governance defines landing zone and guardrails; Control Tower implements multi-account blueprints. IAM Identity Center is access management. CloudTrail is auditing. CloudFormation is provisioning but not governance blueprint."
  },
  {
    "id": "4a82b8a178601142788bf7f53bba0af8c14fc2b3d3d85b49607ffd805f4fde55",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "Which migration approach explicitly includes workload retirement analysis and can reduce total number of applications migrated?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Rehost",
      "B": "Refactor",
      "C": "Replatform",
      "D": "Retire"
    },
    "explanation": "Retire involves identifying and turning off unused or obsolete assets, reducing migration scope. The other Rs focus on moving or transforming assets."
  },
  {
    "id": "2276788485d37d989d49f9d5a1d614abe400c0ca905d06f2c096e970bd580020",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A startup wants to move a data processing pipeline to AWS with minimal upfront cost and elastic scaling. The pipeline code can be modified. Which migration strategy is best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Rehost to EC2 Auto Scaling",
      "B": "Replatform on Elastic Beanstalk",
      "C": "Refactor using AWS Lambda and Amazon S3 event triggers",
      "D": "Repurchase a managed BI SaaS"
    },
    "explanation": "Refactoring to Lambda and S3 events gives serverless scaling and minimal upfront cost. Rehosting and replatforming still incur EC2 costs. Repurchasing SaaS may not fit custom pipeline."
  },
  {
    "id": "94a96740915d7a67cbadc7e39c06012db2ea4cede082935cf1d1330e31cc0b1b",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "Which AWS CAF perspective focuses on defining operational run books, incident management, and continuous monitoring for migrated workloads?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Business",
      "B": "Operations",
      "C": "Platform",
      "D": "Governance"
    },
    "explanation": "Operations perspective addresses run operations, incident management, and monitoring. Platform is architecture. Governance covers policy. Business is not a CAF perspective."
  },
  {
    "id": "653fb0f7d3321c219b8d76aa4c7812ed9f328d76e161666fa5d25691b16a698e",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "An on-premises application has unpredictable spiky traffic. Which migration strategy and AWS service combination offer elastic scaling without application code changes?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Rehost on EC2 instances in an Auto Scaling group behind an Application Load Balancer",
      "B": "Replatform to AWS Lambda with AWS Server Migration Service",
      "C": "Refactor into containers on ECS Fargate using DMS",
      "D": "Repurchase a CDN-based SaaS solution"
    },
    "explanation": "Rehosting in EC2 Auto Scaling with ALB provides elasticity without code change. Lambda requires code modification. ECS Fargate may need containerization. Repurchasing might not handle custom app."
  },
  {
    "id": "a69ff787e34655dbf1d917b7241870f6cc8894fa6e93da57c1627bd824473e7f",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "A company runs a critical web application on-premises with monthly fixed costs of $15,000 for hardware, power, and cooling. They plan to migrate to AWS and estimate the equivalent workload requires a single EC2 m5.xlarge instance running 24\u00d77. On-Demand pricing for m5.xlarge is $0.192 per hour and they need 2 TB of EBS gp2 storage at $0.10 per GB-month. Which statement is true regarding their monthly costs?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Their total AWS cost will be around $4,350 per month, which is still less than the $15,000 on-prem cost",
      "B": "Their total AWS cost will be around $343 per month, making the move significantly cheaper than on-prem",
      "C": "Their AWS cost will be only the $204 storage charge because EC2 compute is free for this workload",
      "D": "Their AWS compute cost alone will be over $4,000 per month, so on-prem remains cheaper"
    },
    "explanation": "Compute: 0.192 \u00d7 24 \u00d7 30 = $138.24; Storage: 2,048 GB \u00d7 0.10 = $204.80; Total \u2248$343.04, far below the $15,000 fixed on-prem cost."
  },
  {
    "id": "49db12d4aa02ff93e05b310eae82db6a8c2738b4bc13b11341b525c1d6a0aa2d",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "An application runs 620 instance-hours per month on EC2 t3.medium. On-Demand cost is $0.0416 per hour. A 1-year No Upfront Standard Reserved Instance reduces the hourly rate to $0.023. Which purchasing option yields the lowest monthly cost and approximately how much will the company save each month compared to the alternative?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "On-Demand; save about $11.53 per month compared to the Reserved Instance",
      "B": "Reserved Instance; save about $11.53 per month compared to On-Demand",
      "C": "Spot Instances; save about $16 per month compared to On-Demand",
      "D": "Convertible Reserved Instance; save about $5 per month compared to On-Demand"
    },
    "explanation": "On-Demand: 620 \u00d7 0.0416 = $25.79; RI: 620 \u00d7 0.023 = $14.26; Savings \u2248$11.53, so the RI is cheaper."
  },
  {
    "id": "c882250fd992f4b6f25c7c7d6b7143741f9b1b16605d907a6a2d8121f150b9a3",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "A company has existing Microsoft SQL Server licenses with active Software Assurance and wants to minimize licensing costs when migrating to AWS. Which licensing strategy should they choose to leverage their existing investment and reduce compute costs?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Use License Included RDS SQL Server",
      "B": "Bring Your Own License on EC2 Dedicated Hosts",
      "C": "Migrate to Amazon Aurora PostgreSQL",
      "D": "Deploy SQL Server on EC2 with License Included"
    },
    "explanation": "With Software Assurance, BYOL on Dedicated Hosts allows reuse of existing licenses and avoids the premium for License Included offerings."
  },
  {
    "id": "4f53b9a1337744eee3c5fcd0c5990f6b23353b209ee3406b9059a736e1b142e2",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A DevOps engineer reviews CloudWatch metrics for an EC2 m5.large instance and observes average CPU utilization of 10% over 30 days with occasional short bursts. The current cost is $0.096 per hour. Which action will most effectively reduce cost while maintaining performance requirements?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Downsize to a t3.large and enable Unlimited Mode to cover burst demands",
      "B": "Convert to a m5.large Reserved Instance at a lower hourly rate",
      "C": "Migrate this instance to a Spot Instance with the same configuration",
      "D": "Use Auto Scaling to add more m5.large instances during bursts"
    },
    "explanation": "Switching to a t3.large lowers baseline cost ($0.0832/hr) and Unlimited Mode uses CPU credits for bursts, meeting performance while reducing spend."
  },
  {
    "id": "f7fb266dd61c9168350db04e1882e8cc43e42e94a259aac985ee19ab9383503e",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "A research team stores 5 TB of data in Amazon S3 that is accessed less than 5% of the time each month. They require millisecond\u2010level retrieval. Which storage class minimizes monthly storage cost while meeting performance needs?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "S3 Standard",
      "B": "S3 Standard\u2010Infrequent Access",
      "C": "S3 One Zone\u2010Infrequent Access",
      "D": "S3 Intelligent\u2010Tiering"
    },
    "explanation": "Standard-IA offers millisecond retrieval at a lower storage rate than Standard. One Zone-IA has lower durability/risk, and Intelligent-Tiering adds monitoring charges for infrequent data."
  },
  {
    "id": "e2f039cc03b35970d3f8329f352593db05229b79a1cf0edbd40f01d6d20f064c",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A development team has EC2 instances running continuously for development and testing but uses them only 10 hours per weekday. How can the team minimize compute costs while ensuring instances are available during business hours?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Write a Lambda function triggered by EventBridge to start instances at 8 AM and stop at 6 PM on weekdays",
      "B": "Purchase Reserved Instances for the development environment",
      "C": "Manually stop and start instances each day to avoid charges",
      "D": "Use AWS Organizations to automatically schedule instance shutdowns"
    },
    "explanation": "Automating start/stop via EventBridge and Lambda ensures instances run only during needed hours and removes human error of manual operations."
  },
  {
    "id": "388371f0f7e2825222f1a58135a9f3b31bdec44892b28e76972513de92f0b768",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "Which AWS characteristic most directly enables substantial economies of scale, allowing AWS to offer lower unit costs compared to on-premises solutions?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Purchasing hardware in bulk and distributing costs across many customers",
      "B": "Using virtualization to isolate tenants",
      "C": "Region-based pricing to capture local demand",
      "D": "Offering a free tier to onboard new users"
    },
    "explanation": "By buying hardware at scale and pooling it across customers, AWS lowers its per-unit costs and passes savings to customers."
  },
  {
    "id": "bd6ec3b554a04f6f64668037f8f48f1d137bb0697d48d93afbb9b6d07093207b",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "When migrating from an on-premises environment to AWS, which cost shifts from being primarily a fixed cost to a variable cost?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Data center cooling and power",
      "B": "Software licensing renewals",
      "C": "Employee salaries",
      "D": "Data transfer out to the Internet"
    },
    "explanation": "On-premises, power/cooling and salaries are fixed. In AWS, data transfer out is charged per GB and thus is a variable cost."
  },
  {
    "id": "12f5919d35cff928c6a42c4e90cb23180c86a78acb58fac59e0a7e23eaac752d",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.1",
    "stem": "A company needs to run a nightly batch job requiring 30 EC2 instances for exactly 6 hours each night. Which pricing model will minimize compute cost with the least operational overhead and acceptable reliability?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Purchase 30 On-Demand instances and run each night",
      "B": "Purchase a 1-year Reserved Instance for 30 instances",
      "C": "Use Spot Instances with a capacity-optimized allocation strategy and fallback to On-Demand if interrupted",
      "D": "Use a 1-year Compute Savings Plan for 30 instance-hours each night"
    },
    "explanation": "Spot Instances offer the deepest discount for short-duration batch workloads. Using a capacity-optimized Spot Fleet with On-Demand fallback balances cost and reliability."
  },
  {
    "id": "f36a65496c863afd76cf37f65b9d71ae8c962e3fcfdc501a09781aad8b23183a",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "A start-up has a 24\u00d77 steady-state workload requiring 5 c6g.large instances. The On-Demand rate is $0.0832 per hour. A 1-year No Upfront Standard Reserved Instance provides a 40% discount, and a 1-year Compute Savings Plan provides a 30% discount. Which option yields the lowest hourly cost for this workload?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Continue On-Demand, which offers the most flexibility",
      "B": "Purchase Standard Reserved Instances to achieve the lowest hourly rate",
      "C": "Purchase a Compute Savings Plan for the optimal balance of flexibility and discount",
      "D": "Use Spot Instances since they are always the cheapest"
    },
    "explanation": "On-Demand: $0.0832; RI: 0.0832\u00d70.6= $0.04992; SP: 0.0832\u00d70.7= $0.05824. Standard RIs yield the lowest rate for a constant workload."
  },
  {
    "id": "6cb50fbd82b11cfb9a0a3cde8d4dcb7e07aa1c9a3625432e94abf49b1e33455b",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A cloud architect wants to identify underutilized EC2 instances and right-size them to reduce costs. Which AWS service provides automated instance right-sizing recommendations based on utilization?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon CloudWatch",
      "B": "AWS Cost Explorer",
      "C": "AWS Compute Optimizer",
      "D": "AWS Trusted Advisor"
    },
    "explanation": "AWS Compute Optimizer analyzes utilization metrics and recommends optimal instance types for cost and performance."
  },
  {
    "id": "c17270dbd6d67be0f197ac38a472808440e0038c5a545609e1362f8c2adac85c",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "Which AWS tool should be used to automatically identify unattached EBS volumes that incur ongoing storage costs and recommend deletion?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Config",
      "B": "AWS Trusted Advisor",
      "C": "AWS Compute Optimizer",
      "D": "AWS CloudTrail"
    },
    "explanation": "Trusted Advisor includes an EBS check that lists unattached volumes and recommends cleanup to reduce storage costs."
  },
  {
    "id": "e9700153748a073c9494fcad5674fb8ffa140642cdd8dd2495da1d9a0ad1e673",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "An enterprise wants the greatest flexibility to change EC2 instance families, regions, and AWS Fargate tasks while still receiving a substantial discount. Which purchasing option best meets this requirement?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Standard Reserved Instances",
      "B": "Convertible Reserved Instances",
      "C": "Compute Savings Plans",
      "D": "EC2 Instance Savings Plans"
    },
    "explanation": "Compute Savings Plans apply to any EC2 family, region, or Fargate usage, offering maximum flexibility with a significant discount."
  },
  {
    "id": "ae25c82f87ea7bb373a140f3600fe7abadc81a8bbb91a6f5f3431ff507f8d4b2",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "An e-commerce website has variable traffic that ranges from 200 to 2,000 EC2 instances daily. To optimize costs for this highly variable workload, which strategy should the company implement?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Purchase 2,000 Standard Reserved Instances to cover peak demand",
      "B": "Use On-Demand only and rely solely on Auto Scaling",
      "C": "Purchase 200 Standard Reserved Instances for baseline and use On-Demand for bursts above 200",
      "D": "Purchase a Savings Plan for only 200 instance-hours"
    },
    "explanation": "Reserving capacity for the predictable baseline (200 RIs) and using On-Demand for spikes balances cost savings with flexibility."
  },
  {
    "id": "5a1300b02272b4d47f6b85e9288fbf97514c24601097e05528b104c5151264f6",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A company notices that as its AWS storage usage grows, its average cost per GB decreases due to tiered pricing discounts. Which economic principle does this illustrate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Elasticity",
      "B": "Economies of scale",
      "C": "Agility",
      "D": "Pay-per-use"
    },
    "explanation": "Economies of scale describe how unit costs decrease as total volume increases, and AWS passes these savings to customers via volume-based pricing tiers."
  },
  {
    "id": "b53fd939f970acb0f9abfbdb31f92ba59375213965f2a96b98a5b9f11ee0ccd0",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "A finance company stores sensitive financial reports in Amazon S3 with server-side encryption using AWS Key Management Service (KMS). They notice that KMS keys must be rotated annually. According to the shared responsibility model, who is responsible for rotating the KMS customer master key (CMK)?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS rotates the CMK on behalf of the customer because S3 uses KMS-managed keys.",
      "B": "The customer rotates the CMK because KMS customer-managed keys are the customer\u2019s responsibility.",
      "C": "AWS rotates the CMK because any AWS-managed CMK is rotated by AWS.",
      "D": "A joint responsibility exists; AWS rotates every two years and the customer completes in alternate years."
    },
    "explanation": "Customer-managed CMKs are under the customer\u2019s control, including rotation. AWS-managed CMKs are rotated by AWS. Since they use a customer-managed CMK, the customer is responsible for rotation."
  },
  {
    "id": "f3cfe1770a93a6dc18e4febbc588aec2a0f7e10efeaf408e3d757213cc8236a1",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "An energy startup runs its web application on Amazon EC2 instances in a custom Amazon VPC. According to the shared responsibility model, which security task is the customer\u2019s responsibility?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Patching the underlying hypervisor to fix a CVE vulnerability.",
      "B": "Configuring AWS Shield Advanced protection against DDoS attacks.",
      "C": "Applying operating system patches on the EC2 instances.",
      "D": "Ensuring the physical security of the data center hosting the instances."
    },
    "explanation": "Customers are responsible for guest OS, applications, and network configurations on EC2. AWS handles hypervisor, physical security, and AWS Shield Advanced infrastructure."
  },
  {
    "id": "38c2146ed3ca7fa9ae74ce2216cd6ca2c71338ab4e2fcea618d50e78bb1a2ca0",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A media company wants to use Amazon RDS for PostgreSQL. Under the shared responsibility model, who is responsible for encrypting the database at rest if they enable RDS encryption?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS, because RDS is a managed service and encryption at rest is entirely AWS\u2019s responsibility.",
      "B": "Joint responsibility: AWS handles the infrastructure encryption; the customer configures the encryption option and manages the keys.",
      "C": "Customer only, because all encryption tasks fall on the customer side when using a managed database.",
      "D": "AWS, but only when using AWS-managed customer master keys; otherwise, a third party must encrypt databases."
    },
    "explanation": "When encryption is enabled, AWS manages encryption at the storage layer, but the customer must enable it and choose or manage the CMK. It\u2019s joint: customer configures and manages CMKs, and AWS performs the encryption."
  },
  {
    "id": "e39225666d2bd43d3f34157d0f0585c5ed2b5d2053e0b025d31500274617ca04",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "Your organization deploys a containerized application using AWS Fargate. Which responsibility shift occurs compared to running containers on EC2 instances?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "The customer no longer manages the underlying EC2 instances\u2019 operating system and container runtime.",
      "B": "AWS no longer secures the network path; the customer must manage VPC security groups.",
      "C": "The customer now manages patching of the container host OS, which was AWS\u2019s responsibility on EC2.",
      "D": "AWS now handles application-level logging, which was previously the customer\u2019s responsibility."
    },
    "explanation": "With Fargate, AWS abstracts and manages the underlying instances and OS. Security groups and network remain customer-managed, and logging remains customer\u2019s job."
  },
  {
    "id": "55182d5e8fd9dbdb6dcfec049c663c74109bf6509e39bc0afc2c71e2fc4cd595",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "A healthcare firm uses Amazon Lambda to process patient data. According to the shared responsibility model, which of the following is the customer\u2019s responsibility?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Ensuring the availability of Lambda infrastructure.",
      "B": "Patching the underlying OS running the Lambda runtime.",
      "C": "Updating the Lambda runtime to the latest AWS-managed version automatically.",
      "D": "Securing the code within the Lambda function and its IAM execution role policies."
    },
    "explanation": "AWS manages Lambda infrastructure, OS, and runtime patching. The customer is responsible for function code security and defining least privilege IAM roles."
  },
  {
    "id": "1ea20af902a0f5662594b5c62282265c9f23bbfe73e15ab1d04e0abe40264425",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "An e-commerce business configures AWS Config rules to record changes in security group rules. Under the shared responsibility model, which component is AWS responsible for and which is customer responsibility?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS provides the Config service and storage of rule compliance logs; the customer defines the rules and interprets the compliance results.",
      "B": "AWS defines and enforces compliance rules; the customer just views the reports.",
      "C": "Customer provides the audit trail storage; AWS defines baseline security group rules.",
      "D": "Customer manages the Config service infrastructure; AWS only provides the rule templates."
    },
    "explanation": "AWS supplies, maintains, and secures AWS Config service. Customers configure rules, apply them to resources, and interpret results."
  },
  {
    "id": "45ba7ad17c0f1001a63b4d597320b8911aec84afe59291e64538c28860968a3a",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "A biotech startup uses Amazon Elastic Kubernetes Service (EKS) with managed node groups. According to the shared responsibility model, which activity is the customer\u2019s responsibility?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Upgrading the Kubernetes control plane versions.",
      "B": "Managing and patching the operating system on the worker nodes.",
      "C": "Ensuring the isolation of AZs for high availability of the control plane.",
      "D": "Scaling the Kubernetes control plane automatically."
    },
    "explanation": "AWS manages the control plane (API server, etcd). Customers manage node group OS updates, container images, and pod security."
  },
  {
    "id": "1ea6e41ef92a4ea7701912ea8896bc29d3ca659ccb97e010a8d8ff8970d4b163",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "A retail company configures an Application Load Balancer in front of EC2 instances. Under the shared responsibility model, which aspect must the customer manage?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Maintaining the Load Balancer\u2019s network hardware.",
      "B": "Ensuring the load balancer\u2019s SSL certificate is trusted by the certificate authority.",
      "C": "Uploading and renewing the SSL/TLS certificate on the load balancer.",
      "D": "Patching the load balancer\u2019s operating system."
    },
    "explanation": "Customers upload and renew certificates on ALB. AWS manages the ALB infrastructure, network hardware, and OS patching."
  },
  {
    "id": "8d2ded67458b5eed647d57aac375b8f0934c70562eea3fdea11d39c34e0ab386",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "A customer wants to use Amazon S3 for long-term storage. They are concerned about data integrity. Under the shared responsibility model, which action is the customer responsible for?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Maintaining tape backups of S3 objects.",
      "B": "Configuring S3 to perform hardware maintenance on storage nodes.",
      "C": "Repairing data corruption in S3 hardware.",
      "D": "Enabling versioning or cross-region replication to protect against accidental deletion or corruption."
    },
    "explanation": "AWS maintains the storage infrastructure and integrity checks. Customers must enable versioning or replication to protect data against accidental deletion or corruption."
  },
  {
    "id": "d40506cf1addfbb0453fa1c92274c4de165d9cbc7b2fe4c5eacec51e6e279ca0",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "Your team configures AWS IAM Identity Center (AWS SSO) integrated with Active Directory. Who is responsible for securing the on-premises Active Directory against credential compromise?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS secures the entire authentication flow including on-prem AD because Identity Center integrates with it.",
      "B": "The customer secures the on-premises Active Directory; AWS secures the Identity Center service.",
      "C": "AWS secures both on-prem and cloud directories when integrated with Identity Center.",
      "D": "It\u2019s a shared responsibility: AWS secures the AD servers, customer secures IAM policies."
    },
    "explanation": "Customers are responsible for their on-prem infrastructure security. AWS secures the cloud Identity Center and associated AWS services."
  },
  {
    "id": "ea0e01376954ff71cb930a774c22c353756d182c2614e9da86b0216e59a882d6",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "A government agency runs a serverless analytics pipeline using AWS Glue and Amazon S3. In case of a security breach in data catalogs, who is responsible for securing the Glue job scripts and IAM roles?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "The customer, because they write and manage the Glue job scripts and IAM roles.",
      "B": "AWS, because Glue is a fully managed extract-transform-load service.",
      "C": "AWS, for the job scripts; the customer, for the IAM roles.",
      "D": "Shared: AWS secures the service; customer only secures data at rest."
    },
    "explanation": "Customers supply the ETL code and define IAM policies. AWS secures the service infrastructure but not custom scripts and role definitions."
  },
  {
    "id": "28b76600b0fc71de174ce3fea555d2d806f72fbb9c603f379e53f7b1b957a12c",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "An IoT company uses AWS IoT Core to receive device telemetry. Under the shared responsibility model, which action is AWS responsible for?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Validating device certificates against the customer\u2019s corporate CA.",
      "B": "Securing the message broker infrastructure and TLS termination for device connections.",
      "C": "Managing device firmware updates once data reaches IoT Core.",
      "D": "Encrypting stored telemetry in the AWS IoT message queue at rest."
    },
    "explanation": "AWS secures the IoT Core infrastructure including the message broker. Customers manage device certificates and encryption policies for storage."
  },
  {
    "id": "e8ff8d576fd70e4fdd1eb2e7838cad88778b75e1d67abc940553e5146c42718f",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "A startup leverages AWS Backup to centrally manage backups across AWS services and on-premises servers. According to shared responsibility, who defines the backup policy and verifies restorations?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS defines default backup policies and tests restores monthly.",
      "B": "AWS configures backup vault encryption and runs restore drills.",
      "C": "A joint responsibility: AWS and the customer collaborate on backup policy creation.",
      "D": "The customer defines backup vaults, backup plans, and performs restore testing; AWS maintains the backup infrastructure."
    },
    "explanation": "Customers must define backup plans, vault configurations, and validate restoration. AWS provides and secures the Backup service infrastructure."
  },
  {
    "id": "a080f04fbea5a3d9656aba8e12239a7a40ec0785b111459339b333b3da742448",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "A multinational organization must determine which AWS Regions are certified for processing GDPR-regulated personal data before deploying new workloads. In which AWS service and section should they look to find region-specific compliance program information?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Artifact \u2013 Compliance Reports",
      "B": "AWS Config \u2013 Managed Rules",
      "C": "AWS Service Health Dashboard",
      "D": "AWS Trusted Advisor \u2013 Cost Optimization"
    },
    "explanation": "AWS Artifact\u2019s Compliance Reports section lists region-specific programs such as GDPR, whereas Config rules check resource compliance, and the Service Health Dashboard and Trusted Advisor do not provide compliance program detail."
  },
  {
    "id": "07f56baf32a566d5d8de9f97fafc66f83cdf04a853444d7c99eba95a0c2e0b32",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A security team needs to automate evidence collection against PCI DSS controls across hundreds of AWS accounts and generate audit-ready reports with minimal manual effort. Which service should they choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Audit Manager",
      "B": "AWS Security Hub",
      "C": "AWS Config",
      "D": "AWS CloudTrail"
    },
    "explanation": "AWS Audit Manager automates gathering evidence for frameworks like PCI DSS and creates audit-ready reports. Security Hub aggregates findings but doesn\u2019t automate evidence collection, Config assesses compliance but doesn\u2019t produce reports against external frameworks, and CloudTrail logs API calls only."
  },
  {
    "id": "05558849f1963a0029965c36deb50f006b4043375b461840aa1bb27c94fee093",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "Your organization mandates that no new Amazon S3 buckets may be created without server-side encryption across all member accounts. You require a preventive control to enforce this at creation time. Which solution meets this requirement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Config rule to detect unencrypted buckets",
      "B": "AWS Organizations Service Control Policy denying s3:PutBucketEncryption when SSE isn\u2019t specified",
      "C": "IAM user policy denying s3:CreateBucket for unencrypted buckets",
      "D": "CloudWatch Event triggering Lambda to encrypt buckets post-creation"
    },
    "explanation": "An AWS Organizations SCP can prevent the API call if SSE parameters are missing. Config rules detect non-compliance after creation, IAM policies can\u2019t inspect request parameters during bucket creation, and remediation via Lambda is reactive, not preventive."
  },
  {
    "id": "845c280ddad7ea6b3553d4d449084c589a0c15e2d73df69c832b4874b6c2483b",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.3",
    "stem": "A security analyst wants to investigate suspicious lateral data movement between EC2 instances by examining network flow logs detailing IP addresses, ports, and traffic accept/deny status. Which AWS service should they query?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS CloudTrail",
      "B": "VPC Flow Logs",
      "C": "AWS Config",
      "D": "Amazon CloudWatch Metrics"
    },
    "explanation": "VPC Flow Logs capture IP traffic flow information (source/destination IP, port, accept/deny). CloudTrail logs AWS API calls, Config logs configuration changes, and CloudWatch metrics show aggregated counters without per-flow detail."
  },
  {
    "id": "a8748b57d11d355d1b205acac8c7253b21eb1cf3c7b201257fbb133e202d6ec7",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "Which AWS service provides a centralized, standardized dashboard that aggregates findings from Amazon GuardDuty, Amazon Inspector, and Amazon Macie and maps them to industry standards such as CIS and PCI DSS?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Security Hub",
      "B": "Amazon Detective",
      "C": "AWS Config",
      "D": "AWS CloudTrail"
    },
    "explanation": "AWS Security Hub aggregates findings from GuardDuty, Inspector, and Macie and aligns them to standards like CIS and PCI DSS. Detective analyzes threat graphs, Config tracks resource state, and CloudTrail logs API calls."
  },
  {
    "id": "a3153332849a216b91da6586606957c33163b7cdf7d14aa30cc5657c506fddf3",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "A financial services firm must retain CloudTrail logs for seven years in an immutable, tamper-proof manner. Which configuration satisfies this requirement?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Deliver logs to an S3 bucket configured with MFA-Delete and Object Lock in Compliance mode",
      "B": "Deliver logs to an S3 bucket with a lifecycle rule transitioning to Glacier after 30 days",
      "C": "Send logs to CloudWatch Logs with SSE-KMS encryption only",
      "D": "Store logs in AWS Backup vault with default retention"
    },
    "explanation": "Enabling S3 Object Lock in Compliance mode plus MFA-Delete makes objects immutable and protected for the retention period. Lifecycle transitions or SSE-KMS alone do not enforce immutability."
  },
  {
    "id": "b58446d70de0c2b66d881ba4c1f249139c94b2b99ab91f8b7ff3b4bca19341b7",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A security architect needs to grant a single IAM role permission to decrypt data using a customer-managed CMK, while ensuring no other identity can use it. Which mechanism enforces this requirement most securely?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Attach an IAM policy to the role allowing kms:Decrypt",
      "B": "Use the CMK\u2019s key policy to grant only that role kms:Decrypt",
      "C": "Create a grant on the CMK for the role",
      "D": "Use an AWS Organizations SCP to restrict kms:Decrypt"
    },
    "explanation": "Key policies are the primary control in KMS and can explicitly restrict usage to a specific role. IAM policies alone are secondary and cannot override a more permissive key policy; grants are temporary and SCPs cannot target specific roles."
  },
  {
    "id": "1f36d7d088367c0ba8e46a03d80c3f335d6852170cb16f7cce83d0c5c6bc6543",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "An operations team requires daily evidence of resource configuration drift\u2014such as public security groups or unencrypted S3 buckets\u2014against the AWS Foundational Security Best Practices standard. Which service and feature combination should they implement?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Config with AWS Config Aggregator and managed rules",
      "B": "AWS Security Hub automated checks",
      "C": "AWS CloudTrail Insights",
      "D": "Amazon CloudWatch Events with custom Lambda rules"
    },
    "explanation": "AWS Config managed rules continuously evaluate resource configurations and, with an Aggregator, provide a daily compliance snapshot across accounts and regions. Security Hub checks run on a weekly or manual cadence and are not as granular."
  },
  {
    "id": "fe7b11bf3b51803f09a553c0640d72e25776cc1249b0c7dbd80467c3d37eccdc",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "A company must ensure that its AWS Lambda function writes sensitive output to Amazon S3 with encryption at rest and transfers data securely in flight. Which encryption options meet both requirements?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Enable SSE-S3 on the S3 bucket and configure Lambda to use HTTPS endpoints",
      "B": "Use SSE-KMS on S3 and store Lambda secrets encrypted with KMS",
      "C": "Use client-side encryption SDK in Lambda and enable SSE-S3 on S3",
      "D": "Enable SSE-KMS for S3 and ensure Lambda uses HTTPS (TLS) for S3 operations"
    },
    "explanation": "SSE-KMS provides customer-managed at-rest encryption, and HTTPS (TLS) ensures in-transit encryption. SSE-S3 doesn\u2019t provide key auditability, and client-side encryption doesn\u2019t address in-transit security by itself."
  },
  {
    "id": "c1c2a50f75200ed77e010c2d6e68fc0674a28c553342df318de1b054a10d9607",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "Which type of AWS KMS key should a compliance-driven organization use if it requires key material to be stored in a corporate HSM appliance under their physical control?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS-managed CMK",
      "B": "Customer-managed CMK (default key store)",
      "C": "Customer-managed CMK in a custom key store",
      "D": "AWS-owned CMK"
    },
    "explanation": "A custom key store CMK links KMS to an AWS CloudHSM cluster you control, ensuring key material resides in your HSM. Default and AWS-owned CMKs do not provide customer-controlled HSM storage."
  },
  {
    "id": "b2ccb006d1f7b769432b10bfde3c60fa2232b7d90974a6483ac83d096564b2bf",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "A security operations team must monitor real-time API activity for sensitive IAM actions (such as CreateUser or DeleteRole) and trigger alerts immediately. Which AWS capability should they enable?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Enable a multi-region CloudTrail trail with delivery to CloudWatch Logs and configure metric filters",
      "B": "Enable AWS Config recording of IAM resources",
      "C": "Deploy Amazon GuardDuty to detect IAM policy changes",
      "D": "Use AWS Config conformance pack for IAM best practices"
    },
    "explanation": "CloudTrail captures API calls and can stream events to CloudWatch Logs, where metric filters and alarms provide real-time alerts. Config records state but not every API call in real time, and GuardDuty focuses on threat anomalies."
  },
  {
    "id": "8b3b85a6272dd89a419a782b081c8d5d900b328c638534846094aed47ee95b40",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "An auditor requests granular logs showing every KMS key usage operation (Encrypt, Decrypt, GenerateDataKey) over the past 90 days for forensic analysis. Which configuration ensures these events are recorded?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable CloudTrail data events for AWS KMS",
      "B": "Enable AWS Config resource recording for KMS keys",
      "C": "Use KMS key rotation logs",
      "D": "Activate AWS CloudWatch Logs for KMS"
    },
    "explanation": "Enabling KMS data events in CloudTrail logs every cryptographic operation. Config records configuration changes only, rotation logs don\u2019t capture each use, and CloudWatch Logs alone isn\u2019t a source for KMS API events without CloudTrail integration."
  },
  {
    "id": "f8fe861eca8d4573d20d7331bd3ba85d1e6deb567fd3e5c4f59464f5650bfadf",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A security architect must prevent the creation of resources in unapproved AWS Regions across all accounts. They prefer a declarative, centralized, and preventive guardrail. Which AWS feature provides this capability?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Config rule to detect unauthorized Regions",
      "B": "AWS Organizations Service Control Policy denying actions in disallowed Regions",
      "C": "Security Hub custom action to disable resources",
      "D": "CloudWatch Events rule to remediate after creation"
    },
    "explanation": "An Organizations SCP can prevent API actions in specified Regions before resources are created. Config rules and CloudWatch remediation are reactive, and Security Hub flags findings but doesn\u2019t prevent creation."
  },
  {
    "id": "96be4cd791de0249ebafe9a4ca549c40857cf7ba5b5b3fd446e6051165fbb0e0",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "To meet tagging compliance requirements, a company mandates that specific tag keys and values be enforced on all new resources in every account. Which feature can centrally validate and enforce tag schema?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Organizations Tag Policies",
      "B": "AWS Config managed rule \u201crequired-tags\u201d",
      "C": "IAM permissions boundary on tag usage",
      "D": "CloudFormation StackSets with tags"
    },
    "explanation": "AWS Organizations Tag Policies define allowed tag keys and values centrally and apply a deny if resources are created without compliance. Config rules can detect after creation, and the other options don\u2019t centrally enforce tagging across accounts."
  },
  {
    "id": "1befd82279fe11000aca20618799098638a38a4303bed168b42a96dd35e42c76",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "An enterprise security team requires an aggregated, cross-account view of AWS Config rule compliance status for regulatory audits. Which service feature meets this need?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Config Aggregator",
      "B": "AWS Config Dashboard in each account",
      "C": "Security Hub compliance scores",
      "D": "CloudWatch cross-account metrics"
    },
    "explanation": "AWS Config Aggregator collects and displays compliance statuses from multiple accounts and regions in a single place, facilitating audit reporting. The per-account dashboard and other services don\u2019t aggregate across accounts by default."
  },
  {
    "id": "6e25d11c0a6d2536ec84d922d85c716550055bf31fc39c2aece9df5fffc7bec3",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "A company uses an external SAML identity provider to federate user access into AWS. Security engineers are concerned about the \"confused deputy\" problem when allowing the external IdP to assume a role in their AWS account. Which mechanism in the AWS STS AssumeRole API and IAM trust policy prevents this issue?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Require and validate an ExternalId parameter in the AssumeRole call and the role\u2019s trust policy.",
      "B": "Configure the trust policy to match the aws:userid SAML attribute to the calling principal.",
      "C": "Add a resource-based policy on each target resource that filters for the external IdP\u2019s issuer URL.",
      "D": "Use an identity-based policy with a NotPrincipal element to block unauthorized requesters."
    },
    "explanation": "By using an ExternalId parameter in both the AssumeRole call and the trust policy condition (sts:ExternalId), you ensure only callers who know that unique ID can assume the role, preventing confused deputy. Matching aws:userid in the trust policy (B) doesn\u2019t stop other principals from reusing assertions. Resource-based policies (C) cannot validate an ExternalId, and NotPrincipal (D) is too broad and cannot ensure only the intended external IdP uses the role."
  },
  {
    "id": "415aba57b81d08c8d8ec4e3dd555e9a9632e8ba0fe3182dd64eda218d2a53386",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.3",
    "stem": "Developers and testers share an S3 bucket for a project: developers need full access under prefix /dev/, testers need read-only access under prefix /test/. To enforce least privilege, what is the simplest IAM design?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Create two IAM groups\u2014Developers and Testers\u2014and attach identity-based policies granting each group only the required S3 permissions on its prefix.",
      "B": "Use a single IAM group with a policy granting developers full access and testers read-only access, relying on user tagging for enforcement.",
      "C": "Attach a resource-based bucket policy that grants each IAM user (by ARN) the appropriate permissions to its prefix.",
      "D": "Define two IAM roles with permission boundaries and require users to assume the appropriate role per operation."
    },
    "explanation": "Option A cleanly separates duties by group, attaches clear policies, and follows least privilege. A single group with mixed permissions (B) cannot distinguish user roles. A resource-based bucket policy using individual ARNs (C) doesn\u2019t scale and mixes policy types. Roles with permission boundaries (D) add unnecessary complexity for this use case."
  },
  {
    "id": "76299ef9c1d614f62da36cd6977492606aaccad3dbf937b0b61c910888543b14",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.3",
    "stem": "An application running on Amazon EC2 needs to read items from Amazon DynamoDB. A developer has embedded long-lived IAM access keys in the code, but company policy forbids storing credentials. What is the most secure and cost-effective solution?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Store the access keys encrypted in AWS Systems Manager Parameter Store and retrieve them at runtime.",
      "B": "Store the access keys in AWS Secrets Manager and reference the secret in the application code.",
      "C": "Attach an IAM instance profile (role) to the EC2 instance with a policy granting only the required DynamoDB actions.",
      "D": "Encrypt the access keys with a customer-managed AWS KMS key and decrypt them in the application."
    },
    "explanation": "Assigning an IAM role to the EC2 instance eliminates the need for long-lived keys (C). Parameter Store (A) and Secrets Manager (B) still require handling long-lived credentials and add cost. Encrypting keys with KMS (D) still leaves them persisted and retrievable by anyone with the decryption permission, violating best practice."
  },
  {
    "id": "c2ee56048ad41b819d401209c2a89adc9e47b94d23c6ecdc9a8b3458f96376ab",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "A security administrator configures an IAM password policy to require rotation every 90 days. After implementation, the root user\u2019s password does not expire. Why and how can the administrator enforce rotation for the root account?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Include the root user ARN in the IAM password policy ExplicitApply list.",
      "B": "Attach a managed policy requiring password rotation to the root user.",
      "C": "Deploy an AWS Organizations service control policy to enforce root password rotation.",
      "D": "There is no IAM password policy for root; root password rotation must be managed manually outside of IAM."
    },
    "explanation": "IAM password policies only apply to IAM users, not the root account. The only way to enforce root password rotation is via operational processes (D). You cannot include root in an IAM password policy (A), attach managed policies to root (B), or enforce root password rotation with an SCP (C)."
  },
  {
    "id": "898f598c735cbbbb700b72a8d46b0c2e501d7890b0f9834b59311d1b3260bbc6",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "Your company federates users from a third-party SAML identity provider into AWS. A requirement is to enforce MFA for all federated sessions. Which solution meets this requirement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a Condition in the IAM identity policy requiring aws:MultiFactorAuthPresent for federated users.",
      "B": "Configure the SAML IdP to include an MFA-AuthnContextClassRef in the assertion and add a Condition in the role trust policy to validate that attribute.",
      "C": "Attach an IAM permissions boundary to federated roles that prevents any action unless MFA is used.",
      "D": "Create a resource-based policy on each resource that denies access if MFA is not present."
    },
    "explanation": "IAM does not enforce aws:MultiFactorAuthPresent for SAML\u2010federated users (A). The correct approach is to have the IdP assert MFA in the SAML assertion (AuthnContextClassRef) and enforce it in the role trust policy (B). Permissions boundaries (C) and resource-based policies (D) cannot inspect SAML authentication context for MFA."
  },
  {
    "id": "e0f5a62ccfddf7d238833d7dd18336ee3734259d513f433f6f8035050488be56",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.3",
    "stem": "A company wants to enable its Azure Active Directory (Azure AD) users to sign in to multiple AWS accounts through AWS IAM Identity Center (AWS Single Sign-On) without creating IAM users. Which steps must the administrator take?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create IAM users for each Azure AD user, then federate them via SAML.",
      "B": "Use AWS Directory Service for Microsoft Active Directory and enable Azure AD Connect synchronization.",
      "C": "Configure Azure AD as a SAML 2.0 identity provider in AWS IAM Identity Center, enable SCIM provisioning, and map Azure AD groups to AWS permission sets.",
      "D": "Use Amazon Cognito to federate Azure AD users into AWS Identity Center."
    },
    "explanation": "AWS IAM Identity Center natively supports SAML 2.0 federation with Azure AD and can auto-provision users and groups via SCIM (C). Creating IAM users (A) defeats central identity. AWS Managed Microsoft AD (B) is for on-prem Windows workloads and not required. Cognito (D) is for application identities, not for AWS account access."
  },
  {
    "id": "1f1451785520f9e6caf4a4788c7798971d9b9eeddf22be28aea9a842afe898ea",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "An EKS cluster uses Kubernetes service accounts with IRSA (IAM Roles for Service Accounts) to grant pods permissions. The pods are failing STS token requests with \u201cInvalidIdentityToken: failed to verify JWT\u201d errors. What is the most likely misconfiguration?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "The IAM role\u2019s permission policy does not include the required dynamodb:* actions.",
      "B": "The OIDC identity provider was created with the wrong audience (aud) claim instead of sts.amazonaws.com.",
      "C": "The cluster\u2019s NodeInstanceRole is not listed in the service account trust policy.",
      "D": "The IAM trust policy uses aws:MultiFactorAuthPresent instead of the Kubernetes token audience."
    },
    "explanation": "When configuring IRSA, the OIDC provider must trust tokens with audience sts.amazonaws.com (B). If the audience is misconfigured, STS cannot validate the JWT. Missing permissions (A) would cause access denied, not token verification errors. NodeInstanceRole (C) is unrelated to pod tokens. Using MFA conditions (D) doesn\u2019t affect web identity token validation."
  },
  {
    "id": "f2b92f73a624a9387ea683b2856d3614c45b114922aada821ca4b61570a916b7",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "Your security team has hired contractors and must ensure they never exceed a minimal set of permissions in any AWS account. Which IAM feature should you use to enforce this across all their IAM principals?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Attach an explicit Deny service control policy in AWS Organizations to each account.",
      "B": "Use resource-based policies to deny actions by contractors.",
      "C": "Apply a permissions boundary to each contractor\u2019s IAM user or role to cap their maximum permissions.",
      "D": "Grant contractors only AWS-managed policies that list specific allowed actions."
    },
    "explanation": "Permissions boundaries define the maximum permissions an IAM user or role can have, regardless of other policies (C). SCPs (A) block at the account level but may affect more principals and cannot be scoped to specific users. Resource-based policies (B) are tied to resources, not identities. Relying solely on AWS-managed policies (D) risks future policy changes expanding permissions."
  },
  {
    "id": "b9b13c7017a5f9d35caf9b694a0dc161ca6540ede4b3b6fe315400388ac52e0f",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "An IAM user with the AdministratorAccess managed policy cannot modify the company\u2019s AWS Support plan or payment methods. Why is this the case?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "A resource-based policy on the billing console is blocking IAM users.",
      "B": "AWS Support plan changes require an explicit AWSConfig rule to allow IAM users.",
      "C": "Service control policies in AWS Organizations prohibit IAM users from billing actions.",
      "D": "Only the root user can change the AWS Support plan and payment methods."
    },
    "explanation": "Modifying the AWS Support plan and payment methods are root-only tasks; no IAM policy, even AdministratorAccess, can grant these actions (D). There is no resource-based policy on the billing console (A). AWS Config rules (B) and SCPs (C) are not required to explain why an admin IAM user cannot perform root-only actions."
  },
  {
    "id": "da7bd1324977f609a891d501f5d8f2395ad076c061992155c290b5b03c720197",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "Your IT team must integrate an on-premises Active Directory for user login to AWS IAM Identity Center. They do not want to replicate any directory data to AWS. Which AWS Directory Service option meets this requirement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AD Connector, because it proxies authentication requests to the on-prem AD without syncing data.",
      "B": "AWS Managed Microsoft AD, because it supports trust relationships with on-prem AD without data replication.",
      "C": "Simple AD, because it provides a lightweight Active Directory that can federate to on-prem AD via SAML.",
      "D": "Amazon Cognito user pool with custom identity provider linkage to Active Directory."
    },
    "explanation": "AD Connector acts as a proxy to on-premises AD and does not store or replicate directory data (A). AWS Managed Microsoft AD trusts on-prem AD but does replicate and synchronize data (B). Simple AD is standalone; it can\u2019t federate to on-prem AD (C). Amazon Cognito (D) is for application-level identity, not directory services for IAM Identity Center."
  },
  {
    "id": "d526830fd10b3b8ce608616ba056a328f064a74f7c9ed48b4a6b00ad8d1175eb",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "A CI/CD system must assume an IAM role to deploy infrastructure in multiple AWS accounts. The CI/CD tool is hosted off-platform and cannot use EC2 instance profiles. What is the most secure way to grant it temporary credentials?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create an IAM user with long-lived access keys and restrict its permissions.",
      "B": "Implement STS AssumeRole with an ExternalId in the trust policy and use short-lived credentials.",
      "C": "Use AWS Single Sign-On to generate session tokens for the CI/CD tool.",
      "D": "Store the access keys in AWS Secrets Manager and retrieve them at each deployment."
    },
    "explanation": "Using STS AssumeRole with an ExternalId prevents the confused deputy problem and issues short-lived credentials (B). A long-lived IAM user key (A) is less secure. AWS SSO (C) is designed for human SSO sessions, not automated CI/CD. Secrets Manager (D) still manages long-lived keys and adds cost."
  },
  {
    "id": "7952b4c9764fdcf9c1b44c8f53d9c69b5852022bb2d2f5fb2ad127285192e1fd",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.3",
    "stem": "Your security team wants to implement contextual, least-privilege access based on an IAM principal\u2019s tags. Which policy condition key allows you to grant or deny actions based on tags assigned to the principal?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "aws:PrincipalARN",
      "B": "aws:TagKeys",
      "C": "aws:PrincipalTag",
      "D": "aws:ResourceTag"
    },
    "explanation": "The aws:PrincipalTag condition key lets you write identity-based policies that restrict actions based on tags assigned to the principal (C). aws:PrincipalARN (A) matches the principal\u2019s ARN, not tags. aws:TagKeys (B) is used for resource tag operations. aws:ResourceTag (D) filters based on the target resource\u2019s tags."
  },
  {
    "id": "b2c67443bfead696f631ebc6565713211dd7bcd77423421a3ff525c14b44c35a",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "When developers assume an IAM role to perform tasks, you need to restrict the session to a subset of the role\u2019s normal permissions for safety. Which feature allows you to enforce this at session creation?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Supply a session policy in the AssumeRole call to further restrict permissions.",
      "B": "Use a permissions boundary on the role to limit its overall privileges.",
      "C": "Create a resource-based policy on AWS resources that filters for sts:SessionName.",
      "D": "Apply an AWS Organizations service control policy to the user\u2019s OU."
    },
    "explanation": "Session policies passed in the AssumeRole API call apply only to that session, further restricting the role\u2019s permissions (A). Permissions boundaries (B) cap maximum permissions but cannot vary per session. Resource-based policies (C) do not inspect session names, and SCPs (D) apply account-wide and cannot enforce per-session conditions."
  },
  {
    "id": "3ab2a08fd3650df967f1a79e5394a43fcc2d2563adba15e5033e108bb44a9820",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A security audit requires that no IAM user or role have more than 24 password reuse entries stored. Which IAM feature and value should you configure?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Set the password policy\u2019s MinimumPasswordLength to 24.",
      "B": "Set the password policy\u2019s MaxPasswordAge to 24 days.",
      "C": "Enable the IAM setting to prevent password reuse across the last 24 characters.",
      "D": "Configure the password policy\u2019s PasswordReusePrevention to 24 previous passwords."
    },
    "explanation": "The PasswordReusePrevention setting in an IAM password policy controls how many previous passwords are disallowed; setting it to 24 enforces the audit requirement (D). MinimumPasswordLength (A) and MaxPasswordAge (B) control length and rotation frequency, not reuse. Option (C) is not a valid IAM setting."
  },
  {
    "id": "74aa4c63cf39b31d42f2073392d5ff69682ff1ca44c0ffc7829dc95a61525bf5",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "Your organization uses AWS IAM Identity Center and wants to automate user and group provisioning from Okta. Which protocol and capability must be enabled in Okta and AWS IAM Identity Center?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Enable OAuth2 authorization code flow between Okta and AWS IAM Identity Center.",
      "B": "Enable SCIM provisioning in Okta and configure AWS IAM Identity Center for SCIM endpoint integration.",
      "C": "Use OpenID Connect federation from Okta directly into each AWS account.",
      "D": "Configure SAML 2.0 single sign-on only; provisioning is not supported."
    },
    "explanation": "SCIM (System for Cross-domain Identity Management) supports automated provisioning of users and groups from an IdP like Okta into AWS IAM Identity Center (B). OAuth2 (A) and OIDC (C) handle authentication, not provisioning. SAML (D) provides SSO but does not automatically provision identities."
  },
  {
    "id": "1fdfb982d694aeb1648a8e606a50cf698afb4df0c60cb5ae3c2084288fe6727f",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "Your security operations team needs to automatically discover and classify sensitive data\u2014such as PII and credentials\u2014across hundreds of objects stored in Amazon S3. Which AWS service would best meet this requirement at scale?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Security Hub",
      "B": "Amazon Macie",
      "C": "Amazon Inspector",
      "D": "AWS GuardDuty"
    },
    "explanation": "Amazon Macie uses ML to automatically discover, classify, and protect sensitive data in S3. Security Hub aggregates findings, Inspector scans EC2 hosts or container images, and GuardDuty detects threats in logs and network traffic."
  },
  {
    "id": "53cd1bd256968a78649d6d916efe04bd65b7a05fe8eb2b340fee4f0c2d637100",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A financial institution wants to evaluate and rapidly procure vetted third-party next-generation firewall solutions from AWS\u2019s partner ecosystem with minimal procurement overhead. Which AWS resource should they use?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Service Catalog",
      "B": "AWS Config",
      "C": "AWS Marketplace",
      "D": "AWS Artifact"
    },
    "explanation": "AWS Marketplace offers a catalog of third-party security solutions from vetted partners. Service Catalog governs internal portfolios, Config tracks resource compliance, and Artifact provides compliance reports."
  },
  {
    "id": "23c1206a0e64ae02380d60bce3cde7233840e5e91a46de332781b578fdfee330",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "Your organization needs a centralized service that aggregates, normalizes, and prioritizes security findings from AWS services and select third-party tools across multiple accounts and Regions. Which AWS service fulfills this requirement?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Security Hub",
      "B": "Amazon Detective",
      "C": "AWS Config",
      "D": "AWS IAM Access Analyzer"
    },
    "explanation": "Security Hub aggregates and prioritizes security findings from AWS services and integrated third-party products. Detective is for investigation, Config tracks resource configurations, and Access Analyzer analyzes IAM permissions."
  },
  {
    "id": "d1b611e7154d57875663fe86030efab67953b0f834c5705ce474b371ada17dc0",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A compliance auditor has requested your organization\u2019s PCI DSS and ISO 27001 audit reports for AWS services. Which AWS resource provides on-demand access to those documents?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Artifact",
      "B": "AWS Config",
      "C": "Amazon Inspector",
      "D": "AWS Security Hub"
    },
    "explanation": "AWS Artifact provides on-demand access to AWS compliance reports and security and compliance documents. Config tracks config changes, Inspector assesses host vulnerabilities, and Security Hub aggregates findings."
  },
  {
    "id": "cf35047ddbd830a6419b128cbfaa3a3d1243c282728165fb6f4759ee2f47df66",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "Your security team wants a continuous, managed threat detection service that uses anomaly detection and integrated threat intelligence to identify unauthorized API calls and potentially compromised credentials. Which service should they deploy?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS GuardDuty",
      "B": "AWS CloudTrail Insights",
      "C": "AWS Security Hub",
      "D": "AWS Config"
    },
    "explanation": "Amazon GuardDuty continuously monitors for unauthorized or anomalous API calls and compromised credentials using threat intelligence. CloudTrail Insights detects unusual activity but isn\u2019t a managed threat detection service, Security Hub aggregates findings, and Config monitors configurations."
  },
  {
    "id": "62c41377bdc1e8ee777adbffde24161d9937820eb1d800e0fcb8814534ac6f3c",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "After GuardDuty generates multiple findings, your team needs an interactive graph-based tool to visualize relationships between AWS resources and suspect activities for deeper investigation. Which AWS service provides this capability?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Security Hub",
      "B": "Amazon Detective",
      "C": "AWS GuardDuty",
      "D": "AWS Config"
    },
    "explanation": "Amazon Detective creates interactive visualizations (graphs) of AWS resource relationships and activity history. Security Hub aggregates findings, GuardDuty generates them, and Config tracks configurations."
  },
  {
    "id": "9b65bcbae0f994a70da5a5cf8e450ee34f4e3ec6b61c02c1fbdbf349ca512077",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A DevSecOps team wants to automatically scan container images stored in Amazon ECR for known OS-level vulnerabilities before deployment. Which AWS service should they use?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Inspector",
      "B": "AWS Config",
      "C": "AWS GuardDuty",
      "D": "AWS Security Hub"
    },
    "explanation": "Amazon Inspector can perform vulnerability assessments on container images in ECR. Config tracks resource configurations, GuardDuty detects threats at runtime, and Security Hub aggregates findings."
  },
  {
    "id": "2e0373b9fd3bae6331eb452c57a0f90dffe052597977da25550c5bead5ed6528",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "Your compliance rules mandate near-real-time detection of any Amazon S3 bucket that becomes publicly accessible. Which AWS service can continuously evaluate bucket policies and alert you when violations occur?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Config",
      "B": "Amazon GuardDuty",
      "C": "Amazon CloudWatch",
      "D": "Amazon Inspector"
    },
    "explanation": "AWS Config continuously evaluates resource configurations against rules (e.g., public S3 buckets) and can trigger notifications. GuardDuty detects threats, CloudWatch monitors metrics and logs, and Inspector assesses host vulnerabilities."
  },
  {
    "id": "0addb31779724ebbeffb14bc93a5a83dd7960828b60d41b44c0ed1a85d606a1b",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "Your network security team needs to centrally deploy and manage stateful inspection and intrusion prevention firewall rules across multiple Amazon VPCs. Which AWS service is designed for this use case?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS WAF",
      "B": "AWS Network Firewall",
      "C": "Security Groups",
      "D": "AWS Shield"
    },
    "explanation": "AWS Network Firewall provides managed, stateful inspection and intrusion prevention across VPCs. WAF protects web applications at the HTTP layer, Security Groups are instance-level firewalls, and Shield defends against DDoS."
  },
  {
    "id": "40ff9bea9d1118832800aedd616b8861d4449db325bd6597f46a9983b69bab64",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "Your security policy requires that any S3 bucket that becomes publicly accessible must be automatically remediated to block public access within minutes. Which combination of AWS services should you use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Config rule with AWS Lambda remediation",
      "B": "AWS WAF",
      "C": "AWS GuardDuty",
      "D": "AWS Backup"
    },
    "explanation": "An AWS Config managed rule can detect public S3 buckets and invoke a Lambda function to remediate. WAF protects web traffic, GuardDuty detects threats, and Backup handles backup and restore."
  },
  {
    "id": "dd80877efb7fd5d5fad41a4aa61d6714f2b28b6fb5a2773a1039223cb24cfa46",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "Your CIO wants real-time, cost-optimized checks against AWS best practices for security, fault tolerance, and performance, with recommendations to improve. Which service should they consult?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Trusted Advisor",
      "B": "AWS Security Hub",
      "C": "AWS Well-Architected Tool",
      "D": "Amazon Inspector"
    },
    "explanation": "AWS Trusted Advisor provides real-time best practice checks and recommendations across cost, performance, security, and fault tolerance. Security Hub aggregates security findings, Well-Architected Tool assesses workloads against architectural best practices, and Inspector scans hosts."
  },
  {
    "id": "59a6eeb969fe994325488a8b49c26cef961a927b5cfeb052b4aded088ac731ef",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "Your team needs detailed technical guidance on configuring envelope encryption and key rotation best practices for AWS Key Management Service (KMS). Which AWS resource is the authoritative source?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS KMS Developer Guide",
      "B": "AWS Artifact",
      "C": "AWS Security Hub Documentation",
      "D": "AWS Marketplace product listings"
    },
    "explanation": "The AWS KMS Developer Guide in AWS Documentation provides authoritative, detailed guidance on envelope encryption and key rotation. Artifact provides compliance reports, Security Hub docs explain findings aggregation, and Marketplace lists partner offerings."
  },
  {
    "id": "9ee64bb04510b911527d8454538fd420af19eec350a8f3a2aaf4612331461df8",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A solutions architect needs AWS\u2019s recommended guidelines for implementing TLS encryption in transit across services like ELB, API Gateway, and S3. Which resource should they consult?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Security Best Practices whitepaper",
      "B": "Technical documentation in each service\u2019s Developer Guide",
      "C": "AWS Knowledge Center article",
      "D": "AWS Support Center"
    },
    "explanation": "The AWS Security Best Practices whitepaper provides prescriptive, consolidated guidance on TLS across AWS services. Developer Guides cover individual service configurations but not aggregated best practices, Knowledge Center has community Q&A, and Support Center handles tickets."
  },
  {
    "id": "095a1ca0c2e71a5106f86007f2357443a8c38548aed5c1e924a4ecf1b197e549",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "Regulatory requirements demand immutable, write-once, read-many (WORM) storage for security logs with compliance enforcement. Which AWS feature should you enable on the S3 bucket storing your logs?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Backup vault lock",
      "B": "S3 Object Lock in compliance mode",
      "C": "Glacier Vault Lock",
      "D": "CloudTrail immutability settings"
    },
    "explanation": "S3 Object Lock in compliance mode enforces WORM storage on an S3 bucket. Backup vault lock and Glacier Vault Lock apply to backup and Glacier vaults, respectively; CloudTrail does not provide native immutability settings on its own."
  },
  {
    "id": "7c49dc5a6912ee00a0f9575c5c325addb39bfa33a962f97d217a5e95fb0eb590",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "Your security team wants to subscribe to AWS\u2019s official security vulnerability disclosures, patch advisories, and anomaly notifications in real time. Which AWS resource should they monitor?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Security Advisories page on the AWS Security Blog",
      "B": "AWS News Blog",
      "C": "AWS Compute Blog",
      "D": "AWS Service Health Dashboard"
    },
    "explanation": "The AWS Security Advisories page on the AWS Security Blog provides official vulnerability disclosures and patch advisories. The News Blog covers general announcements, the Compute Blog covers compute services, and the Service Health Dashboard shows service availability."
  },
  {
    "id": "f0b29fce8ec5cba5e27d309d8741bf30798c1465a35a855916cdd3387b3c5156",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A global financial institution is standardizing their network infrastructure by provisioning VPCs, subnets, and route tables across multiple AWS accounts and Regions. They require strict version control, idempotency, and automated drift detection. Which method of provisioning best meets these requirements?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS CloudFormation templates",
      "B": "AWS CLI scripts executed via Shell",
      "C": "Custom AWS SDK calls written in Python",
      "D": "Manual configuration in the AWS Management Console"
    },
    "explanation": "AWS CloudFormation provides declarative, version-controlled templates that are inherently idempotent and support built-in drift detection. CLI scripts and SDK calls can be automated but require custom idempotency logic and lack native drift detection. Manual console changes are neither repeatable nor version-controlled."
  },
  {
    "id": "933ccbe933d40397edd62a29ce6f48d85bb98a321f441d4da0defb09be43fa0f",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A security team needs to import a corporate root CA certificate into AWS Certificate Manager in an ad-hoc, one-time operation. No repeatability or automation is required. Which method should they choose to minimize overhead and complexity?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Management Console",
      "B": "AWS CloudFormation template",
      "C": "AWS CLI command in a CI/CD pipeline",
      "D": "AWS Config rule"
    },
    "explanation": "For a one-time, manual import, the AWS Management Console is simplest and incurs no scripting or template maintenance. CloudFormation and CLI introduce unnecessary automation complexity. AWS Config is unrelated to resource creation."
  },
  {
    "id": "9e29df7e9971b6d3648b63a3169d3b7cc5884bc60eea5040098043fdf0cb1a44",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "Your company operates in multiple AWS Regions and requires consistent deployment of IAM roles, Lambda functions, and DynamoDB tables as part of a global rollout. Which AWS service enables centralized management and deployment of CloudFormation stacks across accounts and Regions?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS CloudFormation StackSets",
      "B": "AWS Organizations",
      "C": "AWS Control Tower",
      "D": "AWS Config"
    },
    "explanation": "CloudFormation StackSets extends CloudFormation templates across multiple accounts and Regions in a single operation. AWS Organizations manages accounts but not resource stacks. Control Tower bootstraps landing zones but doesn\u2019t deploy arbitrary stacks. AWS Config monitors configuration compliance."
  },
  {
    "id": "11bfbf79a9b1d5fb66040b7dbb0fe259922265949276382462a817825f3aec99",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "A DevOps engineer suspects that manual changes have been made directly to S3 bucket policies outside of CloudFormation templates. They want to identify drift without writing custom scripts. Which method should they use?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use CloudFormation drift detection",
      "B": "Create an AWS Config conformance pack",
      "C": "Run AWS CLI aws s3api get-bucket-policy",
      "D": "Analyze AWS CloudTrail logs"
    },
    "explanation": "CloudFormation drift detection directly compares the actual resource state against the template. Config conformance packs focus on policy compliance rather than template drift. CLI get-bucket-policy and CloudTrail logs provide raw data but require manual comparison."
  },
  {
    "id": "def20768e2aad3f68cdcc4708045dc85163046304f7bceb8c8dca48a7828fd4f",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "To deploy a serverless application with zero downtime and automated rollbacks on failure, which combination of tools provides best integration for packaging, provisioning, and deployment?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS CloudFormation with AWS SAM CLI",
      "B": "AWS CLI with manual Lambda console updates",
      "C": "AWS SDK with custom deployment scripts",
      "D": "AWS CodeCommit with AWS CodeDeploy"
    },
    "explanation": "AWS SAM CLI builds and packages serverless applications into CloudFormation stacks, providing zero-downtime deployments and rollback capabilities. The other options lack integrated infrastructure provisioning or rollback automation for serverless apps."
  },
  {
    "id": "42a7fe78f48f57be16efbf0723f494754713bd7bab4898980695f16ca36f3e59",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "You need to deploy a new version of an EC2-based application with minimal downtime and automatic rollback on failed health checks. Which deployment strategy and toolset should you use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CloudFormation rolling update policy",
      "B": "AWS CodeDeploy Blue/Green with CloudFormation",
      "C": "Manual Auto Scaling group update via console",
      "D": "Custom AWS Lambda function to update instances"
    },
    "explanation": "CodeDeploy Blue/Green integrated with CloudFormation automates traffic shifting, health-check monitoring, and rollback if failures occur. CloudFormation rolling updates don\u2019t support Blue/Green natively. Manual and Lambda solutions require custom scripting and lack built-in rollback."
  },
  {
    "id": "7aacf6c28d15b6e97f2bbb14d7b2b304afedbc12521156eda7dbed51b9a3c97d",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "After switching from the AWS Management Console to the AWS CLI for creating IAM roles, a developer encounters UnauthorizedOperation errors when attaching policies, even though their console user had sufficient permissions. What most likely caused the failure?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "The CLI issues multiple discrete API calls that each require individual IAM permissions",
      "B": "The CLI defaults to a different AWS partition requiring separate credentials",
      "C": "Service Control Policies block CLI but allow console actions",
      "D": "AWS CLI only works with the root account for IAM operations"
    },
    "explanation": "When using the CLI, each IAM API call (CreateRole, AttachRolePolicy, etc.) requires explicit permissions. The console often aggregates actions under broader permissions, masking missing granular privileges. The other options are incorrect."
  },
  {
    "id": "a1352468b97c2cb38ed1f422d5c4274d376f9e80a15580f9d48c46828047364f",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "An enterprise wants to apply security patches and configuration changes to both on-premises servers and EC2 instances using a single operational process. Which AWS service and access method should they use?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Systems Manager Run Command",
      "B": "AWS CloudFormation",
      "C": "AWS OpsWorks",
      "D": "AWS Auto Scaling"
    },
    "explanation": "AWS Systems Manager Run Command can target both on-prem and EC2 instances for ad-hoc or scheduled tasks. CloudFormation and Auto Scaling target only AWS resources. OpsWorks is limited to Chef/Puppet configurations."
  },
  {
    "id": "ec80f030ec72d5e70d3dd4bc2ee4323efdaf595e541e00f21477d37172d026ff",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "To ensure consistent OS configurations across a fleet of EC2 instances without relying on instance user-data scripts, which AWS service or method should you use?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Image Builder",
      "B": "AWS Config Rules",
      "C": "Amazon CloudWatch Events",
      "D": "AWS CodeDeploy"
    },
    "explanation": "AWS Image Builder automates the creation of golden AMIs with pre-baked OS configurations. Config Rules audit compliance but don\u2019t enforce image builds. CloudWatch Events and CodeDeploy are not designed for AMI creation."
  },
  {
    "id": "5094399560577ba138cb2dfcd81a16c6f0365c0c397982ea94e076ecc551a3c3",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A new AWS service lacks native support in CloudFormation. To incorporate it into your automated provisioning pipeline with full infrastructure-as-code capabilities, which approach should you implement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CloudFormation custom resource backed by AWS Lambda",
      "B": "Invoke AWS CLI commands in a build stage",
      "C": "Use AWS Config Remediation",
      "D": "Perform manual configuration in the console"
    },
    "explanation": "A CloudFormation custom resource invokes a Lambda function to call unsupported service APIs, integrating into IaC workflows. CLI in a build stage breaks template consistency; Config Remediation is for compliance, not provisioning."
  },
  {
    "id": "befa32df40470fb514c960a0dc31fdc9d401c7609932192401316898ca6a7c25",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.3",
    "stem": "Your operations team must apply a critical security patch to thousands of EC2 instances across multiple accounts in minutes without SSH. Which programmatic access method should you use?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Systems Manager Run Command",
      "B": "AWS CLI with SSH in a loop",
      "C": "CloudFormation update",
      "D": "AWS Lambda function invoked manually"
    },
    "explanation": "Systems Manager Run Command can execute commands at scale across accounts and Regions without SSH. CLI with SSH is slower and less secure. CloudFormation updates don\u2019t target instance OS. Lambda manual invocation isn\u2019t scalable."
  },
  {
    "id": "dbf4c6ede319afec4a6b8d9cc47ca8bb606a294fa094102e67a2c2aab5c1a5ea",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "As part of an automated CI/CD pipeline, you need to provision and update AWS infrastructure (VPCs, IAM roles, Lambda functions) before deploying application code. Which CodePipeline action type should you include?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS CloudFormation action",
      "B": "AWS CodeDeploy action",
      "C": "AWS CodeBuild action",
      "D": "Generic AWS CLI action"
    },
    "explanation": "The CloudFormation action in CodePipeline natively provisions and updates infrastructure using templates. CodeDeploy and CodeBuild are for application deployment and build phases. A generic CLI action lacks the pipeline integration and infrastructure state management."
  },
  {
    "id": "8e013695c7addee11256142ee69678a29b8098c3d5700d56de093e2baa3378b4",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "A deployment script needs to pause until an Amazon RDS instance reaches available status before applying schema migrations. Which AWS CLI feature accomplishes this without manual polling?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "aws rds wait db-instance-available",
      "B": "Configure a CloudWatch Events rule",
      "C": "Use an AWS Step Functions wait state",
      "D": "Implement a Lambda sleep loop"
    },
    "explanation": "The CLI waiter command aws rds wait db-instance-available blocks until the RDS instance is ready. CloudWatch Events and Step Functions introduce additional services and complexity; a Lambda sleep loop is custom polling."
  },
  {
    "id": "d703002cf4f1633553fc5a68f13032b1f7eea44390be60d52397d3f3d1b31001",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "Developers want to write infrastructure as code using familiar programming languages (TypeScript) and leverage high-level constructs that synthesize into AWS CloudFormation. Which tool should they adopt?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Cloud Development Kit (CDK)",
      "B": "AWS Serverless Application Model (SAM)",
      "C": "AWS Amplify",
      "D": "AWS CloudFormation Designer"
    },
    "explanation": "AWS CDK allows use of TypeScript and other languages to define high-level constructs that synthesize into CloudFormation templates. SAM is specialized for serverless, Amplify focuses on front-end, and CloudFormation Designer is a visual tool, not code-centric."
  },
  {
    "id": "d9cb8addf1512756763c104bca0732d6a147c70695be80db91edf108ef43322c",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "Your organization requires approved, standardized IT products (VPC topologies, IAM configurations) that can be provisioned by business units without giving direct CloudFormation template access. Which AWS service should you use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Service Catalog",
      "B": "AWS Control Tower",
      "C": "AWS Config",
      "D": "AWS Organizations"
    },
    "explanation": "Service Catalog lets administrators define and publish standardized products based on CloudFormation templates, enabling self-service provisioning without exposing templates. Control Tower sets up landing zones; Config is for compliance; Organizations manages accounts."
  },
  {
    "id": "108431bf7a00bd1a5edafef0d9d19a50e3eff60168e3924608462a24b341ac92",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "A global SaaS provider has an active-active multi-region deployment in us-east-1 and eu-west-1. They need to route users to the region with the lowest latency and automatically fail traffic to the healthy region if one region becomes unhealthy. Which Amazon Route 53 routing policy should they implement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Latency-based routing with health checks",
      "B": "Geoproximity routing with traffic weights",
      "C": "Weighted routing with equal weights and health checks",
      "D": "Failover routing with primary-passive configuration"
    },
    "explanation": "Latency-based routing directs users to the region with the lowest latency and, when combined with health checks, automatically fails over if a region becomes unhealthy. The other policies do not optimize for latency in an active-active scenario."
  },
  {
    "id": "e6c790e0abe4ee598ad0889602ba70dcefe6a51ef1f063d91978180da61232b2",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "During a scheduled capacity test, you need to ensure minimal latency and maximum security between EC2 instances in two Availability Zones within a region. A colleague suggests that cross-AZ traffic traverses the public internet. Which statement most accurately describes inter-AZ connectivity?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AZs within the same region are connected by private, high-bandwidth, low-latency AWS network links",
      "B": "AZs communicate over the public internet by default, potentially exposing data to internet-based threats",
      "C": "Cross-AZ traffic requires setting up a virtual private network (VPN) between AZs",
      "D": "AZ traffic is routed through peering relationships managed by AWS Marketplace partners"
    },
    "explanation": "AWS AZs in a region are interconnected with AWS\u2019s private backbone network, offering high bandwidth, low latency, and no exposure to the public internet."
  },
  {
    "id": "e8884086b077f95aef687c545bb18ffc5632d3bb6439958abb633a9b3f01d230",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "To design for high availability with a target of 99.99% uptime and sub-50 ms latency variance, which resource distribution strategy within a single AWS region physically isolates compute resources?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy instances in multiple Availability Zones behind an Elastic Load Balancer",
      "B": "Deploy instances in multiple edge locations across the AWS global edge network",
      "C": "Deploy instances in separate VPCs within the same Availability Zone",
      "D": "Deploy instances across multiple regions using a single Route 53 weighted policy"
    },
    "explanation": "Distributing instances across multiple AZs behind an ELB isolates against AZ failures while maintaining low latency within a region. Edge locations cache content and cannot host persistent compute."
  },
  {
    "id": "05d922bcfad39223a39af6fd2780e665d70b56fc4483927deaa1cdc0eb9ba74b",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A company serves static web assets from an S3 bucket in ap-south-1 via CloudFront. If ap-south-1 becomes unavailable and the CloudFront cache expires, users receive errors. What change ensures users can still receive content from a secondary S3 bucket in another region without manual DNS updates?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure a CloudFront origin group with the primary S3 bucket and a secondary S3 bucket origin for origin failover",
      "B": "Create Route 53 failover routing to switch DNS to the secondary S3 bucket domain when the health check fails",
      "C": "Use a multi-region S3 bucket to automatically failover storage to the nearest region",
      "D": "Enable S3 cross-region replication and set S3 Transfer Acceleration on the secondary bucket"
    },
    "explanation": "A CloudFront origin group automatically fails over to the secondary origin when the primary becomes unhealthy. Route 53 changes would not affect the CloudFront distribution, and there is no multi-region S3 bucket feature."
  },
  {
    "id": "3aacf940f6ed402fd3ce0ffbdaaa8d45825d03eb07fed9dd68a906d7be965296",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.2",
    "stem": "A fintech application must ensure EU customer requests are always served from the EU region, even if latency to other regions might be lower. Which Route 53 routing policy and configuration combination meets this requirement?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Geolocation routing with health checks, directing EU IPs to the EU region, and setting a low TTL",
      "B": "Latency-based routing with a region bias for the EU region",
      "C": "Geo-proximity routing with a large bias towards the EU region",
      "D": "Weighted routing assigning 100% weight to the EU region"
    },
    "explanation": "Geolocation routing guarantees that all requests originating from specified locations go to the designated region. Latency or weighted policies cannot enforce strict geographic boundaries."
  },
  {
    "id": "0b3f5acd4ce9af354891dc6856aa84ea988f3e674a82b6b35643fed3c928c336",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "The RTO for a region-level failure is 15 minutes. The team wants to maintain a scaled-down warm standby environment in a secondary region and route traffic to it only if the primary region fails. Which Route 53 routing policy should they implement?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Failover routing with health checks, primary active, secondary standby",
      "B": "Latency-based routing with health checks, two active endpoints",
      "C": "Weighted routing with weights 100:0 and then switch to 0:100 manually",
      "D": "Geolocation routing directing global traffic to the secondary region"
    },
    "explanation": "Failover routing is designed for active-passive setups and will automatically switch to the standby region when health checks on the primary endpoint fail."
  },
  {
    "id": "ff036e17b5ed86b5dd44485a0369eb40cab14a85aad23b090175970f5f440d4c",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "A global media company sets up a CloudFront distribution with an S3 origin in us-east-1. Describe the request flow when a user in India requests a video for the first time.",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "User \u2192 nearest edge location \u2192 regional edge cache \u2192 origin \u2192 back through regional edge \u2192 edge \u2192 user",
      "B": "User \u2192 nearest edge location \u2192 edge directly contacts origin \u2192 edge \u2192 user",
      "C": "User \u2192 CloudFront global network \u2192 origin \u2192 edge \u2192 user",
      "D": "User \u2192 regional edge cache \u2192 S3 \u2192 edge \u2192 user"
    },
    "explanation": "On a miss, the request goes from the edge location to the regional edge cache, then to the origin, and the response flows back through the regional cache and edge before reaching the user."
  },
  {
    "id": "f299c5d10c795dabeafa7caf57c1b60c18217127c6cac21a82da992b734db77c",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.2",
    "stem": "An organization wants to design a globally distributed application with data sovereignty in APAC and compliance in Europe. They choose to route APAC users to ap-southeast-2, EU users to eu-central-1, and all other traffic to us-east-1. Which routing policy should they use?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Geolocation routing with specific rules for APAC and Europe, plus a default endpoint",
      "B": "Latency-based routing across three regions",
      "C": "Weighted routing with static weights for each region",
      "D": "Failover routing with health checks"
    },
    "explanation": "Geolocation routing can direct specified geographic locations to particular endpoints and use a default for all other traffic."
  },
  {
    "id": "c385ecb6d00f467dfaf404922a3af6dbdfe63df5c147da2d444911c56087e925",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "A company configures Route 53 latency-based routing for endpoints in multiple regions and sets the DNS record TTL to 60 seconds. During a region outage, how long will clients continue to resolve to the failed endpoint before querying for an updated record?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Approximately 60 seconds",
      "B": "Immediately upon endpoint health failure",
      "C": "300 seconds, the default minimum for Route 53",
      "D": "0 seconds, because Route 53 pushes updates to clients"
    },
    "explanation": "Clients respect the DNS TTL they have cached. With a 60-second TTL, they will retry DNS queries approximately every 60 seconds."
  },
  {
    "id": "4734d0db57c6cee2a439b00cf6b9f3aecfd9174e95c2027dccd7e146b64b3ff4",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.2",
    "stem": "A latency-sensitive application requires consistent sub-50 ms user-to-origin times from three continents. They plan to use CloudFront, but they also need to route arbitrary TCP traffic. Which AWS solution uses the global network to route TCP traffic to the optimal regional endpoint based on network performance?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Global Accelerator",
      "B": "Amazon Route 53 weighted routing",
      "C": "AWS Transit Gateway",
      "D": "Amazon CloudFront"
    },
    "explanation": "Global Accelerator provides static anycast IPs and routes TCP/UDP traffic over the AWS global network to the optimal regional endpoint, minimizing latency."
  },
  {
    "id": "01d19162344e292940028a3c7f6e492d672d207df36e7090d092f144392625a7",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.2",
    "stem": "Which of the following best describes the hierarchical relationship between AWS Regions, Availability Zones, and edge locations?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Regions are isolated geographic areas containing multiple AZs; edge locations are separate from Regions/AZs and used for content caching and edge compute",
      "B": "Edge locations are subsets of AZs that cache content; Regions contain multiple edge locations",
      "C": "AZs span across multiple Regions; edge locations are central points in each Region",
      "D": "Regions are logical groupings of edge locations, and AZs are physical data centers within an edge location"
    },
    "explanation": "Regions consist of AZs, which are fault-isolated datacenters; edge locations are separate points of presence for caching and edge compute, not part of AZs or Regions."
  },
  {
    "id": "f23a7f05aa4a2fb7a412e92a2fc260780daf0ad460fff2aae4cfe85f4a2d46fc",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A company replicates data between RDS instances in us-east-1 and eu-west-1. They expect replication traffic to traverse AWS\u2019s private global network. Which statement about this network path is correct?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Cross-region traffic between AWS Regions uses AWS\u2019s private global backbone network by default",
      "B": "Cross-region traffic is routed over the internet and incurs standard internet egress charges",
      "C": "Cross-region replication uses VPN tunnels by default to encrypt over the internet",
      "D": "Cross-region traffic is only available if you set up AWS Direct Connect links between Regions"
    },
    "explanation": "AWS replicates traffic over its private backbone between Regions. There is no requirement to configure VPNs or Direct Connect for standard inter-region replication."
  },
  {
    "id": "bfa5cfc2232717eaa7929b01de9c3c7136116f3a7e594e46bae401a1a95004a4",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "A failover simulation causes two Availability Zones to fail in a single Region. Despite this, the application remains available. What design principle allowed this resilience?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "The application spanned at least three AZs in that Region, so even after two failed, one AZ was still serving traffic",
      "B": "Edge locations automatically took over application traffic when two AZs failed",
      "C": "Route 53 geolocation routing redirected traffic to other Regions without additional configuration",
      "D": "Cross-region replication in S3 continued to serve content from the secondary region without DNS update"
    },
    "explanation": "By distributing the application across three AZs, failure of any two still left one AZ operational. Edge locations and geolocation routing do not provide AZ-level compute failover."
  },
  {
    "id": "678641f5c6de7fff477fcd48f9931366920adb64b6e9cd38c285bbbe78040754",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A latency-sensitive global application uses CloudFront to cache dynamic content with no Cache-Control headers set. What is the maximum time content can remain in edge caches before CloudFront checks the origin for updates, assuming no TTL overrides?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "24 hours",
      "B": "60 seconds",
      "C": "300 seconds",
      "D": "0 seconds"
    },
    "explanation": "If no Cache-Control or expires headers are set, CloudFront uses a default TTL of 24 hours before it revalidates with the origin."
  },
  {
    "id": "da252b0d89c6b6a3280df56fa139dbe48b16e1cc6f06fa812f7cd06886027643",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "A global e-commerce brand must perform A/B testing of website variants in North America only, directing 10% of NA traffic to variant endpoints and 90% to standard, while all other regions receive the standard site. Which Amazon Route 53 approach achieves this?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use a Route 53 Traffic Flow policy: geolocation rule for North America pointing to a weighted record set (10/90), plus a default record for all other regions",
      "B": "Use latency-based routing across two endpoints with weights 10/90",
      "C": "Use weighted routing across all requests and rely on health checks to filter non-NA traffic",
      "D": "Use geoproximity routing with biases to approximate 10/90 split in North America"
    },
    "explanation": "Route 53 Traffic Flow allows combining geolocation routing (to isolate North American traffic) with weighted routing (to split that traffic 10/90). The default record handles all other regions."
  },
  {
    "id": "59f63ffd13e38a59940b0e2bc3bb68444ff60b329825ca6504f435a628d2ad9a",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.7",
    "stem": "A data science team needs to run large CPU-bound batch jobs overnight on AWS at the lowest possible cost. They can tolerate interruptions and reschedule failures. Which compute solution best meets these requirements?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Use AWS Lambda functions triggered in parallel with oversized memory configurations.",
      "B": "Deploy On-Demand M5 instances in a fixed Auto Scaling group with a scheduled scale-in.",
      "C": "Configure EC2 Spot Instances with C5 compute-optimized instances in an Auto Scaling group.",
      "D": "Submit the jobs to AWS Batch running containers on AWS Fargate."
    },
    "explanation": ""
  },
  {
    "id": "be0566a27a133428f66e9936d5679c3101464209ee512d976d0cf7c38b2a1686",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "A financial services API must respond to unpredictable spikes in requests under 200 ms. The team wants minimal server management and pay-per-use pricing. Which compute option is most appropriate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provision an EKS cluster on EC2 and use the Kubernetes Ingress controller.",
      "B": "Deploy AWS Lambda functions behind Amazon API Gateway.",
      "C": "Run containers on AWS Fargate behind an Application Load Balancer.",
      "D": "Host the API on EC2 instances in an Auto Scaling group with an ALB."
    },
    "explanation": ""
  },
  {
    "id": "5ea218c2da48688108e66c09efbf2f3454c2559cb28b878efa3fdc9761565e0d",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "You have microservices deployed in ECS that require HTTP path-based routing and advanced routing rules. Which load balancer should you choose?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Application Load Balancer (ALB)",
      "B": "Network Load Balancer (NLB)",
      "C": "Classic Load Balancer (CLB)",
      "D": "Gateway Load Balancer (GLB)"
    },
    "explanation": ""
  },
  {
    "id": "12d7a865d4e590e84ede4b610308b57a0caaa61040d5b33e7420d2ed724cc95f",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "A machine learning engineer must train models on GPUs. Which compute service and instance family should they select to achieve high GPU throughput with minimal operational overhead?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Lambda with GPU-enabled Lambda layers",
      "B": "EC2 P3 instances provisioned in a managed cluster",
      "C": "ECS tasks on Fargate with GPU support",
      "D": "Amazon SageMaker Studio without specifying instance type"
    },
    "explanation": ""
  },
  {
    "id": "65f0856f5ee416c15e6392d922477251776c696b767808f9a877526bbd994fa5",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A serverless application using AWS Lambda experiences cold-start latency that impacts user experience. The function runs in a VPC. Which feature should be enabled to reduce latency?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Reserved concurrency on the Lambda function",
      "B": "API Gateway caching",
      "C": "Provisioned IOPS for the ENI attachments",
      "D": "Provisioned concurrency on the Lambda function"
    },
    "explanation": ""
  },
  {
    "id": "e47eba307318680ebd2a94644496c0e1628eff5115a4c88e04937e92c1d086ca",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "Your team wants to run containerized workloads without managing any EC2 instances or clusters. The application does not require persistent storage or custom AMIs. Which compute service should you choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon EKS on EC2",
      "B": "Amazon EC2 Auto Scaling group",
      "C": "ECS on Fargate",
      "D": "AWS Lambda with container images"
    },
    "explanation": ""
  },
  {
    "id": "c42eddf275769679f43f42a3f5a152471a1a5cfb86947b71ecfc146476ac4de7",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "A Windows-based container application requires specialized licensing not supported by AWS Fargate. The team still wants container orchestration and minimal clustering overhead. Which option is best?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Deploy containers on AWS Lambda with a Windows runtime",
      "B": "Use Amazon ECS on EC2 Windows instances",
      "C": "Use Amazon EKS with Fargate Windows support",
      "D": "Migrate the container to a Linux base and use Fargate"
    },
    "explanation": ""
  },
  {
    "id": "27cd47c4f6111c11866de91d391bff0c8e03c266d5fb4b90db40618bd14223cb",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "You need to maintain an average CPU utilization of 60% for a fleet of EC2 web servers. Which Auto Scaling policy should you implement?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Simple scaling with a scale-out adjustment of +2 instances",
      "B": "Scheduled scaling to add instances at peak hours",
      "C": "Target tracking scaling policy with target CPU utilization of 60%",
      "D": "Step scaling based on two CPU utilization thresholds"
    },
    "explanation": ""
  },
  {
    "id": "874a9c140249c576848eecb42f8d58b93b21fbb63de6b6577b09d4bf407b30a1",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.7",
    "stem": "A development team needs to perform a blue-green deployment for a Lambda-based image processing service. They require instant rollback capability and traffic weighting. Which approach meets these requirements?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Publish a new Lambda version and use an alias with weighted traffic shifting",
      "B": "Deploy the function to AWS Fargate and swap service definitions",
      "C": "Use CodePipeline to create a CloudFormation stack swap",
      "D": "Deploy to a new EC2 Auto Scaling group and switch the ALB target group"
    },
    "explanation": ""
  },
  {
    "id": "fc8898cb9d8fbf8a4454a864ef2c9a18c745f3e2c0ea7e9feda86f593e33df43",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "A stateful container requires persistent EBS storage and runs critical financial transactions. The team wants automated scaling and health checks. Which compute configuration is most suitable?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon ECS on Fargate with EFS volumes",
      "B": "AWS Lambda with provisioned concurrency and EFS",
      "C": "EKS on Fargate with dynamic PVCs",
      "D": "Amazon ECS on EC2 with EBS volumes and an Application Load Balancer"
    },
    "explanation": ""
  },
  {
    "id": "1ea0ec7db263d5adc210e9c2acccbd58a27213b36feb29a03c83b9cf45ac7cc9",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A memory-intensive in-memory caching application requires high RAM per vCPU. Which EC2 instance family is most appropriate?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Compute-optimized C5 instances",
      "B": "Memory-optimized R5 instances",
      "C": "Storage-optimized I3 instances",
      "D": "General-purpose M5 instances"
    },
    "explanation": ""
  },
  {
    "id": "d335ce53bbf5ce424c3bd5952cb3cdc6d01974675f9fa3ae1076e719596aaa66",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "Your TCP-based application uses end-to-end TLS and requires ultra-low latency and high packet throughput. Which load balancer should you deploy?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Application Load Balancer with TCP listener",
      "B": "Network Load Balancer with TLS listener",
      "C": "Classic Load Balancer in TCP mode",
      "D": "Gateway Load Balancer (GLB)"
    },
    "explanation": ""
  },
  {
    "id": "c3efb536fe9a2e53aec1a8dfcbca3d76fe1c962687044210525d08ae6c44fe2a",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "You want to run ephemeral containerized batch workloads on Spot Instances but minimize interruptions. Which allocation strategy should you choose in your Auto Scaling group?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Capacity-optimized allocation strategy",
      "B": "Lowest-price allocation strategy",
      "C": "Diversified allocation strategy",
      "D": "Prioritized allocation strategy"
    },
    "explanation": ""
  },
  {
    "id": "0c0c0c4ae44cf159d542f0c6bfaf4b2a3989136c3068cee2077e881a7daeab74",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "A scheduled job runs every hour for 2 minutes and performs light processing. The team wants the simplest operational model and pay-per-use billing. Which compute service should they choose?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Batch on EC2 Spot Instances",
      "B": "ECS Scheduled Tasks on Fargate",
      "C": "EC2 instance with a cron job",
      "D": "AWS Lambda function triggered by Amazon EventBridge"
    },
    "explanation": ""
  },
  {
    "id": "7f9f3f7d5c0b7037fcac51f616b97b938c5e6bcc761168d20373b1af8979fdb9",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "A containerized legacy application requires out-of-the-box health checks, automatic scaling, and maintenance windows. The operations team has limited Kubernetes experience. Which compute service should they select?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon EKS on EC2 with managed node groups",
      "B": "Amazon EC2 Auto Scaling with Docker installed manually",
      "C": "Amazon ECS on EC2 with an Application Load Balancer",
      "D": "AWS Fargate with an external orchestration tool"
    },
    "explanation": ""
  },
  {
    "id": "7515fc416a767ef43597c40cb10679e88416e5e19f545fd7fe3e3f2b16e12729",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A global e-commerce platform requires a relational database with seamless cross-region read capability, automated failover, and minimal administrative overhead. Which AWS database service should you select to meet these requirements?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Aurora Global Database",
      "B": "Amazon RDS for PostgreSQL with Read Replicas",
      "C": "Amazon RDS Multi-AZ for MySQL",
      "D": "Amazon DynamoDB Global Tables"
    },
    "explanation": "Only Aurora Global Database combines a single primary writer with low-latency, cross-region read replicas and automated failover; RDS read replicas aren\u2019t managed globally, Multi-AZ is within one region, and DynamoDB is NoSQL."
  },
  {
    "id": "2e887423afe7066246ab32a02da09cba43a34d74145fcf8ee102268999e900cf",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "Your application requires a sub-millisecond, in-memory data store for session state with optional persistence to disk. Which managed AWS database service best fits this use case?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon ElastiCache for Redis",
      "B": "Amazon ElastiCache for Memcached",
      "C": "Amazon DynamoDB",
      "D": "Amazon RDS with Provisioned IOPS"
    },
    "explanation": "ElastiCache for Redis offers sub-millisecond in-memory performance plus optional AOF/RDB persistence; Memcached has no persistence; DynamoDB is disk-based NoSQL; RDS is not an in-memory store."
  },
  {
    "id": "fa791cb5ef2d00f83a4cbfc6c1cf894609d364583d99c8410b601fa3e15effa5",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A knowledge-graph application needs a fully managed graph database that supports both property graph (Gremlin) and RDF (SPARQL) models with ACID transactions. Which AWS service should you choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Neptune",
      "B": "Amazon QLDB",
      "C": "Amazon DynamoDB",
      "D": "Amazon DocumentDB"
    },
    "explanation": "Neptune is the only AWS service supporting both Gremlin and SPARQL graph models with ACID transactions; QLDB is ledger-only, DynamoDB is key-value/NoSQL, DocumentDB is document-only."
  },
  {
    "id": "1da395c4544b72f20eb4754a6ffc441d898c05f7359a2ddecf6ae304aedc9515",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.4",
    "stem": "Your team needs a MongoDB-compatible, fully managed document database with automated backups and multi-AZ durability. Which AWS service meets these criteria?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon DocumentDB (with MongoDB compatibility)",
      "B": "Self-managed MongoDB on EC2",
      "C": "Amazon DynamoDB",
      "D": "Amazon RDS for MySQL"
    },
    "explanation": "DocumentDB provides MongoDB wire-protocol compatibility, automated backups, and multi-AZ management; EC2 requires manual maintenance, DynamoDB isn\u2019t MongoDB-compatible, RDS MySQL is relational."
  },
  {
    "id": "dc35be14470e94902031e9cbb0ed197c15be115b3b088ffebd8ced8bfdfe9664",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "You must migrate an on-premises Oracle database to Amazon Aurora PostgreSQL with minimal downtime and convert schema objects to PostgreSQL. Which combination of AWS tools is most appropriate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS DMS only",
      "B": "AWS SCT for schema conversion and AWS DMS for data replication",
      "C": "AWS Snowball for bulk load and AWS DMS for replication",
      "D": "AWS Database Migration Service with AWS Lambda transformations"
    },
    "explanation": "AWS SCT handles heterogeneous schema conversion (Oracle\u2192PostgreSQL) and AWS DMS performs continuous data replication with minimal downtime; DMS alone can\u2019t convert schema."
  },
  {
    "id": "a33f4aea18cae3f41854ae0d2ae65164d24586b33762b75360417c5197e167cb",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "An application suffers high read latency on a DynamoDB table. You require a transparent, write-through cache for read acceleration without modifying application code. Which service should you deploy?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "DynamoDB Accelerator (DAX)",
      "B": "Amazon ElastiCache for Redis",
      "C": "Amazon CloudFront",
      "D": "Amazon RDS Proxy"
    },
    "explanation": "DAX provides a transparent, write-through cache for DynamoDB without code changes; ElastiCache requires code integration, CloudFront is for HTTP caching, RDS Proxy is for relational DBs."
  },
  {
    "id": "65d3a51043d1ba08616a4ed8413b636cd4b413e8a487b9b967374289d831ef15",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "Your analytics team needs to run complex, petabyte-scale, columnar queries with predictable performance on structured data. Which AWS database service should they use?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Redshift",
      "B": "Amazon Aurora",
      "C": "Amazon Athena",
      "D": "Amazon EMR"
    },
    "explanation": "Redshift is a dedicated, petabyte-scale, columnar data warehouse optimized for complex queries; Aurora is OLTP, Athena is serverless but has variable performance, EMR is Hadoop-based."
  },
  {
    "id": "a72f25f0bc933b2a8e2f4dfdeb90c3786fdf5210de864f699e3bdaef2d24b093",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A financial application requires an immutable, cryptographically verifiable transaction log with SQL-like querying. Which AWS database service should you choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon QLDB",
      "B": "Amazon DynamoDB",
      "C": "Amazon RDS for PostgreSQL",
      "D": "Amazon Neptune"
    },
    "explanation": "QLDB provides a ledger with an immutable, verifiable log and SQL-compatible queries; DynamoDB is general NoSQL, RDS and Neptune don\u2019t provide built-in immutable ledgers."
  },
  {
    "id": "ead467d5784c957ee945788892dde0e6a4db8397d5eb9921fe7e35bc6bc6bc6d",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "Your organization requires Microsoft SQL Server Enterprise with complete OS and database-level control, including custom patching and licensing. Which deployment option is best?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "SQL Server on Amazon EC2",
      "B": "Amazon RDS for SQL Server",
      "C": "Amazon Aurora (MySQL-compatible)",
      "D": "Amazon DynamoDB"
    },
    "explanation": "EC2 allows full OS and DB control, custom patching, and BYOL for Enterprise; RDS is managed with limited OS access, Aurora isn\u2019t SQL Server, DynamoDB is NoSQL."
  },
  {
    "id": "5eac4da145ed3dd5bc3e0c759c789fe6a8715e29cdebae56ebc433946fa7c0d7",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "You need multi-master, read-write concurrency across multiple Availability Zones in a single region for a relational database. Which AWS service supports this?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon RDS Multi-AZ",
      "B": "Amazon DynamoDB Global Tables",
      "C": "Amazon Aurora Multi-Master",
      "D": "Amazon ElastiCache for Redis"
    },
    "explanation": "Aurora Multi-Master allows read-write operations on multiple writer instances across AZs; RDS Multi-AZ is standby only, DynamoDB global tables are NoSQL across regions, ElastiCache is in-memory cache."
  },
  {
    "id": "ae97485dbf61e2f0dfe865d0fb45ae859a3c364e267ee2c311d063a79510bfab",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "A microservices architecture using Amazon Aurora MySQL needs to scale out hundreds of connections and manage failover without code changes. Which service should you implement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon RDS Proxy",
      "B": "Amazon Elastic Load Balancer",
      "C": "Amazon CloudFront",
      "D": "AWS Application Auto Scaling"
    },
    "explanation": "RDS Proxy pools and shares database connections for Aurora/RDS, improving scalability and failover; ELB and CloudFront are for HTTP traffic, Auto Scaling is for compute."
  },
  {
    "id": "d7f9d235abe0a1b60f1f5b34fb8529e0159305382cc741ed5d4c7632a644c721",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "Your application requires a MySQL-compatible, serverless database that automatically scales compute capacity based on demand. Which AWS service should you choose?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Aurora Serverless",
      "B": "Amazon RDS for MySQL",
      "C": "Amazon DynamoDB",
      "D": "Amazon DocumentDB"
    },
    "explanation": "Aurora Serverless automatically scales compute capacity for MySQL-compatible workloads; RDS MySQL requires manual scaling, DynamoDB is NoSQL, DocumentDB is MongoDB-compatible."
  },
  {
    "id": "0e1a1022ddf58f992327a762746072f8e88b5a6eb9be4fa8893f27c053467609",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.3",
    "stem": "You need to migrate a large on-premises Oracle database to Amazon RDS for Oracle with minimal downtime and continuous replication of changes. Which DMS setting should you enable?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Full load plus change data capture (CDC)",
      "B": "Full load only",
      "C": "CDC only",
      "D": "Schema conversion only"
    },
    "explanation": "DMS Full Load + CDC migrates existing data then captures ongoing changes for near zero-downtime migration; full load only delays cutover, CDC only can\u2019t initialize data."
  },
  {
    "id": "80068214cd8d0a3655f24e9444c97e739dd8923d79ddfa15aa001883841ce076",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "A gaming platform needs a globally distributed, key-value store with active-active writes in multiple regions for low-latency user sessions. Which AWS service should you use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon DynamoDB Global Tables",
      "B": "Amazon Aurora Global Database",
      "C": "Amazon ElastiCache for Redis",
      "D": "Amazon RDS Multi-AZ"
    },
    "explanation": "DynamoDB Global Tables provide multi-region, active-active NoSQL key-value replication; Aurora Global is read-only in secondary regions, ElastiCache and RDS Multi-AZ do not support multi-region writes."
  },
  {
    "id": "b24aa678dbc4a68647a5ddbb5f70dcd3171d4d6640764c065c7ff3af032d3b88",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "You must offload read traffic from your Amazon Aurora primary to reduce latency and scale read capacity within a region. Which feature should you add?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Aurora Replicas",
      "B": "Amazon RDS Read Replicas",
      "C": "DynamoDB Accelerator (DAX)",
      "D": "Amazon ElastiCache"
    },
    "explanation": "Aurora Replicas provide low-latency, managed read scaling within the Aurora cluster; RDS replicas don\u2019t apply to Aurora clusters, DAX is for DynamoDB, ElastiCache is separate cache."
  },
  {
    "id": "c4b1140d5d053e65353e04e0cc5afed48e74b99719ed1612e438f4e856b34e13",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A company has a VPC with private subnets that host application servers. These servers must download OS updates from the internet but must not accept any inbound connections. Which configuration best meets these requirements?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Attach an Internet Gateway to the VPC and add a route for 0.0.0.0/0 in the private subnet route table.",
      "B": "Deploy a NAT instance in a private subnet with source/destination checks enabled and point the private subnet route table to it.",
      "C": "Deploy a managed NAT Gateway in a public subnet and configure the private subnet\u2019s route table to direct internet-bound traffic to it.",
      "D": "Create a VPC gateway endpoint for Amazon S3 and point the private subnet route table to it."
    },
    "explanation": "A NAT Gateway in a public subnet allows outbound internet access for private subnets without accepting inbound connections. An IGW on the private subnet would expose instances inbound. A NAT instance with source/dest checks enabled would drop traffic. An S3 endpoint only covers S3, not general OS updates."
  },
  {
    "id": "f75d1fcabebc8f9e95db66c7777f5d06cc8b19b079756132069b1c2235abee0d",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "Your security team wants to block all HTTP traffic (port 80) to a set of EC2 instances in a public subnet, but still allow HTTPS (port 443). You must implement this at the subnet boundary and require minimal maintenance. Which solution achieves this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use security groups on the instances to deny port 80 and allow port 443.",
      "B": "Configure a Network ACL on the public subnet to deny inbound port 80 and allow port 443, ensuring rule numbers guarantee order.",
      "C": "Attach an inline firewall appliance in the subnet to filter port 80.",
      "D": "Implement an AWS WAF web ACL on the Internet Gateway to block port 80 traffic."
    },
    "explanation": "Network ACLs act at the subnet boundary and can explicitly deny port 80 while allowing 443. Security groups are stateful and cannot explicitly deny. WAF works at HTTP layer via ALB or CloudFront, not on IGW directly. A firewall appliance increases maintenance."
  },
  {
    "id": "8fd1cfaf639e157e94aa2454f11b63a6fbb5851a7fe87e5d643c672177766f13",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "A data analytics application in a private subnet needs to pull objects from Amazon S3. The security policy prohibits any direct internet egress or NAT devices. Which design allows S3 access without violating policy?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Create a Gateway VPC endpoint for S3 and update the private subnet route table to use this endpoint.",
      "B": "Use an Internet Gateway with a route for S3 IP ranges in the private subnet route table.",
      "C": "Deploy a NAT Gateway in a separate VPC and peer with the application VPC.",
      "D": "Configure an interface endpoint for S3 in the private subnet."
    },
    "explanation": "A Gateway endpoint for S3 provides private, internet-free access to S3. Interface endpoints cannot be used for S3. Internet Gateway breaks the no-egress policy. NAT Gateway is disallowed by policy and peering would still require egress."
  },
  {
    "id": "e5b004cf59eedd2f762c2bce4ddcb4843ecd4dfc97f122efd4bfb8ce9f58909d",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "Your organization has three VPCs in the same account in different Regions. They need to communicate to a central data processing VPC using transitive routing. Peering is not sufficient. Which network service solves this with minimal overhead?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create VPC Peering connections from each VPC to the central VPC and configure route tables.",
      "B": "Use an AWS Transit Gateway with attachments for each VPC and enable inter-region peering.",
      "C": "Set up a fully meshed Site-to-Site VPN between the VPCs.",
      "D": "Deploy a NAT Gateway in the central VPC and route traffic through it."
    },
    "explanation": "AWS Transit Gateway supports transitive routing and inter-region attachments with minimal peering configuration. VPC peering is non-transitive. A full mesh VPN is complex and costly. NAT Gateway does not route VPC-to-VPC."
  },
  {
    "id": "6d60d5a877a750d8e2fc74381a61390605e73a91a2c02fb9d19c54da4e22f556",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "A hybrid network requires encrypted connectivity between on-premises and AWS with consistent latency and high throughput. You have a Direct Connect connection but need encryption. Which option meets these requirements?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use the Direct Connect public VIF to access AWS services directly over the connection.",
      "B": "Create a Site-to-Site VPN over the Internet as a backup only.",
      "C": "Tunnel the Direct Connect private VIF through an AWS Client VPN endpoint.",
      "D": "Establish a Site-to-Site VPN over the Direct Connect connection using a private virtual interface."
    },
    "explanation": "You can run a VPN tunnel over a Direct Connect private VIF to combine dedicated bandwidth with encryption. Public VIF does not encrypt. Client VPN is for client-to-VPC. An Internet VPN backup doesn\u2019t use Direct Connect bandwidth."
  },
  {
    "id": "a2e6fe8dc335b6952218fec6c571b4c2377748a16608ed8205262b007d09e8bb",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A globally distributed application uses Route 53 to direct users to the lowest-latency Region. One Region must be active alone and only failover on failure. Which routing policy combination should you implement?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Weighted routing with weights set to favor the primary Region.",
      "B": "Latency-based routing with a health check to remove the primary Region on failure.",
      "C": "Geolocation routing to send all traffic to the primary Region.",
      "D": "Latency-based routing for latency optimization and setup a secondary Region with failover via health checks."
    },
    "explanation": "Use latency-based routing for regular traffic to the lowest latency endpoint, with a health check on the primary to failover to the secondary only when unhealthy. Geolocation would not optimize latency. Weighted cannot guarantee failover only on failure."
  },
  {
    "id": "df4c9bb7af348b05409a67037c70e25662f4405758b91b7a5f4de79bd8b60ff9",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "You created a private hosted zone in Route 53 and associated it to a VPC in account A. You now need to resolve its records from a VPC in account B without exposing them publicly. What is required?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a Route 53 record alias in account B pointing to the private zone.",
      "B": "Share the hosted zone with account B via AWS Resource Access Manager and associate the VPC.",
      "C": "Disassociate and re-create the zone as public.",
      "D": "Set up a Route 53 resolver endpoint in account B forwarding to account A Internet Resolver."
    },
    "explanation": "Private hosted zones can be shared to other accounts via AWS RAM and then associated to their VPCs. Alias records cannot cross accounts, and public exposure is not allowed. A resolver forwarding endpoint would still require endpoint in account A."
  },
  {
    "id": "ece79c4a5d58332ea6f6684b63f2e83e8e44fa758ef80b283c68b68e921310bc",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.7",
    "stem": "An application in your VPC must call AWS Kinesis Data Streams. It runs in private subnets that have no internet access. Which network endpoint configuration is most appropriate?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Create a Gateway VPC endpoint for Kinesis Data Streams and update the route table.",
      "B": "Peer the VPC to an AWS-managed VPC for Kinesis access.",
      "C": "Create an interface endpoint (AWS PrivateLink) in the private subnets for Kinesis Data Streams.",
      "D": "Attach an Internet Gateway to the private subnets and assign public IPs."
    },
    "explanation": "Kinesis Data Streams requires an interface endpoint (AWS PrivateLink) for private-access from subnets without internet. Gateway endpoints only support S3 and DynamoDB. Peering to AWS VPC isn\u2019t supported. IGW with public IPs breaks no-internet requirement."
  },
  {
    "id": "009b4190c600cdc25b868e6e32ab1f0c72871db37ccdd5370afe0f53ed60da66",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "A VPC has two subnets: one private and one public. You attach a Site-to-Site VPN and enable route propagation to the main route table. You notice on-premises hosts cannot reach instances in the private subnet. What is the most likely cause?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "The virtual private gateway is misconfigured\u2014use an Internet Gateway instead.",
      "B": "The private subnet\u2019s custom route table does not have the propagated route from the VPN attached.",
      "C": "Security groups are blocking the VPN traffic on the EC2 instances.",
      "D": "NAT Gateway is used in the private subnet instead of IGW."
    },
    "explanation": "If you propagate routes to the main route table but your private subnet uses a separate custom route table that you did not attach propagation to, on-premises routes won\u2019t reach it. Security groups are stateful but normally allow return. A NAT Gateway isn\u2019t used for VPN traffic."
  },
  {
    "id": "1cd8303c397b8e7f184d59f8e98bdd6c2414e9602547eaade8fe8118218e8dcb",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.3",
    "stem": "Your network team requires all HTTP-based requests from on-premises for a specific API endpoint in AWS to traverse a mutual TLS connection. The API is in a private subnet fronted by a Network Load Balancer. Which solution will meet this requirement?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure the NLB to terminate TLS and proxy on plain HTTP to the targets.",
      "B": "Use an Application Load Balancer with a customer-managed certificate.",
      "C": "Place an Internet Gateway in front of the private subnet to accept mTLS.",
      "D": "Use a TLS-enabled NLB with an Amazon Certificate Manager private certificate authority to perform mTLS at the listener."
    },
    "explanation": "A Network Load Balancer can be configured for TLS listeners and mTLS by integrating ACM Private CA certificates at the NLB, ensuring mutual authentication. ALB supports mTLS only in certain versions but the requirement specifies NLB. IGW cannot terminate mTLS."
  },
  {
    "id": "073982d944f28ee2ffe9e9768aec0207ed78eda06b82b61f5d2db99ad423f382",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.3",
    "stem": "You want to enforce least-privilege network access by limiting which IP protocols and ports EC2 instances can use in a subnet. You also want automatic stateful return traffic. Which construct should you use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Network ACLs, since they can deny traffic per rule and are stateful.",
      "B": "Route tables, because you can block unwanted IP protocols.",
      "C": "Security groups, because they are stateful and support allow rules per protocol and port.",
      "D": "AWS WAF on the subnet to filter traffic at the network level."
    },
    "explanation": "Security groups are stateful, allow whitelisting of specific protocols and ports, and automatically permit return traffic. NACLs are stateless and require explicit return rules. Route tables cannot filter by port, and WAF works at the application layer."
  },
  {
    "id": "058480bebfe74f83157ee8f37a7afcf25f00ecb8a876f273d8f0093441ca5d4a",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A VPC is peered with another VPC. Instances in each VPC cannot resolve private DNS hostnames for services in the peer. Which configuration change will enable DNS resolution across the peering connection?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable \"Allow DNS resolution from peer VPC\" and \"Allow DNS hostnames\" in the peering connection settings.",
      "B": "Create Route 53 public hosted zones for the private hostnames and associate them to both VPCs.",
      "C": "Deploy an interface endpoint in each VPC for Route 53.",
      "D": "Use AWS Global Accelerator to front the services and resolve DNS globally."
    },
    "explanation": "Enabling DNS resolution in the VPC peering options allows private hostnames to be resolved across peering. Public hosted zones would expose records publicly. Interface endpoints and Global Accelerator are not used for VPC peering DNS resolution."
  },
  {
    "id": "82b4ecf3f18374366bd9c972a365def68889a4252cbb91e04bc9fcab1acf404b",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "When designing a VPC with multiple subnets, you need to ensure that a NAT device fails over without manual intervention if an Availability Zone goes down. Which design achieves this?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Deploy a single NAT instance in one AZ and enable Auto Recovery.",
      "B": "Deploy one NAT Gateway in each AZ and configure each private subnet\u2019s route table to point to the NAT Gateway in its AZ.",
      "C": "Use a single NAT Gateway in one AZ with an elastic network interface that can fail over to the other AZ.",
      "D": "Use a single NAT instance behind an Application Load Balancer spanning both AZs."
    },
    "explanation": "Placing a NAT Gateway in each AZ and routing each private subnet to its local NAT Gateway provides zone-level resiliency. NAT Gateways cannot be shared across AZs and cannot fail over automatically between AZs."
  },
  {
    "id": "1d0e058f3d965883e4a37253e6c09ae70e3d88291ee8834708c1214880be425b",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.7",
    "stem": "Your enterprise wants to simplify inbound DNS resolution for hundreds of on-premises clients to private AWS namespaces. You must avoid public DNS. What architecture solves this at scale?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Install on-premises recursive DNS servers that forward to the VPC private hosted zone.",
      "B": "Open your private hosted zone to the internet and use public DNS servers.",
      "C": "Deploy Route 53 Resolver inbound endpoints in VPCs and configure on-premises DNS forwarders to forward specific domains to them.",
      "D": "Use AWS Client VPN to allow on-premises clients to join the VPC and use VPC-resident DNS."
    },
    "explanation": "Route 53 Resolver inbound endpoints allow on-premises DNS servers to forward queries for private zones without exposing them publicly. Client VPN is not needed for DNS forwarding. Public hosting is disallowed."
  },
  {
    "id": "44a9648f991be4cfa8fa100bba7aafc7661b8c75786026b04dc86fdef1a3f23c",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.7",
    "stem": "A research institution runs an HPC cluster on EC2 instances. The cluster must mount a POSIX file system backed by Amazon S3 for large scientific datasets, with high throughput and transparent caching of active data. Which AWS storage service best meets these requirements?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon FSx for Lustre",
      "B": "Amazon EFS",
      "C": "Amazon S3 Standard",
      "D": "Amazon FSx for Windows File Server"
    },
    "explanation": "FSx for Lustre provides a POSIX-compliant file system that transparently caches active data from S3 with high throughput, making it ideal for HPC workloads. EFS is a fully managed file system but not optimized for direct S3 integration or HPC throughput. S3 Standard is object storage, not POSIX. FSx for Windows File Server is Windows-only and SMB protocol."
  },
  {
    "id": "6c28415d1464099243a43747c15a9d2bcc5f997ea5bc1ec7e20c247d5164c02e",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.7",
    "stem": "An on-premises application requires an NFS file share that seamlessly stores data in Amazon S3 while caching frequently accessed files locally. Which AWS service should be deployed?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Storage Gateway File Gateway",
      "B": "AWS Storage Gateway Tape Gateway",
      "C": "Amazon EFS",
      "D": "Amazon FSx for Lustre"
    },
    "explanation": "File Gateway presents an NFS interface that caches hot files locally and stores objects in S3. Tape Gateway is for virtual tapes, EFS is in-cloud only, and FSx for Lustre is POSIX but not an on-premises gateway."
  },
  {
    "id": "66a0a08e76031bf699f0c879ad4652a3e436265d9e1551fa9604f742225f0fab",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A company needs to present iSCSI block storage on-premises with local caching for performance and asynchronous snapshots to AWS for DR. Which Storage Gateway configuration meets this need?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Storage Gateway Tape Gateway",
      "B": "AWS Storage Gateway Volume Gateway (Cached mode)",
      "C": "AWS Storage Gateway Volume Gateway (Stored mode)",
      "D": "Amazon EBS"
    },
    "explanation": "Volume Gateway in Cached mode provides local caching of frequently accessed data while storing the full dataset in AWS and enables asynchronous EBS snapshots. Stored mode keeps the primary data on-premises. Tape Gateway presents virtual tapes, and EBS is for EC2 only."
  },
  {
    "id": "f79dabe20840aeece86b08ce1d598ff6e4ced842760f550fb1b5702d9ddb8e09",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "A financial firm must replace its physical tape library with a virtual tape solution that stores archives in Amazon S3 and transitions to Amazon S3 Glacier. Which AWS Storage Gateway type should they use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Storage Gateway File Gateway",
      "B": "AWS Storage Gateway Volume Gateway",
      "C": "Amazon S3 Glacier Instant Retrieval",
      "D": "AWS Storage Gateway Tape Gateway"
    },
    "explanation": "Tape Gateway emulates a tape library and writes virtual tapes to S3, which can then transition to Glacier. File Gateway and Volume Gateway do not emulate tape libraries."
  },
  {
    "id": "3d4b71be501816c4457730f4c2941e8d416d0021d53d42ca19d3ce9e0d27eba6",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "A latency-sensitive OLTP database on EC2 requires a block volume with up to 60,000 IOPS and single-digit millisecond latency. Which Amazon EBS volume type should be selected?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "gp3",
      "B": "io2 Block Express",
      "C": "gp2",
      "D": "sc1"
    },
    "explanation": "io2 Block Express supports very high IOPS (up to 256,000) and sub-millisecond latency for mission-critical workloads. gp3 is sufficient for moderate IOPS but not this high; gp2 and sc1 are lower performance."
  },
  {
    "id": "c83a2b54547f95960d2aecd8d84c685a8eda793f0875ddcac147e88d2228a40c",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.4",
    "stem": "A Windows-based multi-AZ application requires an SMB file share with Active Directory integration and automated backups. Which AWS storage service should be used?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon EFS",
      "B": "Amazon FSx for Lustre",
      "C": "Amazon FSx for Windows File Server",
      "D": "AWS Storage Gateway File Gateway"
    },
    "explanation": "FSx for Windows File Server provides a fully managed, multi-AZ SMB file share with AD integration and native Windows features. EFS uses NFS, Lustre is POSIX, and File Gateway is on-premises only."
  },
  {
    "id": "63ad55f98895c30d1f0c5a5568a03293fb51c7633ec226caaaf58d039724ec21",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "A data lake stores billions of small objects with unpredictable access patterns. To optimize cost without impacting performance, which S3 storage class should be chosen?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "S3 Intelligent-Tiering",
      "B": "S3 Standard-Infrequent Access",
      "C": "S3 One Zone-Infrequent Access",
      "D": "S3 Glacier Instant Retrieval"
    },
    "explanation": "Intelligent-Tiering automatically moves data between frequent and infrequent tiers based on access patterns, optimizing cost. Standard-IA and One Zone-IA require manual tiering. Glacier Instant Retrieval is archival with higher access costs."
  },
  {
    "id": "2a4ed855e3f66584bd56d795afa815a45478fd9229107804bf8df6343d0584c4",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "An analytics team archives logs but requires milliseconds-latency retrieval when investigating issues. Which storage class minimizes cost while meeting this performance?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "S3 Standard-Infrequent Access",
      "B": "S3 Glacier Flexible Retrieval",
      "C": "S3 Glacier Instant Retrieval",
      "D": "S3 Glacier Deep Archive"
    },
    "explanation": "Glacier Instant Retrieval offers low storage cost with millisecond retrieval latency. Standard-IA is more expensive for infrequent access, Flexible Retrieval has minutes-level latency, and Deep Archive has hours-level latency."
  },
  {
    "id": "cdc094e752e7172f156028e7e582b48b090c0c2ae33adac03b2f419fe1365b1b",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A compliance requirement mandates storing data for 7 years with retrieval times under 5 minutes. Which S3 storage class should be used?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "S3 Glacier Deep Archive",
      "B": "S3 Glacier Flexible Retrieval",
      "C": "S3 Glacier Instant Retrieval",
      "D": "S3 One Zone-Infrequent Access"
    },
    "explanation": "Glacier Flexible Retrieval (formerly S3 Glacier) provides low-cost archival storage with retrieval options under 5 minutes. Instant Retrieval is more expensive; Deep Archive retrieval takes hours. One Zone-IA is not archival."
  },
  {
    "id": "5fcf7ae0c25d839d344325d876b0a3e9e6832c63e67961ee71c40ce92cc5b7e4",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "You need a lifecycle policy that transitions objects to S3 Standard-IA after 30 days and then to S3 Glacier Flexible Retrieval after 90 days. Which approach achieves this in the simplest way?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Create two separate lifecycle rules, each with a single transition action",
      "B": "Create one lifecycle rule with two transition actions defined",
      "C": "Use S3 Intelligent-Tiering instead of manual transitions",
      "D": "Use AWS Backup lifecycle policies"
    },
    "explanation": "A single S3 lifecycle rule can define multiple transition actions at different ages. Separate rules are unnecessary. Intelligent-Tiering may incur monitoring charges, and AWS Backup does not manage S3 lifecycle."
  },
  {
    "id": "7b4cd7a73ada66a799c07819a6ede3146816d87bf841bd87bde5ebf3907fa28b",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "An organization wants to centrally manage backups of Amazon EFS file systems, RDS databases, and DynamoDB tables via a single service. Which service provides this capability?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Backup",
      "B": "AWS Data Lifecycle Manager",
      "C": "AWS Storage Gateway",
      "D": "Amazon S3 Lifecycle Management"
    },
    "explanation": "AWS Backup natively orchestrates backups for EFS, RDS, DynamoDB, and other supported resources centrally. Data Lifecycle Manager only handles EBS snapshots and AMIs. Storage Gateway is a data gateway, and S3 Lifecycle Management applies only to S3 objects."
  },
  {
    "id": "41e1dce09b6e931e9b113f5cdc6406fba026430c7d5db1ed513f74c1f5c14c69",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "A staging area for regenerated data can tolerate AZ outages and will be accessed rarely. Data must be retained for 1 year at the lowest possible cost. Which storage class should be selected?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "S3 Standard-Infrequent Access",
      "B": "S3 Glacier Flexible Retrieval",
      "C": "S3 One Zone-Infrequent Access",
      "D": "S3 Glacier Deep Archive"
    },
    "explanation": "One Zone-IA stores data in a single AZ at lower cost than Standard-IA. It tolerates AZ loss and is ideal for noncritical, infrequently accessed data. Glacier classes add retrieval latency."
  },
  {
    "id": "6a13b2391c426b262cdc83927b45b9ffcb599bccd750bb5d9dabf64714f70a61",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "For cross-region disaster recovery of static website content in S3, you need automatic, asynchronous replication of new objects to a secondary region. Which feature should be used?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "S3 Transfer Acceleration",
      "B": "S3 Cross-Region Replication",
      "C": "S3 Batch Replication",
      "D": "AWS CloudFront Origin Failover"
    },
    "explanation": "S3 Cross-Region Replication automatically and asynchronously replicates new objects between buckets in different regions. Transfer Acceleration speeds uploads, Batch Replication is manual, and CloudFront handles CDN failover, not storage replication."
  },
  {
    "id": "7d92c7168caa0edc4dabe80fc76194aeaa5a41b01079551236c2df6a3299565c",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A big data ETL job requires scratch storage on each EC2 instance for intermediate files, but data loss on instance termination is acceptable. Which storage option is most cost-effective?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon EBS gp3",
      "B": "Amazon EFS",
      "C": "Amazon S3",
      "D": "EC2 instance store"
    },
    "explanation": "Instance store provides ephemeral block storage at no extra cost, ideal for temporary scratch space. EBS, EFS, and S3 all incur storage and I/O costs and persist beyond instance termination."
  },
  {
    "id": "d3e933ebc4246450552641aa9754321f062cc13680b0b8b8c1802406aa700fe9",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A Linux application requires a boot volume with encryption at rest using AWS KMS, baseline of 3,000 IOPS, and cost optimization. Which EBS volume type meets these needs?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "gp3 with encryption enabled",
      "B": "gp2 with default encryption",
      "C": "io1 with encryption enabled",
      "D": "st1 with encryption enabled"
    },
    "explanation": "gp3 supports provisioned IOPS, baseline performance above 3,000 IOPS, and encryption via KMS at lower cost than io1. gp2 cannot provision IOPS, io1 is more expensive, and st1 is for throughput-optimized HDD."
  },
  {
    "id": "948d7057675d860e69e74c4d113d88b1331983cb38695ff0b5a1e58e83e0b49a",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A gaming company wants to perform real-time analytics on clickstream data to compute player engagement metrics using SQL. They also require the output to be delivered to a Lambda function for alert generation. Which AWS service best meets these requirements with minimal operational overhead?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Kinesis Data Streams",
      "B": "Amazon Kinesis Data Analytics",
      "C": "AWS Lambda",
      "D": "Amazon Kinesis Data Firehose"
    },
    "explanation": "Amazon Kinesis Data Analytics lets you run continuous SQL queries on streaming data and natively deliver results to Lambda. Data Streams only ingests, Firehose buffers and delivers without SQL transforms, and Lambda alone requires custom ingestion code."
  },
  {
    "id": "ea659760e49da00449066e79ba88bb596b5e6e68ff4f87b3a1f588cfca282105",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "An insurance company needs to extract printed and handwritten text, including tables and forms, from scanned claim documents for downstream processing. Which AWS service should they choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Comprehend",
      "B": "Amazon Textract",
      "C": "Amazon Rekognition",
      "D": "Amazon SageMaker"
    },
    "explanation": "Amazon Textract is designed for OCR of text, handwriting, and table structures. Comprehend analyzes text semantics after extraction, Rekognition focuses on images and face detection, and SageMaker is for custom ML model training."
  },
  {
    "id": "12f638d303d831e289cc43a0ff16469295853deaad0e717f31c5408402703f9c",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "An enterprise wants a fully managed search service that allows business users to ask natural language questions against a repository of technical manuals, PDFs, and web pages and receive accurate, context-aware answers. Which service is best?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon OpenSearch Service",
      "B": "Amazon Kendra",
      "C": "Amazon Athena",
      "D": "Amazon QuickSight"
    },
    "explanation": "Amazon Kendra provides a managed, ML-powered search optimized for natural language queries. OpenSearch requires custom configuration, Athena is SQL-on-S3, and QuickSight is BI visualization."
  },
  {
    "id": "221212729566052e636211931ca5ee9f27d72fda2f3005ca948ebed1a9fc069c",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "A retail analytics team needs to run a multi-node Hadoop cluster with Spark and Hive on petabytes of data in S3, while minimizing cluster setup and management overhead. Which service should they use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon EMR",
      "B": "AWS Glue",
      "C": "AWS Lambda",
      "D": "Amazon Redshift"
    },
    "explanation": "Amazon EMR provides managed Hadoop, Spark, and Hive clusters. Glue is serverless ETL without full Hadoop environment, Lambda is not for large-scale clusters, and Redshift is a data warehouse, not Hadoop ecosystem."
  },
  {
    "id": "429e4f06d9c0d7dd515fb23e7f5170b37dd05c390b9d4e2b8dd10a0fe0983c9c",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A company wants to build, train, tune, and deploy a custom deep learning image classification model with managed infrastructure, automated hyperparameter optimization, and scalable hosting endpoints. Which AWS service should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Comprehend",
      "B": "Amazon Rekognition",
      "C": "Amazon SageMaker",
      "D": "Amazon Textract"
    },
    "explanation": "Amazon SageMaker provides end-to-end managed ML workflows including training, tuning, and hosting. Comprehend and Rekognition are pretrained services, and Textract is OCR only."
  },
  {
    "id": "b746d39c1a2711fe29b727288ce277dba97c924e71e54ac8a2ced96b0a95b3dc",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A financial institution needs to detect explicit or suggestive content and identify faces in user-uploaded images to enforce compliance. Which service is most appropriate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Rekognition",
      "B": "Amazon Comprehend",
      "C": "Amazon Macie",
      "D": "AWS Key Management Service"
    },
    "explanation": "Amazon Rekognition performs image moderation and face detection. Comprehend is text-based, Macie classifies S3 data for PII, and KMS manages encryption keys."
  },
  {
    "id": "2094c7329426c6e52157f71cc96fcf568e6dfc42b7315c7bef0562837be83c63",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.3",
    "stem": "A data analyst needs to run ad hoc SQL queries directly on structured and unstructured data stored in S3 without provisioning or managing any clusters. Which service should they choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Athena",
      "B": "Amazon Redshift",
      "C": "Amazon EMR",
      "D": "Amazon QuickSight"
    },
    "explanation": "Athena provides serverless SQL-on-S3 for ad hoc queries. Redshift requires cluster management, EMR is for Hadoop clusters, and QuickSight is a BI tool, not a query engine."
  },
  {
    "id": "67d80b3a2b9f2a6bcfa7342543dfd4eeeba6b0494a77755a0c5718b22138d2ce",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A marketing team needs to build interactive dashboards with embedded ML-powered anomaly detection and forecasting on time-series sales data stored in S3 and Redshift. Which AWS service meets these requirements?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon QuickSight",
      "B": "Amazon Athena",
      "C": "AWS Glue",
      "D": "Amazon SageMaker"
    },
    "explanation": "Amazon QuickSight supports interactive dashboards, ML insights, and forecasting on data from S3 and Redshift. Athena and Glue don\u2019t provide dashboards, and SageMaker is for custom ML."
  },
  {
    "id": "88c185092ad2055e936795df0ca490797d93cdd50c4a0fc64eb73793ecac339d",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A media company wants to ingest streaming video metadata, transform it into Parquet format, and deliver it to S3 every minute with dynamic partitioning. Which service should they use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Kinesis Data Streams",
      "B": "Amazon Kinesis Data Firehose",
      "C": "Amazon Kinesis Data Analytics",
      "D": "AWS Lambda"
    },
    "explanation": "Kinesis Data Firehose can batch, transform (e.g., to Parquet via Lambda), and deliver streams to S3 with partitioning. Data Streams only transports raw data, Analytics runs SQL, and Lambda alone requires custom orchestration."
  },
  {
    "id": "0ff29f2b77f2d866d1d84c93f703b427b8799b04cd4206f07250ff5b00aa2577",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A healthcare analytics team needs a petabyte-scale, columnar data warehouse that supports high concurrency, mass ingestion, and complex joins for clinical trial data. Which service is most appropriate?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Redshift",
      "B": "Amazon Athena",
      "C": "Amazon EMR",
      "D": "Amazon OpenSearch Service"
    },
    "explanation": "Redshift is a petabyte-scale, managed columnar data warehouse optimized for complex queries and concurrency. Athena is serverless SQL-on-S3 without high-concurrency tuning, EMR is Hadoop-based, and OpenSearch is search focused."
  },
  {
    "id": "0deeabf79789a6ebac7d136a3dd31eb024a5139c92f8e854d48def755f37347f",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "A data engineering team needs a managed data catalog with built-in ETL, schema discovery, and job scheduling for data in S3 and JDBC sources. Which service should they pick?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Glue",
      "B": "AWS Data Pipeline",
      "C": "Amazon EMR",
      "D": "AWS Step Functions"
    },
    "explanation": "AWS Glue provides a serverless data catalog, crawlers for schema discovery, and managed ETL with scheduling. Data Pipeline orchestrates tasks but lacks a unified catalog, EMR isn\u2019t purpose-built for cataloging, and Step Functions orchestrate workflows."
  },
  {
    "id": "27b1cf55afaa12a115d50391b77b9d4f2d0919331414b0b785326510ace2992d",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "A global e-commerce site needs to provide dynamic language translation of product descriptions into multiple target languages on the fly. Which AWS service fulfills this requirement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Translate",
      "B": "Amazon Comprehend",
      "C": "Amazon Polly",
      "D": "Amazon Lex"
    },
    "explanation": "Amazon Translate provides real-time, neural machine translation. Comprehend analyzes text sentiment, Polly synthesizes speech, and Lex builds conversational chatbots."
  },
  {
    "id": "c7b7359a446e91f13224e6e234a6342f56a809c4313d78066be1ffa6377a4b28",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A customer service team wants to automatically detect sentiment and extract key entities (names, dates, amounts) from customer emails for routing and priority tagging. Which service should they use?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Comprehend",
      "B": "Amazon Textract",
      "C": "Amazon SageMaker",
      "D": "Amazon Rekognition"
    },
    "explanation": "Comprehend provides sentiment analysis and entity recognition on text. Textract extracts text from documents, SageMaker is for custom models, and Rekognition handles images."
  },
  {
    "id": "fdb5a172f20fade9f6ffde7b21143034edb34314840aecfeb407b165c544526b",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A call center wants to convert hours of recorded customer support calls into text transcripts to drive downstream NLP analysis. Which AWS service should they choose?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Transcribe",
      "B": "Amazon Comprehend",
      "C": "Amazon Lex",
      "D": "Amazon Polly"
    },
    "explanation": "Amazon Transcribe converts speech to text. Comprehend analyzes text, Lex builds bots, and Polly converts text to speech."
  },
  {
    "id": "2ae2dc17b7fb1be0056c0a0fa50d9fffb260f13e3dd9b01de56cef2fd9e7abdf",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.7",
    "stem": "A DevOps team needs a managed search and analytics engine to index, search, and visualize application logs in near real time with built-in Kibana dashboards. Which AWS service should they use?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon OpenSearch Service",
      "B": "Amazon Athena",
      "C": "Amazon EMR",
      "D": "Amazon QuickSight"
    },
    "explanation": "OpenSearch Service provides a managed Elasticsearch engine with Kibana for real-time log indexing, search, and dashboards. Athena queries S3 logs asynchronously, EMR is for big-data clusters, and QuickSight visualizes data."
  },
  {
    "id": "02838ffe8513971634af3c12f4d5827a084e90d49530cf33bdf8a5d1cf878948",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A multinational organization needs to centralize custom events from multiple AWS accounts and route them to specific targets such as AWS Lambda functions, SQS queues, and third-party SaaS endpoints based on event patterns. Which AWS service provides a fully managed event bus solution for this requirement?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon SNS (Simple Notification Service)",
      "B": "AWS Step Functions",
      "C": "Amazon EventBridge",
      "D": "Amazon SQS (Simple Queue Service)"
    },
    "explanation": "Amazon EventBridge offers a fully managed event bus with pattern-based routing to multiple targets. SNS is pub/sub only, Step Functions orchestrates workflows, and SQS is a queue without event pattern routing."
  },
  {
    "id": "c7afb53c1523dfb3ea9547028dc3cc437b1c2759458d18bd7ce281216518205a",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A financial services application processes transactions that require exactly-once message delivery and strict ordering guarantees between producers and consumers. Which AWS service should be used to meet these requirements?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon SNS standard topics",
      "B": "Amazon SQS standard queues",
      "C": "Amazon SQS FIFO queues",
      "D": "Amazon EventBridge"
    },
    "explanation": "SQS FIFO queues provide exactly-once processing and ordering guarantees. Standard topics/queues offer at-least-once delivery without ordering, and EventBridge is at-least-once delivery only."
  },
  {
    "id": "987d633f968db7182d5a0d7404c0f56632c210695cd206cf867b8b81bfb0565c",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "A retail company wants to send high-volume, personalized, transactional email notifications to its customers\u2019 inboxes with low latency and high deliverability. Which AWS service is the most appropriate choice?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon SNS (Simple Notification Service)",
      "B": "Amazon SES (Simple Email Service)",
      "C": "Amazon Connect",
      "D": "AWS Lambda configured with a self-managed SMTP server on EC2"
    },
    "explanation": "Amazon SES is built for high-volume email with deliverability and low latency. SNS is for simple notifications (not email-centric), Connect is a contact-center service, and a self-managed SMTP on EC2 adds operational overhead."
  },
  {
    "id": "ba65469ac126c7fd81d30142a7cb29b300a86cb25134d6d32d67af206db5317c",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A mobile application requires a backend service that supports real-time GraphQL subscriptions, offline data synchronization, and a single GraphQL endpoint for all client data operations. Which AWS service best meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon API Gateway WebSocket APIs",
      "B": "AWS AppSync",
      "C": "Amazon API Gateway REST APIs with Lambda integration",
      "D": "AWS Amplify hosting"
    },
    "explanation": "AWS AppSync provides managed GraphQL APIs with subscriptions, offline sync, and a single endpoint. API Gateway WebSockets don\u2019t natively support GraphQL, REST APIs lack built-in GraphQL support, and Amplify is hosting only."
  },
  {
    "id": "720c68db1669d806c2b3ceb0f1705ab5c4293314b5ea70bdcac1a4d8fabd6346",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "A development team needs a fully managed build service that scales automatically, removes the need to provision and manage build servers, and integrates natively with AWS CodePipeline. Which service should they choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS CodeDeploy",
      "B": "AWS CodeBuild",
      "C": "AWS CodePipeline",
      "D": "AWS Cloud9"
    },
    "explanation": "AWS CodeBuild is a fully managed build service. CodeDeploy handles deployments, CodePipeline orchestrates pipelines, and Cloud9 is a development environment."
  },
  {
    "id": "ece32eae5d673f9d6d2da3984a38647c769b24ef499c1a202d1aaddc0e49ad9a",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A microservices-based application experiences performance bottlenecks. The team needs a service to trace requests across multiple microservices, visualize service maps, and identify latency issues in production. Which AWS service fulfills this requirement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon CloudWatch Metrics",
      "B": "AWS X-Ray",
      "C": "AWS CloudTrail",
      "D": "Amazon Inspector"
    },
    "explanation": "AWS X-Ray provides distributed tracing and service maps. CloudWatch Metrics shows aggregated metrics, CloudTrail logs API calls, and Inspector is for security assessments."
  },
  {
    "id": "8f8b1253ecd270245de28184eeef6d04d8fc37e673faadbbffa4760c338585f6",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "A company needs to provide persistent desktop experiences to remote employees, including access to corporate applications, home directory storage, and Windows OS. Which AWS service should they provision?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon AppStream 2.0",
      "B": "Amazon WorkSpaces",
      "C": "AWS Client VPN",
      "D": "Amazon EC2 Auto Scaling"
    },
    "explanation": "Amazon WorkSpaces delivers persistent virtual desktops with storage and OS. AppStream provides application streaming only, Client VPN provides network access, and Auto Scaling manages EC2 capacity."
  },
  {
    "id": "e333f393a787d5376a75ba987761ab2a38fa2ca6837bd3d7dee6f49a212776fb",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "An organization wants to provide streaming access to a single Windows application through users\u2019 web browsers without installing it locally. Which service is most appropriate?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon AppStream 2.0",
      "B": "Amazon WorkSpaces",
      "C": "Amazon EC2 with Remote Desktop",
      "D": "AWS Direct Connect"
    },
    "explanation": "AppStream 2.0 streams individual applications to browsers. WorkSpaces is full desktops, EC2 RDP requires client setup, and Direct Connect is a private network link."
  },
  {
    "id": "349d3712e65762153635f91b2f661b861e4391100d4f9c93a83df9b7e7570bf8",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.8",
    "stem": "A media company needs to broadcast breaking news alerts simultaneously to subscribers via SMS, mobile push notifications, and email. Which AWS service should they use to meet these multi-protocol distribution requirements efficiently?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon SQS",
      "B": "Amazon SNS",
      "C": "Amazon SES",
      "D": "AWS Lambda"
    },
    "explanation": "Amazon SNS can push messages to SMS, mobile push, and email endpoints. SQS is a queue, SES is email-only, and Lambda is a compute service."
  },
  {
    "id": "d51838ebf0b47e0b9cb02baa86899de20863e06f3cd7bd85524df7c18dfe7ca6",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.1",
    "stem": "A web development team wants to deploy and host a static single-page application in multiple regions with built-in CI/CD triggered by Git commits. Which AWS service should they choose?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Amplify",
      "B": "Amazon S3 static website hosting",
      "C": "Amazon CloudFront",
      "D": "AWS CodePipeline"
    },
    "explanation": "AWS Amplify provides static web hosting with multi-region CI/CD. S3/CloudFront handles hosting/CDN only, and CodePipeline orchestrates pipelines but doesn\u2019t host."
  },
  {
    "id": "792f89d7c12a52c8e3503ec006b056c1902c521f42cc4248f0bdf97fcaf05e92",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "An enterprise needs a fully managed service to securely connect millions of IoT devices, ingest telemetry data, and route messages to other AWS services. Which service should they select?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS IoT Core",
      "B": "Amazon Kinesis Data Streams",
      "C": "AWS IoT Analytics",
      "D": "Amazon MQ"
    },
    "explanation": "AWS IoT Core securely connects devices at scale and routes messages. Kinesis is streaming only, IoT Analytics analyzes data, and MQ is a general broker."
  },
  {
    "id": "125de9ed9607949fdfe85a52163cb154952af90eed102b630a8b158c4a9c26c1",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "An online retailer needs to coordinate a serverless order-processing workflow that includes inventory checks, payment authorization, and shipping label generation, with built-in error handling and retries. Which AWS service should they use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Lambda",
      "B": "AWS Step Functions",
      "C": "Amazon SWF",
      "D": "AWS Batch"
    },
    "explanation": "Step Functions orchestrates serverless workflows with state management and retries. Lambda executes code, SWF is deprecated/complex, and Batch is for batch compute."
  },
  {
    "id": "2d6ff4ed686e8626570410bb6bf1bace97d5300a91a4c7846f9470450652cacf",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A startup requires 24x7 technical support with access to infrastructure event management, rapid response for urgent cases, and a designated technical account manager. Which AWS Support plan meets these requirements?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Business Support",
      "B": "AWS Developer Support",
      "C": "AWS Enterprise On-Ramp",
      "D": "AWS Enterprise Support"
    },
    "explanation": "AWS Enterprise Support includes a technical account manager and event management. Business lacks a TAM, Developer has limited hours, and On-Ramp doesn\u2019t provide full TAM services."
  },
  {
    "id": "b935eabdd985ddc72446ea50d8964bb05c707b14e2ef69373afdd8bc86153097",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.1",
    "stem": "A team wants to build a continuous delivery pipeline modeling build, test, and deploy stages that triggers automatically on code commits and integrates with CodeBuild, CodeDeploy, and CloudFormation. Which AWS service should orchestrate these stages?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS CodeBuild",
      "B": "AWS CodeDeploy",
      "C": "AWS CodePipeline",
      "D": "AWS CloudFormation"
    },
    "explanation": "CodePipeline orchestrates multi-stage pipelines. CodeBuild builds, CodeDeploy deploys, and CloudFormation provisions resources."
  },
  {
    "id": "d464fdbd38bf379ceb210daa2ccde30f0e811853d072e027532274a6d9d645d0",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "A company plans to set up a cloud-based contact center supporting voice calls, chat interactions, and real-time analytics with minimal custom development. Which AWS service should they employ?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Connect",
      "B": "Amazon SES",
      "C": "Amazon Pinpoint",
      "D": "AWS Chime SDK"
    },
    "explanation": "Amazon Connect is a managed contact center with voice, chat, and analytics. SES is email-only, Pinpoint focuses on campaigns, and Chime SDK requires custom development."
  },
  {
    "id": "aa1bf6bbb9205e28f4a866165de55fa1beeb3a896f4f872fbf9aa220618b8879",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "A company runs a fleet of m5.large EC2 instances that maintain a steady-state utilization of 50% 24x7 and occasionally experiences compute spikes of up to 20% utilization that can tolerate interruption. To minimize cost over the next year, which pricing strategy should they use for the baseline and the spikes?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Purchase 1-year Standard Reserved Instances for the steady 50% baseline and use Spot Instances for the 20% spikes.",
      "B": "Run all workload on On-Demand Instances and purchase a compute savings plan for spikes.",
      "C": "Purchase 1-year Convertible Reserved Instances for baseline and use On-Demand Instances for spikes.",
      "D": "Purchase 1-year Standard Reserved Instances for both baseline and spikes to lock in maximum discount."
    },
    "explanation": "Standard RIs give the highest discount for the predictable 50% baseline. Spot Instances are the cheapest option for interruption\u2010tolerant spikes. On-Demand or Convertible RIs for spikes would be more expensive or insufficiently flexible."
  },
  {
    "id": "6900b66bd591d3babfae2450e9df284b14d94eddb783c0ca9cfabf868a66ccc2",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "A startup anticipates a 3-year commitment to EC2 usage but expects to change instance families, operating systems, and regions over time. Which purchasing option offers the greatest cost savings while preserving that flexibility?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "3-year Standard Reserved Instances with All Upfront payment.",
      "B": "3-year Convertible Reserved Instances with No Upfront payment.",
      "C": "3-year EC2-specific Savings Plan with Partial Upfront payment.",
      "D": "3-year Compute Savings Plan with Partial Upfront payment."
    },
    "explanation": "Compute Savings Plans apply to any EC2 instance family, size, OS, region, and also cover Fargate/Lambda, offering more flexibility than Reserved Instances or EC2-specific Savings Plans while still providing significant discounts."
  },
  {
    "id": "2b8abbe189b5ca4f6989f27bc232a5315cfe8b986a94bb6966d4d0f51ce00004",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "Your organization plans to use AWS Lambda extensively and wants to commit to a lowest possible execution cost. Which pricing model should you select?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "A Lambda-specific Savings Plan.",
      "B": "An EC2 Savings Plan.",
      "C": "A Compute Savings Plan.",
      "D": "A Standard Reserved Instance."
    },
    "explanation": "AWS Lambda compute is covered only by Compute Savings Plans. EC2-specific Savings Plans and Reserved Instances do not apply to Lambda."
  },
  {
    "id": "1bd7d01ecab197385c09fe5f6e98dddaf7005d5024636788eab48aca2319bc16",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "An application replicates 5 TB of data monthly between two EC2 instances in different Availability Zones within the same region. What is the approximate monthly data transfer cost?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Zero, because all intra-region EC2 data transfer is free.",
      "B": "Approximately $50.",
      "C": "Approximately $10.",
      "D": "Approximately $100."
    },
    "explanation": "AWS charges $0.01/GB for data transferred between AZs in the same region. 5 TB \u2248 5,120 GB \u00d7 $0.01/GB = $51.20 \u2248 $50."
  },
  {
    "id": "d41a1662975e1d46fd454eed0aaf1f7a3cad054f5b8dc8e362cb8f3586ec6671",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.2",
    "stem": "A global media company must deliver static video thumbnails stored in S3 to users worldwide with the lowest latency and lowest egress cost. Which architecture should they choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable S3 Transfer Acceleration on the bucket and let users pull directly.",
      "B": "Deploy an Amazon CloudFront distribution with the S3 bucket as origin.",
      "C": "Configure cross-region S3 replication to every major region and serve from the nearest S3 bucket.",
      "D": "Create an EC2 instance in each region behind a Network Load Balancer hosting the thumbnails."
    },
    "explanation": "CloudFront caches content at edge locations, reducing both latency and egress costs. S3 Transfer Acceleration is more expensive for large-scale egress. Cross-region replication increases storage and replication costs, and EC2 self-hosting adds compute and transfer costs."
  },
  {
    "id": "a84953093d0151d58e8f8bfe7c767588cd7c79da1db8cefe4124fc318f1543e9",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "Compliance logs must be retained for 7 years but are accessed less than once per year. Retrieval can take hours. Which S3 storage class yields the lowest monthly cost?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "S3 Glacier Instant Retrieval.",
      "B": "S3 Glacier Flexible Retrieval.",
      "C": "S3 Glacier Deep Archive.",
      "D": "S3 Standard-Infrequent Access."
    },
    "explanation": "S3 Glacier Deep Archive has the lowest storage cost for data accessed less than once per year. Instant Retrieval is more expensive storage, and Standard-IA is more expensive than Glacier classes."
  },
  {
    "id": "6932d319e6922cf95336b12ab9aed4ff2c1dc218c4d5d68c6bc5435fd7ad481c",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "A financial services firm uses EBS volumes that require a minimum of 10,000 IOPS but only 100 GB of storage. Which EBS type meets the performance requirement at the lowest monthly cost?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provisioned IOPS SSD (io1).",
      "B": "General Purpose SSD (gp2).",
      "C": "General Purpose SSD (gp3) with 10,000 provisioned IOPS.",
      "D": "Throughput Optimized HDD (st1)."
    },
    "explanation": "gp3 allows separate provisioning of IOPS and throughput, offering 10,000 IOPS on 100 GB at lower cost than io1. gp2 cannot reach 10,000 IOPS on 100 GB, and st1 is HDD and not suitable for IOPS."
  },
  {
    "id": "293ceb9edfcb181991feba0d51f9e9d3ff1d40ce0fe5231469bc29973eb69f92",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "An analytics team runs large batch jobs totaling 10,000 vCPU-hours per month, but the jobs can be interrupted and rescheduled. To minimize cost, which EC2 purchasing option should they choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "On-Demand Instances.",
      "B": "Spot Instances.",
      "C": "Standard Reserved Instances.",
      "D": "Compute Savings Plan."
    },
    "explanation": "Spot Instances offer the deepest discounts for interruption-tolerant workloads. Reserved Instances and Savings Plans are more expensive for intermittent usage, and On-Demand is more expensive than Spot."
  },
  {
    "id": "03adc53601573da31edbfcdd89bc2a53e67e0d800f8c186ac59a9151ccbd09f3",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "Your organization needs to ingest 20 TB of on-premises data into S3 quickly and at the lowest cost. Internet upload would incur egress charges of $0.09/GB. AWS Snowball Edge is priced at a flat $300 job charge with 50 TB capacity. Which option is cheaper?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Upload directly over the internet.",
      "B": "Order an AWS Snowball Edge device and import.",
      "C": "Set up a VPN connection and use VPC endpoint to S3.",
      "D": "Use S3 Transfer Acceleration over the internet."
    },
    "explanation": "Direct upload cost: 20 TB\u224820,480 GB\u00d7$0.09=~$1,843. Snowball Edge costs $300 flat, making it far cheaper. VPN/VPC endpoints do not avoid egress charges, and Transfer Acceleration is more expensive."
  },
  {
    "id": "e58a6ead3444cb4022c3a1ade3955b2e424fa0f5beb87ca0400bf5914a4c6609",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A data lake stores millions of small objects in S3 with unpredictable access patterns (from 0 to 10,000 requests/day). You want to automatically optimize storage cost without performance impact. Which class should you use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "S3 Standard-Infrequent Access with lifecycle transitions.",
      "B": "S3 Intelligent-Tiering with automatic tiering.",
      "C": "S3 Glacier Flexible Retrieval with instant retrieval API.",
      "D": "S3 One Zone-Infrequent Access with lifecycle transitions."
    },
    "explanation": "Intelligent-Tiering automatically moves objects between frequent and infrequent tiers based on usage without any operational overhead or retrieval fees on first access, ideal for unpredictable patterns."
  },
  {
    "id": "aa55d5112a3e4fc4b88d8d32dec7aa64012e3852d9c6630caa5eecf843d09138",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A backup process writes daily snapshots to S3, retains them for 30 days, and must allow immediate restore. Which S3 storage class minimizes cost while meeting requirements?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "S3 Standard.",
      "B": "S3 Standard-Infrequent Access.",
      "C": "S3 Glacier Instant Retrieval.",
      "D": "S3 Glacier Deep Archive."
    },
    "explanation": "S3 Standard-IA has the same high durability and immediate access as Standard but at a lower storage price for data that is accessed less than once per month. Glacier classes impose retrieval delays and costs."
  },
  {
    "id": "17db5b350b4127abcbf004d62840d97bc62b444690b8ae5d459d884312ba67db",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "EC2 instances in a public subnet use a NAT Gateway in a different Availability Zone for internet access, incurring charges. How can you eliminate data transfer costs for private instances accessing S3?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Configure instances to use the NAT Gateway in their own AZ.",
      "B": "Use an Internet Gateway instead of a NAT Gateway.",
      "C": "Create a VPC gateway endpoint for S3.",
      "D": "Replicate data to a public subnet S3 bucket."
    },
    "explanation": "A VPC gateway endpoint for S3 provides private connectivity to S3 within the region at no data transfer cost. NAT Gateways and Internet Gateways incur egress or inter-AZ costs."
  },
  {
    "id": "35743192832a6b511b0e8cd422e25c01a3d1ea03e3f39f3355d15d4dd209e3dd",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "Your organization anticipates a consistent monthly EC2 spend of $10,000. They want to minimize long-term effective hourly rates without needing to change instance types or regions. Which commitment yields the lowest effective hourly rate?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "On-Demand Instances.",
      "B": "1-year Standard Reserved Instances, No Upfront.",
      "C": "3-year Compute Savings Plan, Partial Upfront.",
      "D": "3-year Standard Reserved Instances, All Upfront."
    },
    "explanation": "A 3-year All Upfront Standard RI locks in the maximum discount off On-Demand rates, delivering the lowest effective hourly price when usage patterns and instance types remain constant."
  },
  {
    "id": "37238b3dd5b96703629db052f2ceb545d285a48aecf3581f53d1d5adabf6ebfe",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.2",
    "stem": "A company transfers 50 TB/month between two S3 buckets in different regions using cross-region replication. Data transfer charges for inter-region S3 replication are $0.02/GB. What is the approximate monthly replication cost?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Approximately $1,024.",
      "B": "Approximately $512.",
      "C": "Approximately $256.",
      "D": "Approximately $2,048."
    },
    "explanation": "50 TB \u2248 50,000 GB (approx 51,200 GB). 51,200 GB \u00d7 $0.02/GB = $1,024."
  },
  {
    "id": "f5544f61643ae15cbe56cf7c96e7649e2355566250022b8af59599d002e8c711",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "A video editing pipeline processes large media files. Storage must support frequent, rapid retrieval and unpredictable access yet minimize cost. Which S3 storage class is most appropriate?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "S3 Intelligent-Tiering with Archive Instant Access tier.",
      "B": "S3 Standard.",
      "C": "S3 Glacier Flexible Retrieval.",
      "D": "S3 One Zone-Infrequent Access."
    },
    "explanation": "Intelligent-Tiering with the frequent archive instant access tier moves infrequently accessed objects to a lower-cost archive tier while providing millisecond access when needed, optimizing cost for unpredictable access."
  },
  {
    "id": "7b19a44eecf34610ae0937b765aed39753a6d516e9e8666de958ee595884afba",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "A finance team wants to receive an alert when the AWS forecasted monthly spend for a department\u2019s consolidated billing account exceeds $50,000. Which combination of services and settings should be used to create an automated notification when the forecast triggers this threshold?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Configure an Amazon CloudWatch alarm on the EstimatedCharges metric for the management account and add an SNS notification at $50,000.",
      "B": "Use AWS Budgets to create a cost budget with a $50,000 forecast threshold and configure an alert subscription to SNS.",
      "C": "Enable Cost Explorer anomaly detection for the management account with a $50,000 sensitivity level and subscribe to the anomaly alert.",
      "D": "Set up an AWS Config rule on billing resources to alert when monthly forecasted cost exceeds $50,000 via Lambda."
    },
    "explanation": "AWS Budgets supports forecasted spend thresholds and SNS alerts; CloudWatch Billing metrics do not include forecasts, and Config cannot directly monitor forecasted costs."
  },
  {
    "id": "88842ad43108064269e978015b9a60747c54487c88022c29eaf253dabe30f367",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "An engineering manager needs to estimate the cost difference between running 10 m5.large On-Demand EC2 instances and the same configuration using a 1-year Standard Reserved Instance with all upfront payment. Which tool and features enable the most accurate comparison?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Cost Explorer\u2019s RI purchase recommendations and compare the recommended adjustments.",
      "B": "Use AWS Budgets to create two cost budgets\u2014one for On-Demand and one for Reserved\u2014and compare actual spend.",
      "C": "Use the AWS Pricing Calculator, configure both On-Demand and 1-year Standard Reserved Instances with all upfront, and compare the estimated monthly costs.",
      "D": "Use AWS Organizations consolidated billing to generate a forecast that contrasts On-Demand and Reserved Instance usage."
    },
    "explanation": "The AWS Pricing Calculator allows explicit configuration of payment options and upfront costs for precise cost estimates; Cost Explorer recommendations focus on existing usage."
  },
  {
    "id": "8fcc6d1fed42ebfd8c195c06968e872c5ff8760478a98de548bc6ac7022f0f38",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "A company uses five AWS accounts under one payer account. They want to track each account\u2019s S3 storage cost with separate cost budgets. What must be done to ensure budgets report costs by individual account?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Enable detailed billing reports and parse costs by account ID in Amazon Athena.",
      "B": "Create a single budget on the payer account with filters for each account monthly.",
      "C": "Use Cost Explorer with linked account filters instead of AWS Budgets.",
      "D": "In AWS Budgets, create a separate budget per linked account and use the Linked Account filter for each budget."
    },
    "explanation": "AWS Budgets supports filtering by linked account. Creating separate budgets per account with the Linked Account filter provides individual spend alerts."
  },
  {
    "id": "3a70423dd19ae94f38432baabd53af17ca0575953d4a135d7be9cdf109855d93",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "An organization wants to enforce cost allocation tags on all EC2 instances across multiple accounts. Which approach ensures resources cannot be created without required tags and that costs appear appropriately in reports?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use a Service Control Policy in AWS Organizations to deny ec2:RunInstances when required tag keys are missing, activate the tags as cost allocation tags in the payer account.",
      "B": "Create an AWS Config rule in each account to detect untagged resources and send an SNS notification.",
      "C": "Implement a Lambda function triggered by CloudWatch Events to inspect and tag resources after creation.",
      "D": "Use AWS CloudTrail to monitor RunInstances and manually enforce tagging in monthly reports."
    },
    "explanation": "A Service Control Policy can block untagged EC2 launches across the OU; activating cost allocation tags in billing ensures tags appear in Budgets and Cost Explorer."
  },
  {
    "id": "5519d29bc19572dafa2eb82c53ee296add66eb80c20d7f8f340432f0f47d1b7e",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A startup uses a single on-demand database in one AWS account. They plan to spin up replicas for testing, but need an up-to-date cost estimate. Which method provides an accurate upfront estimate including anticipated data transfer?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Use AWS Cost Explorer to forecast based on historical usage and add a manual data transfer estimate.",
      "B": "Use the AWS Pricing Calculator to model the replica instances, storage, and cross-AZ data transfer costs.",
      "C": "Enable AWS Budgets with forecast alerts and extrapolate from current cost trends.",
      "D": "Deploy the replicas and monitor costs in Cost Explorer for a week to extrapolate monthly spend."
    },
    "explanation": "The AWS Pricing Calculator allows modeling new resources and data transfer ahead of deployment; Budgets and Cost Explorer rely on existing usage data."
  },
  {
    "id": "a381d04efcd915c2a614a09a3175b640f70e40a338758e19b904e25db7e6f890",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "A DevOps team has enabled user-defined cost allocation tags but sees no data in Cost Explorer. What is the most likely resolution?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Activate the user-defined tags in the Billing console under cost allocation tags before waiting 24 hours.",
      "B": "Enable AWS Config tag tracking in each account for Cost Explorer to pick up tags.",
      "C": "Configure the AWS Organizations master account to propagate tags automatically to member accounts.",
      "D": "Use Resource Groups Tag Editor to reapply tags so that Cost Explorer recognizes them."
    },
    "explanation": "Tags must be activated in the Billing console as cost allocation tags and take up to 24 hours to appear in Cost Explorer; other actions do not enable cost allocation visibility."
  },
  {
    "id": "5d47d723ae88a3dfe69545b9eb168a52390c5c0728c609acf864d046bd613427",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A large enterprise uses consolidated billing and needs to retrieve detailed cost and usage data programmatically. Which AWS service and feature should be used to automate retrieval of hourly resource-level cost data?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Athena with the detailed billing report in S3 and partitioned by hour.",
      "B": "AWS Budgets API to list budgeted and actual spend per linked account.",
      "C": "Enable the AWS Cost and Usage Report with hourly granularity to S3, then use AWS SDK to fetch reports.",
      "D": "Invoke Cost Explorer\u2019s GetCostForecast API for hourly granularity across linked accounts."
    },
    "explanation": "The Cost and Usage Report can be configured for hourly granularity and delivered to S3; you can programmatically retrieve it via the S3 API or AWS SDK."
  },
  {
    "id": "eb0eb84db317aef8c0433781b52790aa080201592c2f550a470c11f48c1ceb6b",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A project lead needs to prevent budget overruns by automatically pausing EC2 instances when the project\u2019s monthly spend exceeds 80% of its budget. Which solution meets this requirement with the least operational effort?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use an AWS Lambda function triggered by Cost Explorer hourly reports to stop instances when cumulative spend >80%.",
      "B": "Create an AWS Budget with an 80% threshold that triggers an AWS Chatbot notification, then invoke a preconfigured Systems Manager Automation document to stop instances.",
      "C": "Configure a CloudWatch Events rule on the AWS/Billing namespace to detect billing metrics >80% and invoke Lambda to pause instances.",
      "D": "Enable anomaly detection in AWS Cost Explorer, and manually stop instances when anomalies exceed budget."
    },
    "explanation": "AWS Budgets supports actions via AWS Chatbot or IAM actions; coupling a budget action with Systems Manager Automation provides a low-effort automated stop."
  },
  {
    "id": "750b5ec5e2f0b89d47eebcaf28a20bd063f41276f6db43b9cd0dc2827a1cd7fa",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "An administrator needs to analyze whether to purchase a 3-year Convertible Reserved Instance or a 1-year All-Upfront RI for their current On-Demand workload that has variable usage. Which AWS tool and report should they use?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Use AWS Pricing Calculator to compare both RI options by inputting usage patterns.",
      "B": "Use AWS Budgets forecast to project costs for each RI option over three years.",
      "C": "Use Cost Explorer\u2019s Savings Plans recommendations to compare Convertible RIs.",
      "D": "Use Cost Explorer\u2019s RI Recommendation report with a custom lookback period to compare cost savings of 1-year All-Upfront versus 3-year Convertible RIs."
    },
    "explanation": "Cost Explorer\u2019s RI Recommendation report can be configured for different terms and payment options and provides comparative savings analysis."
  },
  {
    "id": "1b809c0b5abdb664b1ab8c5e25eb7f7dbe4da148b3c580b1dc27fa75dc10aa4b",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A company uses AWS Organizations with several OUs. They need a single view of budgets across all member accounts in one OU. Which step is required to achieve this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "In the payer account, create an AWS Budget and select the Organization unit filter for the desired OU.",
      "B": "Enable tagging policies in AWS Organizations to forward budget data to member accounts.",
      "C": "Deploy CloudFormation StackSets in all OU accounts to create individual budgets and aggregate reports.",
      "D": "Use Cost Explorer\u2019s linked account filter to manually export and merge data for the OU."
    },
    "explanation": "AWS Budgets supports an Organization unit filter; creating the budget in the payer account and selecting the OU provides consolidated budget tracking."
  },
  {
    "id": "d4807b4b9c4c389e260fc70e08c028f0586ce0d820dbd367677a77f65f726085",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "A resource owner wants to ensure that new AWS resources are automatically tagged with the project tag key \u2018Project\u2019 and the appropriate value for cost tracking. Which AWS service or feature should be used?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Enable AWS Config\u2019s tag compliance rule to enforce the Project tag on resource creation.",
      "B": "Use AWS Budgets\u2019 tag enforcement feature to apply the Project tag.",
      "C": "Implement AWS Systems Manager Automation using a CloudWatch Events rule on CreateTags to add the Project tag.",
      "D": "Use AWS Organizations Tag Policies to auto-apply tags to all newly created resources."
    },
    "explanation": "AWS Organizations Tag Policies define allowed tags but do not auto-apply them; using Systems Manager Automation triggered by resource creation can add tags automatically."
  },
  {
    "id": "a41e8a2443b898d2b2924b807fd563fb69288d367f078efddac8bc38860d1d76",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "A startup with limited budgets needs to project first-year costs for migrating its on-premises web application to AWS, including EC2, RDS, EBS, and data transfer. Which approach offers the most comprehensive estimate?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Use AWS Cost Explorer to forecast costs based on a small pilot environment.",
      "B": "Use AWS Pricing Calculator to model each component\u2014EC2, RDS, EBS, and data transfer\u2014with expected usage and payment options.",
      "C": "Set up AWS Budgets with a baseline equal to on-premises costs and adjust monthly based on actual AWS spend.",
      "D": "Deploy the full architecture and run it for one month; then use Cost Explorer to annualize the cost."
    },
    "explanation": "The AWS Pricing Calculator allows precise modeling of each service, usage, and data transfer to estimate first-year costs before migration."
  },
  {
    "id": "52a0fd7215a85b3f8834f326b4c3890b4a51ef72526cac5ef5082b72b6ca2c65",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "An enterprise enabled AWS Cost and Usage Reports (CUR) but needs to break down costs by application team in each account. Which combination of features supports this analysis?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable AWS Budgets with cost filters per application tag and export CSV budget reports.",
      "B": "Use CloudWatch custom metrics to push cost-per-team metrics to dashboards.",
      "C": "Enable consolidated billing detailed billing reports and parse by linked account and cost center tags.",
      "D": "Activate cost allocation tags for application teams, enable CUR with resource IDs and tags, then query CUR data in Athena by tag and account."
    },
    "explanation": "Activating cost allocation tags and including them in CUR allows querying by tag and account in Athena for team-level cost breakdown."
  },
  {
    "id": "f416226cb06b6370e03939e7f0d1000307bda8ba1f87e043c18dfa5fb12b49d9",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A finance analyst observes that Savings Plan discounts are not applied to some EC2 instance usage. Which investigation steps in Cost Explorer will reveal why those discounts are not applied?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Review the Budget\u2019s actual versus forecasted comparison in AWS Budgets for discrepancy reasons.",
      "B": "In Cost Explorer, filter by Savings Plan utilization reports and group by purchase option to identify on-Demand usage outside the plan scope.",
      "C": "Run AWS Pricing Calculator for Savings Plans versus RI to see unused capacity.",
      "D": "Check AWS Organizations consolidated billing summary to find accounts not covered by the Savings Plan."
    },
    "explanation": "Cost Explorer\u2019s Savings Plans utilization report grouped by purchase option shows usage billed at On-Demand rates indicating scope mismatches."
  },
  {
    "id": "a2df520906f2450d2e695953564189a80c24ac5069fc132b9a4b815d0ae37e91",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A development manager wants to cap the monthly cost of a test environment at $2,000. When the budget threshold is exceeded, all EC2 instances should stop automatically. Which configuration will achieve this?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Create an AWS Budget with a $2,000 threshold and configure a budget action to invoke a role that stops all EC2 instances.",
      "B": "Use Cost Explorer anomaly detection on test account spend with a $2,000 threshold, triggering an SNS topic to notify admins.",
      "C": "Deploy a CloudWatch alarm on EstimatedCharges at $2,000 to invoke Lambda that tags instances for shutdown.",
      "D": "Enable an AWS Config rule for billing that when trigger >$2,000, executes Systems Manager to stop instances."
    },
    "explanation": "AWS Budgets supports budget actions that can invoke IAM roles for EC2 StopInstances when thresholds are breached without additional custom infrastructure."
  },
  {
    "id": "85267e9e93803e2857c5e07f0875546d2e9caa52b63e712dec6eaa3ef53f4c48",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.3",
    "stem": "A multinational e-commerce company has intermittent configuration issues and wants proactive guidance on cost optimization, security, and performance best practices without submitting support cases. Which AWS resource should they use to automatically receive recommendations and alerts?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Trusted Advisor",
      "B": "AWS Support Center",
      "C": "AWS Health Dashboard",
      "D": "AWS Config"
    },
    "explanation": "Trusted Advisor continuously inspects your environment against AWS best practices and generates proactive recommendations. The Support Center is the console for manual case management. The Health Dashboard reports current and historical service events. AWS Config tracks resource configurations but does not provide cost or performance best-practice recommendations."
  },
  {
    "id": "f1222dae6f9eccf531d26e5c3d5c92e18913120bf3ee0f2a431f0e7f5d579c32",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A healthcare startup needs to integrate AWS service health events programmatically into their incident management system to trigger remediation workflows. Which AWS service/API should they call?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon CloudWatch Events for Resource-Level Events",
      "B": "AWS Health Dashboard console",
      "C": "AWS Health API",
      "D": "AWS CloudTrail LookupEvents"
    },
    "explanation": "The AWS Health API provides programmatic access to AWS service health and scheduled events for your account. CloudWatch Events delivers some operational metrics but not detailed health events. The Health Dashboard is console-only. CloudTrail records API calls but doesn\u2019t expose high-level service health insights."
  },
  {
    "id": "21e80b3d44cab9c59ad94df50be67a49015113f68b6a21b497f4d9902b8d9bef",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "A financial institution discovers that one of its S3 buckets has publicly exposed sensitive data. They want AWS to investigate potential abuse but do not need technical support. Which AWS team should they contact via the Support Center case type?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Support \u2013 Technical Account Manager",
      "B": "AWS Support \u2013 Infrastructure event management",
      "C": "AWS Customer Service \u2013 Billing and Account inquiries",
      "D": "AWS Trust & Safety \u2013 abuse report"
    },
    "explanation": "Trust & Safety handles reports of policy abuse, data breaches, and security incidents. Technical Account Managers and Infrastructure Event Management are part of technical support. Billing and Account inquiries do not cover abuse investigations."
  },
  {
    "id": "bb583d9647373111c4a996e3cfb112c3e6885eb0742a711223a1f1eafa263ce1",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "A SaaS provider requires 24x7 access to senior cloud support engineers, a designated Technical Account Manager, and concierge billing support. Which AWS Support plan meets these requirements?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Basic Support",
      "B": "Enterprise Support",
      "C": "Business Support",
      "D": "Developer Support"
    },
    "explanation": "Enterprise Support includes 24x7 access to cloud support engineers, a Technical Account Manager, and billing/account concierge. Business plan gives 24x7 engineers but no TAM or concierge. Developer Support provides limited business hours support. Basic has no technical support."
  },
  {
    "id": "483a9e4c33b17bc0ac7e1ba69b71a7f6b00e2e04cf0a253d56ea45d81696dc9b",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "An ISV wants to join the AWS Partner Network and needs access to AWS funding benefits such as Market Development Funds and technical validation programs. Which APN tier provides these benefits?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Registered Partner",
      "B": "Select Partner",
      "C": "Advanced Partner",
      "D": "Standard Partner"
    },
    "explanation": "Advanced Partners have met revenue and certification requirements to qualify for Market Development Funds and specialized technical programs. Registered and Select tiers provide limited benefits. There is no \u201cStandard\u201d tier in APN."
  },
  {
    "id": "623052fb2fa7e7b08a422ea87def477b8a007028c5b5b97befb9616ffb181852",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.8",
    "stem": "A biotech company needs to deploy a licensed third-party bioinformatics tool without managing licensing infrastructure. Which AWS technical resource should they use to simplify procurement and deployment?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Service Catalog",
      "B": "AWS Marketplace Self-Subscription",
      "C": "AWS License Manager",
      "D": "AWS Marketplace AMI offering"
    },
    "explanation": "An AWS Marketplace AMI offering is a pre-configured, licensed Amazon Machine Image you can launch directly. Service Catalog manages internal catalogs. License Manager tracks existing licenses but doesn\u2019t procure or deploy. Self-subscription offers SaaS or containers but AMI is most direct for EC2."
  },
  {
    "id": "4ee4d244abb10b684615756cd7baac41431651979dfec7c863f8a4b7d00ec786",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "A media company\u2019s production environment just went down, causing revenue loss. They open a support case. Which severity level should they choose for the fastest response under an Enterprise support plan?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Severity Level 1 (Critical)",
      "B": "Severity Level 2 (High)",
      "C": "Severity Level 3 (Normal)",
      "D": "Severity Level 4 (Low)"
    },
    "explanation": "Severity Level 1 is for business-critical systems when downtime causes significant impact and demands <15-minute response under Enterprise. Level 2 is high impact but not full outage. Levels 3 and 4 have progressively longer response times."
  },
  {
    "id": "9cfacb1a053e388cd2e077ea02a04fce3ccf025ecf38a400980e48fc957e2f9c",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A retail company is designing a migration and needs detailed architectural best practices from AWS specialists. Which technical resource provides prescriptive guidance and workshops?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Documentation \u2013 API reference",
      "B": "AWS Prescriptive Guidance",
      "C": "AWS Whitepapers on General Security",
      "D": "AWS re:Post"
    },
    "explanation": "AWS Prescriptive Guidance provides tailored workshops, blueprints, and best practices. Documentation APIs are reference-level. Whitepapers on security are high level. re:Post is a community Q&A forum, not structured workshops."
  },
  {
    "id": "87c8e3821f70ebed73306732a7f0f4104ed6dccb90e3f6a4143d57be1c0350d0",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A startup wants to engage AWS experts for a one-day on-site review of its architecture. Which support offering should they purchase?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Business Support plan",
      "B": "Developer Support plan",
      "C": "Infrastructure Event Management add-on with Enterprise Support",
      "D": "AWS Professional Services engagement"
    },
    "explanation": "Infrastructure Event Management (IEM) is an add-on for Enterprise Support customers for architectural reviews and planning events. AWS Professional Services is direct consulting but not a support add-on. Business and Developer plans do not include IEM or on-site reviews."
  },
  {
    "id": "837f22fd168fcfc1eff5d14ef67187f34ce77b494e9db0ca2b8e9ae98137413c",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A gaming company\u2019s engineers need a collaborative environment to get answers from AWS experts and peers. Which free AWS resource should they use before opening a paid support case?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Support Center",
      "B": "AWS Support Knowledge Center",
      "C": "AWS Documentation",
      "D": "AWS re:Post"
    },
    "explanation": "AWS re:Post is a community Q&A moderated by AWS experts and peers, and is free. The Support Center is for paid cases. The Knowledge Center articles and Documentation may not address specific designs or peer insights as re:Post does."
  },
  {
    "id": "bd7f6e4ff0233e34f6b26a32d0cd7d7e9b785b8f75e2dfdfcd07ffd02d3b9352",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "An enterprise uses an on-prem status dashboard linked to AWS events. They want granular notifications about EC2 status changes. Which service should publish these events to Amazon EventBridge for their dashboard?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Health Dashboard",
      "B": "AWS Health API via CloudWatch Events",
      "C": "AWS CloudTrail Events",
      "D": "AWS Config change notifications"
    },
    "explanation": "AWS Health API can emit operational health events into EventBridge/CloudWatch Events for your account. Health Dashboard is console only. CloudTrail logs API calls but not resource health state changes. AWS Config tracks configuration drift, not health events."
  },
  {
    "id": "9a47655b940eda53178e67367412334c05082f86ee36bbb608cdf62109852e68",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "An organization is evaluating support options to programmatically create support cases and retrieve case history. Which support plan and interface combination will satisfy this requirement?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Developer Support plan via Support Center console",
      "B": "Business Support plan via AWS CLI \u201caws support\u201d namespace",
      "C": "Enterprise Support plan via AWS Support API",
      "D": "Basic Support plan via AWS SDK for Python (boto3)"
    },
    "explanation": "The AWS Support API (and CLI) for case management is available to Business and Enterprise plans, but creating high-severity cases and programmatic TAM workflows require Enterprise. Developer and Basic plans do not have Support API access."
  },
  {
    "id": "1ca8ec12d821cff0e9275f7d25db2d8788ea3fb92e90786489a79acdd81a3725",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "A manufacturing firm experienced intermittent service interruptions across regions. They need near-real-time root cause analysis across multiple accounts. Which AWS technical resource helps identify historical and current service events for all accounts in an Organization?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Health Organizational View in AWS Health API",
      "B": "AWS Config Aggregator",
      "C": "AWS CloudWatch cross-account dashboards",
      "D": "AWS Resource Access Manager"
    },
    "explanation": "The AWS Health API\u2019s organizational view provides aggregated health events across all accounts in an AWS Organization. Config Aggregator tracks resource configurations, not service health. CloudWatch dashboards gather metrics but not service event root cause analysis. Resource Access Manager shares resources."
  },
  {
    "id": "f3fd7fc2c2dfa37bfe8a159f99f32eaac1f864e604e46206604c8b7fa9f7b654",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A biotech lab wants AWS to validate a new life sciences workload against best practices in security, performance, and cost. Which combination should they request through a support case?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Well-Architected Review via Business Support",
      "B": "Well-Architected Review via Infrastructure Event Management add-on",
      "C": "Technical Account Management review via Enterprise Support",
      "D": "Architecture Review via AWS Partner Network"
    },
    "explanation": "Well-Architected Reviews are delivered under the Infrastructure Event Management (IEM) add-on for Enterprise Support. Business Support does not include IEM. TAM provides account guidance but not formal Well-Architected Reviews. APN Partners can review but that\u2019s external to AWS support."
  },
  {
    "id": "e4fa8200a2d79f075694b7d4111180b5a96fe9ea860fd0d16d07db06ddbb610d",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A startup is comparing centralized documentation sources to solve a VPC peering routing issue. Which resource provides the most authoritative, update-controlled content?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Answers",
      "B": "AWS re:Post community threads",
      "C": "AWS Official Documentation on Amazon VPC",
      "D": "AWS Whitepapers"
    },
    "explanation": "The AWS Official Documentation is the single source of truth for service APIs and implementation guidance and is regularly updated. AWS Answers and whitepapers are useful but not as granular or always current. re:Post threads reflect community Q&A and may include outdated or subjective advice."
  },
  {
    "id": "11734f0e5506dc7224d1661a49ce3fa02fdc12b29ef416e85c4477308fb46cdd",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A streaming media company experiences unpredictable global demand spikes during live events and needs to deliver content with low latency to viewers worldwide. They want to avoid provisioning servers in every possible location in advance. Which AWS Cloud benefit should they leverage to meet these requirements?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use Amazon CloudFront\u2019s global edge network to cache content closer to viewers and reduce latency without pre-provisioning servers in each region.",
      "B": "Deploy Amazon EC2 instances in a single AWS Region and rely on Auto Scaling to handle global traffic peaks.",
      "C": "Use AWS Direct Connect to establish dedicated network connections from each viewer location to a central region.",
      "D": "Configure Amazon S3 cross-region replication to replicate media files synchronously to all Regions before a live event."
    },
    "explanation": "Amazon CloudFront\u2019s global edge network provides low-latency delivery by caching content at edge locations worldwide, allowing the company to serve viewers without pre-provisioning servers in every region. Auto Scaling in a single Region does not address geographic latency. AWS Direct Connect reduces latency for enterprise networks, not global end-user delivery. S3 cross-region replication is asynchronous and doesn\u2019t provide edge caching."
  },
  {
    "id": "9729ddd0f39d179d46c0de85b1e4e02a41ce5d0bc9d9661568f0111fe028399b",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "An online retailer experiences sharp increases in traffic during holiday sales. They need to maintain consistent user experience while minimizing infrastructure costs between peaks. Which combination of AWS benefits and services addresses both requirements?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use EC2 Auto Scaling with On-Demand Instances and AWS Lambda functions to scale resources based on demand and pay only for what you use.",
      "B": "Deploy additional EC2 instances in each Availability Zone and maintain them idle between peaks to ensure readiness for holiday traffic.",
      "C": "Purchase a 3-year Reserved Instance capacity in each Region to save costs during occasional traffic spikes.",
      "D": "Configure Multi-AZ Amazon RDS with a Read Replica to scale read workloads and reduce costs between spikes."
    },
    "explanation": "EC2 Auto Scaling with On-Demand Instances and Lambda provides elasticity\u2014automatically adding and removing capacity\u2014and a pay-as-you-go model avoids idle costs. Pre-provisioning idle instances or long-term Reserved Instances incur costs between peaks. RDS Read Replicas help read scaling but don\u2019t address compute scaling of the web tier."
  },
  {
    "id": "6b62af6896b91c6758472904d8ac6b01a3b906fe8e45803d51893127b5afae4d",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A global financial services firm requires an RTO of under 15 minutes and an RPO of under 5 minutes for its trading platform. They currently replicate data across two data centers with manual failover. How does AWS infrastructure benefit help meet these requirements while minimizing operational overhead?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Use Amazon S3 cross-region replication to asynchronously replicate transaction logs to another Region for disaster recovery.",
      "B": "Deploy Amazon RDS in Multi-AZ configuration to synchronously replicate the database across Availability Zones with automated failover.",
      "C": "Configure scheduled snapshots of the database every 5 minutes and restore to a new instance when needed.",
      "D": "Use Amazon Route 53 latency-based routing to redirect traffic to an alternate Region during failures."
    },
    "explanation": "RDS Multi-AZ synchronously replicates data across AZs and provides automated failover within seconds, meeting strict RTO/RPO with minimal overhead. S3 replication and snapshots are asynchronous and prone to data loss. Route 53 latency routing does not replicate data."
  },
  {
    "id": "86142512ba429c34aaefcca762665fc3800682e680be6fb7d8935b541f6bd997",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A software development team needs to provision an isolated test environment identical to production in under an hour, then tear it down after testing to minimize costs. How does AWS improve the team's agility compared to their on-premises data center?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "By allowing developers to submit a hardware purchase request online and tracking fulfillment electronically.",
      "B": "By deploying always-on standby servers in each data center for immediate use.",
      "C": "By using AWS CloudFormation templates to deploy and tear down production-identical stacks on demand and paying only for used resources.",
      "D": "By purchasing Reserved Instances for the test environment to guarantee capacity."
    },
    "explanation": "CloudFormation templates enable on-demand, repeatable environment provisioning and teardown, delivering rapid agility and pay-as-you-go cost control. Hardware procurement requests and standby servers delay agility. Reserved Instances lock in capacity and cost."
  },
  {
    "id": "3bde0cbbce56b5e3cd00eb1c66b6295885f7d99527d0723f9bd2e35db4d83de7",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A startup estimates its infrastructure costs will decrease over time as its customer base grows. Which AWS principle explains this phenomenon?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Elasticity allows resources to expand and contract automatically.",
      "B": "Global reach provides infrastructure worldwide.",
      "C": "Pay-as-you-go eliminates upfront costs.",
      "D": "Economies of scale enable AWS to lower per-unit costs as its footprint grows."
    },
    "explanation": "Economies of scale allow AWS to negotiate better component pricing and pass savings to customers as per-unit costs decrease with volume. Elasticity, global reach, and pay-as-you-go describe other benefits but not the reduction in cost per unit over time."
  },
  {
    "id": "e0770b4236a5f7747d6507d9f6ae199500db6911ed5f79dddb4a348e6554e73d",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A healthcare company must store patient data in specific geographic locations to comply with local privacy regulations. They also need the ability to expand into new markets quickly. Which AWS benefit and feature combination best meets both requirements?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Use AWS Direct Connect to each data center combined with Reserved Instances.",
      "B": "Leverage AWS Regions to store data locally and deploy new infrastructure on demand in additional Regions with pay-as-you-go.",
      "C": "Purchase long-term colocation space in each geography.",
      "D": "Use Multi-Region S3 replication to synchronously replicate data across all Regions continuously."
    },
    "explanation": "AWS Regions provide isolated geographic locations for data residency compliance, and the pay-as-you-go model allows rapid expansion without long-term commitments. Direct Connect and colocation do not address rapid market entry. S3 replication is asynchronous and not intended for synchronous compliance."
  },
  {
    "id": "fcca77368d0625f9d7862aaacf51f29fe89bd9a8565feb2e9ef75d527b9c7cb8",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A media analytics firm runs batch processing jobs on thousands of EC2 instances nightly. They want to reduce costs by approximately 30% without compromising the ability to scale up for occasional peak workloads. Which purchasing strategy best achieves this?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Purchase Standard Reserved Instances for baseline capacity and use On-Demand Instances for peak workloads.",
      "B": "Use EC2 Spot Instances exclusively to run all batch jobs.",
      "C": "Purchase Convertible Reserved Instances and prepay all capacity for one year.",
      "D": "Deploy all batch processing on On-Demand Instances and rely on Auto Scaling."
    },
    "explanation": "Standard Reserved Instances for baseline workloads provide predictable savings (~30%), while On-Demand Instances handle peaks. Exclusive Spot usage risks interruption. Convertible RIs reduce flexibility in scaling and prepayment increases upfront costs. On-Demand alone does not optimize baseline costs."
  },
  {
    "id": "06f0ef6925ad64971089e5968b1442f73ecae14e60fec6a024542b8a6e156050",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.7",
    "stem": "An online payment gateway requires uninterrupted service during an Availability Zone (AZ) outage. They have two EC2 instances behind an Elastic Load Balancer in a single Region. How does the AWS infrastructure benefit of Availability Zones help ensure no service interruption during an AZ failure?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure EC2 instances in one AZ and rely on Auto Scaling for rapid replacement.",
      "B": "Use Route 53 health checks to fail over to a different Region.",
      "C": "Deploy the EC2 instances and the load balancer across multiple AZs so that if one AZ fails, traffic is routed to instances in another AZ.",
      "D": "Use S3 cross-region replication to automatically replicate traffic to other AZs."
    },
    "explanation": "Deploying resources across multiple AZs within a Region leverages isolated failure domains; the load balancer automatically routes traffic to healthy instances in remaining AZs. Auto Scaling in one AZ does not protect against that AZ\u2019s physical failure. Route 53 failover adds cross-Region latency and complexity. S3 replication is for storage, not compute failover."
  },
  {
    "id": "83ac852649195effdf9bb20e5f74cc7ec0f6b2ed5bd89fdffd5f285a6e9436c0",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "A photo-sharing application experiences unpredictable, spiky workloads that can spike by 100x at any time. The development team wants to offload the undifferentiated heavy lifting of provisioning servers. Which AWS benefit and service combination should they choose to optimize scalability and operational overhead?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use AWS Lambda functions to run code in response to events with automatic scaling, eliminating server management.",
      "B": "Launch EC2 instances with On-Demand capacity at the start of each spike.",
      "C": "Deploy containers on Amazon ECS with a fixed number of tasks.",
      "D": "Use Reserved Instances and schedule instance runs to coincide with predicted spikes."
    },
    "explanation": "AWS Lambda provides serverless compute that scales automatically in response to incoming requests, removing the need to provision or manage servers. On-Demand EC2 or ECS with fixed tasks requires capacity planning. Reserved Instances and scheduling do not address unpredictable spikes."
  },
  {
    "id": "885b84d9658392d045d63874eb393125d6124c418daa6df3d69b4bd0d3523bcd",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A biotech research team needs to quickly provision GPU-enabled instances to test machine learning models during several short experiments and then destroy them when done. Which AWS Cloud benefit best describes this capability compared to on-premises?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Economies of scale achieved by purchasing GPUs in bulk.",
      "B": "Rapid elasticity and agility to provision specialized resources on demand and release them when experiments complete.",
      "C": "Capacity reservation for GPUs using Reserved Instances.",
      "D": "High availability across Availability Zones for GPU workloads."
    },
    "explanation": "AWS provides rapid elasticity and agility, enabling the team to provision GPU-enabled instances in minutes on demand and release them immediately after, without hardware procurement delays. Economies of scale reduce costs but don\u2019t address provisioning speed. Reserved Instances require upfront commitment. High availability is a resilience benefit, not provisioning agility."
  },
  {
    "id": "a390a78c146c401bdfa5edd5d8467fd772d3e64a74e7035e5b5290017fc565f0",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.3",
    "stem": "A news website requires minimal downtime during regional disruptions, automatically routing readers to healthy Regions. Which AWS infrastructure benefit and service accomplish this?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon CloudFront with origin failover for edge caching.",
      "B": "AWS Global Accelerator to manage traffic across AWS Regions.",
      "C": "Amazon S3 Cross-Region Replication for static site hosting.",
      "D": "Amazon Route 53 health checks with latency-based or failover routing policies across Regions."
    },
    "explanation": "Amazon Route 53 can perform health checks and automatically fail over or route traffic based on latency to healthy endpoints in other Regions, leveraging AWS\u2019s global infrastructure. CloudFront origin failover is limited to predefined origins, and Global Accelerator optimizes performance but requires endpoints. S3 replication doesn\u2019t handle dynamic routing."
  },
  {
    "id": "27f5f19a8ce4b8017243b60dedb46fc6a13ffb7b7dd1ddee90040670203d75cc",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "A startup no longer maintains physical servers and no longer performs capacity planning for CPU, memory, or storage. Which AWS Cloud economic benefit allows this?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Global infrastructure eliminates capacity concerns.",
      "B": "Pay-as-you-go pricing shifts responsibility for capacity planning to AWS, allowing customers to consume only what they need.",
      "C": "Economies of scale enable AWS to manage capacity internally.",
      "D": "Elasticity allows dynamic resizing of server hardware."
    },
    "explanation": "Pay-as-you-go pricing means customers only pay for provisioned resources, with AWS handling the underlying capacity management. While elasticity resizes resources, pay-as-you-go is the key economic benefit eliminating capacity planning for customers. Economies of scale relate to AWS\u2019s cost structure, not customer capacity planning."
  },
  {
    "id": "9f3c648a2910be9fcfd3fd8affed86322f2f3c565a20b1f33fc922eb97eb9b46",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A retail chain needs to spin up infrastructure in under 10 minutes for pop-up stores during the holiday season, but procurement and hardware setup on-premises takes weeks. Which AWS benefit addresses this business requirement?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Economies of scale deliver lower hardware costs.",
      "B": "Elasticity scales hardware up and down automatically.",
      "C": "On-demand self-service enables provisioning of resources instantly without human intervention.",
      "D": "Global infrastructure provides presence in multiple markets."
    },
    "explanation": "On-demand self-service allows business units to provision compute, storage, and networking instantly via the AWS Console or APIs, meeting tight timeframes. Elasticity manages scaling in response to demand, but self-service is the mechanism for instant provisioning. Economies of scale affect cost, not provisioning speed."
  },
  {
    "id": "ad82f3372e29c515a4db9983433e3da1df40b91c83312986e89570b10e611ceb",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.2",
    "stem": "A multiplayer gaming platform uses Amazon Aurora for its backend. When one Availability Zone becomes unreachable, the application must automatically fail over within seconds to avoid impact. Which AWS feature leverages the benefit of high availability?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Aurora Multi-AZ deployments with a writer endpoint and reader endpoints that automatically fail over to a replica.",
      "B": "Aurora cross-Region replicas that asynchronously replicate data to another Region.",
      "C": "Manual point-in-time restores using snapshots stored in S3.",
      "D": "Using Amazon S3 event notifications to trigger new instance launches."
    },
    "explanation": "Aurora Multi-AZ deployments synchronously replicate data within a Region and automatically fail over to a standby replica in another AZ within seconds, ensuring high availability. Cross-Region replicas have higher latency and are asynchronous. Snapshots and S3 events are not designed for automated, low-RTO failover."
  },
  {
    "id": "09e5cdb3707da91f1108a7ab2a2bc910100c36e6f89b0b27a5bcbec0fee6b6be",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "A software vendor wants to expand into Latin America and Middle East markets but wants to test market uptake without significant upfront investment. Which AWS benefit should they leverage to meet this need?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Purchase Reserved Instances in those Regions and amortize cost over one year.",
      "B": "Use AWS Regions in those geographies on a pay-as-you-go basis to deploy infrastructure quickly and pay only for what is used.",
      "C": "Sign multi-year hardware hosting contracts with local data centers.",
      "D": "Use AWS Outposts to install on-premises racks in those markets."
    },
    "explanation": "AWS Regions in Latin America and the Middle East provide local infrastructure that can be provisioned instantly on a pay-as-you-go model, minimizing upfront investment and risk. Reserved Instances require commitment. On-premises contracts and Outposts involve capital expense and longer lead times."
  },
  {
    "id": "76226610306cac7856f8b9f419c41d1f38231b85a2a3319ac9f369d7ba8fbc06",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.1",
    "stem": "A company needs to deploy an identical VPC configuration across five AWS accounts in three different Regions, ensuring the process is fully automated, repeatable, and integrates seamlessly into their existing CI/CD pipeline. Which deployment method best satisfies these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Executing AWS CLI commands wrapped in a shell script for each account and Region",
      "B": "Using an AWS CloudFormation template triggered by AWS CodePipeline",
      "C": "Running a custom Python script with the AWS SDK (Boto3) from a developer\u2019s workstation",
      "D": "Manually configuring the VPC through the AWS Management Console using CloudFormer exports"
    },
    "explanation": "AWS CloudFormation provides an idempotent, template-driven approach that integrates natively with CodePipeline for automated, repeatable multi-account, multi-Region deployments."
  },
  {
    "id": "ceecf2ddef3b51ebd54142e1a6d1310fceacb5ef6a09e4b5fee126c8a89d1d36",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "An operations team must apply a critical security patch to a fleet of EC2 instances on a recurring schedule with minimal manual intervention. Which AWS service or deployment method should they choose to meet this requirement?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Use an SSH script invoked by a cron job from a bastion host",
      "B": "Configure AWS Systems Manager Patch Manager with a recurring patch baseline",
      "C": "Create a CloudFormation stack update that replaces instances monthly",
      "D": "Deploy a Lambda function that uses the AWS SDK to log into each instance"
    },
    "explanation": "AWS Systems Manager Patch Manager automates patching on a schedule, ensuring instances converge to the desired state without custom scripting or manual SSH access."
  },
  {
    "id": "f0d3cce9ca7b6a264140e61fc9da694b028c85cd8948ce50d8b179668f2b0337",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.3",
    "stem": "A security audit prohibits SSH access to EC2 instances. Which method enables administrators to obtain secure shell access to instances without opening inbound SSH ports?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Install a third-party VPN appliance in the VPC",
      "B": "Configure a bastion host with port forwarding and multi-factor authentication",
      "C": "Use AWS Systems Manager Session Manager to start an interactive shell session",
      "D": "Enable AWS CloudHSM and connect through its console"
    },
    "explanation": "Session Manager provides secure, audited shell access over the AWS API without requiring SSH ports or bastion hosts."
  },
  {
    "id": "59d750519d132872929ef5bd4358e3f1e5f547b627b5b13dca965e9bfc9fa3ee",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "An enterprise requires a hybrid deployment model where low-latency processing occurs on-premises and archival storage resides in AWS. Which AWS service best supports this hybrid deployment scenario?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Outposts for on-premises compute and Amazon S3 for archival storage",
      "B": "AWS Snowmobile for on-premises compute and AWS Glacier for storage",
      "C": "AWS Direct Connect for compute and AWS Storage Gateway for archival",
      "D": "Amazon CloudFront for on-premises edge processing and S3 Glacier Deep Archive"
    },
    "explanation": "AWS Outposts extends AWS hardware and services on-premises for low-latency compute, while S3 can house archival data in the cloud."
  },
  {
    "id": "7ca530deec4e7006c170bfe0f2597e385142ea1bbcc6ab3a3146a0d3b19fec53",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "A developer notices that AWS CLI commands fail because they\u2019re targeting the wrong Region, even though their AWS SDK-based application uses the intended Region. Why is the CLI behavior different, and how can it be corrected?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "The AWS SDK defaults to us-east-1 unless overridden in code; the CLI reads the AWS_PROFILE default",
      "B": "CLI uses EC2 instance metadata for Region and ignores the AWS config file",
      "C": "CLI commands require explicit --endpoint-url; SDK infers endpoints automatically",
      "D": "CLI reads the Region from ~/.aws/config under the active profile; setting region there or using --region fixes the issue"
    },
    "explanation": "The AWS CLI sources Region from the ~/.aws/config file for the active profile; adding or correcting the region entry or using --region aligns CLI behavior with the SDK."
  },
  {
    "id": "cda958e6e5dd3aac46a9095e927ce5f340336191d31178d92dceb3eeece61f08",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A solutions architect must deploy updates to an API Gateway and Lambda-backed service. They need to preview changes before affecting production. Which deployment approach accomplishes this requirement?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Directly update the production API and monitor with CloudWatch",
      "B": "Deploy manually via the AWS Management Console in production",
      "C": "Use CloudFormation change sets to preview and then execute the stack update",
      "D": "Execute AWS CLI commands in a dry-run mode against the production stack"
    },
    "explanation": "CloudFormation change sets allow a preview of proposed modifications to live stacks before execution, ensuring safe production updates."
  },
  {
    "id": "72eea4fa563f64d5dc420211f4d11a8d9b71c41b4088916f28eb24178f6c82d5",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "An organization is building a serverless application and wants to apply infrastructure-as-code with minimal boilerplate. Which tool or framework most reduces repetitive template code?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Serverless Application Model (SAM)",
      "B": "Raw CloudFormation JSON templates",
      "C": "AWS Management Console",
      "D": "AWS CLI imperative scripts"
    },
    "explanation": "AWS SAM extends CloudFormation with simplified syntax for serverless resources, reducing template boilerplate and supporting CI/CD integration."
  },
  {
    "id": "899f4ceef38a37e414874178b18cbe8cb4c942ff4da74f9b291137d4ab5beceb",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A DevOps engineer needs to detect and correct configuration drift in an existing CloudFormation stack that provisions database and networking resources. Which feature or service should they use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Config multi-account aggregator with remediation",
      "B": "CloudFormation drift detection and automated remediation via stack update",
      "C": "AWS Systems Manager State Manager to enforce resource configurations",
      "D": "Manual inspection via AWS CLI and patch scripts"
    },
    "explanation": "CloudFormation drift detection identifies stack resources that differ from the template; automated stack updates can then remediate drift to the declared state."
  },
  {
    "id": "51c805a3c3b2c1561d1cc04d5024068a624cd5755037cb00bb434090951bb508",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A startup needs to provision ephemeral test environments on demand and tear them down after use with zero lingering resources. Which deployment method aligns best with this requirement?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Console to manually clone resources for each test",
      "B": "Run AWS SDK scripts from a user\u2019s laptop when needed",
      "C": "Automate creation and deletion via CloudFormation stacks",
      "D": "Leverage AWS CLI commands executed interactively"
    },
    "explanation": "CloudFormation stacks allow rapid provisioning and clean removal of entire environments through single-stack create and delete operations, avoiding orphaned resources."
  },
  {
    "id": "1f0c3775dd947f3a64c0dc6dd12f1e9f0e12a362d726d00521742514d57be10f",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "Developers want to use a general-purpose programming language to define and deploy AWS infrastructures. Which tool supports this imperative approach while generating CloudFormation under the hood?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS CloudFormation Designer",
      "B": "AWS SAM CLI",
      "C": "AWS CDK Toolkit in TypeScript without synthesis",
      "D": "AWS Cloud Development Kit (CDK)"
    },
    "explanation": "AWS CDK lets developers author infrastructure in languages like Python or TypeScript, synthesizes into CloudFormation templates for deployment."
  },
  {
    "id": "b04894105b7d0aac9d808515d3e6adae639479a4b89c6a8420dc5f0874a3c1ca",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.2",
    "stem": "A global media company must push emergency configuration changes to hundreds of IoT devices in the field with intermittent connectivity. Which AWS deployment method supports secure, asynchronous updates?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS CodeDeploy to each device fleet",
      "B": "AWS IoT Device Management job documents",
      "C": "AWS Lambda functions invoked manually",
      "D": "AWS Systems Manager State Manager"
    },
    "explanation": "AWS IoT Device Management jobs deliver and apply configuration updates securely when devices reconnect, handling intermittent connectivity."
  },
  {
    "id": "eabf51969ce0c4fba3d2331389088465530a46922a638650d45e9f2433c8737e",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.3",
    "stem": "An engineer needs ad-hoc CLI access to AWS without installing any local software and wants credentials scoped to their IAM identity. Which AWS service or feature should they use?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS CloudShell",
      "B": "AWS Console embedded CLI",
      "C": "AWS Config snippet runner",
      "D": "AWS Systems Manager Automation"
    },
    "explanation": "AWS CloudShell provides a browser-based shell with preconfigured AWS CLI and credentials scoped to the user\u2019s IAM identity without local setup."
  },
  {
    "id": "2d3ff27ac0b741d60bdf71b987aa77c14f25eeec10af29801a92e27fc8fbf147",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A compliance requirement states that no manual changes are allowed to production infrastructure. Which combination of deployment and governance tools enforces this policy?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy via AWS Console and monitor with CloudWatch Events",
      "B": "Use AWS CLI for provisioning and guard with AWS Config rules",
      "C": "Provision exclusively via CloudFormation within AWS Organizations Service Control Policies",
      "D": "Apply changes via AWS SDK scripts signed by a CI/CD user"
    },
    "explanation": "Restricting production modifications to CloudFormation within an AWS Organizations SCP prevents manual console or API changes, ensuring all changes flow through versioned templates."
  },
  {
    "id": "fb16cd9b743ce0210e16673cfef9796863978c9da715ddd84678ba4f459bb461",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A network operations team must perform a one-time audit of all security group rules across multiple VPCs and Regions without writing custom code. Which method provides the simplest path?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Log into each account and use the AWS Console\u2019s search feature",
      "B": "Run AWS Config advanced query to retrieve security group configurations",
      "C": "Invoke AWS SDK scripts from an EC2 bastion host",
      "D": "Call DescribeSecurityGroups via AWS CLI per Region"
    },
    "explanation": "AWS Config\u2019s advanced queries enable cross-Region, cross-account resource inspection with a single predefined SQL-like query, eliminating custom scripting."
  },
  {
    "id": "96bf23a00fbe24f85594729bc8584a1fc35bde49b770235ea5efe281210f3d5d",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.3",
    "stem": "A CLI-based automated deployment is failing due to missing permissions when executed by an EC2 instance role. Which practice ensures least-privilege programmatic access while enabling the deployment?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Embed long-lived IAM user credentials in the CLI profile on the instance",
      "B": "Attach administrator-managed policies to the instance role",
      "C": "Grant wildcard permissions for all resources in the instance role\u2019s policy",
      "D": "Create a dedicated IAM role with a fine-grained policy and assign it to the instance profile"
    },
    "explanation": "Assigning a purpose-built IAM role with only the required permissions to the EC2 instance profile enforces least privilege while enabling CLI-based deployments."
  },
  {
    "id": "842674f2e2888ffca572c7c1edd3acb896cc49fb704f5c41ead47c91d2e7f0d4",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A media processing application needs to automatically analyze images uploaded to an S3 bucket. Each analysis invocation takes less than 3 seconds, requires no custom OS, and must scale to thousands of events per second with no server management overhead. Which AWS compute service should you choose?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Lambda",
      "B": "Amazon EC2 Auto Scaling group",
      "C": "Amazon ECS on EC2 launch type",
      "D": "AWS Batch"
    },
    "explanation": "AWS Lambda provides event-driven execution with automatic, massive scaling and no server management. EC2 Auto Scaling and ECS on EC2 require you to provision and manage instances. AWS Batch is for long-running batch jobs and has higher cold-start latency and management complexity."
  },
  {
    "id": "09d79e044b162d12b5bbb4a6463ddfe91de50cdc8cfece0dd38776a8a75e35fd",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "Your team needs to run a series of high-performance computing jobs that have complex interdependencies, run for several hours, and can tolerate Spot interruptions. They want a service that manages the job queue, retries, and compute provisioning. Which AWS compute service is most suitable?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon ECS on Fargate",
      "B": "AWS Lambda",
      "C": "Amazon EC2 Spot Instances with user-managed scheduling",
      "D": "AWS Batch"
    },
    "explanation": "AWS Batch automatically handles dependency resolution, retries, and provisioning of Spot or On-Demand compute for long-running jobs. ECS on Fargate lacks built-in job queuing and dependency features. Lambda can\u2019t run multi-hour jobs. Managing EC2 Spot yourself shifts orchestration burden to you."
  },
  {
    "id": "b0d05b2b98c1cd2769abe545a3b77457cfc36ef3d1311d5bc55100bc382a5d46",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A microservices architecture requires full control over Kubernetes configurations, custom networking, and cluster upgrades. You need minimal interruption and full Kubernetes API compatibility. Which compute service should you select?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon ECS",
      "B": "Amazon EKS",
      "C": "AWS Fargate",
      "D": "AWS Lambda"
    },
    "explanation": "Amazon EKS provides a fully managed Kubernetes control plane while giving you direct Kubernetes API access and control. ECS is native AWS container orchestration, not Kubernetes. Fargate abstracts away nodes and doesn\u2019t expose full cluster control. Lambda isn\u2019t container orchestration."
  },
  {
    "id": "47389e9df2f5b979a777c503dce0330fe63b1d060ff63271bf980e35192428d2",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.1",
    "stem": "You must rapidly deploy a .NET Core web application across multiple Availability Zones with built-in load balancing, health monitoring, and auto-scaling without provisioning infrastructure manually. Which AWS compute service fits this requirement?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon EC2 instances with Auto Scaling",
      "B": "Amazon Lightsail",
      "C": "AWS Elastic Beanstalk",
      "D": "AWS Lambda"
    },
    "explanation": "Elastic Beanstalk automates provisioning and scaling of a .NET Core app with health monitoring and load balancing. EC2 with Auto Scaling requires manual configuration. Lightsail doesn\u2019t support multi-AZ auto-scaling. Lambda is for event-driven functions, not traditional web apps."
  },
  {
    "id": "7e2f55314ec5105f85c3e13ab99455ceb54f67f65942b3444f44a05378566982",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.8",
    "stem": "A global e-commerce site requires personalization code executed at edge locations to minimize latency and offload origin. Which AWS compute service should you use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Lambda",
      "B": "Amazon EC2 with Local Zones",
      "C": "Amazon ECS on EC2",
      "D": "AWS Lambda@Edge"
    },
    "explanation": "Lambda@Edge runs functions at CloudFront edge locations, reducing origin latency. A regional Lambda still runs in a region, not at edges. EC2 Local Zones are compute but require instance management. ECS on EC2 has similar limitations."
  },
  {
    "id": "cd923f1cc001a5507802a21340218dc28fab1f649f4f15b31a708536bc72cf68",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A financial modeling workload requires GPU acceleration for parallel computations and strict control over driver installations. Which compute option should you provision?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Fargate",
      "B": "Amazon EC2 P3 instance",
      "C": "AWS Lambda",
      "D": "AWS Batch"
    },
    "explanation": "EC2 P3 instances provide dedicated GPUs and full OS control for drivers. Fargate cannot provision GPUs. Lambda has limited GPU support and runtime constraints. Batch could orchestrate on EC2 but you\u2019d still choose the underlying EC2 P3 for GPU."
  },
  {
    "id": "2c932a09c2ccd68a2b6818fbe73681f52d8463fe402a626e8a163dcb85ecdb3f",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "Your organization must run an on-premises video processing application that uses the same AWS APIs and management tools as in the cloud, due to data residency requirements. Which compute service meets this need?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Snowball Edge",
      "B": "AWS Local Zones",
      "C": "AWS Outposts",
      "D": "Amazon EC2 with VPN"
    },
    "explanation": "AWS Outposts extends AWS infrastructure on-premises with the same APIs, management, and hardware. Snowball Edge is for data transfer appliances. Local Zones are AWS data centers in metro areas, not on your site. EC2 with VPN still runs in AWS."
  },
  {
    "id": "0d4711216b2cc479752c515c2b7ac8dbea5092fb78b7d4847eab652964ba70fd",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "A small static website needs a predictable monthly cost and simple DNS, SSL, and content hosting without deep AWS expertise. Which compute offering should you recommend?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon EC2 with S3 static hosting",
      "B": "Amazon Lightsail",
      "C": "AWS Elastic Beanstalk",
      "D": "AWS Lambda"
    },
    "explanation": "Lightsail provides a simple, fixed-price package including DNS and SSL for basic websites. EC2 plus S3 requires management of multiple services. Elastic Beanstalk is more complex and Lambda isn\u2019t suited for full static site hosting."
  },
  {
    "id": "e21dd5091f9fd9026513d974685b6c6151938b198814474ab438c2cf9f6101d5",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "Your machine learning team needs to host containerized inference endpoints on GPU-enabled hardware, using custom AMIs and a familiar Docker-based workflow. They want AWS to manage the cluster overhead. Which compute combination do you choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Fargate",
      "B": "Amazon EKS on Fargate",
      "C": "Amazon ECS on EC2 launch type",
      "D": "AWS Lambda"
    },
    "explanation": "ECS on EC2 launch type lets you provision GPU-enabled EC2 instances with custom AMIs and Docker tasks, while AWS manages the orchestration. Fargate does not support GPUs. EKS on Fargate also doesn\u2019t support GPUs. Lambda cannot run GPU workloads."
  },
  {
    "id": "88fa55fca6b22d409a1b9162f89b26c34fb10bc7feb0c6879410c0ea22e09726",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "You have long-running containerized data transformations that exceed 15 minutes and require up to 10 vCPU and 30 GB memory without server provisioning. Which compute service should you select?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Lambda",
      "B": "Amazon EC2 Spot Instances",
      "C": "Amazon ECS on EC2",
      "D": "AWS Fargate"
    },
    "explanation": "AWS Fargate supports long-running containers with custom resource requirements and abstracts server management. Lambda is capped at 15 minutes. EC2 Spot and ECS on EC2 require you to manage instances."
  },
  {
    "id": "172aed2cc4eeea6e9266a16fe0c63bcc06b16faf845d13024bf94ead9d208587",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "Which AWS compute service supports provisioned concurrency to pre-warm execution environments and reduce cold-start latency for functions?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Lambda",
      "B": "AWS Fargate",
      "C": "Amazon EC2 Auto Scaling",
      "D": "AWS Batch"
    },
    "explanation": "AWS Lambda\u2019s provisioned concurrency feature keeps function instances initialized. Fargate, EC2 Auto Scaling, and Batch don\u2019t offer this specialized serverless pre-warming capability."
  },
  {
    "id": "bc78fdd3af6dd130d635238c830c5aa1926b7aa6485cd34c6f8e4e3ead72f1fd",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "You are running a Kubernetes cluster on AWS but want pods from one namespace to launch without managing nodes. Which compute configuration should you use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon EKS on EC2 launch type",
      "B": "Amazon EKS with Fargate profile",
      "C": "Amazon ECS on Fargate",
      "D": "AWS Lambda"
    },
    "explanation": "EKS with a Fargate profile allows pods in specified namespaces to run serverless without EC2 node management. EKS on EC2 requires managing nodes. ECS is not Kubernetes. Lambda is function-based."
  },
  {
    "id": "490604baf8986bbb36c16859c7bd167723a663593cf3d70fc7f652e733630532",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.3",
    "stem": "A serverless function needs access to resources in a private VPC. Which compute service variant supports direct VPC configuration?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Lambda (regional)",
      "B": "AWS Lambda@Edge",
      "C": "AWS Fargate",
      "D": "Amazon Lightsail"
    },
    "explanation": "Regional AWS Lambda functions can be configured to access a VPC. Lambda@Edge cannot be attached to a VPC. Fargate and Lightsail aren\u2019t serverless functions and require different network setups."
  },
  {
    "id": "f059d4ac4107e8bdf89582373c710b319ec26aa9240460a13f4216e41f16aa0c",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "For containerized batch jobs that must use Spot Instances to minimize cost, automatically handle retries, and support a job queue, which service do you select?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon ECS on Fargate",
      "B": "Amazon EKS",
      "C": "AWS Batch",
      "D": "AWS Lambda"
    },
    "explanation": "AWS Batch natively orchestrates container jobs on Spot or On-Demand instances, manages job queues, and retries. ECS on Fargate doesn\u2019t support Spot. EKS requires manual node management. Lambda can\u2019t run long-running Spot workloads."
  },
  {
    "id": "f574fd02577e612a7fc7280d64e9c09c0b6b5d00cac96801e2eba8a2ef9c275e",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A data processing pipeline consists of many parallel tasks that need shared file system access (up to 5 GB), can run up to 14 minutes, and should incur cost only when running. Which compute service satisfies these requirements?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Lambda with Amazon EFS",
      "B": "AWS Fargate",
      "C": "Amazon EC2",
      "D": "AWS Batch"
    },
    "explanation": "Lambda can now mount EFS for shared storage and runs functions up to 15 minutes, billing per execution. Fargate and EC2 require instance management. Batch adds queuing complexity and may incur compute overhead."
  },
  {
    "id": "2c3d1c495e491ef1361b7ca0af7a76de8686409c261c89229e9561c7a880aef0",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A global e-commerce company experiences dramatic spikes in order volume during promotional events. To handle sudden traffic surges, they distribute microservices across multiple Availability Zones and decouple order processing workflows using Amazon SQS. Which AWS Well-Architected Framework pillar is primarily demonstrated by these design choices?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Operational Excellence",
      "B": "Performance Efficiency",
      "C": "Reliability",
      "D": "Security"
    },
    "explanation": "Distributing across AZs and decoupling for fault tolerance and recovery are core practices of the Reliability pillar."
  },
  {
    "id": "14d40c5f739c46ec69e2d529bdb6e51b5ee6af6e606076b6b49a38b49d5031a1",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "An online marketplace has implemented AWS CloudFormation templates and a CI/CD pipeline using AWS CodePipeline, CodeBuild, and CodeDeploy. They collect deployment and application metrics to refine runbooks and automatically roll back failed changes. Which pillar does this practice exemplify?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Cost Optimization",
      "B": "Operational Excellence",
      "C": "Performance Efficiency",
      "D": "Reliability"
    },
    "explanation": "Automating processes, measuring outcomes, and evolving runbooks align with the Operational Excellence pillar."
  },
  {
    "id": "7f2ebcb580f17be24a0daa2d9ac943bc040d2730696de393d6c4fedb3029368b",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.2",
    "stem": "A streaming analytics service serves global customers requiring sub-50 ms playback startup times. The architecture uses Amazon CloudFront edge caching, Route 53 latency-based routing, and DynamoDB Accelerator (DAX). Which pillar of the Well-Architected Framework is being addressed?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Performance Efficiency",
      "B": "Operational Excellence",
      "C": "Security",
      "D": "Cost Optimization"
    },
    "explanation": "Employing edge caches and latency-based routing to meet strict performance targets is central to the Performance Efficiency pillar."
  },
  {
    "id": "a5861b1848691ec3772ddfec6f94671a986713c1aee83cfbc56b18129448c87e",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A healthcare application must enforce encryption for all data at rest and in transit, apply strict IAM policies with least privilege, and record all API activity using AWS CloudTrail. Which pillar is primarily focused on these design requirements?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Reliability",
      "B": "Security",
      "C": "Cost Optimization",
      "D": "Performance Efficiency"
    },
    "explanation": "Data protection, least-privilege access, and audit trails are fundamental to the Security pillar."
  },
  {
    "id": "76f371938aec3d6ea1cbe100f00c493f8595ff6743cda444ec55b014d3aa83f6",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A SaaS provider analyzes usage patterns with AWS Cost Explorer, implements AWS Compute Optimizer recommendations for right-sizing, and purchases Reserved Instances for steady-state workloads. Which Well-Architected Framework pillar does this strategy support?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Performance Efficiency",
      "B": "Security",
      "C": "Operational Excellence",
      "D": "Cost Optimization"
    },
    "explanation": "Rightsizing, usage analysis, and capacity commitments directly support the Cost Optimization pillar."
  },
  {
    "id": "8f96199cac38ffe9b993107ef1081f25a314041e9cdf779351395051af51d3f9",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "An image processing pipeline migrates from EC2 instances to AWS Lambda functions behind API Gateway to eliminate server management and automatically scale with incoming requests. Which pillar does this design choice illustrate?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Operational Excellence",
      "B": "Reliability",
      "C": "Cost Optimization",
      "D": "Performance Efficiency"
    },
    "explanation": "Leveraging serverless services for automatic scaling and no capacity planning aligns with Performance Efficiency."
  },
  {
    "id": "f251fd367c06832143cd7a1427c012d344a490a02e9aab7d268cc05fca7b9da1",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A SaaS vendor instruments end-to-end request tracing with AWS X-Ray, aggregates application logs, and updates runbooks based on anomalies discovered during tracing sessions. Which pillar is demonstrated by this practice?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Security",
      "B": "Performance Efficiency",
      "C": "Operational Excellence",
      "D": "Reliability"
    },
    "explanation": "Capturing telemetry to analyze and improve operations is a core tenet of the Operational Excellence pillar."
  },
  {
    "id": "9ba36c46f87d202e1e5c43c7736f9bb5d695026076552990523bc8c08b02369c",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A logistics startup builds an event-driven testing environment where developers deploy experimental features with feature flags, capture metrics via Amazon CloudWatch, and roll back experiments automatically if error rates exceed thresholds. Which pillar\u2019s design principles best support this practice?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Reliability",
      "B": "Operational Excellence",
      "C": "Cost Optimization",
      "D": "Security"
    },
    "explanation": "Rapid experimentation, automated rollback, and metrics-driven decisions are hallmarks of Operational Excellence."
  },
  {
    "id": "267f7d8baeee0f54f0df08768a212ed39036260891433c8da31e6dae86e7a6cb",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A research institution uses Amazon EC2 Spot Instances and implements application-level checkpointing to handle interruptions. They process large batch workloads while minimizing infrastructure costs. Which pillar is most directly addressed by their use of spot capacity and checkpoint-restart mechanisms?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Security",
      "B": "Reliability",
      "C": "Cost Optimization",
      "D": "Performance Efficiency"
    },
    "explanation": "Using spot capacity and checkpointing to reduce expense aligns with the Cost Optimization pillar."
  },
  {
    "id": "af8206886638dc61fbc8849ee9480c0f888ede8f054266baef79d1f5c97193f4",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A bank employs AWS Config rules to detect non-compliant resources, triggers AWS Lambda to correct drift automatically, and alerts operators when manual approval is required. Which pillar does this automated remediation and governance reflect?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Performance Efficiency",
      "B": "Security",
      "C": "Operational Excellence",
      "D": "Cost Optimization"
    },
    "explanation": "Automating detection, remediation, and governance for continual improvement is part of Operational Excellence."
  },
  {
    "id": "124805d43942291785e1bf00f9f485ae4c0637d4954e7b418cd6f136bc885a27",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.2",
    "stem": "A real-time chat application leverages Amazon API Gateway for WebSocket connections, AWS Lambda functions for message routing, and DynamoDB for state management. It scales transparently under spiky, unpredictable load without manual capacity planning. Which pillar\u2019s underlying principle is best exemplified?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Reliability",
      "B": "Security",
      "C": "Operational Excellence",
      "D": "Performance Efficiency"
    },
    "explanation": "Automatically scaling serverless components to meet load without manual intervention exemplifies Performance Efficiency."
  },
  {
    "id": "e02f67b5ea07f372447fa1b6b28fc1e170bb49f8aa4904d41d7daba4c7cd7068",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A global financial service uses active-active deployment across two AWS Regions with synchronous data replication to maximize uptime (Reliability). However, clients report increased write latency due to replication overhead. Which pillar highlights this trade-off and should guide further optimization?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Security",
      "B": "Performance Efficiency",
      "C": "Operational Excellence",
      "D": "Cost Optimization"
    },
    "explanation": "Balancing latency overhead against availability is a concern of the Performance Efficiency pillar."
  },
  {
    "id": "58863bd20751cc60abc26fa814dc050817e51937e0c37aab755bd977394dbc4a",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.2",
    "stem": "A media analytics platform uses Amazon CloudWatch Auto Scaling with predictive scaling based on machine learning forecasts to adjust capacity proactively. Which pillar\u2019s design principle is being applied by leveraging predictive scaling?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Operational Excellence",
      "B": "Reliability",
      "C": "Performance Efficiency",
      "D": "Cost Optimization"
    },
    "explanation": "Using data-driven, predictive scaling to meet performance requirements aligns with Performance Efficiency."
  },
  {
    "id": "6b2043d09559aa1c1cb714579e150369c051bb290756444a79626b21fdb1330a",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A critical service deploys across multiple Availability Zones and uses Elastic Load Balancing health checks to replace unhealthy EC2 instances automatically. Which pillar focuses on these self-healing and high-availability practices?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Performance Efficiency",
      "B": "Reliability",
      "C": "Cost Optimization",
      "D": "Security"
    },
    "explanation": "Automatic recovery and fault isolation through health checks are core aspects of the Reliability pillar."
  },
  {
    "id": "47d41e25f507065644f5fabb36e97fd4ee15f27cf3e0de20a8bbbb4f27b10b45",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A web application team adopts blue/green deployments using AWS CodeDeploy to minimize deployment risk and enable safe rollbacks. Which pillar and design principle does this practice best represent?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Reliability \u2013 Design for failure and recovery",
      "B": "Operational Excellence \u2013 Automate change and validation",
      "C": "Security \u2013 Implement defense in depth",
      "D": "Cost Optimization \u2013 Manage demand-based resource allocation"
    },
    "explanation": "Blue/green deployments and automated rollbacks are practices of Operational Excellence\u2019s change management."
  },
  {
    "id": "27d8dde4a76b1133e4eadca97570f897f4e9f9e89f87c0e66162fbace7653298",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A financial firm plans to run 50 m5.large EC2 instances continuously for five years to support a stable analytics pipeline. They are willing to commit to a long-term contract. Which purchasing option will minimize their total compute cost?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Five-year Standard Reserved Instances",
      "B": "Five-year Convertible Reserved Instances",
      "C": "Five-year EC2 Instance Savings Plan",
      "D": "Five-year Compute Savings Plan"
    },
    "explanation": "Standard RIs for a specific instance type offer the deepest discount (~72%) compared to Convertible RIs (~54%) or Savings Plans (~66%). Convertible RIs and Savings Plans provide flexibility at a lower discount."
  },
  {
    "id": "4425cfb4c5438cefbb33890cce35a8ebedf3ec415440d883da4fd8b5c963660a",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A SaaS startup anticipates unpredictable spikes and lulls in EC2 usage across multiple instance families and regions. Which pricing model yields the greatest cost savings while preserving maximum flexibility?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Standard Reserved Instances",
      "B": "Convertible Reserved Instances",
      "C": "EC2 Instance Savings Plan",
      "D": "Compute Savings Plan"
    },
    "explanation": "Compute Savings Plans apply discounts across any EC2 instance family or region, making them more flexible than Instance Savings Plans (which are tied to a family) and RIs (which are tied to instance types/regions)."
  },
  {
    "id": "7c460eb41c4e9267649e6917830774369e0f0df5ddb6116d02634fd054a349d5",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A regulatory audit requires you to archive 100 TB of data with infrequent access (once per quarter) and retrieval possible within minutes. Which S3 storage class minimizes monthly cost while meeting retrieval SLAs?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "S3 Standard\u2013Infrequent Access",
      "B": "S3 Glacier Deep Archive",
      "C": "S3 Glacier Instant Retrieval",
      "D": "S3 One Zone\u2013Infrequent Access"
    },
    "explanation": "Glacier Instant Retrieval offers the lowest storage cost (~$0.004/GB-month) with millisecond retrieval. IA classes are more expensive and Glacier Deep Archive retrieval times (hours) don\u2019t meet minutes SLA."
  },
  {
    "id": "e7033e1431cf4095dde824c1798c1e26b6da232f010fd6dd8f7a0d9b9b21de3b",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "You host a multi-region static website and expect truly unpredictable read patterns. You want to minimize storage costs without manual tiering or risk of availability impact. Which S3 storage class is most appropriate?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "S3 Standard",
      "B": "S3 Intelligent-Tiering",
      "C": "S3 Standard\u2013Infrequent Access",
      "D": "S3 Glacier Flexible Retrieval"
    },
    "explanation": "Intelligent-Tiering automatically moves objects between tiers based on access patterns and maintains multi-AZ durability with no operational overhead, unlike IA (manual) or Glacier."
  },
  {
    "id": "a7e8f0daa42416e1dca7036f00b9807e377a8a68f42f002ba1b0d4e69b4f9e0a",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A data scientist pushes 5 TB of data each month into an S3 bucket via the public internet. How will AWS bill the data transfer?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "$0.02/GB data transfer in",
      "B": "Free\u2014data ingress is not charged",
      "C": "$0.01/GB PUT request data transfer",
      "D": "Charged at inter-AZ data transfer rates"
    },
    "explanation": "AWS does not charge for data transferred into AWS (ingress) from the public internet. Charges apply only for data egress."
  },
  {
    "id": "14f0e7eedbaf13eea108149824b0ad6e89d577c24aa966d123f66560d6c60440",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "You replicate EBS snapshots from us-east-1 to eu-west-1 daily. Which cost components apply?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Snapshot storage in source region only",
      "B": "Data transfer in destination plus storage",
      "C": "Inter-region data transfer out from us-east-1 plus snapshot storage in eu-west-1",
      "D": "No data transfer fees, destination storage only"
    },
    "explanation": "Cross-region snapshot copy incurs inter-region data transfer charges on the source plus snapshot storage charges in the destination region."
  },
  {
    "id": "8fa0d0a1e88b78a4cfb6d4748132ed25826a45ebe7dee67341c2dbfa8803faf4",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "Your global application uses CloudFront with an S3 origin. Compared to direct S3 access, what data transfer fee applies when delivering content from CloudFront edge to end users?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Standard S3 data egress rate",
      "B": "Regional data transfer rate",
      "C": "Inter-AZ data transfer rate",
      "D": "Edge-to-Internet at CloudFront rates (lower than S3 egress)"
    },
    "explanation": "CloudFront delivers content to viewers at CloudFront data transfer pricing, typically lower than S3 data egress rates."
  },
  {
    "id": "a3b1175938ca899c21e1c3567826e293c6ab438b2e495f65b4c0bfcba1a3e3e4",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A large financial service firm needs consistent low-latency, high-volume transfers (20 TB/month) between their datacenter and AWS. Which option provides the lowest network cost per GB?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Direct Connect dedicated circuit",
      "B": "Internet VPN over public internet",
      "C": "AWS Site-to-Site VPN",
      "D": "VPC peering"
    },
    "explanation": "Direct Connect provides a dedicated circuit with lower per-GB transfer rates compared to internet-based transfers or VPN tunnels."
  },
  {
    "id": "4b8f04699ce60e6a3b281ba871c43d3323966092888c4f42270fdb4936fab491",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "Your logs average 128 KB per object stored in S3 Standard-IA. A rogue process deletes an object after 10 days. Which cost applies for that deletion?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "No early deletion fees\u2014paid per GB used",
      "B": "Early deletion charge = storage cost for remaining 20 days (30-day minimum)",
      "C": "Full month\u2019s storage only, no penalty",
      "D": "API DELETE request fee only"
    },
    "explanation": "Standard-IA enforces a 30-day minimum storage charge per object; deleting before 30 days incurs charges for the unused days."
  },
  {
    "id": "b51aaec72ee27ab533d00a502a235faba96aff1df21571e02b72d9284e2adba2",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A workload writes 1 TB to Amazon EFS One Zone once per day and rarely re-reads data. You want lowest monthly cost. Which file storage option?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "EFS Standard",
      "B": "EFS Standard-Infrequent Access",
      "C": "EFS One Zone\u2013Infrequent Access",
      "D": "EBS gp2 volume"
    },
    "explanation": "EFS One Zone-IA charges lower storage rates and IA rates apply since data is rarely read; EBS and Standard EFS are more expensive."
  },
  {
    "id": "969fb0a97d1b78af5df0a2fe6d127da3b971e555223ad820bec448972beef949",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A media company wants predictable billing for Lambda functions invoked by API Gateway 1 million times per month with 512 MB memory at 200 ms average. Which describes cost?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Pay per request plus GB-second duration (billed monthly)",
      "B": "Flat monthly fee for reserved concurrency",
      "C": "Pay per EC2 instance-hour for the underlying hosts",
      "D": "Included free tier covers entire usage"
    },
    "explanation": "Lambda pricing consists of a per-request fee plus compute duration charged in GB-seconds. There is no flat EC2 instance or reserved fee for Lambda."
  },
  {
    "id": "3e5b38b3aab78bd41a90bc2880c1676ce2e7d8dbd480d188d9f7e4576c5f73bd",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "Your team provisions a 1 TiB gp3 EBS volume with 4,000 IOPS and 250 MB/s throughput. Which cost components will you incur?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Storage only (per GB-month)",
      "B": "Storage plus data transfer rate",
      "C": "Storage per GB-month, provisioned IOPS per IOPS-month, provisioned throughput per MB/s-month",
      "D": "Storage plus per-IO request fee"
    },
    "explanation": "gp3 volumes separate billing: per-GB storage, per-provisioned IOPS, and per-provisioned throughput."
  },
  {
    "id": "7505d8de62edcab7e5680948117393cc4e23ee68a283b49ad51d97310a0265a4",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "You need to copy 500 GB of data daily between S3 buckets in the same region. Which pricing applies?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Cross-region transfer rates",
      "B": "S3 PUT request charges only (data transfer in same region is free)",
      "C": "Standard data egress from source bucket",
      "D": "INTER-AZ bandwidth rates"
    },
    "explanation": "Data transferred between S3 buckets in the same region is free; you pay only for PUT requests on the destination."
  },
  {
    "id": "34b656221c7f81a899ad6d5fa2dfe555e9dbb3b3596523ef49e2fb80812e1599",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.2",
    "stem": "A team is evaluating storage options for infrequent backups requiring millisecond retrieval. They compare S3 IA, Glacier Flexible Retrieval, and Glacier Instant Retrieval. Which cost component differentiates Glacier Flexible vs Instant?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Storage per GB-month (Instant cheaper)",
      "B": "Retrieval per GB (Flexible has none)",
      "C": "Minimum storage duration (Instant shorter)",
      "D": "Retrieval latency SLA (Instant supports milliseconds, Flexible minutes to hours)"
    },
    "explanation": "Glacier Instant Retrieval supports millisecond access at a higher storage cost, while Flexible Retrieval has minute-to-hour latency; they also differ in retrieval fees and minimum durations."
  },
  {
    "id": "6262d018736350182353cb8a46c6a3e838ffb327e18a83c9cbb9622e9cb4feed",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "Your architecture uses API Gateway private integration to invoke a Lambda in the same region. Which data transfer charges apply?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "API Gateway data transfer out to internet rates",
      "B": "Inter-region data transfer rates",
      "C": "No data transfer charges within the same region between services",
      "D": "Data transfer charges for VPC traffic"
    },
    "explanation": "Data transfer between services (API Gateway and Lambda) in the same region is free; egress charges apply only to transfers outside the region or to the internet."
  },
  {
    "id": "f0cb96621e6a13479abedc77bf75da06fea4df2218d141bc2d4179d3cf7ded92",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "A startup on the Developer Support plan needs to receive programmatic notifications when their AWS resources experience account-specific health events. Which support plan must they subscribe to enable the AWS Health API and integrate with Amazon EventBridge for automated alerts?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Basic Support",
      "B": "Developer Support",
      "C": "Business Support",
      "D": "Enterprise Support"
    },
    "explanation": "The AWS Health API\u2014and its integration with EventBridge for account-specific health events\u2014is available only on Business and Enterprise Support. Business is the minimum required plan."
  },
  {
    "id": "57dc38283800f27d84b260c77ffb247dd3b73e98acd81c3c8947c29718082e35",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A financial services company is planning a major product launch and requires proactive architectural guidance and event-management support from AWS. Which support plan and feature combination provides access to a designated Technical Account Manager (TAM) and Infrastructure Event Management (IEM)?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Business Support with Infrastructure Event Management",
      "B": "Business Support with a Technical Account Manager",
      "C": "Developer Support with Infrastructure Event Management",
      "D": "Enterprise Support with both a Technical Account Manager and IEM"
    },
    "explanation": "Only the Enterprise Support plan includes both a designated TAM and the IEM engagement. Business Support does not include TAM or IEM."
  },
  {
    "id": "f54d7b227e9a4d6baf061ff475f679f1537c80642301e4a76ea2811ef0653106",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "A DevOps engineer attempts to call the AWS Support API (for example, DescribeCases) from the us-west-2 region and receives a \u201cService not found\u201d error. What is the most likely cause?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "The engineer lacks the necessary IAM permissions for support:*",
      "B": "API access must be enabled manually in the Support Center console",
      "C": "The AWS Support API endpoint exists only in the us-east-1 region",
      "D": "Their Support plan does not include API access"
    },
    "explanation": "The AWS Support API is only available via the us-east-1 regional endpoint. Attempts from other regions return a service not found error."
  },
  {
    "id": "48f9af6af2c43d7c9993db8ff8e89a1fa3c0f905a3f60d0c09572a9d139d2c76",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A solutions architect needs an automated pipeline that routes AWS account-specific health notifications into an Amazon SNS topic for downstream processing. Which combination should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Export Service Health Dashboard events as CSV into an S3 bucket",
      "B": "Stream CloudWatch metrics for service-limit breaches to SNS",
      "C": "Use AWS Personal Health Dashboard events via Amazon EventBridge to SNS",
      "D": "Poll the AWS Health API in a Lambda function and then publish to SNS"
    },
    "explanation": "The recommended integration is Personal Health Dashboard \u2192 EventBridge \u2192 SNS. Service Health Dashboard cannot be exported this way; polling the API in Lambda adds unnecessary complexity."
  },
  {
    "id": "82846630de27c666e0cd1d0467efc252ae056cf3a9c9a51ebe656e770cb62294",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A security compliance officer needs to download audited compliance reports such as SOC 2 and ISO 27001 for AWS services used in their environment. Which AWS resource provides this information?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Artifact",
      "B": "AWS Trusted Advisor",
      "C": "AWS Security Hub",
      "D": "AWS Config"
    },
    "explanation": "AWS Artifact is the central resource for on-demand access to AWS\u2019s compliance reports and agreements. Trusted Advisor, Security Hub, and Config are for operational checks, not compliance report distribution."
  },
  {
    "id": "1bd9608fc30a14f64398ca46e3b800ca586edd091b3d72f0958eba0f1a4998ce",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A developer encounters an uncommon AWS SDK error not documented in the official docs. They want to ask AWS experts and the user community for assistance. Which resource should they use?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Support Center",
      "B": "AWS re:Post",
      "C": "AWS Knowledge Center",
      "D": "AWS Developer Forums"
    },
    "explanation": "AWS re:Post is the modern, peer-reviewed Q&A community moderated by AWS. AWS Developer Forums have been deprecated, the Support Center is for paid-support cases, and the Knowledge Center contains AWS-published articles."
  },
  {
    "id": "8849af21e7d3087a2a7c24a342802aa5eabc199f12b7ba2227a921d5c9e9c0c3",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A DevOps engineer wants to find AWS-published troubleshooting articles and one-step remediation guides for Amazon EC2. Which resource should they consult?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Documentation",
      "B": "AWS re:Post",
      "C": "AWS Knowledge Center",
      "D": "AWS Solutions Library"
    },
    "explanation": "The AWS Knowledge Center provides targeted troubleshooting articles and step-by-step remediation guides. The general Documentation includes API references but not the curated troubleshooting content."
  },
  {
    "id": "4e7f97c4ea4cd9ca3ee42726b0d8ba6b14c42769971438ef4326ed72888137a3",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A cloud architect is designing a fault-tolerant multi-Region application and needs AWS\u2019s recommended architectural patterns. In which resource will they find the authoritative Well-Architected guidance?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Blogs",
      "B": "AWS Whitepapers & Guides",
      "C": "AWS Solutions Library",
      "D": "AWS Architecture Center videos"
    },
    "explanation": "The Well-Architected Framework whitepaper resides in AWS Whitepapers & Guides. Blogs and videos provide supplemental content but aren\u2019t the formal framework document."
  },
  {
    "id": "3319ea77346b053ef96576df4e30e85351b3ece861249a6c89ed57f37daf338f",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.3",
    "stem": "An enterprise IT lead needs to find AWS Consulting Partners with expertise in Well-Architected reviews in the APN. Which AWS resource helps them locate qualified partners?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Marketplace",
      "B": "AWS Support Center",
      "C": "AWS Partner Solutions Finder",
      "D": "AWS Documentation"
    },
    "explanation": "The AWS Partner Solutions Finder (in the APN portal) lets customers search for Consulting Partners by competency, including Well-Architected reviews. Marketplace is for software listing, not consulting."
  },
  {
    "id": "31625629cae360887f3bc276b2aa7cb7d6021e8f7f2d670454652881c1847021",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A software vendor wants to offer its SaaS web-application firewall through AWS so that customers can procure and deploy it directly. Which AWS channel should they use?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Partner Network (APN) portal",
      "B": "AWS Marketplace",
      "C": "AWS Service Catalog",
      "D": "AWS CodePipeline"
    },
    "explanation": "AWS Marketplace is the storefront where customers procure third-party software and SaaS offerings. APN portal is for partner management, Service Catalog is for internal provisioning, and CodePipeline is a CI/CD service."
  },
  {
    "id": "0c407c48dbc83ca240cf124c86f84f552aa494f2f734c48442ae2fdaac6b0d74",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "An engineer needs the ability to open and track support cases from an iOS device with push notifications for updates. Which AWS resource should they deploy?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Support Center web dashboard",
      "B": "AWS Chatbot integrated with Amazon Chime",
      "C": "AWS Console mobile browser",
      "D": "AWS Support App"
    },
    "explanation": "The AWS Support App (iOS/Android) provides native case management and push notifications. The web dashboard and mobile browser do not support push notifications, and Chatbot only relays messages in chat rooms."
  },
  {
    "id": "0ffcfc32dbb3e4963dd63828e0a8393ec95179ec78adb0c7fa3bf998ded8c3f5",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "A security team wants early notifications of AWS security advisories and vulnerability disclosures (e.g., CVE notices). Which AWS resource should they subscribe to?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Security Bulletins (Security Blog) RSS feed",
      "B": "AWS Shield Advanced alerts",
      "C": "AWS Config compliance notifications",
      "D": "AWS Personal Health Dashboard"
    },
    "explanation": "AWS publishes security advisories via the AWS Security Bulletin on the AWS Security Blog (with RSS). Shield Advanced and Config relate to operational protections, not disclosure bulletins."
  },
  {
    "id": "da406e8e01386ed48186a75328103bd48726e0819a4be91606858a0e4a6328a6",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A development team needs official, open-source AWS SDK code examples across multiple languages to accelerate prototyping. Which resource should they use?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Knowledge Center",
      "B": "AWS Samples GitHub repositories",
      "C": "AWS Support Center code attachments",
      "D": "AWS Marketplace code bundles"
    },
    "explanation": "AWS hosts official code samples in the aws-samples GitHub organization. The Knowledge Center offers troubleshooting articles, not code libraries, and Marketplace isn\u2019t for code samples."
  },
  {
    "id": "81dd3f9501aad7f947dde12afe5039eac69e91b95f358672301bc15b8ea70fb7",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "An operations manager must reference AWS\u2019s official service-level agreements (SLAs) and uptime commitments. Which resource contains this information?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Service Level Agreements page in AWS Documentation",
      "B": "AWS Artifact",
      "C": "AWS Trusted Advisor",
      "D": "AWS Pricing Calculator"
    },
    "explanation": "The AWS SLAs page in the official AWS Documentation lists uptime commitments per service. Artifact provides compliance reports, Trusted Advisor offers operational checks, and the Pricing Calculator estimates costs."
  },
  {
    "id": "61edeb58fc963452724d4ee2a7e6530caf8bbcfa6c27314c253be1c78307d45e",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A partner company is evaluating the benefits and requirements of upgrading from APN Registered Tier to Advanced Tier. Where should they look to find detailed program guidelines and benefits?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Marketplace partner section",
      "B": "AWS Training and Certification portal",
      "C": "APN Partner Central on the AWS Partner Network website",
      "D": "AWS Support Center"
    },
    "explanation": "APN Partner Central (within the AWS Partner Network portal) provides detailed information on partner tiers, requirements, and benefits. Marketplace and Support Center are unrelated to APN program specifics."
  },
  {
    "id": "59a537e157a2893a6b637182a4a7614ec6e8f0509678ce20b40ea0a32f3f5aff",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "A financial software vendor needs to deploy a MySQL-compatible database on AWS that allows installation of custom plugins and OS-level performance monitoring agents. The solution must minimize operational overhead while granting root access to the database server. Which deployment option meets these requirements?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon RDS for MySQL with custom parameter groups",
      "B": "Amazon Aurora MySQL with Performance Insights",
      "C": "Self-managed MySQL on an Amazon EC2 instance",
      "D": "Amazon DynamoDB with custom encryption keys"
    },
    "explanation": "Amazon EC2 self-managed MySQL provides full OS and root-level access for installing custom plugins and agents. RDS and Aurora are managed and restrict OS-level access, and DynamoDB is a NoSQL service unsuitable for MySQL."
  },
  {
    "id": "04c9e0e336dcd79a8617e9f5356c42cb7a3b977877559e20ac199a30bfc7c05a",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "An internal audit team requires a relational database to record financial transactions with strong ACID guarantees for a small department. The database does not need to scale massively, and cost predictability is important. Which AWS database service should they select?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon RDS for MySQL Multi-AZ deployment",
      "B": "Amazon Aurora Serverless v1",
      "C": "Amazon DynamoDB",
      "D": "Amazon DocumentDB"
    },
    "explanation": "Amazon RDS Multi-AZ provides a managed relational MySQL instance with strong ACID compliance and predictable pricing. Aurora Serverless is overkill, DynamoDB is NoSQL, and DocumentDB is for MongoDB workloads."
  },
  {
    "id": "e1002d3bd7870beb36721282a11b33ce28e4af8a3e9123fef2bb41cb00054f8f",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A migration project involves moving a heterogeneous on-premises Oracle database schema to Amazon RDS for PostgreSQL. The solution must automate schema and code conversion with minimal manual effort. Which AWS service should they use first?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Database Migration Service (AWS DMS)",
      "B": "AWS Schema Conversion Tool (AWS SCT)",
      "C": "Amazon Aurora MySQL replication",
      "D": "AWS Data Pipeline"
    },
    "explanation": "AWS SCT analyzes and automatically converts database schema and code from one engine to another. DMS migrates data but does not convert schema."
  },
  {
    "id": "918cb147e1ddefa351d0202e4afb95b64ffd646d5377edca33af7bb20ae20963",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "A rapidly growing e-commerce application uses Amazon RDS for its product catalog. The catalog experiences spikes of read traffic during promotions, causing high latency. Which AWS service should you implement to reduce the RDS read load and improve response times?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon ElastiCache for Redis",
      "B": "AWS DataSync",
      "C": "Amazon DynamoDB Accelerator (DAX)",
      "D": "AWS Database Migration Service"
    },
    "explanation": "ElastiCache for Redis provides an in-memory cache to offload reads from RDS. DAX is for DynamoDB acceleration, DataSync is for file transfer, and DMS is for migration."
  },
  {
    "id": "24db237f8cd0fac3ead56010342d371f012ad35c4e03627ee5cab8a3929fa132",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "A social networking platform needs to efficiently execute complex queries to find friends-of-friends relationships across billions of records. Which AWS database service is most appropriate?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Neptune",
      "B": "Amazon DynamoDB",
      "C": "Amazon RDS for MySQL",
      "D": "Amazon DocumentDB"
    },
    "explanation": "Amazon Neptune is a purpose-built graph database with optimized traversal queries. DynamoDB and RDS are not optimized for graph traversals, and DocumentDB is for document stores."
  },
  {
    "id": "ff7f0c64c8936a592a4b53c2da11539d38d07ecb393bc1dfcaeb43740582a764",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "You need to continuously replicate changes from an on-premises MySQL database to Amazon DynamoDB for near real-time analytics. Which AWS service should you use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Database Migration Service with change data capture",
      "B": "AWS DataSync with scheduled transfers",
      "C": "AWS Schema Conversion Tool",
      "D": "Amazon Redshift Spectrum"
    },
    "explanation": "AWS DMS supports ongoing replication (CDC) from MySQL to DynamoDB. DataSync is for file-based transfers, SCT does schema conversion, and Redshift Spectrum queries S3."
  },
  {
    "id": "c8c7ecdb35f4b30e330551c97f002f27722f6d8dfa2bdcdf35957aaa24465497",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "Your development team is building a JSON-based content management system and needs a managed database with MongoDB-compatible APIs and native JSON query capabilities. Which AWS database service should they choose?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon DocumentDB",
      "B": "Amazon RDS for PostgreSQL",
      "C": "Amazon Aurora MySQL",
      "D": "Amazon DynamoDB"
    },
    "explanation": "Amazon DocumentDB provides MongoDB-compatible APIs and native JSON query. RDS and Aurora are relational, and DynamoDB supports JSON but lacks MongoDB API compatibility."
  },
  {
    "id": "d472bd20a54bce453ac111c623e7903e842ca75a37d5cddb035f4c6b65cb39dc",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "An application requires sub-millisecond latency for read and write operations and must offer optional data persistence across node restarts. Which AWS database service best meets these requirements?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon ElastiCache for Redis",
      "B": "Amazon ElastiCache for Memcached",
      "C": "Amazon DynamoDB",
      "D": "Amazon RDS for MariaDB"
    },
    "explanation": "ElastiCache for Redis supports sub-millisecond latency and offers persistence. Memcached is in-memory only without persistence, DynamoDB has higher latency, and RDS is disk-based."
  },
  {
    "id": "578195c520abc91d3b09e1ec29e76291111063754d49fe902ebe10531e23529b",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "A variable workload application needs a relational database that automatically scales compute capacity based on demand without manual provisioning of new instances. Which AWS database offering provides this capability?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Aurora Serverless v1",
      "B": "Amazon RDS with Auto Scaling",
      "C": "Amazon DynamoDB Autoscaling",
      "D": "Amazon Redshift Concurrency Scaling"
    },
    "explanation": "Aurora Serverless v1 automatically adjusts ACUs for relational workloads. RDS requires manual instance changes, DynamoDB is NoSQL, and Redshift is a data warehouse."
  },
  {
    "id": "04cb70fcbbbe454b8192cc9f5bc8d6d4c81241cff1b344f4a8c192a1526543d3",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "An enterprise wants to offload heavy analytical queries from their Amazon Aurora OLTP cluster. They require a managed data warehouse with columnar storage and high query performance on large datasets. Which AWS service should they use?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Redshift",
      "B": "Amazon RDS for PostgreSQL",
      "C": "Amazon Athena",
      "D": "Amazon EMR"
    },
    "explanation": "Redshift is a managed, columnar data warehouse optimized for OLAP queries. RDS is OLTP, Athena queries S3, and EMR is a Hadoop framework requiring more management."
  },
  {
    "id": "6ee5e7b96b9efe87bd426698946e40820d613274dd729b57175924ea10963ee2",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "During migration of a 10 TB on-premises Oracle database to Amazon RDS for Oracle, minimal downtime is required. Which replication strategy with AWS DMS meets this goal?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use one-time full load only",
      "B": "Use ongoing replication only",
      "C": "Perform a full load followed by change data capture (CDC)",
      "D": "Take an EBS snapshot and restore to RDS"
    },
    "explanation": "DMS full load plus CDC migrates existing data then replicates ongoing changes, minimizing downtime. Full load alone stops updates, CDC alone cannot load initial data, and EBS snapshots are for EC2, not RDS."
  },
  {
    "id": "40cfb44e1ba307a7368826a42891fd594015146ee3cb79c3d7af76bc86f3e8a1",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "A global gaming application requires multi-master, active-active writes across AWS Regions for player state stored in a NoSQL database. Which AWS database feature supports this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon DynamoDB global tables",
      "B": "Amazon RDS Multi-AZ deployments",
      "C": "Amazon DocumentDB global clusters",
      "D": "Amazon ElastiCache cross-region replication"
    },
    "explanation": "DynamoDB global tables provide multi-master writes across regions. RDS Multi-AZ is standby only, DocumentDB global clusters are primary-secondary, and ElastiCache supports read-only replicas."
  },
  {
    "id": "7a02f7b95e2884fdbed54f9ec61d4acdfb9185c2ded508b5b2333ce13555f37b",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "A latency-sensitive application uses Amazon DynamoDB for storage but experiences occasional read bottlenecks on hot keys. Which AWS service can accelerate DynamoDB reads with a compatible caching layer?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "DynamoDB Accelerator (DAX)",
      "B": "Amazon ElastiCache for Redis",
      "C": "Amazon RDS Proxy",
      "D": "AWS Global Accelerator"
    },
    "explanation": "DAX is a fully managed, in-memory cache specifically for DynamoDB. ElastiCache is separate and would require custom integration, RDS Proxy is for RDS, and Global Accelerator is network-level."
  },
  {
    "id": "5ccb67e936fdc863014f1fcc528be5691cd491c040e3c3d74586b9e4ed7d7e67",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "A development team needs to rapidly provision multiple isolated copies of a production PostgreSQL database for testing. Which AWS database service feature provides the fastest database cloning capability?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Aurora fast database cloning",
      "B": "Amazon RDS Read Replicas",
      "C": "AWS DMS snapshot replication",
      "D": "Amazon S3-backed export/import"
    },
    "explanation": "Aurora's fast database cloning uses copy-on-write to create near-instant clones. RDS replicas maintain replication, DMS is for migration, and S3 export/import is slower."
  },
  {
    "id": "6edacfcd09c0b6fff045041c6ed9bc4fd7f61a5c1c949083aa332573efb8b49e",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "A bioinformatics team uses Amazon Neptune and needs to choose the appropriate query engine. Their datasets model chemical compounds as vertices and interactions as edges, and they plan to use Gremlin traversals. Which Neptune engine should they use?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Neptune Gremlin engine",
      "B": "Neptune SPARQL engine",
      "C": "Amazon Neptune ML",
      "D": "Amazon Neptune Streams"
    },
    "explanation": "For property graph models and Gremlin traversals, use Neptune's Gremlin engine. The SPARQL engine is for RDF triples, Neptune ML is for ML tasks, and Streams is for change streams."
  },
  {
    "id": "486025a18d088fe0bf5d7fdb1b5f44b256111e80b8c52cf6da2c87904391f666",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "Your organization uses AWS Organizations with a management account and three member accounts. The finance team needs a single monthly budget of $10,000 for the total organization, and also wants individual alerts when any member account reaches 50% of its share (i.e., $3,333). Which approach meets these requirements with the least operational overhead?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "In the management account, create one AWS Organizations cost budget scoped to all member accounts for $10,000, and three separate cost budgets scoped to each member account for $3,333, each with its own email alert.",
      "B": "In each member account, create a cost budget for $10,000 and set a 50% threshold alert; the alerts will automatically cascade to the management account.",
      "C": "In the management account, create a single cost budget for $10,000 and set two alerts: one at 50% for the overall spend and another at 50% for each member account.",
      "D": "Enable AWS CloudWatch EstimatedCharges metrics in each account and configure a 50% threshold alarm for $3,333; use an SNS topic to notify finance."
    },
    "explanation": "Option A uses AWS Organizations budgets to cover the entire organization and individual member budgets for per-account thresholds. B is incorrect because budgets in member accounts cannot cover the entire organization. C is invalid: a single budget cannot have separate thresholds per account. D is suboptimal and requires per-account CloudWatch setup and does not provide consolidated budgeting in AWS Budgets."
  },
  {
    "id": "28cafa9db109c43fd05329dce25d3da3923aa6b460244f4176a7cf13e927d06b",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.8",
    "stem": "A team wants to estimate the monthly cost impact of deploying 20 m5.large EC2 instances in us-west-2 for six months, plus 10 TB of S3 Standard storage in the same region, and 100 GB/month of Data Transfer Out to the internet. They also want to compare that to a similar deployment in us-east-1. Which AWS tool should they use and why?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Cost Explorer forecasting feature, because it will project future costs based on the new hypothetical usage profile.",
      "B": "AWS Budgets, by creating a usage budget and adjusting the usage threshold to reflect the new deployment in both regions.",
      "C": "AWS Pricing Calculator, because it allows you to input hypothetical resource quantities, select regions, and compare cost estimates side by side.",
      "D": "AWS Cost and Usage Report, by generating a detailed report and then manipulating it in Athena to simulate the new deployment."
    },
    "explanation": "The AWS Pricing Calculator is specifically designed for what-if cost estimation across regions and services. Cost Explorer forecasts historical trends, not hypothetical deployments. Budgets monitor actual or forecasted costs but don\u2019t produce what-if estimates. The Cost and Usage Report provides raw data but does not simulate future deployments."
  },
  {
    "id": "ff90c5232893b48e2d18a84afa8eb08046a877eeca23e76169067bb98d87dc71",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "Your security team has tagged all resources with CostCenter, Environment, and Department tags. The finance department needs to break down monthly costs by Department in Cost Explorer and Budgets. What steps must you perform to make these tags available for cost reporting?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "In each AWS account, define tag policies via AWS Organizations so only Department tags are allowed, then group by tag in Cost Explorer.",
      "B": "Enable AWS Config rules to aggregate tags centrally, then use AWS Budgets with tag filters for Department.",
      "C": "Create Resource Groups for each Department tag value, and AWS Budgets will automatically pick up those Resource Groups.",
      "D": "In the Billing console, activate the CostCenter, Environment, and Department tags as cost allocation tags, wait for AWS to process them, then use Cost Explorer and AWS Budgets to filter and group by the Department tag."
    },
    "explanation": "After tagging resources, you must activate tags in the Billing console as cost allocation tags. AWS then includes them in Cost Explorer and Budgets. Tag policies, Resource Groups, or AWS Config do not activate tags for cost allocation."
  },
  {
    "id": "dd0a4dfd65c867ed3aed4c631e54ff39f3c0e795569f958cec6c76204613e34e",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "Your finance team requests a monthly report of actual and forecasted costs by account and by service for a consolidated billing setup. Which AWS service and configuration will satisfy this with minimal custom tooling?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Budgets to create multiple budgets per service and per account, then export the alerts CSV each month.",
      "B": "Use AWS Cost Explorer, group costs by Linked Account and by Service, and enable forecasts for each grouping.",
      "C": "Enable AWS Cost and Usage Reports to S3, process them with AWS Glue and Athena, and build a custom dashboard.",
      "D": "Use the AWS Pricing Calculator, input historical usage data per account and service, and generate a monthly report."
    },
    "explanation": "Cost Explorer supports grouping by Linked Account and Service and includes forecasts. Budgets cannot report across multiple dimensions easily. Cost and Usage Reports require significant ETL. Pricing Calculator cannot report on historical or forecasted actual accounts."
  },
  {
    "id": "db3df5b441a7c2933f079b941ee1c5aad1caebd60f049b2eae1bcbe7b1727cf3",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "A development team accidentally launched several high-cost On-Demand instances. To automatically alert finance and dev leads when their EC2 spend in any account exceeds $2,000 in a month, which approach is the most reliable?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create an AWS Cost budget in each affected account scoped to EC2 service costs with a $2,000 actual cost threshold and subscribe both email addresses.",
      "B": "Use CloudWatch billing metric alarms on EstimatedCharges in each account and add dev leads to the alarm SNS topic.",
      "C": "In the management account, create a usage budget scoped to EC2 usage quantity with a threshold equivalent to $2,000.",
      "D": "Use AWS Pricing Calculator to forecast EC2 costs daily and send notifications when estimates project $2,000."
    },
    "explanation": "A Cost budget scoped to EC2 actual costs delivers precise notifications at $2,000. CloudWatch billing alarms are only for total spend and have a latency. Usage budgets track quantity, not cost. Pricing Calculator is manual and not for dynamic alerts."
  },
  {
    "id": "e31c77a0200899b92ce679a00b66b008b6de8bb79289e9b7fcc33b9467a0305e",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "Your company purchased several Savings Plans and wants to include the Savings Plan amortized fees in their budget calculations to see total effective net cost. Which budget configuration should you choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create an Unblended cost budget, because it shows the list price without RI or Savings Plan discounts.",
      "B": "Create a Blended cost budget, because it mixes RI upfront and hourly rates across accounts.",
      "C": "Create an Amortized cost budget, because it spreads the upfront and recurring Savings Plan costs over the usage period.",
      "D": "Create a usage budget for normalized usage quantity, since Savings Plans apply automatically to usage."
    },
    "explanation": "An Amortized cost budget distributes upfront and recurring Savings Plan fees evenly over the period, showing the true net cost. Unblended ignores amortization, Blended mixes purchase types but doesn\u2019t amortize upfront fees, usage budgets track quantities only."
  },
  {
    "id": "a7983b471208e1189b9bea8b8b4eaec10b81e99e4828a21070e139af3c6697a5",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A finance lead wants to be notified not only when actual monthly spend hits 80% of a $5,000 budget, but also when the forecasted month-end spend is projected to exceed 80%. What settings are required in AWS Budgets to achieve both notifications?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set a single actual cost threshold at 80% with two notifications: one for Actual spend and one for Forecasted spend.",
      "B": "Create two separate cost budget notifications: one with Threshold Type = Actual and Threshold = 80%, another with Threshold Type = Forecasted and Threshold = 80%.",
      "C": "Create one notification with Threshold Type = Actual, Threshold = 80%, and enable \"Forecasted alert\" in the same notification.",
      "D": "Use AWS Cost Explorer alert on forecasted spend and AWS Budgets alert on actual spend separately."
    },
    "explanation": "AWS Budgets requires separate notifications for Actual vs Forecasted threshold types. You cannot mix threshold types in a single notification. Cost Explorer cannot send alert emails directly."
  },
  {
    "id": "58f4d3c981cb21139fcd8cdb3332fcfc8d13b44ac2365c07f0c7f4a25af26a80",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "You have a cost allocation tag called ProjectCode. You enabled it in the Billing console but see no tag data in Cost Explorer. What is the most likely reason?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Cost Explorer only shows AWS-generated tags; ProjectCode must be an AWS reserved tag.",
      "B": "AWS Budgets must be configured before activated tags appear in Cost Explorer.",
      "C": "You must delete and re-create the tag on each resource to force it to appear.",
      "D": "It can take up to 24 hours after activation before Cost Explorer includes the tag in reports."
    },
    "explanation": "After activating a user-defined tag as a cost allocation tag, AWS processes it and it appears in Cost Explorer within 24 to 48 hours. There are no requirements to use AWS-generated tags or re-create tags for Cost Explorer."
  },
  {
    "id": "4fbb7003932a681a4957481500238bdd023f29d7f156d9107de4566b4ee5de89",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "An analyst needs to segment costs by application owner across four AWS accounts. The team already tags resources with Owner and Team tags. To avoid waiting for tag activation in Billing console, which feature allows grouping costs immediately based on those tags?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Create separate AWS Budgets for each owner, scoped by tag in each account.",
      "B": "Use Cost Categories in AWS Cost Management with rules mapping Owner tag values to custom categories, then group by Cost Category in Cost Explorer.",
      "C": "Use AWS Config to aggregate tag data and customize the Cost and Usage Report to group by Owner tag.",
      "D": "Use consolidated billing and apply Service Control Policies (SCPs) to prevent untagged resources."
    },
    "explanation": "Cost Categories can map tag values to custom categories immediately once tags exist, without waiting for cost allocation processing. Budgets still require activated cost allocation tags. AWS Config and CUR require processing, SCPs do not group costs."
  },
  {
    "id": "a71c46634a286976fe42ac571f3b58c9bfcf0762e71f7d28e4127531a8c365ea",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "Your team plans to automate a monthly cost forecast by calling an AWS API. They want both a 3-month historical view and a projection for the next month. Which API and sequence should they use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Call AWS Pricing API to retrieve historical pricing and use local code to extrapolate next month.",
      "B": "Use AWS Budgets API GetBudgetPerformanceHistory for historical data and AWS Pricing Calculator API for forecasts.",
      "C": "Use AWS CostExplorer GetCostAndUsage for the last three months and CostExplorer GetTags for next month forecast.",
      "D": "Use AWS CostExplorer GetCostAndUsage for 3-month historical data and AWS CostExplorer GetCostForecast for a 1-month forecast."
    },
    "explanation": "The AWS Cost Explorer API provides GetCostAndUsage for historical data and GetCostForecast for projecting future costs. Pricing API and Budgets API do not provide these combined capabilities."
  },
  {
    "id": "063c7f57c38bb427866ac268d3b10d0a0e3ceee4928209fe6d5bcf285c3fc371",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A vice president asks you to allocate the $3,000 monthly AWS Support plan fee to four business units. The plan is purchased at the management account level. Which approach will yield an accurate chargeback?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Apply a cost allocation tag to the Support plan resource and group by tag in Cost Explorer.",
      "B": "Divide the support fee equally and create four fixed cost budgets of $750 each in the management account to track allocation.",
      "C": "Use Reserved Instance purchase cost allocation to spread the support fee proportionally based on compute usage.",
      "D": "Enable Cost and Usage Reports and use Athena to filter the AWS Support line item by linked account."
    },
    "explanation": "AWS Support fees are charged at the management account and cannot be tagged or attributed to linked accounts. Creating fixed cost budgets of $750 each provides a manual chargeback model. The Support line item in CUR is only in the payer account, so it cannot be filtered by linked account."
  },
  {
    "id": "e5922bd7a7be34a2f0b34612d0f77265317a62d189b1afd518f71e793da04e0b",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A startup wants to ensure they rarely exceed their monthly free tier limits. They need both usage and cost notifications. Which combination of AWS resources should they configure?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Create a usage budget for free tier service hours and a cost budget for $0 threshold; subscribe to both budget alerts.",
      "B": "Use AWS CloudWatch billing metrics for free tier usage and AWS Budgets for cost alerts at $1.",
      "C": "Create two AWS Budgets: one usage budget scoped to the free tier service quotas and one cost budget with a $0.01 threshold; set alerts for both.",
      "D": "Enable AWS Cost Anomaly Detection for free tier usage patterns and AWS Pricing Calculator to forecast when free tier will be exhausted."
    },
    "explanation": "Budgets can monitor both usage and cost: a usage budget for service hours and a cost budget for near-zero spend. CloudWatch billing metrics do not track free tier usage. Cost Anomaly Detection is for anomalies, not free tier limits."
  },
  {
    "id": "0f1ab7cfcb17ace278f1b373ac73bf9e437cbf7d20824201690e3a4a8d867ff0",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "Your team is evaluating three what-if scenarios for data transfer costs between multiple VPCs in different regions. They need line-item estimates for each region pair. Which resource is best suited for this scenario?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Pricing Calculator, creating three separate estimates with inter-Region data transfer line items.",
      "B": "AWS Cost Explorer custom report with historical inter-Region transfer costs extrapolated.",
      "C": "AWS Budgets usage forecast for data transfer with region-based filters.",
      "D": "AWS Cost and Usage Report with tagging of data transfer metrics and Athena queries."
    },
    "explanation": "The AWS Pricing Calculator allows you to explicitly define inter-Region data transfer line items per scenario. Cost Explorer cannot simulate hypothetical inter-Region transfers. Budgets monitor actual/forecasted costs. CUR requires complex ETL and does not simulate."
  },
  {
    "id": "5383a51499c4c97d8385339af6bd5fcc439e1286bda341b0f87200dea887fd63",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "A manager wants to automatically reallocate unused Reserved Instance (RI) discounts to a new project tag after launching new resources. Which resource should you configure to facilitate this in your cost management processes?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create an AWS Budget with a filter for unused RI discounts and tag the budget with the new project code.",
      "B": "Use AWS Pricing Calculator to manually apply RI discounts to the new project estimate.",
      "C": "Activate a cost allocation tag for RIs and assign it to the new resources; Cost Explorer will automatically reallocate.",
      "D": "Use Cost Categories with rules that map RI unused amounts to the project\u2019s cost category based on tags, and view the allocation in Cost Explorer."
    },
    "explanation": "Cost Categories can redistribute costs, including unused RI discounts, based on tag-based rules. Budgets and Pricing Calculator do not redistribute existing unused discounts. Cost allocation tags cannot reallocate dynamic unused RI discounts automatically."
  },
  {
    "id": "a07f1a80fa1eeab82d5678dcafc6d1dd5a4746c655a7df6259c04dc35f422df1",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "Your security policy requires that only specific IAM roles can access billing data across the organization. Billing data resides in the management account. Which action must you take to grant member account roles access to AWS Budgets and Cost Explorer?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Enable IAM Access Analyzer in each member account to generate cross-account policies.",
      "B": "In the management account, attach an IAM policy to the member roles granting aws-portal and cost-explorer permissions, and in AWS Organizations enable \"IAM Access to Billing Information\" for member accounts.",
      "C": "Create a Service Control Policy in Organizations that permits billing actions on all member accounts.",
      "D": "Use AWS Single Sign-On to map member account roles to the management account\u2019s BillingReadOnlyAccess managed policy."
    },
    "explanation": "Billing data is only accessible from the payer (management) account when you enable \"IAM Access to Billing Information\" in Organizations. You then attach policies to roles there. Access Analyzer, SCPs, or SSO mappings alone do not grant billing console/API access."
  },
  {
    "id": "83c89565d7382b26295c35243ce824d94e6bda8e804472eaa7088f342650f962",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.7",
    "stem": "Your company\u2019s web application is deployed in two AWS Regions (us-east-1 and eu-west-1), each with two Availability Zones. Despite using Route 53 latency-based routing, users in Asia continue to experience average latencies of 180 ms. Which architectural change will most effectively reduce latency for Asian users without provisioning additional Regions?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy an Amazon CloudFront distribution in front of your application to cache static content at edge locations closest to Asian users.",
      "B": "Add a third Region in ap-southeast-1 and configure active-active routing to that Region.",
      "C": "Increase the number of Availability Zones in the existing Regions from two to four to improve global reach.",
      "D": "Implement inter-region VPC peering between us-east-1 and eu-west-1 to optimize network path for Asian traffic."
    },
    "explanation": "CloudFront uses edge locations globally to cache content, reducing latency for Asian users without adding new Regions. Adding a new Region adds operational overhead and doesn\u2019t leverage existing edge caching. Expanding AZ count in existing Regions does not bring compute closer to Asian users. VPC peering optimizes private traffic between Regions but doesn\u2019t serve end users from nearer locations."
  },
  {
    "id": "68a779c5f6f486e73cdd7c4f5ab6dadbcae6114d07ca4c48881f74df6b37cf4c",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "A financial application uses Amazon RDS with Multi-AZ in a single Region. You notice write latency spikes during peak hours. Which characteristic of AWS\u2019s global infrastructure is most likely contributing to this latency?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Synchronous replication between Availability Zones occurs over physical distances within a Region, introducing inherent latency.",
      "B": "Traffic encryption between AZs uses CPU-intensive algorithms, causing write delays.",
      "C": "Data is replicated over the public Internet between AZs, resulting in unpredictable latency.",
      "D": "AZ-to-AZ traffic is throttled by AWS to enforce isolation, causing latency spikes."
    },
    "explanation": "Multi-AZ RDS uses synchronous replication over AWS\u2019s private fiber network across geographically separated AZs, so physical distance contributes to latency. Encryption overhead is minimal compared to network distance. Replication does not traverse the public Internet, and AWS does not throttle intra-Region AZ traffic for isolation."
  },
  {
    "id": "fa6af0f49165c6f5f9d866895d85e9f1dda486c0fc220bd1545bc187a11a447a",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "Your text-processing service requires 99.99% availability. Each Availability Zone in your primary Region has a historical availability of 99.5%, and you plan an active-passive deployment across two Regions, each with two AZs. Which deployment meets the availability target with the lowest operational cost?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Active-active across both Regions, each with two AZs (load balanced across all four AZs).",
      "B": "Active-passive with two AZs in primary Region and two AZs in secondary Region kept warm.",
      "C": "Active-active with two AZs only in the primary Region; configure automatic failover to secondary Region with two AZs if both AZs in primary fail.",
      "D": "Active-active with one AZ in each Region and Route 53 failover routing."
    },
    "explanation": "Option C yields availability: primary Region two-AZ active-active = 1\u2013(0.005\u00b2)=99.9975%. Secondary Region only activated if both primary AZs fail, so overall meets 99.99% with lower cost than continuously running in both Regions. A is more expensive. B keeps secondary warm and costs more. D with one AZ per Region yields 0.995\u00b2=99.00% insufficient."
  },
  {
    "id": "39a46ce9c6936375842a6f2f34571a195190e1af749d364cc257af327c10811b",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.2",
    "stem": "You must meet a regulation that requires customer data at rest to reside within the EU. Your application is globally distributed. Which AWS global infrastructure feature helps you meet this requirement while still serving global traffic?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon CloudFront to serve cached content from edge locations worldwide.",
      "B": "Store data in an Amazon S3 bucket in eu-central-1 and configure global access via CloudFront with origin access identity.",
      "C": "Configure Route 53 to geo-DNS route non-EU traffic to a bucket in us-east-1.",
      "D": "Enable cross-Region replication for your S3 bucket to replicate data to us-east-1 for improved durability."
    },
    "explanation": "Keeping the S3 bucket and its data only in eu-central-1 ensures compliance, while CloudFront edge locations can still serve content globally. Geo-DNS alone doesn\u2019t cache content, and replicating to us-east-1 violates the EU data-residency requirement."
  },
  {
    "id": "c49a624f1ad7a77170d0e3b00201d4a891b77b2b1860b74868e87cbf9e20a5c9",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.3",
    "stem": "An IoT analytics pipeline ingests sensor data in near real time from devices across Latin America. You need the lowest latency ingestion point without provisioning full Regions. Which combination of AWS global infrastructure is most appropriate?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy an endpoint in a single Region in us-east-1 and configure VPC peering from on-premises gateways.",
      "B": "Use AWS Snowball Edge devices at each site to batch-upload data hourly to the nearest Region.",
      "C": "Create custom edge locations using AWS Outposts in each country.",
      "D": "Configure AWS IoT Core with regional edge endpoint placements (edge locations) to route data to the nearest AWS edge location before forwarding to a central Region."
    },
    "explanation": "AWS IoT Core edge endpoints leverage AWS\u2019s managed edge locations to minimize ingestion latency without full Region deployments. Snowball Edge is for offline batch, Outposts require hardware racks and is overkill, and VPC peering doesn\u2019t affect sensor-to-cloud latency."
  },
  {
    "id": "726db40d7da2c8c6bdbbc3e11d1c7aedaa3d5866b9147352bea03263176cc9e0",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.7",
    "stem": "Your e-commerce application uses Amazon CloudFront for static assets and Route 53 latency-based routing for dynamic APIs hosted in two Regions. You notice stale content served from an edge location after an API schema update. What explains this behavior?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Edge locations automatically purge cache on origin schema changes, but propagation can take hours.",
      "B": "Latency-based routing caches DNS records at edge locations, causing schema mismatch.",
      "C": "CloudFront edge caches static content based on TTL; without invalidation, updated assets won\u2019t reflect until TTL expiry.",
      "D": "Route 53 health checks fail after schema change, forcing fallback to old edge caches."
    },
    "explanation": "CloudFront caches static assets at edge locations until their TTL expires or an invalidation is issued. Route 53 routing and health checks do not impact CloudFront cache behavior."
  },
  {
    "id": "fbf981e841c8897de33655dddd1b940f0f4cf311a20e674b7a4f61a0d9b67e2e",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.2",
    "stem": "A streaming media platform must deliver live video content with minimal global latency. Which AWS global infrastructure design yields the lowest end-user latency?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Host the live origin in a single Region with two AZs and stream directly to users.",
      "B": "Use Amazon CloudFront with an origin in one Region, leveraging edge locations for live streaming.",
      "C": "Deploy media origin servers in each Region without edge caching.",
      "D": "Configure Route 53 geolocation routing to send users to regional origins only."
    },
    "explanation": "For live streaming, CloudFront edge locations ingest and deliver content with lower latency than direct routing to regional origins. Multiple regional origins without edge caching provide higher latency, and geolocation routing still requires streaming over long paths."
  },
  {
    "id": "f6820ddb63f553a14b744e902091f9371e70d4c502197c791038e1b71ed6c406",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "You architected an application across three AZs in a single Region for high availability but observed correlated failures when a nearby power substation tripped. Which AWS global infrastructure concept addresses this risk?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Availability Zones are physically isolated data centers; you should deploy across multiple AZs to mitigate correlated failures but also across Regions when necessary.",
      "B": "Edge locations provide compute failover to handle power outages in AZs.",
      "C": "Regions automatically redistribute resources if one AZ loses power.",
      "D": "Using Direct Connect resilient connections prevents AZ power failures."
    },
    "explanation": "AZs are physically separate, but a Region-level event (like a nearby power grid issue) can impact multiple AZs. To mitigate Region-wide risks, you must deploy across multiple Regions. Edge locations and networking services do not replace AZ/Region redundancy."
  },
  {
    "id": "5c5cdcd7d522745916015e7bbd17a95a25cf62851959e8225f7a51bebb146ccf",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "Your global DNS failover uses Route 53 health checks across two Regions. You notice that edge locations cached the DNS answers, delaying failover by up to 10 minutes. How can you minimize this delay?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Enable DNSSEC on the hosted zone to force edge locations to revalidate records.",
      "B": "Increase the number of Availability Zones in each Region to distribute health checks.",
      "C": "Use a shorter health check interval in Route 53 to detect failure faster.",
      "D": "Lower the TTL of the DNS records so edge locations refresh cached DNS answers more frequently."
    },
    "explanation": "Lowering the TTL ensures edge locations expire DNS records sooner and fetch updated answers quickly. DNSSEC does not control caching TTL. Adding AZs doesn\u2019t affect DNS caching, and shorter health check intervals do not override DNS TTL on edge caches."
  },
  {
    "id": "033b0a4ce85fe644a4e153f5683b92f614c9b919862389093cfd101c900def31",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A serverless API is deployed in one Region but must handle global requests with low latency. You consider Lambda@Edge. Which constraint must you account for regarding AWS global infrastructure?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Lambda@Edge requires you to specify a second Region for standby replication.",
      "B": "Lambda@Edge functions replicate automatically to CloudFront edge locations but cannot access VPC resources in the home Region.",
      "C": "Lambda@Edge can only run in the Region where the CloudFront origin is located.",
      "D": "Lambda@Edge functions have unrestricted access to any AZ in all Regions."
    },
    "explanation": "Lambda@Edge functions run in CloudFront edge locations and can\u2019t directly access resources inside a VPC in the origin Region. You must use public endpoints or AWS PrivateLink. They replicate automatically to edges; there\u2019s no need to specify Regions."
  },
  {
    "id": "6d5ee9ceee911d735c83d22d24019609b5ba6c382292931ba7a6119ebae19b28",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "Your cross-Region disaster recovery plan uses asynchronous replication between two Regions. Which trade-off is introduced by choosing asynchronous over synchronous replication?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Lower replication consistency, leading to a higher RPO but reduced write latency impact.",
      "B": "Increased cross-AZ latency due to synchronous writes between AZs.",
      "C": "Increased cost from the higher bandwidth consumption of asynchronous replication.",
      "D": "Risk of split-brain scenarios when using asynchronous between AZs."
    },
    "explanation": "Asynchronous cross-Region replication decouples write operations, reducing latency impact but increasing RPO risk. Synchronous replication across AZs would add latency within a Region. Asynchronous uses similar bandwidth and does not cause split-brain."
  },
  {
    "id": "0232ee6142715749e67e4d996158f441bbe1408335d00593adfc6cbf79ee492d",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "An application uses edge locations for content caching but must execute custom logic on requests without adding latency for end users. Which AWS global infrastructure service supports this requirement?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS WAF deployed at each Availability Zone.",
      "B": "AWS Lambda in the origin Region triggered by API Gateway.",
      "C": "Lambda@Edge deployed to CloudFront edge locations for request/response manipulation.",
      "D": "AWS Outposts deployed at each edge location."
    },
    "explanation": "Lambda@Edge runs your code at CloudFront edge locations, minimizing additional latency. AWS WAF is for security filtering and doesn\u2019t run custom logic. Lambda in origin Region adds round-trip delay, and Outposts are hardware racks, not edge compute."
  },
  {
    "id": "d7528fad39418c334d9a1b07cdc92be9636ba731471e611daf7c6fc0204ced32",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.2",
    "stem": "Your application requires sub-100 ms global read latency for DynamoDB. Which AWS global infrastructure feature enables you to achieve this?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Create a global DynamoDB table in a single Region and rely on AZ replication.",
      "B": "Use DynamoDB Global Tables to replicate data across multiple Regions and serve reads locally.",
      "C": "Use DAX in one Region to cache reads and distribute responses via edge locations.",
      "D": "Enable cross-Region read replicas for DynamoDB Streams."
    },
    "explanation": "DynamoDB Global Tables replicate data automatically across specified Regions, allowing local reads with low latency. AZ replication within one Region doesn\u2019t serve global users. DAX is a regional cache and doesn\u2019t extend to edges, and DynamoDB Streams read replicas are not designed for cross-Region low-latency reads."
  },
  {
    "id": "64dc9c6f4079c597cc55786ed3a7f55a2d6b266dd84c3aa5ee820242dc39b47f",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A compliance audit requires that your API endpoints used in Singapore (ap-southeast-1) remain available even if that entire Region fails. You cannot host in multiple Regions. Which AWS global infrastructure feature provides a compliant failover option?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Deploy a second copy of the API behind a CloudFront regional edge cache in ap-southeast-1 with origin fallback to another Region.",
      "B": "Use an Outposts rack in ap-southeast-1 to serve traffic when the Region fails.",
      "C": "Configure an additional Availability Zone in ap-southeast-1 for active-passive failover.",
      "D": "Install a VPN between Singapore datacenter and AWS to serve requests during Region outage."
    },
    "explanation": "CloudFront regional edge caches can serve from cache if the origin Region is unavailable, satisfying availability while keeping data in ap-southeast-1. Outposts require AWS managed hardware and won\u2019t operate if the parent Region is down. You cannot add AZs arbitrarily, and a VPN to on-premises doesn\u2019t meet the AWS global infrastructure model."
  },
  {
    "id": "a799826defeadb89f7c30c231d2e0b28b164aa8a6fc06279917095249db4779c",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "A startup runs 2,000 instance-hours per month of t3.medium Linux instances across multiple AZs. They anticipate switching to t3.large or another family within the same region to handle growing load. Which option provides the deepest cost savings while preserving maximum instance-type flexibility?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Purchase a 1-year Standard Reserved Instance for t3.medium Linux.",
      "B": "Purchase a 1-year Convertible Reserved Instance for t3.medium Linux.",
      "C": "Purchase a 1-year Scheduled Reserved Instance for t3.medium Linux.",
      "D": "Commit to a 1-year Compute Savings Plan covering their monthly compute spend."
    },
    "explanation": "A Compute Savings Plan applies to any EC2 instance family, size, and OS in the region, giving the deepest savings with full flexibility. Standard and Scheduled RIs lock you to specific instance families or schedules, and Convertible RIs offer less discount."
  },
  {
    "id": "fe8993597ded7db6f8bc42e00de2c69ffb9f8053979945063c727b940ab33ac3",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "A research lab needs to run 5,000 CPU-hours per year for batch simulations. The jobs are delay-tolerant and can be interrupted. Which purchasing option yields the lowest overall compute cost?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "On-Demand instances for all 5,000 CPU-hours.",
      "B": "Spot Instances for all CPU-hours, allowing interruptions.",
      "C": "1-year Standard Reserved Instances for a baseline of 5,000 CPU-hours.",
      "D": "1-year Convertible Reserved Instances for 5,000 CPU-hours."
    },
    "explanation": "Spot Instances can be up to 90% cheaper than On-Demand and are ideal for interruption-tolerant, batch workloads. Reserved Instances and On-Demand incur higher per-hour fees for the same capacity."
  },
  {
    "id": "84e56686829e6e9f3a7c9c43af7f2a86254ab14fa8d69d96de87eb5327c47db5",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "Your company transfers 50 TB of data per month from on-premises to Amazon S3 and pays high Internet data charges. Which solution most effectively reduces monthly data transfer costs?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Order AWS Snowball devices and import data monthly.",
      "B": "Establish an AWS Direct Connect connection and use it for data uploads.",
      "C": "Configure an S3 VPC gateway endpoint for uploads.",
      "D": "Use AWS DataSync over the Internet."
    },
    "explanation": "Direct Connect provides a dedicated link with lower per-GB data transfer rates than Internet. Snowball is for one-time transfers, a VPC gateway endpoint only helps EC2 within a VPC, and DataSync over Internet still incurs Internet egress charges."
  },
  {
    "id": "69b2bd29f6258580e8593ded2635d53c737b2880c5eb6e415d2e76a27c181a1c",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "A backup process replicates 500 GB per day of data between AZs in the same AWS Region. If AWS charges $0.01 per GB for cross-AZ data transfer, what is the approximate monthly cost?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "$150",
      "B": "$1,500",
      "C": "$5",
      "D": "$1,536"
    },
    "explanation": "500 GB/day\u00d730 days=15,000 GB; 15,000 GB\u00d7$0.01/GB=$150 total per month."
  },
  {
    "id": "29027256cc71fda21d990111c2c521b15cac9b877dffce41169979299206c16e",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A research firm stores 100 TB of critical datasets in Amazon S3 accessed <1% per month but requires sub-second retrieval when accessed. Which storage class is most cost-effective?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "S3 Standard-IA",
      "B": "S3 Glacier Instant Retrieval",
      "C": "S3 One Zone-IA",
      "D": "S3 Intelligent-Tiering Frequent Access"
    },
    "explanation": "S3 Glacier Instant Retrieval offers the lowest storage costs for infrequently accessed data while still providing millisecond retrieval. Standard-IA and One Zone-IA cost more per GB for this access pattern, and Intelligent-Tiering adds monitoring fees."
  },
  {
    "id": "330cfcb9a5ac2ff5e71ef0d96022090ef986200cb7ff94145e5f7d411c88c047",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "A development team uses gp2 EBS volumes for Linux EC2 and sees throughput plateau at 160 MiB/s, but needs 250 MiB/s baseline at lower cost. Which volume type should they choose?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "Allocate larger gp2 volumes to gain more burst credits.",
      "B": "Migrate to gp3 volumes and provision 250 MiB/s throughput.",
      "C": "Switch to st1 (throughput-optimized HDD) volumes.",
      "D": "Use standard magnetic (sc1) volumes for low cost."
    },
    "explanation": "gp3 volumes provide a baseline 125 MiB/s throughput (which can be increased by provisioning) independent of size and are cheaper per GiB than gp2. st1 and sc1 are HDD-based and have high latency."
  },
  {
    "id": "1adfaf12dba0d3b983f71f8fc1aedd764811304396c171e4351abff5c6b88ab2",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "Your company archives 24 TB of data quarterly and may need retrieval within 12 hours during audits. Access frequency is <0.05%. Which S3 storage class yields the lowest monthly cost?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "S3 Intelligent-Tiering",
      "B": "S3 Glacier Instant Retrieval",
      "C": "S3 Glacier Flexible Retrieval",
      "D": "S3 Glacier Deep Archive"
    },
    "explanation": "S3 Glacier Deep Archive has the lowest storage cost and can retrieve data within 12 hours. Glacier Flexible Retrieval is more expensive monthly, and Instant Retrieval is designed for millisecond access."
  },
  {
    "id": "b2a8cc8de7203dd6812e50ad558a26f436d361ab27ceb41ac9896ef8a9a2d368",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A shopping platform runs services on ECS Fargate and EC2 instances across various families. They want a 1-year commitment to minimize compute costs across all compute types. Which option should they choose?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "Purchase 1-year Standard Reserved Instances for EC2 only.",
      "B": "Commit to a 1-year Compute Savings Plan.",
      "C": "Purchase 1-year Convertible Reserved Instances for EC2 only.",
      "D": "Purchase 1-year Scheduled Reserved Instances for EC2 only."
    },
    "explanation": "A Compute Savings Plan applies to EC2, Fargate, and Lambda across families and regions, delivering savings for all compute consumption. Reserved Instances apply only to EC2."
  },
  {
    "id": "982fcbbe95bf08a20aa0fdbd19f132cba1e9036bc871bec0aecb503921ba34eb",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "After 6 months of steady m5.large usage, your team anticipates switching to m5.xlarge in the same region. Which Reserved Instance type provides the largest discount while allowing size modification within the family?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "A Regional Standard Reserved Instance for m5.large (instance size flexibility).",
      "B": "A Regional Convertible Reserved Instance.",
      "C": "A Scheduled Reserved Instance.",
      "D": "A Zonal Standard Reserved Instance without size flexibility."
    },
    "explanation": "Regional Standard RIs automatically apply across sizes within the same instance family (size flexibility) and offer deeper discounts than Convertible RIs. Zoned RIs tie you to one AZ but not affect size flexibility."
  },
  {
    "id": "a0be9b12eae172456bec26d2935a177217dbd656ad1644a7005942844375a537",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "A high-traffic video streaming service serves 100 TB/month of data to global users. Egress from S3 costs $0.085/GB, but CloudFront egress is $0.02/GB after the first TB. Which solution most reduces monthly egress costs?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable S3 Transfer Acceleration for faster uploads.",
      "B": "Use Amazon CloudFront with the S3 bucket as a restricted origin.",
      "C": "Deploy a multi-CDN solution across providers.",
      "D": "Use AWS Global Accelerator in front of the S3 bucket."
    },
    "explanation": "CloudFront caches content at edge locations, reducing S3 egress fees to the low CloudFront rate. Transfer Acceleration speeds up uploads but doesn\u2019t reduce downloads, and Global Accelerator is for TCP/UDP traffic acceleration, not cost savings."
  },
  {
    "id": "680e546de861ea46f2d4274358737b7566255694f5184985e789dfd5ec7865a6",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "Your team considers migrating 20 TB/month of on-premises uploads to S3 via AWS Direct Connect (port charge $0.25/hour + $0.02/GB) versus over the public Internet (no charge). Which is more cost-effective monthly?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Direct Connect with a VPN for private uploads.",
      "B": "AWS Direct Connect without VPN for uploads.",
      "C": "Using an S3 VPC endpoint from on-premises.",
      "D": "Uploading directly over the public Internet to S3."
    },
    "explanation": "Public Internet uploads to S3 have no data transfer or port charges. Direct Connect incurs both port-hour and per-GB fees. VPC endpoints only benefit EC2 within VPCs, not on-premises."
  },
  {
    "id": "00e1734fb10a96c923eead23f74149e6900b700d6b3e579ab4494b6a92e2aa2c",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A development pipeline writes data to S3 via EC2 using public Internet gateways, incurring data transfer out charges. Which change eliminates these transfer charges?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Route through a NAT Gateway and maintain the public gateway.",
      "B": "Continue using the Internet Gateway but tag traffic for free tier.",
      "C": "Deploy an Interface VPC Endpoint for S3.",
      "D": "Deploy a Gateway VPC Endpoint for S3."
    },
    "explanation": "A Gateway VPC Endpoint for S3 routes traffic within the AWS network and has no data transfer or gateway charges. Interface endpoints incur hourly and per-GB charges, and NAT gateways still charge for data."
  },
  {
    "id": "05dfa54971c6a138831c2290c84db7a94deeff096f1fd2ebeee29e2d59b60079",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.3",
    "stem": "A CI/CD pipeline uses 500 GB of gp2 EBS volumes each month requiring 2,000 IOPS. gp2 costs $0.10/GiB-month and baseline IOPS scales with size. gp3 costs $0.08/GiB-month with 3,000 baseline IOPS and $0.005 per extra IOPS. Which volume type saves the most monthly?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Stick with gp2 and rely on burst credits.",
      "B": "Migrate to gp3 without provisioning extra IOPS.",
      "C": "Use st1 (throughput-optimized HDD) volumes.",
      "D": "Use sc1 (cold HDD) volumes."
    },
    "explanation": "gp3 at $0.08/GiB\u2010month for 500 GiB costs $40 and includes up to 3,000 IOPS at no extra cost, versus $50 on gp2. HDD options don\u2019t meet IOPS requirements."
  },
  {
    "id": "f3a62950d86975a7d44fa45a72a2421f631bfbc9fb214ecfdd8734a8ab543c05",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A caching system stores curated data in one AZ and can be regenerated if that AZ fails. Data is accessed less than 1% monthly. Which S3 storage class minimizes cost?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "S3 Standard-IA",
      "B": "S3 One Zone-IA",
      "C": "S3 Intelligent-Tiering",
      "D": "S3 Glacier Instant Retrieval"
    },
    "explanation": "S3 One Zone-IA stores data in a single AZ at lower cost than Standard-IA. Since data can be regenerated if the AZ fails and access is infrequent, One Zone-IA is optimal."
  },
  {
    "id": "b43a22dff5662aa09eca2b58ace85b5f237c1317c7d7a51eecc211fb058e0920",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A logging service stores 10 million objects of 10 KB each in S3 and retrievals are <1% monthly. Which storage option is most cost-efficient?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "S3 Intelligent-Tiering",
      "B": "S3 Standard-IA",
      "C": "S3 Glacier Flexible Retrieval",
      "D": "S3 One Zone-IA"
    },
    "explanation": "Intelligent-Tiering charges per-object monitoring fees, which at 10 million objects outweigh storage savings. Standard-IA offers low storage fees without per-object monitoring costs."
  },
  {
    "id": "76f37d6fbfffcf9eff3bbdf657b1ce7708c1912171019da1005f61d5ae1698c6",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.7",
    "stem": "A large data engine on EC2 sends data to S3 via an Internet Gateway, incurring data transfer out fees. Which architecture change removes these fees and minimizes cost?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Add a NAT Gateway for EC2 to reach S3 privately.",
      "B": "Use an Internet Gateway but optimize packets to reduce fees.",
      "C": "Use an Interface VPC Endpoint for S3.",
      "D": "Use a Gateway VPC Endpoint for S3."
    },
    "explanation": "A Gateway VPC Endpoint for S3 routes traffic over the AWS backbone with no data transfer charges. NAT Gateways and Interface Endpoints both incur data or hourly charges."
  },
  {
    "id": "b10fd573fdbcde2e63e2257e5ba32130fc0396b3f62d6b4a43059143df420784",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A company has a monolithic on-premises application that it wants to migrate to AWS as a set of containerized microservices with minimal downtime. Which AWS migration strategy (one of the 6 R\u2019s) is most appropriate for this scenario?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Rehost (lift and shift) to EC2 instances",
      "B": "Replatform (lift, tinker, and shift) to a managed container service",
      "C": "Refactor (rearchitect) into microservices deployed in containers",
      "D": "Repurchase by adopting a commercial SaaS alternative"
    },
    "explanation": "Refactor (rearchitect) is required to break the monolith into microservices for container deployment. Replatform still retains much of the existing architecture, and rehost simply lifts and shifts the monolith. Repurchase replaces the application entirely, not containerizes it."
  },
  {
    "id": "fac8566e3dd95f0bd7e03304c45905a68695a82d51bc722e689ed5b31bde8f0b",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "An organization must migrate a 24x7 on-premises Oracle database to Amazon RDS for Oracle with near-zero downtime. Which AWS service and configuration should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Database Migration Service with a full load only",
      "B": "AWS Server Migration Service replicating the VM image",
      "C": "AWS Database Migration Service using full load plus ongoing change data capture",
      "D": "AWS DataSync to transfer periodic backups to S3"
    },
    "explanation": "DMS with full load + CDC enables ongoing replication for minimal downtime switchover. Full load only incurs downtime, SMS replicates VMs (not optimized for DB), and DataSync transfers files without continuous capture."
  },
  {
    "id": "678f1f7087c69e47bfc0c1a8b4d3f8eb87f3bea4f356b302940b48f546bf5b6a",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A company needs to import 100 TB of on-premises archives into Amazon S3 but has limited network bandwidth and strict transfer time requirements. Which approach best meets these requirements?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Establish AWS Direct Connect and stream uploads",
      "B": "Enable Amazon S3 Transfer Acceleration over the internet",
      "C": "Use AWS Snowball Edge to ship data and import into S3",
      "D": "Configure AWS DataSync to transfer files over the VPN"
    },
    "explanation": "Snowball Edge is optimized for large-scale data transfer when bandwidth is constrained. Direct Connect or Transfer Acceleration still rely on network and DataSync is for incremental sync, not bulk offline transfer."
  },
  {
    "id": "2191fb45a450e8edce93967ea76cfcf04ee7007063670d2af4b3440d39f0f5b1",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "A business running Microsoft Exchange on-premises wants to move to Office 365 with minimal management overhead. Which AWS migration strategy category applies?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Rehost on EC2 Windows instances",
      "B": "Replatform to Amazon WorkMail",
      "C": "Refactor into microservices on AWS Lambda",
      "D": "Repurchase by subscribing to Office 365 SaaS"
    },
    "explanation": "Repurchase involves replacing the existing application with a new SaaS offering (Office 365). Rehost and refactor keep management overhead high; WorkMail is AWS email, but Office 365 is the vendor SaaS choice."
  },
  {
    "id": "237de838ee74913a619b9feda5c82dd7dd58cb948ea70f54e70ab64fec665269",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "An IT manager needs a single place to view progress of multiple heterogeneous server and database migrations across AWS and on-premises. Which AWS service should they use?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Application Discovery Service",
      "B": "AWS Migration Hub",
      "C": "AWS Systems Manager",
      "D": "AWS Control Tower"
    },
    "explanation": "Migration Hub provides a central dashboard for tracking progress of various migration tools and tasks. Discovery Service only collects inventory, Systems Manager manages resources, and Control Tower sets up landing zones."
  },
  {
    "id": "f16248e4c47c14047c8d214818b6d24e02bc4a7b0a48ddbd7a5cedd1b80ab107",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A legacy .NET application requires a Windows environment and cannot be modified. The company wants the fastest path to run it on AWS. Which strategy is recommended?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Rehost by migrating the VM to Amazon EC2 Windows servers",
      "B": "Refactor into Lambda functions",
      "C": "Replatform to Amazon Elastic Beanstalk",
      "D": "Repurchase a new SaaS solution"
    },
    "explanation": "Rehost (lift-and-shift) to EC2 Windows servers preserves the environment with no application change. Elastic Beanstalk still requires some compatibility consideration, refactor requires code change, and repurchase replaces the app."
  },
  {
    "id": "ea56733bf67d7c387b9ca67ebfa046e7afe26386584eea2cf88f54c6047ea952",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A database team plans to upgrade their on-premises SQL Server to a newer version during migration, without rewriting the application. Which 6 R\u2019s strategy does this align with?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Rehost",
      "B": "Replatform",
      "C": "Refactor",
      "D": "Repurchase"
    },
    "explanation": "Replatform (lift, tinker, and shift) allows an operating system or database engine upgrade without major architecture changes. Rehost moves as-is, refactor rewrites, repurchase replaces entirely."
  },
  {
    "id": "5bee0e4af8fa45b39f0af125c9537316b76bbb4ae57bf75179555322add89007",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "Which AWS tool should a company use to build a detailed total cost of ownership (TCO) analysis when comparing existing on-premises costs to AWS operating costs before migration?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Pricing Calculator",
      "B": "AWS Cost Explorer",
      "C": "AWS TCO Calculator",
      "D": "AWS Budgets"
    },
    "explanation": "The AWS TCO Calculator is designed to model on-premises vs AWS costs. Pricing Calculator estimates costs for new AWS deployments, Cost Explorer analyzes actual AWS spend, and Budgets sets alerts."
  },
  {
    "id": "4232eb210496b7616f14a996c170b39b2ab91996c4d97e5c44c885cec0701eff",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "During migration planning, the team needs to understand server utilization patterns and application dependencies in their data center. Which AWS service provides this information?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Migration Hub",
      "B": "AWS Application Discovery Service",
      "C": "AWS Config",
      "D": "AWS DataSync"
    },
    "explanation": "Application Discovery Service collects on-premises server metrics and mappings. Migration Hub tracks migration status, Config records AWS resource config, and DataSync transfers data."
  },
  {
    "id": "3e8ce8a585eb992a1d7f0c77179fcf0182ff475113a45f68e7078b508b161283",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "In the AWS Cloud Adoption Framework (CAF), which perspective focuses on organizational change management, skills, and roles during a migration?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Platform",
      "B": "Governance",
      "C": "People",
      "D": "Security"
    },
    "explanation": "The People perspective addresses staffing, training, organizational change, and skills. Platform covers technology infrastructure, Governance covers policies, and Security covers controls and compliance."
  },
  {
    "id": "1ce7b6ee00dfd3e11f0831e3e0bae31abe9fc082e9a5abf84d4b741d26cd23c0",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "Which AWS CAF perspective addresses policies, risk management, and compliance controls to support migration governance?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Process",
      "B": "Governance",
      "C": "Platform",
      "D": "Security"
    },
    "explanation": "The Governance perspective covers policies, risk management, and compliance. Security focuses on controls implementation, Platform on infrastructure, and Process on workflows and lifecycle."
  },
  {
    "id": "8e2744693848e6edc533660ceaf5315d9bbc6f7b399851df26d4c88a61585160",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A company wants to reduce migration scope by first eliminating unused on-premises servers. Which 6 R\u2019s strategy should they apply?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Retire servers that are no longer needed",
      "B": "Rehost servers in AWS",
      "C": "Replatform critical servers",
      "D": "Repurchase equivalent SaaS services"
    },
    "explanation": "Retire removes decommissioned or underutilized servers before migration, reducing scope. Rehost and replatform move servers to AWS, and repurchase replaces them with SaaS."
  },
  {
    "id": "74f14350910d68544cfa7dd4d7d4a3dd73c38dce5a315550a4b715817e13d16f",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A migration engineer wants to automate the provisioning of AWS infrastructure for repeatable migrations. Which AWS service is best suited for this purpose?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS CloudFormation",
      "B": "AWS CodeDeploy",
      "C": "AWS OpsWorks",
      "D": "AWS Systems Manager"
    },
    "explanation": "CloudFormation uses Infrastructure as Code templates for automated, repeatable provisioning. CodeDeploy deploys application code, OpsWorks manages Chef/Puppet, and Systems Manager automates operations tasks."
  },
  {
    "id": "d30301478f647eaaad56bef790584072561529f76bbf2fe954b7f915a1ed5c17",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "An operations team needs to perform incremental replication of on-premises virtual machines to AWS for migration testing. Which AWS service should they use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Server Migration Service",
      "B": "AWS Database Migration Service",
      "C": "AWS DataSync",
      "D": "AWS Migration Hub"
    },
    "explanation": "Server Migration Service automates incremental replication of VM images for testing and cutover. DMS is for databases, DataSync for files, and Migration Hub tracks migrations."
  },
  {
    "id": "fa52f13142a652fb427f70110b540bf970c6f04d673ccd0cca3d47c2cfbfe73b",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A security requirement mandates that data must be encrypted both in transit and at rest during bulk transfer of on-premises data to AWS. Which solution best meets this requirement?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Direct Connect with MACsec",
      "B": "AWS DataSync over TLS with SSE-S3",
      "C": "AWS Snowball Edge using AWS KMS\u2013managed keys",
      "D": "Amazon S3 Transfer Acceleration with HTTPS"
    },
    "explanation": "Snowball Edge encrypts data at rest and in transit using AWS KMS\u2013managed keys. DataSync over TLS encrypts in transit but SSE-S3 doesn\u2019t use customer KMS keys for at-rest, Direct Connect with MACsec addresses link encryption only, and Transfer Acceleration only secures in transit."
  },
  {
    "id": "52afbe55d1ca03257ccfba54c70e78d38ef7f3929b8fc82fb513718d047a6b63",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "Your organization wants to manage AWS console access for 500 employees in a multi-account AWS Organizations environment using corporate credentials. You need to minimize administrative overhead and avoid duplicating user definitions in each account. Which solution meets these requirements?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS IAM Identity Center integrated with AWS Organizations and configure the corporate identity provider as an external identity source.",
      "B": "Create IAM users in the master account, assign them to permission sets, and enable federation via AWS STS.",
      "C": "In each account, configure SAML federation directly with the corporate identity provider and maintain centralized groups.",
      "D": "Use AWS Directory Service for Microsoft Active Directory in each account, and configure cross-account trusts."
    },
    "explanation": "AWS IAM Identity Center (formerly AWS SSO) integrated with AWS Organizations and your corporate IdP provides centralized management of user definitions and permissions. The other options duplicate configuration or use heavy directory deployments."
  },
  {
    "id": "f47744d210f14bf7685a48537c6186ebef46f5c911cfc3d6ea5065a05b1dd731",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "Which of the following tasks can only be performed by the AWS account root user?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Creating an IAM user with administrative privileges.",
      "B": "Enabling AWS CloudTrail in an AWS Region.",
      "C": "Changing the AWS Support plan for the account.",
      "D": "Attaching a service-linked role to an Amazon EC2 instance."
    },
    "explanation": "Only the root user can change the AWS Support plan (billing operations). The other tasks can be delegated to IAM principals with appropriate permissions."
  },
  {
    "id": "3707a90120f85a0fdb39a0878d30e571d24add899819e0e8ea05e6e5b339cf10",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "A third-party vendor requires programmatic access to your AWS resources in a separate account. To adhere to best practices and defend against the confused deputy problem, you decide to use an IAM role. Which combination of configurations should you implement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create an IAM role in the target account with a trust policy specifying the vendor's AWS account ID, and provide the vendor with the role ARN.",
      "B": "Create an IAM role in the target account with a trust policy that includes the vendor's AWS account ID and a unique external ID, and share the external ID with the vendor.",
      "C": "Create an IAM user in the target account with programmatic access keys and share the keys with the vendor over a secure channel.",
      "D": "Create an IAM role in the vendor's account and assume it from your account by adding your account ID as a trusted principal without external ID."
    },
    "explanation": "Including a unique external ID in the trust policy prevents the confused deputy scenario. Simply trusting the account ID does not provide this protection, and IAM users with long-term keys are less secure."
  },
  {
    "id": "abc783d29a45bcaf4c39cb0aeed6142a281c805514f104ea41b2f9df20ef8833",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.3",
    "stem": "You need to ensure that IAM users must authenticate with MFA before performing any Write operations on S3 buckets. Which IAM policy condition should you include?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "\"Condition\": {\"Bool\": {\"aws:MultiFactorAuthPresent\": \"true\"}}",
      "B": "\"Condition\": {\"Bool\": {\"aws:MultiFactorAuthAge\": \"true\"}}",
      "C": "\"Condition\": {\"Null\": {\"aws:MultiFactorAuthPresent\": \"true\"}}",
      "D": "\"Condition\": {\"Bool\": {\"aws:MultiFactorAuthUsed\": \"true\"}}"
    },
    "explanation": "The aws:MultiFactorAuthPresent condition key must be true to enforce MFA. The other keys are invalid or incorrect."
  },
  {
    "id": "9512aed1901e87b691f4adea92a885632def657ac42a0505e44254baad6c88c0",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "Which AWS service is specifically designed to securely store and manage access keys, passwords, and other credentials with built-in rotation capabilities?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Systems Manager Parameter Store",
      "B": "AWS Secrets Manager",
      "C": "AWS Identity and Access Management (IAM) Credential Report",
      "D": "AWS Key Management Service (KMS)"
    },
    "explanation": "AWS Secrets Manager is built for secure storage and rotation of credentials. Parameter Store can store secrets but lacks native rotation. IAM Credential Report is a reporting tool; KMS is a key-management service."
  },
  {
    "id": "413ce5683a69b102f340b0065eb7fa9fc2820fdf81020286f7873dd8ddbc437d",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "Which combination of methods can be used for IAM user authentication in AWS?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "X.509 certificates, access keys, username/password, federated SAML credentials",
      "B": "SSH key pairs, access keys, AWS CloudHSM certificates, hardware MFA device",
      "C": "Kerberos tickets, AWS Managed Microsoft AD authentication, passwordless email link",
      "D": "X.509 certificates, biometric fingerprints, IAM role assumption"
    },
    "explanation": "IAM supports X.509 certificates, access keys, console username/password, and SAML/OIDC federation. SSH keys and biometrics are not direct IAM authentication methods."
  },
  {
    "id": "0d79e6cb9ac53978ae04247ea1df995944b524c68a1b75f1b0d40574e7217fba",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.3",
    "stem": "A user requires read-only access to all objects under the 'logs/' prefix in an S3 bucket named 'data-bucket'. The current IAM policy grants s3:GetObject on arn:aws:s3:::data-bucket/* but inadvertently allows ListBucket on unrelated buckets due to an overbroad resource. How should you refine the policy to adhere to least privilege?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Specify the GetObject Resource as \"arn:aws:s3:::data-bucket/logs/*\" and add a separate statement for s3:ListBucket on \"arn:aws:s3:::data-bucket\" with a Condition restricting Prefix to \"logs/\".",
      "B": "Change the Resource wildcard to \"arn:aws:s3:::data-bucket/*\" and add a NotResource for other buckets.",
      "C": "Grant s3:ListAllMyBuckets on \"arn:aws:s3:::*\" and s3:GetObject for \"arn:aws:s3:::data-bucket/logs/*\".",
      "D": "Remove the ListBucket permission entirely since GetObject implicitly allows listing."
    },
    "explanation": "To list and read only the 'logs/' prefix, you must scope GetObject to that prefix and scope ListBucket with a prefix condition. The other options are incorrect or overbroad."
  },
  {
    "id": "d8c96071c805bf84448afdfaf62c5c506214ab3d9c1acae824ba1f2b8407b16a",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "You notice that an AWS-managed policy does not include permissions for a new AWS service you require. To extend permissions while adhering to best practices, which approach should you take?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Clone the AWS-managed policy into a new customer-managed policy, then add the required permissions.",
      "B": "Attach an inline policy directly to each IAM user for the new service permissions.",
      "C": "Modify the AWS-managed policy directly to include the new permissions.",
      "D": "Attach the AWS-managed policy and separately attach a second AWS-managed policy with the new permissions."
    },
    "explanation": "AWS-managed policies cannot be modified. Best practice is to create a customer-managed policy based on the AWS-managed one and extend it. Inline policies are harder to manage."
  },
  {
    "id": "567c825de917c65196c68a26c33d385fc4d50102b648f9360bfeabe7bfb85012",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "Which MFA device options are officially supported for IAM user or root account authentication in AWS?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SMS-based One-Time Password and TOTP virtual device.",
      "B": "FIDO2 security key and hardware TOTP device.",
      "C": "Hardware TOTP device and SMS-based One-Time Password.",
      "D": "Biometric fingerprint sensor and FIDO U2F security key."
    },
    "explanation": "AWS supports TOTP virtual devices, hardware TOTP, and FIDO/U2F (FIDO2) security keys. SMS-based OTP and biometrics are not supported."
  },
  {
    "id": "d57b018d6ff3cdaf89bbc370206a846c8e4fc48a90b13dcd7172fa98723b1be1",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "Which AWS credentials are temporary, automatically expire, and are obtained by assuming an IAM role via AWS STS?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "IAM user access keys.",
      "B": "Console username and password.",
      "C": "Session tokens.",
      "D": "X.509 certificates."
    },
    "explanation": "Assuming a role via AWS STS returns temporary security credentials (access key, secret key, and session token). IAM user access keys are long-term; others are not temporary STS credentials."
  },
  {
    "id": "b8f99723444a849e11d9c652c5bb037c2cc1f98f33ef7333697108fb20f9b9e0",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A developer needs to grant Amazon EC2 permission to call AWS Systems Manager on their behalf. They want the simplest solution that follows AWS best practices. Which IAM entity should they use?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Create a custom IAM role, attach an inline policy granting Systems Manager API calls.",
      "B": "Create a service-linked role for EC2 and assign it to the instance profile.",
      "C": "Create an IAM user, store its access keys on the instance.",
      "D": "Use the EC2 default instance profile with administrator access."
    },
    "explanation": "Service-linked roles are pre-defined by AWS for specific services (EC2 calling Systems Manager) and are managed automatically. The other options are less secure or require more management."
  },
  {
    "id": "55c68cbd74e886f6f1da9cd6ae9b0108cf108276708fb69e5fbe1d47934d2cd1",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.3",
    "stem": "You are writing a resource-based policy for an Amazon S3 bucket that grants access to IAM principals. Which principal type cannot be specified directly as a Principal in a resource-based policy?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "IAM role.",
      "B": "IAM user.",
      "C": "IAM group.",
      "D": "AWS account root user."
    },
    "explanation": "Resource-based policies accept AWS accounts, IAM users, IAM roles, federated principals, and services, but IAM groups cannot be principals in resource-based policies."
  },
  {
    "id": "9612acaa04d5f431c28afc0ebeb06623b5ee3587ba6007ecba54f5e82bfafe37",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.3",
    "stem": "Your company needs to provide employees with single sign-on access to multiple AWS accounts in AWS Organizations. They want to avoid writing custom federation code and leverage existing corporate Active Directory identities. Which AWS service provides this capability?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Cognito.",
      "B": "AWS IAM.",
      "C": "AWS IAM Identity Center.",
      "D": "AWS Directory Service for Microsoft Active Directory."
    },
    "explanation": "AWS IAM Identity Center (formerly AWS SSO) integrates with corporate AD and provides SSO to multiple AWS accounts without custom code. Cognito is for application users."
  },
  {
    "id": "585b42546cae8ed694a77592858897d3e109e555273e574f23a4d4a0abab8464",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "An IAM user belongs to two groups: GroupA grants s3:ListBucket permission on a bucket, while GroupB explicitly denies s3:ListBucket for the same bucket. When the user tries to list the bucket, what is the outcome and why?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "The list operation succeeds because both policies are in effect and allows override.",
      "B": "The list operation fails because an explicit deny in any policy overrides allow.",
      "C": "The list operation succeeds because GroupA's allow overrides GroupB deny due to policy attachment order.",
      "D": "The list operation fails because only group policies are evaluated, not user policies."
    },
    "explanation": "In IAM, an explicit deny always overrides any allow, regardless of how policies are attached or their order."
  },
  {
    "id": "04fc32f0c7f5e6cd84565025979c68333dfe14f4965067893faa3f2f163d2063",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "Your AWS account root user has two access keys created years ago that have never been used. To follow security best practices, what should you do next?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Rotate the keys every 90 days to ensure security.",
      "B": "Delete the unused root access keys immediately.",
      "C": "Deactivate the keys temporarily and re-enable if needed.",
      "D": "Store the keys in AWS Secrets Manager with rotation enabled."
    },
    "explanation": "Root access keys represent a high risk if compromised. AWS best practice is to remove (delete) them if unused. They cannot be deactivated or rotated like IAM user keys."
  },
  {
    "id": "df0e97b9e513e6cc9c2f9a96a96c8356ec29e039fa41e122f7d223b5490825bd",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A global retailer needs to provision and maintain a fleet of 200 EC2 instances with consistent network settings and IAM roles. They anticipate frequent updates to the instance configuration. Which deployment approach ensures idempotent, version-controlled, and repeatable provisioning across multiple environments?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Store an AWS CloudFormation template in version control and deploy it via a CI/CD pipeline.",
      "B": "Run AWS CLI commands within custom shell scripts executed manually by administrators.",
      "C": "Configure instances interactively in the AWS Management Console for each update.",
      "D": "Use ad-hoc Boto3 scripts executed from developers\u2019 local machines."
    },
    "explanation": "AWS CloudFormation provides idempotent, declarative templates that are versioned and can be deployed consistently via CI/CD. Manual or ad-hoc approaches lack guaranteed repeatability and version control."
  },
  {
    "id": "3f9c635086ca4a725f5ad43ac21190a2bf21fb0c17ebdcb843081dceb5d6829d",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.1",
    "stem": "A team is performing a one-time migration of several low-priority S3 buckets and does not plan to reuse any scripts or templates. Which provisioning method minimizes setup overhead while meeting the migration requirement?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use the AWS Management Console to create and configure the buckets manually.",
      "B": "Author and deploy an AWS CloudFormation stack.",
      "C": "Develop Python scripts using the AWS SDK and schedule them with AWS Lambda.",
      "D": "Deploy a CloudFormation StackSet across multiple accounts."
    },
    "explanation": "For a one-time, low-priority task with no intent to reuse code, the console offers the least setup overhead. IaC or programmatic solutions introduce unnecessary complexity for a single use."
  },
  {
    "id": "fceb11d7d64a6f3c7d8fc7aef1a8fa1298d53deafc3f0a5c23c445a7c8045dc2",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "Developers need daily programmatic access to AWS services from their workstations without managing long-lived access keys. Which approach is most secure and minimizes credential management overhead?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Configure the AWS CLI with temporary credentials obtained via AWS IAM Identity Center (SSO) that refresh automatically.",
      "B": "Generate long-term IAM access keys for each developer and store them in their ~/.aws/credentials file.",
      "C": "Embed IAM user credentials directly within scripts stored in the internal Git repository.",
      "D": "Have developers manually copy session tokens from the AWS Console each morning."
    },
    "explanation": "IAM Identity Center issues short-lived credentials that rotate automatically and enforce least privilege, avoiding long-lived keys and manual token handling."
  },
  {
    "id": "dfecffa3702b2ae713f5c5b75647410d31300dde8a150773d7de02e278c6ea5d",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A sysops engineer needs to run AWS commands without installing any clients locally and wants persistent home directory storage across sessions. Which option best meets these requirements?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS CloudShell",
      "B": "AWS Systems Manager Session Manager",
      "C": "AWS CLI v2 installed in Docker",
      "D": "AWS SDK for Java with embedded REPL"
    },
    "explanation": "AWS CloudShell is a browser-based shell environment with persistent storage and no local installation. Session Manager requires an agent and does not provide a home directory for AWS commands."
  },
  {
    "id": "7bc1e28b5088569e8a0118e8e1eaa5ec670682cf939e5a966836eb9c0eda5d27",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A company must deploy identical networking, compute, and storage resources in five AWS Regions from a single control plane. Which AWS-native deployment method reduces manual effort and ensures consistency?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use AWS CloudFormation StackSets to deploy a single template across multiple Regions.",
      "B": "Manually configure each Region\u2019s resources via the AWS Management Console.",
      "C": "Write a single Terraform configuration with state stored on-premises.",
      "D": "Execute custom Boto3 scripts sequentially for each Region."
    },
    "explanation": "CloudFormation StackSets enable centralized deployment of identical templates across Regions, ensuring consistency and reducing manual steps."
  },
  {
    "id": "82fba351e6e6f65a3e84cf0d79f0d9208c87fc1dd63aff65f90b61dae03d8f92",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "During a security audit, an engineer needs to perform a one-time inventory of EC2 instance tags across all accounts. They want minimal persistent code changes. Which approach is most efficient?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Run AWS CLI commands within AWS CloudShell to list tags using a simple script.",
      "B": "Deploy a CloudFormation stack that queries and exports instance tags.",
      "C": "Create a Lambda function invoked on a schedule to retrieve and store tags.",
      "D": "Use Systems Manager Automation Documents to gather tag information."
    },
    "explanation": "For a single, ad-hoc inventory, using AWS CLI in CloudShell avoids persistent infrastructure or code, meeting the requirement with minimal overhead."
  },
  {
    "id": "aa6a166c4ad79f74c1ef64cdc44ecd795480ff0488d688c36661c0ee9d8de961",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.1",
    "stem": "An application\u2019s infrastructure requires conditional resource creation and intrinsics (loops, mappings). Which AWS deployment method natively supports these constructs?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS CloudFormation with macros and intrinsic functions",
      "B": "Bash scripts invoking AWS CLI commands",
      "C": "Manual provisioning in the AWS Console",
      "D": "AWS SDK scripts written in Python"
    },
    "explanation": "CloudFormation templates support declarative constructs, intrinsic functions, and macros for conditional logic and looping, which imperative scripts or the console do not natively provide."
  },
  {
    "id": "1fc9caa0984074dbcc0456e1e67ef2b802abe520ef5cfe13e8da4e67f2d31f21",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A team needs to detect and reconcile drift between declared infrastructure and the actual state in AWS. Which method provides native drift detection and remediation?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS CloudFormation Drift Detection",
      "B": "A cron job running AWS CLI describe- commands",
      "C": "Custom AWS SDK scripts comparing template to live state",
      "D": "Manual verification in the AWS Management Console"
    },
    "explanation": "CloudFormation Drift Detection is a built-in feature that identifies when actual resource configurations diverge from the template and supports remediation."
  },
  {
    "id": "83cc351e189aaecb01b06fdb556291840d322cadfb9d37704978dfc375637a49",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A consulting firm must rapidly provision isolated environments for multiple clients on demand, each enforcing corporate standards and guardrails. Which AWS-native deployment method should they use?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Service Catalog with approved CloudFormation portfolios",
      "B": "Direct provisioning in the AWS Console using IAM roles",
      "C": "Standalone Terraform modules executed by client engineers",
      "D": "Ad-hoc AWS CLI scripts stored in a shared bucket"
    },
    "explanation": "Service Catalog lets administrators define approved products (CloudFormation templates) and enforce constraints, ensuring standardized, repeatable, and policy-compliant provisioning."
  },
  {
    "id": "a86590d5f4e097361f1ef098f2c9a4bb9fd092b737b5cd3cdbb9c42f6f563de5",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "Developers want code commits to trigger automatic provisioning of an S3 bucket and DynamoDB table. Which approach integrates best with a Git-based CI/CD workflow?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS CDK definitions deployed via AWS CodePipeline on merge events",
      "B": "Manually create resources in the console after each merge",
      "C": "Run AWS CLI commands locally after pull requests are merged",
      "D": "Deploy CloudFormation templates by manually invoking the AWS SDK"
    },
    "explanation": "The AWS CDK integrates with CodePipeline to translate infrastructure code into CloudFormation templates and deploy them automatically on code merges."
  },
  {
    "id": "43a219faed36ce8a91d1a9855e025130c1685f6c1ed04c04c4c45035bd6b92cc",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A QA team requires temporary test environments that are automatically destroyed after each test cycle without additional cleanup scripts. Which approach simplifies this lifecycle management?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Provision resources with a CloudFormation stack and delete the stack at test completion via pipeline.",
      "B": "Schedule AWS CLI delete commands in a cron job on Cloud9.",
      "C": "Invoke a Lambda teardown function triggered by a CloudWatch alarm.",
      "D": "Manually remove resources in the AWS Console after testing."
    },
    "explanation": "Using a CloudFormation stack for provisioning allows the entire environment to be torn down cleanly and automatically by deleting the stack, avoiding custom cleanup code."
  },
  {
    "id": "4cee04aec72052c9f574c8c481d49ad146f350358015065a2cf9a9b1bb2b30d8",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A security team wants to enforce that all resource changes go through version-controlled templates and block direct console or CLI modifications. Which combination of features achieves this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Manage resources exclusively via CloudFormation in CodePipeline, and restrict direct IAM permissions for console/CLI changes.",
      "B": "Deploy AWS Config rules to auto-remediate unauthorized changes.",
      "C": "Use Service Control Policies (SCPs) to deny CloudFormation usage.",
      "D": "Monitor CloudTrail and send alerts for manual changes."
    },
    "explanation": "Combining strict IAM policies that only allow CloudFormation deployments with a CodePipeline workflow ensures all changes are version controlled and blocks manual modifications."
  },
  {
    "id": "61d6e0a42aa0431551e6631717c570e6b3793022f9da9144cad02647468572b8",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "An operations team needs to propagate outputs (like VPC IDs) from one CloudFormation stack into dependent stacks in a different account. Which native feature supports this use case?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Export outputs and import them via Fn::ImportValue in the dependent templates.",
      "B": "Hardcode VPC IDs into the dependent templates.",
      "C": "Use AWS CLI to retrieve outputs and manually update templates before deployment.",
      "D": "Invoke a Lambda function to patch the templates at deploy time."
    },
    "explanation": "CloudFormation exports allow stacks to share outputs via Fn::ImportValue across accounts or regions, providing a native, automated mechanism."
  },
  {
    "id": "26a1a218caf4cbca403d09be70b84709f7bb3b742f0662b7ad91639f4b851e68",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A team must provision AWS IAM Identity Center (SSO) permission sets and assignments programmatically as part of environment setup. Which IaC tool natively supports these AWS SSO resources?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS CloudFormation with AWS::SSO::* resource types",
      "B": "AWS CLI only, since CloudFormation doesn\u2019t support SSO resources",
      "C": "Custom AWS SDK scripts; IaC is not supported for SSO",
      "D": "AWS Control Tower Account Factory to configure SSO assignments"
    },
    "explanation": "CloudFormation now includes AWS::SSO resource types, enabling native provisioning of SSO permission sets and assignments within templates."
  },
  {
    "id": "1d3fc44930001bc19e47867200aabf34bee9ddc3d380e7011db799ea5f3508a0",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A social media startup anticipates millions of users across Asia, Europe, and the Americas, requiring low-latency performance and compliance with local data-residency laws. Which AWS Cloud benefit most directly addresses both of these requirements?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Elasticity of compute resources to handle variable load",
      "B": "Global infrastructure with multiple Regions and Availability Zones",
      "C": "Pay-as-you-go pricing to optimize costs",
      "D": "Managed services to reduce operational overhead"
    },
    "explanation": "Global infrastructure (Regions/AZs) lets you deploy close to users for low latency and choose data-residency locations for compliance."
  },
  {
    "id": "543c09bf77f6bae07f258aecb99a3e5877bab9d1d740bd8d3a6e76e6e64caaf3",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.7",
    "stem": "An online retailer expects sudden traffic spikes during flash sales, followed by long periods of low activity. Which combination of AWS Cloud benefits should the architect leverage to optimize performance during spikes and cost during lulls?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Global reach and managed services",
      "B": "High availability and reserved instances",
      "C": "Elasticity for scaling out and pay-as-you-go pricing",
      "D": "Edge caching and capacity pooling"
    },
    "explanation": "Elasticity lets you automatically scale compute up/down for spikes; pay-as-you-go avoids paying for idle capacity."
  },
  {
    "id": "62b2d430e20ee300e46ff3028f0ac430ccde6d72e01dd63e8638719f862c646c",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "A financial services firm must achieve an RPO of 5 minutes and RTO of 10 minutes across geographically distant sites. Which AWS Cloud benefit is most critical to meet these recovery objectives?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Serverless computing for automatic fault isolation",
      "B": "Reserved Instances for predictable capacity",
      "C": "Edge locations for content delivery",
      "D": "Multi-Region deployment for disaster recovery"
    },
    "explanation": "Deploying applications in multiple Regions enables quick failover (RTO) and cross-Region replication (RPO) aligned with tight recovery objectives."
  },
  {
    "id": "8ffe06af6d0b9badd9cdd4838fb38ddd27412ab9d044cda7d37f8cfe5b0870d6",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A biotech research company needs to run large genome-sequencing workloads only a few hours per week. Which AWS Cloud benefit and pricing model combination minimizes cost while allowing on-demand scaling?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Elasticity paired with on-demand instances",
      "B": "High availability paired with reserved instances",
      "C": "Global infrastructure paired with spot instances",
      "D": "Managed services paired with dedicated hosts"
    },
    "explanation": "Elasticity lets you provision resources only when needed; on-demand instances avoid long-term commitments for intermittent workloads."
  },
  {
    "id": "2d0964f80440ec47c9b3f62f057b85da86af2f7b3017607b1e01ff960b1ff6ef",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.1",
    "stem": "A SaaS vendor wants to reduce time to market by automating provisioning of dev/test environments in minutes. Which AWS Cloud benefits most directly deliver this capability?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Edge locations and cost optimization",
      "B": "High availability and consolidated billing",
      "C": "Agility through automation (Infrastructure as Code) and elasticity",
      "D": "Global infrastructure and reserved capacity"
    },
    "explanation": "Agility via Infrastructure as Code automates environment creation in minutes; elasticity adjusts resources as needed."
  },
  {
    "id": "f946df710a344ddd90b3d37dac6484a5df886c18420a6cea7cc4bba0fe61d7ef",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "A video streaming platform must ensure uninterrupted playback even if an Availability Zone fails. Which AWS Cloud benefit enables this requirement?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Elasticity to add more servers on demand",
      "B": "High availability across multiple Availability Zones",
      "C": "Global infrastructure for data residency",
      "D": "Pay-as-you-go pricing to control costs"
    },
    "explanation": "High availability achieved by deploying resources across AZs ensures service continuity when one AZ fails."
  },
  {
    "id": "f516fdf0e59d68f46aa0fd9c869297abc4b579206e7dc3739a967835ccd6dd56",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A gaming company deploys leaderboards, matchmaking, and chat services. Traffic surges unpredictably by 1000% during tournament launches. Which AWS benefit helps handle these surges without service degradation?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Elasticity with auto scaling groups",
      "B": "Global infrastructure with edge caching",
      "C": "High availability with multi-Region failover",
      "D": "Consolidated billing for unified invoices"
    },
    "explanation": "Elasticity via Auto Scaling dynamically adjusts capacity to match unpredictable traffic spikes."
  },
  {
    "id": "27c2036dd56e84047169c06e047e4c54e9cf14b8294cc5b49b8f158f38e93aac",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "A global analytics platform must process data in near real time, using resources close to where data is generated. Which two AWS benefits work together to meet this need?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High availability and managed services",
      "B": "Reserve Instances and consolidated billing",
      "C": "Serverless computing and dedicated hosts",
      "D": "Global infrastructure (edge locations) and elasticity"
    },
    "explanation": "Edge locations provide local processing points; elasticity scales up processing power in real time."
  },
  {
    "id": "ac5d10941e9596ce59763d35b99119b88a896a4b88e429d0957af7915160fce2",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A multinational corporation wants to standardize its deployment process to reduce human error and deploy identical environments across multiple AWS accounts in different Regions. Which AWS Cloud benefit supports this goal?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Pay-as-you-go pricing",
      "B": "Elasticity via Auto Scaling",
      "C": "Agility through Infrastructure as Code and global infrastructure",
      "D": "High availability across Availability Zones"
    },
    "explanation": "Infrastructure as Code provides repeatable, standardized deployments; global infrastructure lets you apply them in any Region/account."
  },
  {
    "id": "d7a0428d3d1f653c204096e1f8b520ab2b1c95bfbb4c8c87dce373ab215f3878",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A healthcare startup must maintain uptime during unpredictable load, while also minimizing capital expenditure. Which AWS Cloud benefit-pricing combination should they choose?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Global infrastructure with reserved instances",
      "B": "High availability with pay-as-you-go pricing and elasticity",
      "C": "Edge locations with spot instances",
      "D": "Dedicated hosts with elasticity"
    },
    "explanation": "High availability paired with elasticity and pay-as-you-go lets them handle unpredictable loads without CAPEX."
  },
  {
    "id": "e68c509e78721e4bec15cd64a94749573ecbe73672b1055839e7e4186ca339f6",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A company requires sub-second deployment of hundreds of test environments simultaneously. Which AWS feature combination demonstrates the benefits of speed of deployment and elasticity?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "CloudFormation templates with Auto Scaling",
      "B": "AWS Marketplace AMIs with Reserved Instances",
      "C": "Edge locations with spot instances",
      "D": "Dedicated hosts with consolidated billing"
    },
    "explanation": "CloudFormation automates rapid provisioning; Auto Scaling provides elasticity to manage resource count per environment."
  },
  {
    "id": "7b3b43838d73f5e59a09a2af5148e4f5ec8e148d976548b8b628b85dd5747564",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A real-time IoT analytics application ingests millions of device messages per hour. Which AWS Cloud benefit ensures that the ingestion layer can absorb erratic device bursts without manual intervention?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Elasticity with managed message-ingestion services",
      "B": "Global infrastructure with multi-Region data collection",
      "C": "Pay-as-you-go pricing to handle burst costs",
      "D": "High availability across Availability Zones"
    },
    "explanation": "Elasticity in services like Amazon Kinesis or SQS automatically scales throughput during bursts without manual action."
  },
  {
    "id": "d8ef10956cb37449a627e808558fa0ab09d28f869d90af64fab808dd4fe181e6",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "An e-learning platform offers courses worldwide and must localize content delivery to reduce latency. Which AWS Cloud benefit should they leverage, and why might a simple VPC deployment not suffice?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Elasticity, because scaling in one Region automatically reduces latency globally",
      "B": "Pay-as-you-go pricing, because it optimizes cost across Regions",
      "C": "High availability, because multiple AZs cover global users",
      "D": "Global infrastructure with edge locations, because a VPC only covers a single Region"
    },
    "explanation": "Edge locations (CloudFront) provide global edge caching; a VPC is Region-scoped and cannot serve global low-latency delivery."
  },
  {
    "id": "45834f1842ec876b4291e9cbe18f3b32ed307d0681efa5673b8a153cf0b3048d",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A batch processing job must complete within a fixed time window daily. The workload varies by 50% day to day. Which AWS benefits combination ensures timely completion at minimum cost?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Reserved Instances and high availability",
      "B": "Elasticity with spot and on-demand instances",
      "C": "Global infrastructure with consolidated billing",
      "D": "Serverless computing with dedicated hosts"
    },
    "explanation": "Elasticity lets you scale capacity; using spot instances reduces cost for batch jobs, falling back to on-demand for reliability."
  },
  {
    "id": "1d04a0853ef016ed460ab8d243a4dc9ea8f0b4645c4c6f5d3a586f60f7a0b95d",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.1",
    "stem": "A multinational enterprise needs homogeneous application performance in new markets launched overnight. Which two AWS Cloud benefits combined support rapid, consistent deployment and local performance?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "High availability and spot instances",
      "B": "Elasticity and reserved instances",
      "C": "Agility via Infrastructure as Code and global infrastructure",
      "D": "Consolidated billing and edge caching"
    },
    "explanation": "Infrastructure as Code automates consistent deployments; global infrastructure lets you provision locally in each new market."
  },
  {
    "id": "2836e86ff6c829fd5653803ba8af6df633fce94c3e117b4fcf2c339a21b99273",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "Your company has deployed a three-tier web application using Amazon EC2 instances in a public subnet, an Application Load Balancer, and an Amazon RDS MySQL instance in a private subnet. According to the AWS shared responsibility model, which one of the following tasks is the sole responsibility of AWS?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Installing security patches on the Windows operating system of the EC2 instances",
      "B": "Patching and maintaining the underlying hypervisor that hosts your EC2 instances",
      "C": "Hardening the database schema and user accounts within the RDS MySQL instance",
      "D": "Configuring security groups to restrict inbound traffic to only HTTP and HTTPS"
    },
    "explanation": "In the shared responsibility model, AWS is responsible for the security of the cloud, including patching and maintaining the hypervisor. Installing OS patches, hardening your database schema, and configuring security groups are customer responsibilities."
  },
  {
    "id": "58aa4af064253c5f4dbe5ac360f0ff57110389289d90c90e33ee358849cff6d3",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "Your security team wants all objects in an S3 bucket encrypted at rest. They decide to use server-side encryption with AWS Key Management Service (SSE-KMS). Under the shared responsibility model, which of these tasks must your organization perform?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Rotate the AWS-managed KMS key used by SSE-KMS once per year",
      "B": "Ensure that AWS automatically rotates the KMS key every 30 days",
      "C": "Verify that AWS securely stores the S3 encryption keys in hardware security modules",
      "D": "Configure bucket policies and IAM permissions so only authorized users can use the KMS key"
    },
    "explanation": "With SSE-KMS, AWS manages the HSM infrastructure and rotation schedule (annual by default), but you must configure IAM and bucket policies to control which principals can use or manage the KMS key."
  },
  {
    "id": "43cb4707b60d8258998562ac1077f97b53859f075884bae1258eb6d877b527ae",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "You are using AWS Lambda to process incoming messages from a Kinesis stream. Which of the following best describes your responsibility under the shared responsibility model?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Patching the underlying EC2 instances that run the Kinesis service",
      "B": "Maintaining the physical security of the AWS data center where Lambda runs",
      "C": "Implementing IAM policies and execution roles that grant Lambda the minimum privileges it needs to read from the Kinesis stream",
      "D": "Upgrading the Lambda runtime environment when AWS releases new language versions"
    },
    "explanation": "Customers are responsible for the security in the cloud, including IAM roles and least-privilege policies. AWS manages the underlying infrastructure, physical security, and runtime environment upgrades."
  },
  {
    "id": "92b3c3036ffb06074213d8aa3ed441838ab5f80b48b96c19808edc1ca149e58a",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "Your organization uses Amazon RDS for PostgreSQL. You need to ensure the database engine version remains up to date with security fixes. According to the shared responsibility model, who must apply these engine patches and when?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS automatically applies minor engine patches during your specified maintenance window",
      "B": "Your organization must manually log into the RDS host and install engine patches",
      "C": "AWS notifies you and you must upload and install patches via AWS CLI",
      "D": "AWS applies patches immediately after release regardless of maintenance windows"
    },
    "explanation": "AWS manages patching of the managed RDS engine during the maintenance window you define. You cannot log into the host, and patches are not applied immediately by default without your maintenance window configuration."
  },
  {
    "id": "d223e3c0a32289a71077b954d3956b68ca4391a48191ce3843d560ceff7b3c25",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "Your team wants to employ Amazon S3 client-side encryption to store PII data. Under the shared responsibility model, which of the following is your organization\u2019s responsibility?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Ensuring that AWS rotates the encryption keys every 90 days",
      "B": "Securely generating, distributing, and storing the encryption keys used on the client",
      "C": "Retrieving the envelope keys from AWS KMS and applying them automatically",
      "D": "Configuring Amazon S3 to automatically decrypt objects when accessed"
    },
    "explanation": "For client-side encryption, the customer is fully responsible for encryption key management. AWS manages server-side encryption only; it does not manage client-side keys or distribution."
  },
  {
    "id": "f6f803ba6b0dd3238307b1c5f79ce2ec167ea5e0eddbc29b673a0b3d28277398",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "A compliance auditor requests evidence of how Amazon EC2 hypervisors are patched against CPU microcode vulnerabilities. Under the AWS shared responsibility model, which action should you take?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Provide the auditor with logs of your OS-level patch management system",
      "B": "Share your EC2 instance cloud-init scripts that install security patches",
      "C": "Present IAM policies that enforce patch compliance on EC2 instances",
      "D": "Download and provide the AWS SOC or PCI compliance report that describes hypervisor patching practices"
    },
    "explanation": "AWS provides compliance reports (SOC, PCI, etc.) describing its responsibilities, including hypervisor patching. Customer patch logs and IAM policies cover only the guest OS, not underlying hypervisor."
  },
  {
    "id": "5deeb583e3afce05e19319b2713a9961171031c27ec7a2535cd36973403a03ff",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "Your company uses Amazon DynamoDB and wants to audit all changes to table configurations. Which part of this process is your responsibility and which is an AWS responsibility under the shared responsibility model?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS enables AWS CloudTrail logging; your company must review and archive the logs",
      "B": "Your company must enable CloudTrail for DynamoDB Data Definition Language (DDL); AWS stores and protects the log files",
      "C": "AWS reviews your table configuration changes daily and notifies you of deviations",
      "D": "Your company configures IAM roles for DynamoDB; AWS grants read-only access to CloudTrail logs automatically"
    },
    "explanation": "Customers must enable CloudTrail logging for DynamoDB table DDL events. AWS is responsible for storing, securing, and retaining the log files once enabled."
  },
  {
    "id": "4e2cfbe1831437d21f5496ce8affdb197714519e5d5e496ded03d13ef36d5e4c",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "You run containers on Amazon ECS with EC2 launch type. Your security policy requires that the container host OS be hardened and patched. According to the shared responsibility model, which combination of tasks falls under your responsibility?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Selecting a hardened AMI, deploying OS patches, and configuring the container host firewall",
      "B": "Maintaining the ECS control plane and replacing EC2 instances for updates",
      "C": "Patching the Docker daemon that AWS installs on the hosts",
      "D": "Managing hypervisor updates for the EC2 instances running your containers"
    },
    "explanation": "With ECS EC2 launch type, customers are responsible for patching and securing the host OS (hardening AMI, patching, host-level firewall). AWS manages the ECS control plane and hypervisor."
  },
  {
    "id": "9e5a5e7b4f70eb84b1ec6f6670e0fac5b15952bd1b60df2f912aeef388e9077b",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "Your organization uses AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD). Which security task remains your responsibility under the shared responsibility model?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Applying Windows OS patches to the directory domain controllers",
      "B": "Ensuring physical access control at AWS data centers",
      "C": "Managing the user and group permissions within AD objects",
      "D": "Patching the Amazon Linux instances that host the directory controllers"
    },
    "explanation": "AWS manages Windows OS patching and underlying infrastructure for AWS Managed Microsoft AD. Customers manage directory objects, user and group permissions within the directory."
  },
  {
    "id": "76fca779a2fb612aa4559bdc1e5318091ba39ded5a256310b99a67e51eceb7a7",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "You are storing sensitive health data in Amazon EFS file systems. According to the shared responsibility model, which encryption mechanism and responsibility pairing is correct?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Client-side encryption only; AWS redeems and stores your keys",
      "B": "AWS-managed encryption at rest using AWS KMS; you manage IAM policies to control KMS key usage",
      "C": "Server-side encryption with customer-provided keys; AWS rotates the keys monthly",
      "D": "Server-side encryption with S3-managed keys; you must request key rotation from AWS support"
    },
    "explanation": "EFS server-side encryption uses AWS KMS-managed keys. AWS handles key material storage; the customer must manage IAM policies granting use of the key. AWS-managed key rotation occurs annually."
  },
  {
    "id": "47a9a92e56ad30671033ef019788b915a5170a21c893b4bfbf9db913f02bda58",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.2",
    "stem": "Your company enabled AWS Config to record all changes across AWS resources. A new compliance requirement states that configuration snapshots must be encrypted at rest. Under the shared responsibility model, what must you do?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Rely on AWS to automatically encrypt Config snapshots since AWS manages the service",
      "B": "Submit a support ticket to AWS requesting encryption be enabled",
      "C": "Specify an AWS KMS key in the AWS Config recorder settings and grant your Config role permission to use it",
      "D": "Upload your own certificate to SSL-encrypt the snapshots during transport"
    },
    "explanation": "While AWS manages the Config service, you must configure the recorder to use a KMS key and grant IAM permissions so Config can encrypt snapshots at rest."
  },
  {
    "id": "4bca090770d0ee3a94d92481c9ef48a481f3548d269eff8cc2b04cfc9f751224",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.1",
    "stem": "A financial services firm uses Amazon Managed Streaming for Kafka (MSK). They must ensure data at rest encryption key rotation every 90 days. Under the shared responsibility model, which approach meets this requirement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create a customer-managed KMS key with automatic rotation enabled and associate it with the MSK cluster",
      "B": "Enable AWS-managed KMS key rotation for the MSK cluster and implement a CloudWatch event every 90 days",
      "C": "Provide your own encryption key material to SSE-S3 and attach it to the MSK cluster",
      "D": "Trust AWS to rotate the default AWS KMS key used by MSK quarterly"
    },
    "explanation": "For MSK, you supply a customer-managed KMS key and enable automatic rotation (annual) or manually rotate at your interval. AWS-managed keys cannot be rotated on a custom schedule."
  },
  {
    "id": "8305ba0a36519adf959f0ebd456048f4c678c7aa580e511ca3586bae375a6696",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.1",
    "stem": "Your team configures AWS Shield Advanced to protect against DDoS attacks. Which aspect of DDoS protection is the customer\u2019s responsibility under the shared responsibility model?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Patching the network edge routers to filter malicious traffic",
      "B": "Configuring AWS\u2019s global edge network to drop invalid packets",
      "C": "Maintaining switch firmware in AWS data centers",
      "D": "Configuring security group and network ACL rules on protected resources to limit exposure"
    },
    "explanation": "AWS Shield Advanced handles network-level DDoS mitigation at edge. Customers remain responsible for in-VPC controls like security groups and NACLs to limit protocol and port exposure."
  },
  {
    "id": "40401723e453443dba187221a2df4f21db095ccdc6e8b1fdf937a780bb37202b",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.1",
    "stem": "You need to grant a team in a separate AWS account (Account B) read-only access to an S3 bucket in your management account (Account A) without creating IAM users in Account A. Which solution follows the principle of least privilege and requires minimal administrative overhead?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "In Account A, create an IAM role with a trust policy that allows sts:AssumeRole from the specific IAM role or root principal in Account B, attach a policy granting s3:GetObject on the bucket, and have the team assume that role.",
      "B": "In Account A, create IAM users for each member of the team, attach AmazonS3ReadOnlyAccess to them, and share their credentials securely.",
      "C": "Use AWS Resource Access Manager (RAM) to share the S3 bucket with Account B and rely on RAM permissions to grant read-only access.",
      "D": "Add a bucket ACL granting READ permission to the canonical IDs of the IAM roles in Account B, then require the team to access the bucket using that ACL."
    },
    "explanation": "The least-privilege cross-account pattern is to create a role in the bucket\u2019s account with a trust policy for the external account and attach a minimal S3 permission. ACLs and RAM aren\u2019t designed for fine-grained cross-account access, and creating IAM users in Account A increases management overhead."
  },
  {
    "id": "708ab04445175f7475961f4b295bfd8dc6cc01a805aa39ccbf3afc3e11181037",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.1",
    "stem": "A developer in a child OU of your AWS Organization runs aws iam delete-user --user-name TestUser but receives AccessDenied. The org\u2019s SCPs do not explicitly mention iam:DeleteUser. You verify the developer\u2019s IAM policy allows iam:DeleteUser. What is causing the denial and how do you resolve it?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "The developer\u2019s permission boundary denies iam:DeleteUser. Remove or update the permission boundary attached to the developer\u2019s principal.",
      "B": "AWS Organizations prevents any IAM user from deleting IAM users by default; you must disable Organizations IAM Access Control.",
      "C": "A higher-level SCP in the OU or root implicitly denies iam:DeleteUser (deny overrides allow). Update the SCP to allow this action for the OU, or move the account to an OU without that SCP denial.",
      "D": "The AWS account must be enrolled in IAM Access Analyzer for Organizations to allow user deletions; enable the feature and grant appropriate permissions."
    },
    "explanation": "SCPs are evaluated before IAM policies and any explicit or implicit deny in an SCP overrides IAM allows. Even if the developer\u2019s IAM policy permits deletion, the SCP at a parent OU is denying it."
  },
  {
    "id": "149bfbabea6d20a3a58b3558558c91645863a9a68e1c4dbf3b177074d483e10a",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.1",
    "stem": "Your security team requires MFA for all console logins across all member accounts in your organization. Which implementation enforces this requirement at scale?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Deploy an IAM policy in each account requiring aws:MultiFactorAuthPresent in a condition block for iam:* on Resource \"*\".",
      "B": "Create a preventive Service Control Policy (SCP) at the root of the Organization with a Deny for \"Action\": \"sts:AssumeRole\" and \"Condition\": {\"BoolIfExists\":{\"aws:MultiFactorAuthPresent\":\"false\"}}.",
      "C": "Use AWS Config Conformance Packs to monitor IAM users without MFA and trigger Lambda to disable their console access.",
      "D": "Attach an AWS Managed Policy (IAMReadOnlyAccess) to all users that implicitly requires MFA for console access."
    },
    "explanation": "An SCP with a Deny on sts:AssumeRole when MFA is not present centrally enforces MFA for all role and console sessions. IAM policies per account are impractical at scale, and Config only monitors rather than enforces."
  },
  {
    "id": "d21a4e577e7eb0cfc411ef474fe41f76c4ddd0aa6d71d8db8c61a1751287b451",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.1",
    "stem": "Account B needs to send messages to an SQS queue in Account A but must not receive or delete any messages. Both accounts use IAM roles for service interactions. Which combination of policies meets the requirement with least privilege?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "In Account A, attach a resource-based policy on the queue granting sqs:SendMessage to Account B\u2019s root account; no IAM policy changes in Account B.",
      "B": "In Account B, grant an IAM role sqs:SendMessage on the queue ARN; no changes to Account A\u2019s queue policy are needed because IAM policies alone allow cross-account access.",
      "C": "In Account A, add a queue policy granting sqs:SendMessage to the specific IAM role ARN in Account B. In Account B, attach an IAM policy to that role allowing sqs:SendMessage on the queue ARN.",
      "D": "Use AWS Resource Access Manager to share the SQS queue; Account B\u2019s principals automatically receive send-only permissions."
    },
    "explanation": "Cross-account SQS access requires both a resource policy on the queue allowing the specific principal to SendMessage and an IAM policy in the calling account permitting the action. RAM doesn\u2019t support SQS."
  },
  {
    "id": "4b857484de7a0ecc8e7b0bb97da3463d4959937938b1d83f3ddecfab0f535acf",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.1",
    "stem": "An application running on EC2 in Account A must access an API in Account B. You decide to use an IAM role in Account B that trusts the EC2 instance role. Which trust policy configuration and steps are correct?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "In Account B\u2019s role trust policy, set Principal to the EC2 instance profile ARN and allow sts:AssumeRole. On the EC2 instance, use its instance profile credentials directly to call the API.",
      "B": "In Account B, create an IAM user for EC2, embed its access key in the instance, and grant it API permissions in Account B.",
      "C": "In Account A, attach AmazonApiGatewayInvokeFullAccess to the EC2 role. In Account B, do nothing; IAM policies automatically allow cross-account API calls.",
      "D": "In Account B, create an IAM role with a trust policy Principal set to the EC2 instance role ARN in Account A allowing sts:AssumeRole. Attach the API-call permissions to that role. On EC2, call STS AssumeRole using the instance profile to obtain temporary credentials."
    },
    "explanation": "The correct cross-account pattern is to define a role in the target account (B) with a trust policy for the source EC2 role in Account A, attach needed permissions to that role, then have the EC2 instance assume it via STS."
  },
  {
    "id": "5cbe4e6a68451597fd0cfb05034818dc6e02808fc1b3f65032046bc0a9461b4a",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.1",
    "stem": "Your organization uses AWS IAM Identity Center (successor to AWS SSO) integrated with Active Directory. You need to assign AD groups to permission sets across multiple AWS accounts, but some AD groups are nested. What approach ensures Identity Center assignments work as expected?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Import only top-level AD groups into IAM Identity Center; nested groups are automatically flattened and assignments cascade.",
      "B": "Flatten nested AD groups so that only direct members of a group appear in Identity Center. Create separate permission sets for each flattened group to ensure correct account assignments.",
      "C": "Enable nested group support in IAM Identity Center via an advanced setting. Identity Center will then resolve nested memberships at runtime.",
      "D": "Use AWS Directory Service Simple AD instead of AD Connector; it preserves nested group membership for Identity Center."
    },
    "explanation": "IAM Identity Center does not resolve nested AD groups. You must flatten or map each group individually so that assignments apply to the correct principals."
  },
  {
    "id": "f2f2509d3b78576256b100885a696b57be4ffef24e6b606ed8509cf6c73773f9",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.1",
    "stem": "A Lambda function in Account A needs to decrypt data encrypted with a KMS key in Account B. You have created a key policy in Account B granting the Lambda\u2019s execution role Decrypt permissions. The function still fails with AccessDenied. What else must you configure?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Enable cross-region replication on the KMS key so that the Lambda can use the replicated key.",
      "B": "In Account A, attach an IAM policy to the Lambda role granting kms:Decrypt on the key ARN in Account B; no additional changes in Account B are needed.",
      "C": "In Account B, update the KMS key policy to allow the Lambda role\u2019s ARN specifically and ensure an associated IAM policy is attached to that role permitting kms:Decrypt on the key ARN.",
      "D": "Change the Lambda function\u2019s execution role trust policy to include the KMS service principal (kms.amazonaws.com)."
    },
    "explanation": "Both the KMS key policy and an IAM policy on the execution role must grant kms:Decrypt on the key ARN. A key policy alone is insufficient if the role\u2019s IAM policy doesn\u2019t allow the action."
  },
  {
    "id": "8193d0135ac40f6cdc832ca62b855530dae7588276411bf1cf9817641e77a472",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.1",
    "stem": "You want to restrict a critical IAM role so it can only be assumed from specific corporate IP ranges. Where and how do you implement this restriction?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "In the IAM role\u2019s permission policy, add a Condition on aws:SourceIp for all actions to the corporate IP CIDRs.",
      "B": "In the IAM role\u2019s trust policy, add a Condition \"IpAddress\":{\"aws:SourceIp\":[\"203.0.113.0/24\"]} under the sts:AssumeRole statement.",
      "C": "Enable an AWS WAF IP restriction on sts.amazonaws.com to block AssumeRole calls from outside the corporate IP ranges.",
      "D": "Create a Service Control Policy in Organizations denying sts:AssumeRole when aws:SourceIp is not in the corporate CIDRs."
    },
    "explanation": "Trust policy conditions apply to the sts:AssumeRole call and enforce where the request originates. Permission policies can\u2019t block sts:AssumeRole; SCPs apply at account level and are harder to scope to a single role."
  },
  {
    "id": "d7a1627e6bda013f6b33317b5d53a743a6a52062158140b3b2038240a370d726",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.1",
    "stem": "Your organization federates users from ADFS (SAML 2.0) into AWS. You need session durations up to 12 hours, but users are being signed out after 1 hour. What change enables longer sessions?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase the MaxSessionDuration parameter on the IAM roles to 43200 seconds, and include the DurationSeconds attribute in the SAML assertion when assuming the role.",
      "B": "Configure the ADFS server to issue longer SAML assertion time attributes; IAM roles automatically honor assertion durations.",
      "C": "Modify the AWS CLI configuration on users\u2019 machines to set credential_source to assume-role with a 12-hour session.",
      "D": "Enable the AWS SSO Identity Provider in IAM and set session duration there to 12 hours; SAML federation sessions then inherit this setting."
    },
    "explanation": "By default, IAM roles allow only 1 hour. You must explicitly set MaxSessionDuration on the role to up to 12 hours and request that duration in the SAML assertion. ADFS settings alone don\u2019t change the IAM role\u2019s maximum."
  },
  {
    "id": "9d824a63f10196fdb51991d198f5f40d04054d286fa21e18bda62863191b9897",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.1",
    "stem": "You grant a third-party vendor access to your account via an IAM role with sts:AssumeRole. To prevent the confused-deputy problem, what must you include in the trust policy and in the vendor\u2019s assume-role request?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A Condition in the trust policy requiring aws:PrincipalArn to match the vendor IAM user, and no additional request parameters.",
      "B": "An ExternalId in the vendor\u2019s AWS account settings, and no trust policy changes; AWS will validate automatically.",
      "C": "A Condition that checks aws:SourceIp against the vendor\u2019s corporate IP, and have the vendor call AssumeRole from that IP.",
      "D": "A Condition in the trust policy requiring sts:ExternalId equals a shared value, and have the vendor include that ExternalId parameter when calling sts:AssumeRole."
    },
    "explanation": "To mitigate the confused-deputy attack, you add a Condition for sts:ExternalId in the trust policy, and the vendor must supply that same ExternalId in the AssumeRole request."
  },
  {
    "id": "f6bcc83e42e24ae2e00c22cfee57c35cc4c1214d40c5e11ac61c2105fa220ad4",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.1",
    "stem": "You configure cross-account S3 replication from BucketA in Account A to BucketB in Account B. After enabling versioning and roles, replication still fails with AccessDenied. What is missing?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable cross-region replication on BucketB, even if it\u2019s in the same region.",
      "B": "Add a bucket policy on BucketA granting the replication role in Account B permissions for s3:ReplicateObject and s3:ReplicateDelete.",
      "C": "Attach an S3 ACL on each object in BucketA granting full control to the destination bucket\u2019s account.",
      "D": "In Account B, add an IAM policy allowing the replication service principal to call s3:ListBucket on BucketA."
    },
    "explanation": "Cross-account replication requires a source bucket policy that explicitly grants the destination replication role s3:ReplicateObject (and related) on the source bucket. ACLs and IAM policies alone aren\u2019t sufficient."
  },
  {
    "id": "2851a75ddc2145748915477ac693409701d944afb74c1994eedaddd822ba5b0a",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.1",
    "stem": "Your security team wants to ensure any new IAM user or role cannot have more permissions than the security baseline. Which combination of policies enforces that all new principals are bound by a permission boundary?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Apply an SCP at the Organization root that Denies iam:CreateUser and iam:CreateRole unless the request includes aws:PermissionsBoundary in the condition, and use a baseline boundary policy.",
      "B": "Attach an IAM policy to each administrator user that explicitly Denies iam:CreateUser and iam:CreateRole unless they specify aws:PermissionsBoundary.",
      "C": "Use AWS Config rules to detect IAM principals without a boundary after creation and remediate by attaching one.",
      "D": "In each account, require that the default permission boundary for IAM entities is set to the baseline boundary policy in Account A."
    },
    "explanation": "An SCP can require that creation of IAM users and roles include a PermissionsBoundary, enforcing the security baseline organization-wide before the identity is created."
  },
  {
    "id": "b39487211f7bd8cf5f7453119cd63124c58b8baef0a4e27e95ceb831571ec316",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.1",
    "stem": "A critical DynamoDB table must only be accessed through your VPC endpoint. Which policy enforces this restriction?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "In the table\u2019s IAM resource policy, add a Condition StringEquals aws:SourceIp to the VPC CIDR.",
      "B": "Use a Service Control Policy that denies dynamodb:* when aws:SourceVpc is not the secure VPC ID.",
      "C": "In the DynamoDB table resource policy, add a Condition StringEquals aws:SourceVpce to the VPC endpoint ID.",
      "D": "Attach an IAM policy to all roles accessing the table requiring aws:SecureTransport and aws:VpcId conditions."
    },
    "explanation": "To restrict DynamoDB to a VPC endpoint, you add a Condition on aws:SourceVpce in the table\u2019s resource policy specifying the VPC endpoint ID. IP or VPCId conditions aren\u2019t supported on DynamoDB resource policies."
  },
  {
    "id": "8907ab41fad354c97f968c3f40769ee1aa733544b3c49a34a4eff49aecd54ab7",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.1",
    "stem": "You centrally log CloudTrail events to an S3 bucket in your logging account. You must prevent anyone (even root) from deleting objects in that bucket except for the CloudTrail service. Which bucket policy enforces this?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Deny s3:DeleteObject on the bucket for all principals where aws:PrincipalIsAWSService is false.",
      "B": "Deny s3:DeleteObject on the bucket for Principal \"*\" with a Condition ArnNotEquals aws:PrincipalArn to \"arn:aws:cloudtrail:*:*:trail/*\".",
      "C": "Deny s3:DeleteObject on the bucket for Principal \"*\" without specifying any condition, then add an Allow for the CloudTrail service principal.",
      "D": "Use an SCP at the organization root denying s3:DeleteObject on the logging bucket\u2019s ARN for all accounts except the logging account."
    },
    "explanation": "A bucket policy Deny for all principals with ArnNotEquals set to the CloudTrail trail ARN ensures only the CloudTrail service principal can delete objects. An SCP can\u2019t scope to service principals."
  },
  {
    "id": "e0b1105a93768fadb4cc1d2e4749d26f737907e66901dd4be7eb27bdf55efcb5",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.1",
    "stem": "Your security team wants to use session tags in a cross-account role to implement attribute-based access control. After configuring the trust policy to allow sts:TagSession, sessions still lack tags. What IAM policy change fixes this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "In the principal account, add an IAM policy statement on the user or identity provider allowing sts:TagSession on the role ARN.",
      "B": "Modify the trust policy to include a Condition that maps aws:PrincipalTag keys to the session tags.",
      "C": "Enable the TagSession feature in AWS Organizations to propagate tags automatically.",
      "D": "Include the session tags in the AssumeRoleWithSAML request and no further IAM policy changes are needed."
    },
    "explanation": "Even if the trust policy permits tagging, the caller\u2019s IAM policy must also allow the sts:TagSession action on the role. Without it, tags are silently dropped."
  },
  {
    "id": "6d331515b302f8e5b93a1bf1fc508d7182629580cfa95b7b6cad85e81a57b196",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.2",
    "stem": "A company runs a three-tier application in a single VPC with public subnets for web servers, private subnets for application servers, and private subnets for a database. The application servers must fetch updates from the internet but must never be directly reachable from the internet. Which combination of configurations will meet these requirements with the least administrative overhead?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Place a NAT gateway in each private subnet and update the application servers\u2019 route tables to direct 0.0.0.0/0 to the NAT gateways.",
      "B": "Place a NAT gateway in each public subnet, update the private subnets\u2019 route tables to direct 0.0.0.0/0 to the NAT gateways, and ensure the application servers have no inbound security group rules from the internet.",
      "C": "Create a public NAT instance in each public subnet, configure elastic IPs, update private route tables to use these NAT instances, and add security group rules to block inbound internet traffic.",
      "D": "Deploy an internet gateway and a NAT gateway in each AZ, associate the private subnets directly with the internet gateway, and use network ACLs to block inbound access to the application servers."
    },
    "explanation": "B uses managed NAT gateways in public subnets and private route tables, with security groups blocking inbound internet traffic; this meets requirements with minimal overhead."
  },
  {
    "id": "b32146fc3dc101722ea6b48cbf8eb6a4c3abe45546a35323fd27262122e3dcd2",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.2",
    "stem": "An organization requires that all HTTP traffic to its web application be inspected for SQL injection attacks before reaching the web servers in their VPC. Which architecture should the Solutions Architect recommend to achieve this requirement?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Deploy AWS WAF on an Application Load Balancer in front of the web servers.",
      "B": "Install ModSecurity on the EC2 instances hosting the web servers and configure custom SQL injection rules.",
      "C": "Place an Application Load Balancer with AWS WAF rules in front of the web servers, and configure a rule group that inspects for SQL injection.",
      "D": "Use AWS Shield Advanced to detect SQL injection attacks and forward legitimate traffic to the web servers."
    },
    "explanation": "AWS WAF on an ALB can inspect and block SQL injection; Shield Advanced protects against DDoS, not application-level threats."
  },
  {
    "id": "ed54980523200a9c52b934c3e2f05d875f61be864c84bf3922ff518ee0ecfbbc",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.2",
    "stem": "A critical application in your VPC runs on EC2 instances that must only accept SSH from your corporate network IP range and must use multi-factor authentication for administrative access. Which solution provides the strongest security with minimal ongoing maintenance?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Deploy AWS Systems Manager Session Manager with IAM policies requiring MFA. Use a VPC endpoint for Systems Manager. Restrict SSH security group rules to the corporate CIDR and disable SSH on the instances.",
      "B": "Configure SSH on EC2 to use public key authentication, restrict security group inbound SSH to the corporate CIDR, and require users to enable MFA on their IAM console sessions.",
      "C": "Place a Bastion host in a public subnet, restrict its security group to the corporate CIDR, install MFA software on the Bastion host, and allow SSH from the Bastion host to the private EC2 instances.",
      "D": "Use an AWS Directory Service\u2013backed Microsoft Active Directory for SSH authentication with MFA, and restrict the EC2 instances\u2019 security groups to the corporate CIDR."
    },
    "explanation": "Session Manager with IAM/MFA and a VPC endpoint eliminates bastion hosts and SSH keys, providing strong security and low maintenance."
  },
  {
    "id": "8b1fd496e8bad29c77d98cfd94a15d19bda06317e172723c63a52d61932e9c76",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.2",
    "stem": "Your VPC has two private subnets without internet gateway access. An application in one subnet must securely call an AWS API (e.g., S3) without using a NAT gateway. Which design meets this requirement while adhering to the principle of least privilege?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Configure a public NAT instance in a public subnet, update the private subnet route tables, and restrict IAM policies for the application to S3 access.",
      "B": "Attach an internet gateway to the VPC, update route tables for the private subnet, and use security groups to allow only port 443 outbound.",
      "C": "Use AWS Direct Connect through the corporate network to get internet access and call the AWS API.",
      "D": "Create an S3 VPC endpoint, update the application\u2019s IAM role with least-privilege S3 permissions, and configure the private subnet route table to include the endpoint."
    },
    "explanation": "An S3 VPC endpoint allows private access to S3 without NAT. IAM role enforces least privilege."
  },
  {
    "id": "e8694768e8f50530245b59fd173f5bfb207804008a8a5b99e22aab9dd822b30f",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.2",
    "stem": "An application requires secure, encrypted connections between its on-premises network and a private subnet in AWS. The on-premises firewall only allows IPsec VPN over UDP 500 and 4500. Which architecture will satisfy the requirement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set up AWS Direct Connect with a private VIF and use TLS tunnels to encrypt traffic.",
      "B": "Configure a Site-to-Site VPN connection over an existing Customer Gateway using an AWS Virtual Private Gateway, with IPsec phase 1 and 2 over UDP 500/4500.",
      "C": "Establish a VPN on EC2 instances running OpenVPN in a public subnet and route traffic through an internet gateway.",
      "D": "Use AWS Client VPN endpoint in the VPC and have on-premises clients initiate SSL VPN connections."
    },
    "explanation": "A Site-to-Site VPN uses IPsec over UDP 500/4500 by default and integrates with customer firewalls."
  },
  {
    "id": "aa442e8a94aaabe07c8afa21b6349767950ad62ab787d909a54220002b1bec3e",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.2",
    "stem": "You must centralize secrets management for hundreds of microservices deployed across multiple accounts. Each service needs access to its own secrets and must rotate credentials automatically. Which solution is most appropriate?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use AWS Secrets Manager in a shared security account. Create secrets per microservice, configure IAM resource policies to grant cross-account access, and enable automatic rotation for each secret.",
      "B": "Deploy HashiCorp Vault on EC2 in each account, replicate secrets between clusters, and use a bastion host for secret distribution.",
      "C": "Store secrets in AWS Systems Manager Parameter Store in each account. Use Parameter Store to rotate secrets manually via scripts.",
      "D": "Embed encrypted secrets in container images and rotate by building new images on a schedule."
    },
    "explanation": "AWS Secrets Manager supports cross-account access, per-secret rotation, and centralized management with minimal maintenance."
  },
  {
    "id": "05caec9b8552834ae416873df485f81b626d9ccfcfa63e5a339f3b8519efbb12",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.2",
    "stem": "A security audit reveals that your web servers in a public subnet can inadvertently send traffic to other AWS services without encryption. You must enforce TLS for all outbound calls to AWS APIs and services while minimizing code changes. Which configuration achieves this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Install and configure a proxy server on each web server that terminates and initiates TLS for outbound requests.",
      "B": "Use AWS WAF to inspect and block non-TLS outbound traffic from the VPC.",
      "C": "Enable VPC endpoints (interface) for required AWS services with private DNS enabled, forcing traffic over TLS-encrypted AWS backbone.",
      "D": "Attach an egress-only internet gateway to the subnet and configure security groups to allow only port 443 outbound."
    },
    "explanation": "VPC interface endpoints use TLS on AWS network, require minimal code changes (same DNS names). Egress-only IGW is for IPv6 only."
  },
  {
    "id": "ab02911c5a3937c77924ef087e99d94b776f509d776a6c4555d969da027036f0",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.2",
    "stem": "Your application tier in a private subnet must receive events from Amazon SNS topics in a different AWS account without using the public internet. Which setup meets this requirement securely and with low operational overhead?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set up a Site-to-Site VPN between the two accounts and subscribe the application to the SNS topic via public endpoints.",
      "B": "Use AWS Transit Gateway peering across accounts, route SNS traffic through the Transit Gateway to the private subnet.",
      "C": "Configure the SNS topic to publish to an HTTPs endpoint in the application tier with mutual TLS authentication.",
      "D": "Create an SNS VPC endpoint in the subscriber account\u2019s VPC and configure cross-account SNS topic policy to allow the publisher account to send messages internally."
    },
    "explanation": "An SNS VPC endpoint with cross-account permissions allows internal delivery over AWS private network without VPN."
  },
  {
    "id": "3e8194dfa3ae272c6784b03f4239284f85296c5995c366e71e53f8a8ce647883",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.2",
    "stem": "An application in a private subnet needs to validate user JWT tokens using Amazon Cognito before processing requests. The application servers must not have any public IP addresses. Which architecture should you implement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Expose Cognito user pool endpoints via a NAT gateway and allow the private subnet to route traffic through it.",
      "B": "Configure an interface VPC endpoint for Amazon Cognito in the VPC, update route tables, and restrict security groups to only allow HTTPS to the endpoint.",
      "C": "Deploy an API Gateway private integration with Cognito authorizer and route traffic from the application servers through the API Gateway.",
      "D": "Use AWS Directory Service for AD to sync Cognito tokens and validate against the directory internally."
    },
    "explanation": "An interface endpoint for Cognito User Pools allows private HTTPS access; no public IP required."
  },
  {
    "id": "c71fcd3888ef055289d35058a61a101cf2f9ebc3b883eeae817ce73d718b7c82",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.2",
    "stem": "You need to enforce network-level segmentation between application environments (dev, test, prod) within a single VPC while still allowing secure, centralized logging to a SIEM server in a logging subnet. Which approach meets these requirements?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Network Firewall with stateless rules to isolate each environment\u2019s subnets and stateful rules to allow traffic only from approved environments to the logging subnet.",
      "B": "Apply security groups on each EC2 that reference the SIEM security group and block all other traffic between environments.",
      "C": "Use route tables with specific routes per subnet to direct approved traffic to a central NAT Gateway for the SIEM, and use NACLs to block other routes.",
      "D": "Deploy separate VPCs per environment and use VPC peering to a centralized logging VPC, securing with security groups."
    },
    "explanation": "Network Firewall can enforce subnet-level segmentation and allow only authorized flows to the logging subnet; minimal per-instance config."
  },
  {
    "id": "e44f90abdc85da42872d3293049952f853c6f39c5e0a4333c58abe0a7a40f783",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.2",
    "stem": "An organization\u2019s compliance requires that all database credentials are never stored on disk and only retrieved at runtime. Which solution best secures credentials for an application running in an ECS cluster on Fargate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set environment variables in the task definition with encrypted credentials and enable IAM roles for tasks.",
      "B": "Store credentials in AWS Systems Manager Parameter Store encrypted with a KMS key and fetch them on container start via HTTPS.",
      "C": "Use AWS Secrets Manager with an IAM role for ECS tasks; grant the task role permissions to retrieve secrets at runtime and mount secrets as environment variables.",
      "D": "Embed credentials in Docker images built from a private CodeCommit repository and use AWS KMS to encrypt the entire image."
    },
    "explanation": "Secrets Manager integrates with ECS Fargate task roles to fetch secrets at runtime; environment injection avoids disk storage."
  },
  {
    "id": "a67add6a446847c3710adf0de6069250b402cd586fe66be56c05ab05c369ec04",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.2",
    "stem": "Your VPC spans two AZs. To protect application servers from network-based threats, you plan to deploy AWS WAF, AWS Shield Advanced, and AWS Network Firewall. Which placement ensures all traffic is inspected in real time before reaching the servers?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Front the application with an internet-facing Application Load Balancer with AWS WAF and Shield. Place Network Firewall in a centralized inspection VPC via Transit Gateway, routing all traffic through it before reaching application subnets.",
      "B": "Attach Network Firewall appliances in each private subnet, deploy WAF on each EC2 instance, and enable Shield Advanced on the VPC.",
      "C": "Set up a public NAT gateway with Network Firewall inline, use WAF at the regional API Gateway level, and enable Shield Advanced on the VPC.",
      "D": "Use a Gateway Load Balancer endpoint for Network Firewall in the public subnets, attach WAF to the internet gateway, and enable Shield on the EC2 instances."
    },
    "explanation": "Transit Gateway with centralized Network Firewall ensures all ingress/egress flows go through inspection; WAF and Shield at ALB protect application layer."
  },
  {
    "id": "343dca9bc987c117074605ba8aa6af31930674bea8aa2ed6aa9c5096ea75edab",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.2",
    "stem": "A data processing application in a private subnet requires connections to an internal corporate web service hosted on-premises. The on-premises service is only reachable via an HTTP proxy. You must ensure that the application only communicates with the on-premises service. Which design meets the requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure the application servers to use the proxy\u2019s public endpoint over the internet, and use security groups to limit outbound HTTP.",
      "B": "Deploy a proxy server in a public subnet in AWS that tunnels traffic over a Site-to-Site VPN to the on-premises proxy, restrict route tables and security groups to force traffic through this proxy.",
      "C": "Use AWS Global Accelerator to route traffic from the application to the on-premises proxy and configure a VPC endpoint for Accelerated traffic.",
      "D": "Leverage AWS Transit Gateway Multicast to send traffic from the private subnet directly to the on-premises proxy."
    },
    "explanation": "A Site-to-Site VPN with a proxy in a public subnet forces all application HTTP through the corporate proxy; security groups and route tables enforce it."
  },
  {
    "id": "1af75d59a8a2996b0bcbbb5d38db9872505e33b4cb6d69ef864fb1d2934512ff",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.2",
    "stem": "You must ensure that an Internet-facing API hosted in a VPC cannot be invoked from any countries except the United States and Canada. You also need to block IP-spoofed requests. Which solution provides geofencing with minimal operational overhead?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Use security groups with IP ranges for US and Canada and enable Packets Inspection in Network ACLs to block other sources.",
      "B": "Deploy NACLs with AWS-managed country-specific IP lists and configure them to allow only US/Canada ranges.",
      "C": "Configure AWS WAF geographic match conditions on an internet-facing Application Load Balancer to allow only US and Canada, enable WAF managed rule to block IP-spoofed traffic.",
      "D": "Use AWS Shield Advanced with custom protection groups specifying country codes, and attach it to the ALB."
    },
    "explanation": "AWS WAF geolocation rules easily allow/block by country and can include managed rules to detect spoofed IPs."
  },
  {
    "id": "1123e976b81c974ee6fc8d9dbff2b84f5b73e3ff0830ae1a264005674af8218a",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.1",
    "stem": "A company stores sensitive research data in an S3 bucket encrypted with a symmetric, customer managed AWS KMS key (CMK). The bucket and the CMK reside in Account A. The Analytics team in Account B assumes a cross-account IAM role to access the objects. Attempts to download objects result in AccessDenied errors for kms:Decrypt. What should the Solutions Architect do to resolve this?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Add the Analytics role\u2019s ARN as a principal in the CMK key policy with kms:Decrypt permission.",
      "B": "Create an IAM policy in Account B granting kms:Decrypt on the CMK and attach it to the Analytics role.",
      "C": "Add a bucket policy on the S3 bucket in Account A granting s3:GetObject to the Analytics role.",
      "D": "Create an alias of the CMK in Account B and grant the Analytics role kms:Decrypt on that alias."
    },
    "explanation": "Cross-account KMS access must be granted in the CMK key policy. IAM policies alone cannot override a key policy. Bucket policies control S3 actions but cannot grant KMS permissions."
  },
  {
    "id": "5745ed022f116ad6abfc141980a74692c0b81e0eb74a4f16fec02d3ded6cfafc",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.3",
    "stem": "A security requirement mandates using a customer-supplied key (BYOK) imported into AWS KMS and enforcing annual rotation. After importing the key material and attempting to enable automatic key rotation, the console reports that rotation cannot be enabled. Which design change meets both requirements with the least operational overhead?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "Use a symmetric CMK generated by AWS KMS and enable automatic rotation.",
      "B": "Periodically import new key material under the same imported CMK each year to simulate rotation.",
      "C": "Use an asymmetric CMK for encryption and enable automatic rotation via AWS KMS.",
      "D": "Create a new imported CMK each year and update applications to reference the new CMK alias."
    },
    "explanation": "AWS KMS only supports automatic rotation on AWS-generated symmetric CMKs. To rotate an imported key, you must manually import new key material under the same CMK. Creating a new CMK each year increases operational overhead."
  },
  {
    "id": "1b5687b094b682f1b3e996fa1f0f9e7594ee99d235aab0cccaebdf505f3100ed",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.3",
    "stem": "An organization must enforce WORM retention on S3 objects for exactly 7 years so that objects cannot be deleted or overwritten during the retention period, and then allow administrators to delete objects after 7 years. Which S3 feature configuration meets this requirement?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Enable S3 Object Lock in compliance mode with a 7-year retention period.",
      "B": "Enable S3 versioning and apply a lifecycle rule to expire delete markers after 7 years.",
      "C": "Enable S3 Object Lock in governance mode and apply a lifecycle expiration of 7 years.",
      "D": "Enable default SSE-KMS encryption and apply a lifecycle rule to delete objects after 7 years."
    },
    "explanation": "Compliance mode Object Lock enforces WORM retention for the specified period and then allows object deletion after expiration. Governance mode can be bypassed by privileged users and versioning alone does not enforce WORM."
  },
  {
    "id": "d1d32a86d99e3496a7475887ee7d1ca215ab27778381456de5d31b20698bbb35",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.1",
    "stem": "A Solutions Architect configures cross-region replication (CRR) for an S3 bucket using server-side encryption with AWS KMS (SSE-KMS). Replicated objects appear in the destination bucket, but accessing them fails with AccessDenied on kms:Decrypt. What action resolves this error?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Modify the destination CMK key policy to grant the S3 replication role kms:Decrypt.",
      "B": "Modify the source CMK key policy to grant the S3 replication role kms:GenerateDataKey permission.",
      "C": "Switch the destination bucket to use the AWS managed S3 key (SSE-S3).",
      "D": "Enable automatic cross-region replication of CMK materials."
    },
    "explanation": "S3 replication with SSE-KMS calls GenerateDataKey on the source CMK to encrypt the replication data key. The replication IAM role must be granted kms:GenerateDataKey in the CMK key policy."
  },
  {
    "id": "8c355c5d09e450a91b5b712d5e3256418d2e90208451aa7dcef279dc902622d6",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.3",
    "stem": "A database administrator has enabled SSL encryption on an Amazon RDS MySQL instance. The application team installed the RDS CA certificate on their client servers and configured the MySQL client to use SSL, but connections still appear unencrypted. Which action ensures that all connections to the database are encrypted in transit?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set rds.force_ssl=1 in the DB parameter group and reboot the DB instance.",
      "B": "Require TLS 1.2 in the client connection string.",
      "C": "Import the SSL certificate into AWS Certificate Manager and attach it to RDS.",
      "D": "Place an Application Load Balancer in front of the RDS instance with SSL termination."
    },
    "explanation": "Enabling rds.force_ssl=1 forces all connections to use SSL. Without it, clients may still connect over plain text even if SSL is available."
  },
  {
    "id": "26159cf3f4e371eb08333adf9ac59b260b77d8dac9378314c590e810beb97366",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "2.1",
    "stem": "An organization uses AWS Backup to manage EFS file system backups. The backup vault default encryption is the AWS managed key, but compliance requires backups be encrypted with a customer managed CMK. Despite specifying the CMK in the backup plan, backups remain encrypted with the AWS managed key. What should the architect do?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Create a new backup vault encrypted with the desired CMK and update the backup plan to use that vault.",
      "B": "Specify the CMK in the EFS resource tags used by AWS Backup.",
      "C": "Update the EFS file system encryption settings to use the CMK.",
      "D": "Add a resource-based policy on the CMK to allow AWS Backup service principal encryption."
    },
    "explanation": "AWS Backup encrypts backups according to the vault\u2019s encryption configuration. You must create or use a vault that is encrypted with the required CMK and assign backups to that vault."
  },
  {
    "id": "82838d82de167a0f7d2466e047a79952f209456db7250aad0919d1b681e4237d",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.3",
    "stem": "A Solutions Architect creates a DynamoDB global table spanning two regions. Each region uses a customer managed CMK for at-rest encryption. After enabling replication, replication fails with AccessDenied on kms:Encrypt. What must the architect do to fix this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add the DynamoDB service principal to each regional CMK key policy with kms:Encrypt and kms:Decrypt permissions.",
      "B": "Use an AWS owned CMK for the global table, since custom CMKs are not supported.",
      "C": "Disable encryption on the global table and recreate it with custom CMKs.",
      "D": "Share the CMK from the primary region with the secondary region via multi-region replica."
    },
    "explanation": "DynamoDB replication uses the regional CMK to encrypt data. The service principal (dynamodb.amazonaws.com) must be granted kms:Encrypt/Decrypt in each CMK\u2019s key policy."
  },
  {
    "id": "1a9959b4895dc3d231fe71d29ceab835c49df18a0b10db5fc675188d99b62f6f",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.1",
    "stem": "A company uses SSE-C (customer-provided encryption keys) to encrypt objects in an S3 bucket. They want to serve these objects through CloudFront. Access attempts via CloudFront fail. Which approach allows CloudFront to serve the objects securely with minimal operational effort?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "Migrate to SSE-KMS and configure CloudFront to forward the KMS decryption header.",
      "B": "Migrate to SSE-S3 encryption and configure CloudFront to use HTTPS.",
      "C": "Configure CloudFront to forward the customer-provided key in request headers.",
      "D": "Use signed CloudFront URLs including SSE-C parameters."
    },
    "explanation": "CloudFront does not support SSE-C. Migrating to SSE-S3 simplifies integration, and CloudFront will retrieve objects over HTTPS without additional header handling."
  },
  {
    "id": "f0cc2a09fc0c7d0337f3d8f3536fa7c50f29f21e3c939821678f38417658fc17",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.1",
    "stem": "An application in a private subnet accesses an S3 bucket over a VPC endpoint. A security audit shows data is sent over plaintext HTTP. Which change ensures that data is encrypted in transit without adding NAT gateway costs?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a VPC endpoint policy requiring aws:SecureTransport=true.",
      "B": "Modify the application to use the bucket\u2019s HTTPS endpoint (https://bucket.s3.amazonaws.com).",
      "C": "Enable S3 Transfer Acceleration.",
      "D": "Add a network ACL rule to block outbound port 80."
    },
    "explanation": "Using the HTTPS endpoint for S3 encrypts traffic over the VPC endpoint. Requiring aws:SecureTransport only blocks HTTP but does not enforce encryption, and blocking port 80 would break connectivity."
  },
  {
    "id": "81591d07155c1abc9505a38803bb1d9499269de2049c01469d9cff194824f776",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.1",
    "stem": "A Solutions Architect generates a pre-signed URL for an S3 object encrypted with SSE-KMS. Users receive AccessDenied: missing x-amz-server-side-encryption header when using the URL. How can the architect generate a working pre-signed URL without modifying bucket policies?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Include x-amz-server-side-encryption and x-amz-server-side-encryption-aws-kms-key-id headers in the pre-signed URL.",
      "B": "Use a pre-signed URL assuming SSE-S3 encryption instead of SSE-KMS.",
      "C": "Switch the object to SSE-S3 and generate a new pre-signed URL.",
      "D": "Grant kms:Decrypt permission to each user\u2019s IAM policy."
    },
    "explanation": "For SSE-KMS objects, the request must include the encryption headers. Pre-signed URLs must be generated with those headers so that S3 can perform decryption."
  },
  {
    "id": "73f746d2689ec566972a6560fa6774e19b8fb19b49b3a6b972e488ca5b3d590d",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.4",
    "stem": "A company uses AWS Backup with a plan that moves EBS snapshots to cold storage after 30 days and expires them after 90 days. After 90 days, snapshots remain in cold storage and are not deleted. Why?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Cold storage lifecycle retention is calculated from the time the recovery point enters cold storage (30 days + 90 days).",
      "B": "AWS Backup does not support deletion of cold storage recovery points.",
      "C": "Lifecycle expirations only apply to standard storage; cold storage resources are retained indefinitely.",
      "D": "The backup vault enforces a default 365-day retention override for cold storage."
    },
    "explanation": "In AWS Backup, the expiration clock for cold storage begins when a recovery point is moved to cold storage. Thus expiration occurs 30+90 days after backup creation."
  },
  {
    "id": "6bc9ffa1fbf3f4dbd252d5d6b9979965a135c1118e7114ca082b4910cff12be5",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "2.1",
    "stem": "A Solutions Architect imported a TLS certificate into ACM and attached it to an Application Load Balancer. ACM shows the certificate status as 'issued', but the load balancer continues serving the old certificate. Why?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Imported ACM certificates do not auto-deploy on ELBs; you must manually re-associate them upon rotation.",
      "B": "The certificate must be in the us-east-1 region to serve via Application Load Balancer.",
      "C": "DNS validation is required for imported certificates to auto-rotate.",
      "D": "The ALB needs to be rebooted to pick up the new certificate."
    },
    "explanation": "Only ACM-issued certificates (generated by ACM) can be auto-rotated and auto-deployed to ELBs. Imported certificates require manual re-association when updating."
  },
  {
    "id": "500c55bf46f767d7655fe3e994dc242309d60acd76941d3922217bd9131dce7f",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.1",
    "stem": "An application team in Account X creates encrypted EBS snapshots using a custom CMK. A DR team in Account Y can see the shared snapshots but cannot create volumes from them due to encryption errors. What must the Solutions Architect do?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add the DR account\u2019s IAM principal to the CMK key policy in Account X with kms:Decrypt and kms:ReEncrypt permissions.",
      "B": "Share the CMK alias with Account Y and let them reference it when restoring.",
      "C": "Create a multi-region replica CMK in Account Y and update the snapshot to use it.",
      "D": "Enable cross-account key sharing in AWS Backup settings."
    },
    "explanation": "To restore encrypted snapshots in another account, the source account\u2019s CMK key policy must explicitly grant the target account principal decrypt and re-encrypt permissions."
  },
  {
    "id": "c675986ffd847f45d46bdb9820d013baf895756584013e2a8be8b8208a3b6928",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.2",
    "stem": "A Solutions Architect configures AWS Secrets Manager to use a customer-managed CMK for secret encryption and enables automatic rotation via the built-in Lambda. Rotation fails with AccessDenied on kms:GenerateDataKey. What change resolves this?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Update the CMK key policy to allow the Secrets Manager service principal and the rotation Lambda\u2019s execution role to call kms:GenerateDataKey and kms:Decrypt.",
      "B": "Switch to the default AWS managed CMK for Secrets Manager.",
      "C": "Attach an IAM policy to the rotation Lambda granting kms:GenerateDataKey.",
      "D": "Disable automatic rotation and perform manual rotation via AWS CLI."
    },
    "explanation": "Secrets Manager and the rotation Lambda require CMK permissions in the key policy. IAM policies alone are insufficient to grant KMS access when the key policy denies access."
  },
  {
    "id": "0fc84eb231da170115e4a9d8d8e12c3d4718d6f641b98fefb0bd184d0d8adb7c",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.1",
    "stem": "A Kinesis Data Firehose stream delivers logs to an S3 bucket with SSE-KMS encryption using a customer-managed CMK. Policy dictates that only Firehose can decrypt the logs. Despite granting kms:Decrypt only to the Firehose role, developers still decrypt objects. Which feature enforces that only Firehose can decrypt?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a condition in the CMK key policy requiring kms:EncryptionContext:aws:firehose:arn to match the Firehose stream ARN.",
      "B": "Add an explicit IAM deny for kms:Decrypt on all developer roles.",
      "C": "Switch to SSE-C and let Firehose supply the key during delivery.",
      "D": "Add an S3 bucket policy denying GetObject unless the request comes from Firehose."
    },
    "explanation": "Using an encryption context condition in the key policy binds decryption to Firehose\u2019s context, preventing other principals\u2014even if granted kms:Decrypt\u2014from decrypting without the correct context."
  },
  {
    "id": "766f8dc75ae818b306abbe2a1478c47dbd72a27b356de7d90959918d31ed394e",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "2.1",
    "stem": "A fintech application needs to process transaction events for customer trades. Each trade\u2019s events must be processed in order and independently scaled per customer ID, while ensuring loose coupling between services. Which architecture meets these requirements with minimal operational overhead?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon Kinesis Data Streams with a single stream and a Lambda consumer to process records.",
      "B": "Use an Amazon SQS FIFO queue with MessageGroupId set to the customer ID and trigger a Lambda function off the queue.",
      "C": "Publish events to an Amazon SNS topic and have multiple Lambda subscribers share a DynamoDB table for ordering.",
      "D": "Stream trades into DynamoDB and use DynamoDB Streams with a Lambda processor for ordering."
    },
    "explanation": "SQS FIFO with MessageGroupId guarantees ordering per customer and allows parallel Lambda processing across groups with minimal operations. SNS doesn\u2019t guarantee order; Kinesis requires shard management; DynamoDB Streams isn\u2019t designed for fine\u2010grained per\u2010customer scaling."
  },
  {
    "id": "30779d3e397fa9fbe71243ba2f7c0c65b694c7a619de0f43f5fc1391cc392093",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "2.1",
    "stem": "A media distribution API uses API Gateway backed by Lambda to serve content metadata. During peak traffic, Lambda throttling and high invocation rates drive up cost. Without modifying the backend code, which solution most effectively reduces Lambda invocations and lowers cost?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable API Gateway response caching on the GET method with an appropriate TTL.",
      "B": "Place a CloudFront distribution in front of API Gateway with default caching behavior.",
      "C": "Increase the Lambda reserved concurrency to handle the peak load.",
      "D": "Use AWS Global Accelerator to direct traffic to the lowest\u2010latency region."
    },
    "explanation": "API Gateway caching directly reduces invocations by serving cached responses. CloudFront can cache but requires custom behaviors and invalidation. Increasing concurrency doesn\u2019t reduce invocations; Global Accelerator only optimizes routing."
  },
  {
    "id": "8604483f364602879dc1da2d40f43c54ffcd510af4ba93eb9e2326fc5550cc48",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "2.1",
    "stem": "Your development team needs to run a CPU\u2010intensive image processing service that may use up to 4 vCPUs and can run for up to 5 minutes per job. You want minimal infrastructure management and automatic scaling. Which compute option is most appropriate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Package the logic as an AWS Lambda function configured with maximum memory to achieve the required CPU.",
      "B": "Deploy the service as containers on Amazon ECS with AWS Fargate and configure task CPU and memory.",
      "C": "Launch an EC2 Auto Scaling group of instances and run the service in Docker containers.",
      "D": "Submit the jobs to AWS Batch with a managed compute environment."
    },
    "explanation": "ECS on Fargate lets you define precise CPU and memory for containers with no server management and automatic scaling. Lambda may not provide consistent CPU; EC2 ASG adds operational overhead; AWS Batch is overkill for a microservice."
  },
  {
    "id": "be77e4b5903ca9afed2733d876ca3463f31cae4cf485d6909d68bb3de8918a22",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "2.1",
    "stem": "A data analytics pipeline must process 10,000 records per minute, running an identical workflow per record and limiting concurrent executions to 100 to prevent downstream overload. You need granular status tracking and error handling. Which solution meets these requirements without custom polling?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Use AWS Step Functions Standard Workflow with a Map state configured with MaxConcurrency set to 100.",
      "B": "Send records to an SQS FIFO queue and invoke Lambda functions with reserved concurrency of 100.",
      "C": "Orchestrate tasks using Amazon SWF with concurrency throttles.",
      "D": "Submit each record as a job to AWS Batch with a max\u2010running\u2010jobs limit of 100."
    },
    "explanation": "Step Functions Map state natively supports per\u2010item workflows, status tracking, error handling, and a MaxConcurrency setting. SQS+Lambda lacks granular multi\u2010step orchestration; SWF is deprecated; Batch doesn\u2019t provide first\u2010class workflow visibility."
  },
  {
    "id": "c997d47f37b778cc70ec6d3ef9e73d7fcdfe50f71f2672e4e3b79d2d5b9175f5",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "2.1",
    "stem": "You are migrating a monolithic application into containers on AWS. The team requires tight integration with an Application Load Balancer, automatic scaling per service, and minimal cluster management overhead. Which orchestration choice best fits?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy containers in Amazon EKS on self\u2010managed EC2 nodes.",
      "B": "Deploy containers in Amazon ECS on AWS Fargate with an ALB integration.",
      "C": "Deploy containers in Amazon ECS on EC2 with an Auto Scaling group.",
      "D": "Deploy containers in Amazon EKS on AWS Fargate."
    },
    "explanation": "ECS on Fargate integrates seamlessly with ALB and handles scaling without cluster management. EKS on Fargate is also serverless but introduces Kubernetes control\u2010plane complexity; ECS on EC2 or EKS on EC2 requires managing nodes."
  },
  {
    "id": "9598171a74c617912e8bd29aa1c24fe472aef59dd31f5f73a658dcef9f1aeefa",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "2.1",
    "stem": "Account A must publish inventory update events to Account B. The solution must guarantee at-least-once delivery, enforce cross-account isolation, and avoid continuous polling. Which architecture is most appropriate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create an EventBridge event bus in Account B, grant Account A permission to PutEvents, and use a rule to invoke Lambda.",
      "B": "Publish to an SNS topic in Account A and subscribe an SQS queue in Account B.",
      "C": "Create an SQS FIFO queue in Account B and grant Account A IAM permissions to SendMessage.",
      "D": "Deploy an API Gateway in Account B and have Account A send HTTP POST requests."
    },
    "explanation": "A cross-account SQS FIFO queue with a SendMessage IAM policy offers at-least-once delivery, ordering if needed, and no polling by Account A. EventBridge retention is limited; SNS+SQS adds extra components; API Gateway is synchronous."
  },
  {
    "id": "aec5b24ab011b86a379a9868c4878e26a353d91b3867032028c8f5501fcecc6b",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "2.1",
    "stem": "A global API must deliver sub-100ms latency to users worldwide and be protected by AWS WAF. Which endpoint type and deployment pattern should you choose?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Create a Regional API in API Gateway, deploy a CloudFront distribution in front of it, and associate AWS WAF.",
      "B": "Create an edge-optimized API Gateway endpoint (uses CloudFront) and enable WAF directly.",
      "C": "Create a Private API Gateway endpoint in a VPC and use AWS Global Accelerator.",
      "D": "Create an HTTP API in API Gateway and enable WAF integration."
    },
    "explanation": "Regional API + CloudFront + WAF provides global low latency and centralized protection. Edge-optimized endpoints cannot directly attach WAF before CloudFront; Private APIs are internal; HTTP APIs have limited WAF support."
  },
  {
    "id": "9a8226457e83bb4bc86fc47cad17cba8065115e7c93bb0928984d1eafd0cbc1e",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "2.1",
    "stem": "Your web tier uses an Application Load Balancer routing to ECS tasks, but sudden traffic spikes cause ECS to lag when provisioning tasks, dropping requests. You want to fully decouple spikes and buffer requests. Which change accomplishes this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Introduce an Amazon SQS queue between the ALB and ECS: use a Lambda target for the ALB to enqueue requests, and have ECS poll the queue.",
      "B": "Replace the ALB with API Gateway REST API to front ECS tasks.",
      "C": "Configure AWS Global Accelerator in front of the ALB.",
      "D": "Enable ECS predictive scaling for the service."
    },
    "explanation": "By enqueuing requests in SQS, the web tier is decoupled from ECS and can absorb spikes. API Gateway adds cost and latency; Global Accelerator only optimizes routing; predictive scaling still allows dropped requests during scale-out."
  },
  {
    "id": "5e2e5d6fccf545a58b98ddc3fd28b183209fdf83b73b8fe25e533686d4a323ec",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "2.1",
    "stem": "A stateful application running on AWS Fargate requires a POSIX-compliant shared filesystem mountable by multiple tasks concurrently. Which storage solution should you use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon EFS with Fargate mount target.",
      "B": "Amazon EBS volume attached to each Fargate task.",
      "C": "Amazon S3 mounted via a FUSE driver.",
      "D": "Amazon FSx for Lustre mounted on Fargate."
    },
    "explanation": "Amazon EFS natively integrates with Fargate for POSIX-compliant shared file storage. EBS can\u2019t be attached to Fargate; S3 via FUSE isn\u2019t supported; FSx for Lustre isn\u2019t directly supported on Fargate."
  },
  {
    "id": "e21e4713037396f9dc33180b69a88de1399e00d5c56de5e89ae4f81c6023db14",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "2.1",
    "stem": "A microservices-based system using Lambda functions suffers from noisy-neighbor issues: one function spikes and overloads a shared database. You need to protect the database while maintaining loose coupling. What do you implement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set a reserved concurrency limit on the noisy function to throttle its invocations.",
      "B": "Use AWS WAF to block excessive requests to the function\u2019s API.",
      "C": "Migrate orchestration to AWS Step Functions to serialize calls.",
      "D": "Place an Application Load Balancer in front of the function with rate limiting."
    },
    "explanation": "Reserved concurrency on a Lambda function guarantees it can only run up to the set limit, preventing it from overwhelming shared resources. WAF and ALB rate limiting don\u2019t apply to Lambda; Step Functions serializes calls, reducing decoupling."
  },
  {
    "id": "43e56ee2823e0c8036d1060b91be4ec196db4f06f2d9e7d631b782a022a85736",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "2.1",
    "stem": "A DynamoDB Streams consumer Lambda sends updates to an Amazon OpenSearch cluster. When the cluster is unavailable, retries exhaust Lambda timeouts and drop records. How do you redesign for reliability and decoupling?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Stream DynamoDB changes into Kinesis Data Streams and process with Lambda with manual checkpointing.",
      "B": "Configure the Lambda failure destination to an SQS dead\u2010letter queue and process retries asynchronously.",
      "C": "Migrate the pipeline to Amazon MSK Connect to buffer records.",
      "D": "Use EventBridge to capture DynamoDB events and route to the cluster."
    },
    "explanation": "Using a DLQ decouples processing failures from the Lambda invocation and allows later retries. Kinesis adds complexity; MSK Connect is overkill; EventBridge doesn\u2019t integrate with DynamoDB Streams."
  },
  {
    "id": "74179cd36ac553020b1b632971e598aaf791516114fe5b8d102679db1bcc0c0e",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "2.1",
    "stem": "A workflow publishes two types of notifications: one for thumbnail generation and another for audit logging. The thumbnail service is slower, but audit logs must never be delayed. How do you design a loosely coupled, parallel fan-out?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Publish messages to an SNS topic and subscribe two SQS queues: one for thumbnails, one for audit logs.",
      "B": "Publish to two separate SNS topics, one for each subscriber.",
      "C": "Use a Step Functions parallel state to invoke both Lambda functions synchronously.",
      "D": "Use Amazon MQ with two consumers."
    },
    "explanation": "SNS with multiple SQS subscriptions decouples subscribers, lets each consume at its own pace, and ensures audit logs aren\u2019t blocked. Two SNS topics duplicates publisher logic; Step Functions is synchronous; Amazon MQ introduces brokers to manage."
  },
  {
    "id": "5ed135df6e7623e7045270d1bf3a3b0ee9f05a3680444976d4946064c7afe4ff",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "2.1",
    "stem": "An application is deployed in us-east-1 and eu-west-1 behind regional API Gateway REST APIs. Users must use a single DNS name, benefit from edge caching for static assets, and fail over automatically if one region becomes unhealthy. Which architecture fulfills these requirements?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Deploy a CloudFront distribution with both regional API Gateway endpoints in an origin group (for failover) and use a Route 53 alias to the distribution.",
      "B": "Configure Route 53 latency\u2010based routing directly to each API Gateway endpoint and enable caching on the REST APIs.",
      "C": "Use an edge-optimized API Gateway endpoint type and configure failover in API Gateway.",
      "D": "Place an Application Load Balancer in each region and use AWS Global Accelerator for failover."
    },
    "explanation": "CloudFront origin groups support automatic failover and caching at edge locations. Route 53 latency records don\u2019t provide caching or automatic failover of origin; edge\u2010optimized API Gateway doesn\u2019t handle multi\u2010origin failover; Global Accelerator routes traffic but doesn\u2019t cache."
  },
  {
    "id": "b2ea153eff740899959b1b901509ccf95b15ffe756e798b7bd7eb7d01efb898b",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "2.1",
    "stem": "An Aurora MySQL cluster experiences unpredictable read traffic spikes. You need to auto-scale read capacity to meet demand while minimizing operational complexity. Which solution should you implement?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Enable Aurora Auto Scaling for Aurora Replicas based on average CPU utilization or replica lag.",
      "B": "Deploy an Amazon ElastiCache cluster in front of the database and cache all reads.",
      "C": "Migrate the database to DynamoDB for automatic scaling.",
      "D": "Implement client-side caching in the application layer."
    },
    "explanation": "Aurora Auto Scaling automatically adds or removes replicas based on metrics, providing read scaling with no infrastructure to manage. ElastiCache adds another layer; migration to DynamoDB is disruptive; client caching cannot always guarantee freshness."
  },
  {
    "id": "458219aeb0c5eff7d2bc1d170e6fd0fbeabbe4d8687c460833e2a744d8830df4",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "2.1",
    "stem": "A mobile application uploads user-generated images to a backend. You need to decouple the upload process, minimize backend compute, and ensure direct, secure, and scalable uploads to Amazon S3. What architecture do you choose?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Have clients request S3 pre-signed PUT URLs from a Lambda function, then upload directly to S3.",
      "B": "Use API Gateway REST API to accept binary image payloads and proxy them to S3 via Lambda.",
      "C": "Set up AWS Transfer Family for S3 and have clients use SFTP.",
      "D": "Deploy an AWS DataSync agent on users\u2019 devices to sync images to S3."
    },
    "explanation": "Pre-signed S3 URLs decouple uploads from your backend, allow direct secure uploads to S3, and scale without compute. API Gateway/Lambda proxies consume backend resources; Transfer Family and DataSync aren\u2019t designed for mobile HTTP uploads."
  },
  {
    "id": "f465649defbc868133c93ec473f9c7d55ae0b6c18707da99daf8889ffd530b25",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "2.2",
    "stem": "A financial services company runs a critical trading platform in us-east-1. They currently use an RDS Multi-AZ PostgreSQL deployment and must achieve an RTO of under 15 minutes and an RPO under 5 minutes in case of a full region loss. Which disaster-recovery strategy should you implement to meet these requirements with minimal cost and operational overhead?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure a cross-region read replica in us-west-2 and promote it upon failure.",
      "B": "Use automated backups and restore the DB instance in us-west-2 when us-east-1 fails.",
      "C": "Maintain a warm-standby deployment in us-west-2 with a scaled-down Multi-AZ RDS instance continuously running and sync data via automated snapshots or logical replication.",
      "D": "Implement a pilot-light architecture in us-west-2 by storing snapshots in S3 and spinning up a full Multi-AZ deployment only after failover."
    },
    "explanation": "A warm standby keeps a live, scaled-down Multi-AZ database in another region, enabling fast promotion (RTO <15 min) and near-zero data loss (RPO <5 min). Cross-region read replicas aren\u2019t fully writable, backups/restore and pilot light have longer RTOs."
  },
  {
    "id": "1e9f968a58b60ea418108797c00467af530f87fdbff07ce623c5d09d6ed3bce8",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "2.2",
    "stem": "A global media streaming service needs to serve content with the lowest latency worldwide and maintain continuous availability even if an entire AWS region becomes unreachable. Which design best meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy a single CloudFront distribution with origin failover pointing to an S3 bucket in another region.",
      "B": "Deploy the application in multiple regions, register endpoints in Route 53 with latency-based routing and health checks, and fail over to healthy regions automatically.",
      "C": "Use a central ALB in us-east-1 with cross-region Application Load Balancer endpoints in other regions.",
      "D": "Use Amazon Global Accelerator with a single origin in us-east-1 and rely on edge caching for availability."
    },
    "explanation": "Latency-based routing across active-active regional deployments ensures global low latency and failover if a region fails. CloudFront origin failover handles only static content; a central ALB or single accelerator origin still has a single region dependency."
  },
  {
    "id": "924854bc5729bac6347319dc4944604d4a87beabfec4f65679813c2061321901",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "2.2",
    "stem": "An online gaming company experiences connection timeouts and errors during RDS Multi-AZ failovers because client libraries retry existing connections rather than acquiring new endpoints. How can they minimize failover impact and maintain session routing with minimal code changes?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Introduce Amazon RDS Proxy between the application and the database to pool and manage connections across failovers.",
      "B": "Add Route 53 DNS failover records pointing to the standby endpoint.",
      "C": "Use custom failover logic in the application to refresh endpoints after a DNS TTL expires.",
      "D": "Implement a cross-region read replica and update connection strings upon failover."
    },
    "explanation": "RDS Proxy abstracts the database endpoint and automatically manages and reuses connections during failover, minimizing retries and errors. DNS failover or custom logic increases complexity and TTL delays."
  },
  {
    "id": "c40107062364c6018d92dd556bf1398f2312266d90e5dae1ba5ee7b6fa400a5d",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "2.2",
    "stem": "A marketing site hosted in S3 must fail over to a secondary S3 bucket in another region if the primary bucket becomes unavailable. The solution must automate origin switching and require no application changes. Which architecture meets this requirement?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Route 53 failover record sets pointing to each bucket\u2019s static website endpoint with health checks.",
      "B": "Configure S3 Cross-Region Replication and rely on S3 to auto-switch on failure.",
      "C": "Deploy an ALB in front of each bucket and use health checks to route traffic.",
      "D": "Use a CloudFront distribution with an origin group: primary S3 origin in region A and secondary origin in region B."
    },
    "explanation": "CloudFront origin groups automatically fail over between origins without DNS changes. Route 53 records lack built-in bucket health checks, and S3 replication alone doesn\u2019t switch endpoints."
  },
  {
    "id": "af2a1224133b2838c0f64beedfd4198d068d71b2890cb42116abb6b2c8d7b52a",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "2.2",
    "stem": "A genomic research application uses an Amazon EFS file system in us-east-1. To protect against a full-region outage, the file system must be available with minimal data loss in another region. Which solution provides the fastest restore with minimal operational effort?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Take EFS backups to S3 and, upon failure, restore and mount a new EFS in the secondary region.",
      "B": "Use AWS DataSync to replicate the EFS file system continuously to another EFS in the secondary region.",
      "C": "Configure EFS replication (launched via CLI) to asynchronously replicate data to the secondary region.",
      "D": "Use AWS Backup cross-region vault copy from the primary EFS to restore after region failure."
    },
    "explanation": "DataSync can continuously replicate EFS file systems for near-real-time sync. AWS Backup is periodic and slower; EFS lacks native cross-region replication; snapshot/restore has higher RTO."
  },
  {
    "id": "63324b122377e0b936a10783094252521ffb399530b4d6976f81bebf1bfe9377",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "2.2",
    "stem": "A global user profile database must remain available even if a region is lost. The application requires read/write capability in any region with <1 second replication lag. Which database solution should be used?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Aurora Global Database with multi-region read/write and fast regional failover.",
      "B": "RDS MySQL with cross-region read replicas promoted on failure.",
      "C": "DynamoDB global tables with eventual consistency.",
      "D": "Self-managed database on EC2 with custom replication scripts."
    },
    "explanation": "Aurora Global Database provides low-latency, multi-region replication (<1 second lag) and can fail over quickly. RDS read replicas are read-only until promotion and have higher lag; DynamoDB global tables are eventually consistent; self-managed adds complexity."
  },
  {
    "id": "54e33d1f2de47bc9dadfc3d6bee72de99a364c4128baf627023b3ca28034c329",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "2.2",
    "stem": "A legacy JMS-based messaging application requires broker high availability across AZs without code changes. Failover should be automatic if the active broker fails. Which AWS service/configuration achieves this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy ActiveMQ on EC2 instances in each AZ behind an NLB.",
      "B": "Migrate to Amazon SQS FIFO queues with client-side JMS adapters.",
      "C": "Use Amazon MQ for ActiveMQ with a single broker in multi-AZ deployment.",
      "D": "Use Amazon SNS topics and subscribing clients."
    },
    "explanation": "Amazon MQ multi-AZ ActiveMQ brokers provide automatic, transparent failover to a standby broker without code changes. EC2 deployments require custom clustering; SQS/SNS require application refactoring."
  },
  {
    "id": "92a8adf5bd9aa393a1329cc15aa8730f23d0794830b63b81cd64d30e66c3f332",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "2.2",
    "stem": "Teams have manually baked AMIs and launched EC2 instances in each AZ. Configuration drift occurs, compromising availability. Which combination most effectively enforces immutable, consistent compute environments across AZs?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS OpsWorks to define Chef recipes for each AZ and automate deployments.",
      "B": "Use AWS Image Builder to produce standardized AMIs and deploy via an Auto Scaling group with instance refresh across all AZs.",
      "C": "Implement AWS Config rules to detect drift and trigger SSM Automation to patch instances.",
      "D": "Use AWS Systems Manager State Manager to push configuration changes to running instances."
    },
    "explanation": "Image Builder plus Auto Scaling with instance refresh creates immutable, identical AMIs and ensures Auto Scaling deployments across AZs are consistent. Configuration management or drift detection doesn\u2019t achieve immutability."
  },
  {
    "id": "9b8b31a9680c696760d74660f5245330394b08403a37bd13f84b2915144d498b",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "2.2",
    "stem": "A multi-region database must allow writes in one region and near-instant reads in another region with an RPO under 5 seconds. The database engine should be MySQL-compatible. Which AWS feature best satisfies these requirements?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "MySQL RDS cross-region read replicas promoted on failover.",
      "B": "MySQL RDS Multi-Master across regions.",
      "C": "Aurora MySQL with Multi-AZ within the same region.",
      "D": "Amazon Aurora Global Database configured between the primary and secondary regions."
    },
    "explanation": "Aurora Global Database provides MySQL compatibility, cross-region write forwarding, and replication lag typically under 1 second. RDS cross-region read replicas are read-only until promotion; RDS Multi-Master isn\u2019t supported cross-region; Multi-AZ is same-region only."
  },
  {
    "id": "e2c148f5b36b8c87d45515a261be6b36b1478d161d41de4ca0292de4c2ab87b9",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "2.2",
    "stem": "A 3-tier application uses an EBS-backed instance for shared file storage across AZs. To eliminate this single point of failure, the file store must be highly available across AZs natively. Which change meets this goal with minimal application modifications?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create EBS snapshots every 5 minutes and mount restored volumes when AZ fails.",
      "B": "Migrate the file storage to Amazon EFS in regional, multi-AZ mode.",
      "C": "Deploy a third EC2 file server in a separate AZ with replication scripts.",
      "D": "Use AWS Storage Gateway in each AZ against a central S3 bucket."
    },
    "explanation": "EFS is a regional service providing multi-AZ file storage with minimal application changes. EBS snapshots and EC2 servers add operational overhead; Storage Gateway requires new integration."
  },
  {
    "id": "dc4ae04b02155fc787c215fd42241d4a7f50b6f611fa530471cb3ed8873fed9a",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "2.2",
    "stem": "A serverless image-processing API must remain available if an entire region fails. The API is fronted by API Gateway and invokes Lambda functions. Which design pattern provides the most resilient multi-region failover?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Deploy the API Gateway and Lambda functions in two regions, configure a custom domain with Route 53 active-passive failover and health checks.",
      "B": "Use an edge-optimized API Gateway endpoint and rely on Lambda regional replication.",
      "C": "Front the API Gateway with CloudFront and trust it to serve from the secondary region automatically.",
      "D": "Use AWS Global Accelerator in front of a regional API Gateway endpoint in the primary region only."
    },
    "explanation": "Regional deployments in two regions with Route 53 failover health checks ensure application-level failover. Edge-optimized endpoints and CloudFront don\u2019t automatically switch Lambda backends across regions."
  },
  {
    "id": "17142bfb790dc0ea742fd8787fe60a276e0468909d3b183f8d2009aa52abd78e",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "2.2",
    "stem": "A data center hosting critical ERP VMs must fail over to AWS with minimal downtime (<10 minutes). You need continuous block-level replication without modifying guest OS. Which AWS service should you use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Snowball Edge for bulk data transfer and on-demand import.",
      "B": "AWS Database Migration Service in CDC mode.",
      "C": "VM Import/Export of periodic AMIs.",
      "D": "AWS Application Migration Service for continuous block-level replication."
    },
    "explanation": "AWS Application Migration Service (CloudEndure) offers continuous block-level replication of VMs with automated failover in <10 minutes. Snowball and VM import are offline/batch; DMS is for databases only."
  },
  {
    "id": "2f3a8d34af2e980423d9a5d2942473306185b0cd02d636d01e05308a98d79563",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "2.2",
    "stem": "A team discovered that they are approaching the default limit for Application Load Balancers in a region. To prevent failed deployments that would reduce availability, what automation should be implemented?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Trusted Advisor to alert before limits are reached.",
      "B": "Use AWS Service Quotas integrated with CloudWatch alarms and an AWS Lambda function to request quota increases automatically.",
      "C": "Manually track limits in a spreadsheet and submit requests when 80% threshold is crossed.",
      "D": "Enable AWS Config rules to detect when the limit is reached and pause deployments."
    },
    "explanation": "Service Quotas with CloudWatch and Lambda can automate quota increase requests before limits are hit, avoiding manual errors. Trusted Advisor provides alerts but no automation; spreadsheets and Config rules are manual or reactive."
  },
  {
    "id": "1059d457cacfc06720e20664a6b0d56c0f4340c54d9d7dde70d9be90885b2915",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "2.2",
    "stem": "An enterprise backup policy requires daily snapshots of RDS databases in us-east-1 to be available in us-west-2 for recovery. The solution must be fully managed and support point-in-time restores. Which approach is best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Schedule a daily AWS DMS task to migrate snapshots to us-west-2.",
      "B": "Use S3 replication to copy snapshots from the backup bucket in us-east-1 to us-west-2.",
      "C": "Use AWS Backup vault with cross-region backup copy configured from us-east-1 to us-west-2.",
      "D": "Create manual RDS snapshot copy operations via Lambda daily."
    },
    "explanation": "AWS Backup cross-region copy automates snapshots to another region and supports point-in-time restore. DMS is for live migrations; S3 replication doesn\u2019t handle RDS snapshots; manual Lambda ops add operational overhead."
  },
  {
    "id": "fd9c770935ac47016e3a687ac64bb6725df24ed3b6866a4564d90a4472128206",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "2.2",
    "stem": "Your Application Load Balancer is still sending traffic to instances in AZ-B even though health checks show 5xx errors. You want to prevent AZ-B from receiving any requests when all targets there are unhealthy. What should you do?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Adjust the target group health-check settings (protocol, path, thresholds) so the ALB marks targets as unhealthy and stops routing traffic to AZ-B.",
      "B": "Enable cross-zone load balancing on the ALB so healthy targets in AZ-A serve all traffic.",
      "C": "Create a second ALB in AZ-A and use Route 53 weighted records to shift traffic.",
      "D": "Switch to a Network Load Balancer which will stop sending traffic to unhealthy instances automatically."
    },
    "explanation": "Properly configuring target group health checks ensures the ALB removes unhealthy targets and stops sending traffic. Cross-zone LB only balances; creating a new ALB adds complexity; NLB isn\u2019t suited to HTTP health checks."
  },
  {
    "id": "5c3771697fdff47764fff9626796558009db17f95b620008daba7a38ad403cde",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.1",
    "stem": "A  production relational database on EC2 requires sustained 20,000 IOPS with consistent sub-millisecond latency. Which EBS volume type should you provision to satisfy these requirements at the lowest cost?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "General Purpose SSD (gp3) with provisioned throughput",
      "B": "Throughput Optimized HDD (st1)",
      "C": "Provisioned IOPS SSD (io2)",
      "D": "Magnetic (standard) volumes"
    },
    "explanation": "gp3 maxes at 16,000 IOPS, st1 and standard are HDD and can\u2019t deliver low latency; only io2 supports sustained 20,000 IOPS with sub-millisecond latency."
  },
  {
    "id": "d6ac06c8a93b5014f4a09b9959e86ba990469e9ab72c662c3038754ad427a8c3",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.1",
    "stem": "An analytics application needs 400 MiB/s of sustained sequential throughput on block storage for 24/7 batch processing. Which EBS configuration achieves this most cost-effectively?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "gp3 with IOPS set to 3,000 and throughput set to 400 MiB/s",
      "B": "io2 with default IOPS",
      "C": "st1 with a 1 TiB volume",
      "D": "gp2 with a 1 TiB volume"
    },
    "explanation": "st1 baseline throughput is 80 MiB/s per TiB (only 80 MiB/s), gp2 is burst-based; only gp3 lets you provision 400 MiB/s directly at lower cost than io2."
  },
  {
    "id": "5328d27d61deea63c99610c4b447bcb67793bf39a17e3d7185211372a734d32f",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.1",
    "stem": "A cluster of 5,000 stateless Linux containers in multiple AZs must share a POSIX-compliant file system with high metadata operation rates. Which configuration meets this requirement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "EFS Standard with General Purpose performance mode",
      "B": "EFS Standard with Max I/O performance mode",
      "C": "Amazon FSx for Windows File Server",
      "D": "Amazon S3 with S3FS client"
    },
    "explanation": "Max I/O mode scales metadata operations beyond General Purpose; FSx Windows is SMB, S3FS is non-POSIX and high-latency."
  },
  {
    "id": "8c1c6509f60875f6c737377d4fe8fc069a77d49747fad0b2f5e88bd82a67e4e4",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.1",
    "stem": "An HPC workload requires massively parallel, sub-millisecond I/O and integration with an S3-based data lake for temporary scratch storage. Which storage solution is optimal?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "EFS with Provisioned Throughput",
      "B": "Amazon S3 Transfer Acceleration",
      "C": "Amazon FSx for Windows File Server",
      "D": "Amazon FSx for Lustre"
    },
    "explanation": "FSx for Lustre provides parallel POSIX I/O with S3 integration; EFS can\u2019t match HPC I/O, FSx Windows is SMB, Transfer Acceleration only improves upload speeds."
  },
  {
    "id": "4dcee3151ef4ef5a19b927f082016b915fcb1fb4a242dc7ce612fa07821b11bb",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.1",
    "stem": "A multi-AZ web tier requires a shared file system with low-latency access and automatic cross-AZ replication. Which service should you choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon EFS Standard",
      "B": "Amazon EFS One Zone",
      "C": "Amazon FSx for Windows File Server",
      "D": "Amazon S3"
    },
    "explanation": "EFS Standard replicates across AZs and provides POSIX semantics; One Zone is single-AZ, FSx Windows is SMB, S3 isn\u2019t a file system."
  },
  {
    "id": "398077c493c20f33a474b8a5ca41ea86ad75abe24829ecace53b5feebbbe6636",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.1",
    "stem": "A 3 TiB EFS file system experiences unpredictable peak workloads requiring bursts above its baseline throughput. You need to guarantee a minimum of 200 MiB/s. What do you do?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase the file system size to raise the burst credit balance",
      "B": "Switch the file system to Bursting Throughput mode",
      "C": "Enable EFS Provisioned Throughput at 200 MiB/s",
      "D": "Use EFS One Zone to reduce latency and increase baseline"
    },
    "explanation": "Only Provisioned Throughput guarantees a minimum rate; size increase only affects bursting credits, Switching throughput modes doesn\u2019t provide guaranteed rate, One Zone isn't relevant to throughput."
  },
  {
    "id": "0def8c2a848efdf402786659de9c0ee71d4fc1803a5153095383e3ca4aeffea2",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.1",
    "stem": "An application experiences over 10,000 GET requests per second for 1 KiB objects stored in S3. You need to minimize latency without modifying the application. Which approach is best?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Enable S3 Transfer Acceleration",
      "B": "Prepend object keys with a random hash prefix",
      "C": "Switch to S3 Standard-Infrequent Access",
      "D": "Deploy Amazon CloudFront with the S3 bucket as origin"
    },
    "explanation": "CloudFront caches objects at edge locations to reduce latency; Transfer Acceleration improves uploads, prefixing isn\u2019t needed on modern S3, IA increases latency."
  },
  {
    "id": "3e719d5ba2f9408e2298120ea1d19442d123e9d7512ded8cf30152e091606328",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.1",
    "stem": "You are migrating containerized workloads on AWS Fargate that require a shared POSIX file system for runtime state. Which storage option meets the requirements?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Attach a gp3 EBS volume to each Fargate task",
      "B": "Mount an Amazon EFS file system",
      "C": "Use Amazon S3 with the AWS SDK",
      "D": "Use AWS Systems Manager Parameter Store"
    },
    "explanation": "Fargate tasks can mount EFS for shared POSIX storage; you cannot attach EBS, S3 is object storage, Parameter Store is configuration store."
  },
  {
    "id": "eb9cd9e4defea365b8b8f085d6b652530dbe5a51f010e76e0aad9a9b7d711724",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.1",
    "stem": "An on-premises application requires a local NFS mount with low-latency access to a multi-TB data set and asynchronous replication to S3. Which service should you deploy?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Storage Gateway (File Gateway)",
      "B": "Amazon EFS with VPN",
      "C": "Amazon FSx for Windows File Server",
      "D": "S3 with Direct Connect"
    },
    "explanation": "File Gateway provides a local NFS mount and backs data to S3; EFS can\u2019t be mounted on-prem without a gateway, FSx Windows is SMB, Direct Connect isn\u2019t a file gateway."
  },
  {
    "id": "8573c11fd868d8667ff4e8f3496ca2503016b043163dc025cabcf9f84537cf31",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.1",
    "stem": "A Windows line-of-business application needs an SMB file share with 50,000 IOPS and auto-scaling throughput. Which AWS storage do you use?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Amazon EFS in One Zone",
      "B": "Amazon FSx for Lustre",
      "C": "Amazon FSx for Windows File Server",
      "D": "gp3 EBS volumes in striped RAID"
    },
    "explanation": "FSx for Windows provides SMB shares with scalable IOPS; EFS is NFS, Lustre is POSIX, EBS striping is not managed SMB share."
  },
  {
    "id": "f22c86729dd90e7815082c735987a46937774d98656ac8d08958c49a0f5b1b20",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.1",
    "stem": "A petabyte-scale data warehouse runs large sequential scans and requires cost-efficient block storage. Which EBS volume type is most appropriate?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Provisioned IOPS SSD (io2)",
      "B": "Throughput Optimized HDD (st1)",
      "C": "General Purpose SSD (gp2)",
      "D": "Cold HDD (sc1)"
    },
    "explanation": "st1 is designed for large, sequential workloads at lower cost; io2 and gp2 are SSD (more expensive), sc1 has too low throughput baseline."
  },
  {
    "id": "e87dc872184055e3065a7e1172d95c5bb4c0e1d0b94fb86babe33670402cd5eb",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.1",
    "stem": "You need sub-millisecond latency and over 500,000 IOPS for a financial trading application\u2019s block storage. Which EBS design meets this?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "io2 Block Express with sufficient IOPS configured",
      "B": "Standard magnetic (st1) volumes striped in RAID",
      "C": "gp3 with IOPS at 500,000",
      "D": "EBS gp2 with bursting enabled"
    },
    "explanation": "Only io2 Block Express supports over 500,000 IOPS with sub-ms latency; gp3 max is 16k IOPS, st1 is HDD, gp2 uses burst credits."
  },
  {
    "id": "3f1353dd8838f25ebc3058a94c4fcf35ae7e395b6e60efb720443621a66fd975",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.1",
    "stem": "A file service must enforce WORM compliance, support NFS clients, and deliver tens of thousands of IOPS. Which AWS storage solution do you choose?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Amazon EFS with Lifecycle Management",
      "B": "Amazon S3 with Object Lock",
      "C": "Amazon FSx for Windows File Server",
      "D": "Amazon FSx for NetApp ONTAP"
    },
    "explanation": "FSx for ONTAP supports NFS, WORM (SnapLock), and scalable IOPS; EFS and FSx Windows don\u2019t support WORM, S3 Object Lock is object storage not NFS."
  },
  {
    "id": "a118c913dc081fc9d0265ae9b9f49753150d4879bf22ff48d90531f3de08d3a9",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.1",
    "stem": "An application performs tens of thousands of small file operations per second and requires POSIX semantics. Which storage fits best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon EFS Standard",
      "B": "Amazon S3 with S3FS",
      "C": "Amazon FSx for Windows File Server",
      "D": "gp3 EBS volumes"
    },
    "explanation": "EFS provides native POSIX file system with high metadata performance; S3FS is unreliable, FSx Windows is SMB, EBS can\u2019t be shared across instances."
  },
  {
    "id": "d9cb347945ce0f65a30425edf5ded96a92b7b4b2f9ea7c09d965b397ba7a1b75",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.1",
    "stem": "A high-throughput ingestion pipeline writes large objects to storage every minute. The application cannot be modified. You need to minimize upload time. Which feature do you enable?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "S3 Intelligent-Tiering",
      "B": "EFS Bursting Throughput",
      "C": "S3 Multipart Upload",
      "D": "EBS gp3 provisioned throughput"
    },
    "explanation": "Multipart Upload splits large objects into parallel parts to reduce time; Intelligent-Tiering is cost optimization, EFS and EBS aren\u2019t used for object ingestion."
  },
  {
    "id": "5ac79246c3d32e8692561cda2e75411cc9eb0c213d6445a3ec56ea8ce17f381a",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.2",
    "stem": "A media company needs to process high-resolution images (~15 MB each) in parallel. The processing routine depends on a native C library not supported by AWS Lambda, requires 200 MB of memory, and must scale to 1,000 concurrent tasks at unpredictable intervals. They want minimal infrastructure management and no need to pre-provision EC2 capacity. Which compute option best meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Lambda with provisioned concurrency set to 1,000",
      "B": "Amazon ECS on AWS Fargate with an Application Auto Scaling policy",
      "C": "AWS Batch running on EC2 Spot Instances in a managed compute environment",
      "D": "An Amazon EC2 Auto Scaling group running a container-based Amazon ECS cluster"
    },
    "explanation": "Lambda can\u2019t use the native C library and has payload and execution limitations. AWS Batch would require EC2 capacity management. EC2 Auto Scaling adds management overhead. Fargate provides serverless containers, supports custom libraries, and can scale transparently."
  },
  {
    "id": "1ed7c2083370807dd2601c6c88f801be8c05d5eeb9ea021ab7315f740d55c04c",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.2",
    "stem": "You run a memory-intensive microservice in Amazon ECS on the EC2 launch type. CPU utilization rarely exceeds 20%, but memory usage spikes to 85% under load, causing task failures. Which scaling policy should you implement to ensure stable performance?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Target Tracking policy on CPU utilization set to 70%",
      "B": "Step Scaling policy triggered by NetworkIn exceeding a threshold",
      "C": "Target Tracking policy on memory utilization set to 80%",
      "D": "Step Scaling policy triggered by DiskReadOps exceeding a threshold"
    },
    "explanation": "Since the service is memory-bound, autoscaling on CPU or network won\u2019t address out-of-memory errors. A target tracking policy on memory utilization automatically adds tasks when memory usage is high."
  },
  {
    "id": "a44ea4ccb379976e3c56357d499ea77e758c6767ec2745d430a6cc0df531f9cf",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.2",
    "stem": "An API uses Amazon API Gateway fronting AWS Lambda. Cold starts during unpredictable traffic spikes are causing up to 1 second of added latency, violating SLAs. You need a solution that minimizes cold starts while allowing dynamic scaling without managing servers. What do you implement?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Provisioned Concurrency for the Lambda function",
      "B": "Run the API backend on Amazon ECS Fargate with a minimum task count",
      "C": "Schedule a CloudWatch Events rule that pings the Lambda function every minute",
      "D": "Switch API Gateway integration to an HTTP endpoint on an EC2 Auto Scaling group"
    },
    "explanation": "Provisioned Concurrency pre-initializes execution environments to eliminate cold starts. ECS Fargate introduces container management and minimum tasks overhead. Scheduled pings are unreliable, and EC2 adds server management."
  },
  {
    "id": "f4ee94d8e83870d86f283d2a84f4ae0c39ab4e4fd0cc6e0ae753e9ea24be03f1",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.2",
    "stem": "A data analytics team needs to run ad-hoc MapReduce jobs on petabyte-scale logs. They want minimal management overhead, cost optimization using Spot capacity, and dynamic scaling based on job queue depth. Which solution should you choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon EMR on EC2 Spot Instances with Auto Scaling policies",
      "B": "AWS Lambda in a fan-out pattern to process log batches",
      "C": "AWS Batch in a multi-node parallel job definition",
      "D": "Amazon Kinesis Data Analytics for SQL-based streaming"
    },
    "explanation": "EMR is designed for MapReduce and can scale Spot nodes based on workload. Lambda can\u2019t handle extremely large splits, Batch is less suited for iterative MapReduce, and Kinesis Data Analytics targets streaming, not batch MapReduce."
  },
  {
    "id": "137a878e0f164d182312e9cd77bcea2cd657168777fa95bb07190f23ad4d26eb",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.2",
    "stem": "Your team runs GPU-accelerated machine learning training jobs that are parallelizable across nodes. You need fault-tolerant orchestration with automatic retries, instance scaling based on queue depth, and cost optimization using Spot Instances. Which compute service meets these needs?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Batch with a managed compute environment using GPU Spot Instances",
      "B": "Amazon SageMaker training jobs with automatic model tuning",
      "C": "Amazon EC2 Auto Scaling group with a custom MPI scheduler",
      "D": "Amazon EMR with GPU instances in a YARN cluster"
    },
    "explanation": "AWS Batch offers managed job queues, retry logic, Spot integration, and scaling out GPU instances automatically. SageMaker hides orchestration, EC2 Auto Scaling requires custom scheduler, and EMR\u2019s YARN isn\u2019t optimized for ML training workloads."
  },
  {
    "id": "df22c0c184c9fa1df75edd1a1c460d64701be86cbac45fd075b4578e78833441",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.2",
    "stem": "A microservice deployed as containers requires custom Linux kernel modules and host networking. It must process high-throughput requests with minimal latency. Which AWS compute option do you choose?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon ECS on EC2 with a custom AMI and host networking mode",
      "B": "Amazon ECS on AWS Fargate with awsvpc networking",
      "C": "AWS Lambda with a Lambda function layer containing the kernel modules",
      "D": "AWS Batch with a Docker image containing the kernel modules"
    },
    "explanation": "Fargate doesn\u2019t support custom kernel modules or host networking. Lambda can\u2019t load arbitrary kernel modules. AWS Batch still uses ECS/Fargate under the hood. Only ECS on EC2 lets you manage the host OS and networking."
  },
  {
    "id": "4b77a1d6baaf6526fdbb928f3b822ce9f230793c2a67e04bb74a4aada44c03b4",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.2",
    "stem": "A Lambda function CPU-bound for 500 ms per invocation is too slow. You need to minimize execution time per request. Which action yields the greatest performance improvement?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Reduce allocated memory to lower cost",
      "B": "Enable Provisioned Concurrency at current memory size",
      "C": "Increase reserved concurrency",
      "D": "Increase memory allocation to 3,008 MB"
    },
    "explanation": "Lambda\u2019s CPU is proportional to memory allocation. Increasing memory to 3,008 MB boosts CPU, reducing execution time. Provisioned Concurrency reduces cold starts but doesn\u2019t speed CPU-bound code."
  },
  {
    "id": "12480bc2353fbd10b72f30a66b316212ee1a2ec298bc868c5779357da7e1179f",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.2",
    "stem": "Your Lambda function is throttled during occasional spikes, dropping events. You need to protect downstream services without blocking business-critical workloads. What configuration do you apply?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set Reserved Concurrency at a high value and leave the rest unreserved",
      "B": "Use Provisioned Concurrency equal to peak throughput",
      "C": "Enable burst concurrency mode in the function configuration",
      "D": "Configure event source mapping with a small batch size"
    },
    "explanation": "Reserved Concurrency ensures the function can scale up to that limit and no other functions can consume it. Provisioned Concurrency only pre-warms. Burst mode is default. Batch size adjustment doesn\u2019t prevent throttling due to concurrency limits."
  },
  {
    "id": "2c230eed905e2015532b78c4adb38449925f42a9b1d8a8f3ffcfc98617724490",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.2",
    "stem": "A real-time streaming application reads from 200 Kinesis shards, each peaking at 1,000 records/sec. AWS Lambda can\u2019t scale to that shard count. Which architecture supports parallel processing and stateful aggregation?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Use Lambda with an enhanced fan-out consumer per shard",
      "B": "Deploy Amazon ECS on Fargate running the Kinesis Client Library",
      "C": "Use AWS Batch to ingest and process stream files in bulk",
      "D": "Switch to Amazon SQS as the streaming source and Lambda"
    },
    "explanation": "The Kinesis Client Library on Fargate scales to high shard counts with multiple workers per container and supports stateful processing. Lambda has shard limits and SQS isn\u2019t designed for ordered streaming."
  },
  {
    "id": "3c1f64c8a8328d8f4f568b70b63d21b6d4d9780aaae45757dd458d335e1a8c6e",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.2",
    "stem": "You operate an ECS service with unpredictable batch jobs and steady-state services. You want to use Spot Instances for batch and Fargate for steady load, minimize costs, and maximize resource utilization. What do you configure?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Two separate ECS clusters: one on Spot EC2, one on Fargate",
      "B": "One ECS cluster with EC2 On-Demand capacity provider only",
      "C": "One ECS cluster with both FARGATE and EC2 Spot capacity providers",
      "D": "One ECS cluster using AWS Batch for batch jobs and Fargate for services"
    },
    "explanation": "Using a single ECS cluster with multiple capacity providers allows mixing Spot EC2 for batch and Fargate for services, with per-service capacity provider strategy. Separate clusters increase management overhead."
  },
  {
    "id": "ee2d0143a9abc0c1d2f9e09b4bbb37fdd8f53239ba7072e7b75b40c24ebc8684",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.2",
    "stem": "Your organization needs to run long-running, compute-intensive tasks (up to several hours) in response to job submissions. Jobs are parallelizable, retryable, and vary in size. Which AWS service should you choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Batch in a managed compute environment",
      "B": "Amazon ECS on Fargate with Service Auto Scaling",
      "C": "AWS Lambda functions invoked by Step Functions",
      "D": "Amazon EMR with YARN cluster"
    },
    "explanation": "AWS Batch is designed for long-running batch jobs with retries, queueing, and parallelization. Fargate isn\u2019t optimal for multi-hour runs, Lambda has a 15-minute limit, and EMR is for Hadoop/Spark workloads."
  },
  {
    "id": "c91549355397539f0e61205cc038a8741f9414ada8d7b15082ee9ccfefa4cbe9",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.2",
    "stem": "A file-conversion pipeline asynchronously receives tasks via Amazon SQS. Each task may run up to 20 minutes. You need horizontal scaling without hitting Lambda\u2019s time limit, and minimal orchestration complexity. What architecture do you implement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SQS \u2192 AWS Lambda with a batch window of 5 minutes",
      "B": "SQS \u2192 Amazon ECS on Fargate with an event-driven scheduler",
      "C": "SQS \u2192 AWS Step Functions expressing each conversion as a state machine",
      "D": "API Gateway \u2192 Lambda synchronous conversions"
    },
    "explanation": "Fargate tasks can run for extended durations and scale in response to SQS messages. Lambda\u2019s 15-minute limit is too short, Step Functions adds orchestration complexity, and API Gateway synchronous calls aren\u2019t needed."
  },
  {
    "id": "f6e909e0c60c707c067cdc39917300294ae7b3bbdf6ea807d0327f5e8eb1ece7",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.2",
    "stem": "You run an EKS cluster to serve microservices with highly variable demand (from 5 to 500 pods). You need rapid scale-out/in, efficient bin packing, and Spot utilization. Which autoscaler provides the best mix of performance and cost?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "EKS Managed Node Groups with EC2 Auto Scaling",
      "B": "Kubernetes Cluster Autoscaler with mixed node groups",
      "C": "AWS Karpenter provisioner on the cluster",
      "D": "Amazon EC2 Auto Scaling directly on worker nodes"
    },
    "explanation": "Karpenter rapidly provisions optimal instance types, supports Spot, and packs pods efficiently. Cluster Autoscaler is slower and needs pre-defined groups. Managed Node Groups are less flexible."
  },
  {
    "id": "3bc3c5123cad6b156c1ebef5ac036c4a2075e86b92d59a2902773d8313e30b77",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.2",
    "stem": "You have a distributed simulation requiring low-latency inter-node communication and MPI support. Which EC2 compute option should you choose to minimize network jitter and maximize throughput?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "C5 instance family with Enhanced Networking (ENA)",
      "B": "C5n instance family with Elastic Fabric Adapter (EFA)",
      "C": "M5 instance family with standard networking",
      "D": "P3 instance family with GPU-optimized networking"
    },
    "explanation": "C5n with EFA provides OS-bypass, low-latency, high-throughput connectivity ideal for MPI. ENA alone doesn\u2019t meet HPC requirements, M5 is general purpose, P3 is for GPU but adds unnecessary cost."
  },
  {
    "id": "32bef996f330ef2e3d85b4b23b18425f6bfcd177970d34c82862550063e62fb9",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.2",
    "stem": "You need to orchestrate thousands of short-lived, independent tasks in parallel, with error handling and dynamic fan-out. You also want to avoid managing compute infrastructure. Which AWS service pattern should you use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Step Functions Map state invoking AWS Lambda for each task",
      "B": "Amazon Simple Notification Service with multiple Lambda subscribers",
      "C": "Amazon EMR with Spark to parallelize tasks",
      "D": "AWS Batch with array jobs"
    },
    "explanation": "Step Functions Map simplifies fan-out at massive scale with built-in retries and error handling, and uses Lambda to avoid server management. SNS fans out but lacks orchestration control, EMR/Spark is heavyweight, and Batch array jobs add queueing delays."
  },
  {
    "id": "0c877e1824bd2968e1c08ed784ad691f02381702c1aebc6a85a820ff0976b1e2",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.3",
    "stem": "A global e-commerce application uses a relational database with high-volume read traffic across multiple AWS Regions. Users globally require low-latency read access and strong read consistency, while all writes originate from a primary Region. Which AWS database solution best meets these requirements?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Create an Amazon Aurora Global Database with read-only Aurora Replicas in secondary Regions.",
      "B": "Configure Amazon RDS for MySQL cross-Region read replicas in each Region.",
      "C": "Deploy Amazon DynamoDB global tables with strongly consistent reads.",
      "D": "Use an Amazon Aurora Multi-Master cluster spanning multiple Regions."
    },
    "explanation": "Aurora Global Database provides a single writer and low-latency, strongly consistent reads in secondary Regions. RDS read replicas are asynchronous. DynamoDB global tables don\u2019t offer the relational model. Aurora multi-master only spans one Region."
  },
  {
    "id": "b6979b91f25e92b34ac55f9a8d388d289ef6a94ad4f1df3f9facad362f4ef124",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.3",
    "stem": "A social media platform backed by Amazon RDS for PostgreSQL experiences unpredictable read-intensive spikes. To reduce load and achieve sub-50 ms read latency for user profile lookups, which architecture design is most appropriate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Integrate Amazon ElastiCache for Redis as a read-through cache in front of RDS.",
      "B": "Increase the RDS instance\u2019s buffer cache size to leverage query caching.",
      "C": "Use Amazon DynamoDB Accelerator (DAX) in front of RDS for PostgreSQL.",
      "D": "Convert the database to Amazon Aurora with read replicas to serve reads."
    },
    "explanation": "ElastiCache for Redis provides a high-performance read-through cache for RDS workloads. DAX is specific to DynamoDB. Increasing buffer cache size has limited effect on unpredictable spikes. Migrating to Aurora adds complexity and does not address burst caching as effectively."
  },
  {
    "id": "ddba6f7b16c20c96bd68e823faecaca30a8edaa630d0de88ac628f7efd0f88fb",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.3",
    "stem": "A financial services company must migrate an on-premises Oracle database to AWS, preferring an open-source engine for cost savings. The migration must minimize downtime and maintain transactional integrity. Which target database engine is most appropriate?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Aurora PostgreSQL with AWS Database Migration Service (AWS DMS) ongoing replication.",
      "B": "Amazon RDS for PostgreSQL using native logical replication.",
      "C": "Amazon Aurora MySQL with native binary log replication.",
      "D": "Amazon RDS for MySQL with AWS DMS full load and change data capture."
    },
    "explanation": "Aurora PostgreSQL with AWS DMS supports heterogeneous migration with minimal downtime and high performance. Native logical replication in RDS Postgres is less mature. Aurora MySQL or RDS MySQL target engines don\u2019t directly support Oracle features."
  },
  {
    "id": "5171fb68beb4344a18273305304d42b0783a4a1914c3b222308bb540971ce8ff",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.3",
    "stem": "A gaming application requires a fast, in-memory session store for millions of players. Sessions are short-lived, require millisecond read/write latency, and must scale seamlessly with unpredictable traffic. Which AWS database solution should you choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon DynamoDB with DynamoDB Accelerator (DAX).",
      "B": "Amazon ElastiCache for Redis cluster-mode enabled.",
      "C": "Amazon DynamoDB with on-demand capacity.",
      "D": "Amazon Aurora Serverless v2 with a high-memory instance class."
    },
    "explanation": "ElastiCache for Redis provides a native in-memory store with sub-millisecond latency and cluster-mode auto-sharding. DAX accelerates DynamoDB, not RDS. DynamoDB is not in-memory. Aurora Serverless is relational and higher latency."
  },
  {
    "id": "70c88b9918432b21b4a112a736b327683239899ecb299f280de9a6d928f49572",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.3",
    "stem": "A microservices application uses Amazon RDS for PostgreSQL and opens many concurrent database connections, causing connection saturation. You cannot modify application code. How can you improve database connection scalability?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Deploy Amazon RDS Proxy to pool and share connections across application instances.",
      "B": "Increase the max_connections parameter on the RDS instance.",
      "C": "Add a read replica and distribute connections evenly between writer and replica.",
      "D": "Migrate the database to Amazon Aurora MySQL which supports higher connection limits."
    },
    "explanation": "RDS Proxy transparently pools connections without code changes. Increasing max_connections may exhaust memory. Read replicas don\u2019t alleviate writer connection saturation. Migrating engines is high effort."
  },
  {
    "id": "49a7d08ae1e352cb88f9f2da70864de560b63ec08cf78c5e635b0f4bc9035e6a",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.3",
    "stem": "An application requires storing and querying highly connected social network relationships with low-latency traversals. The data model consists of nodes and edges. Which AWS database service is most appropriate?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Neptune.",
      "B": "Amazon DynamoDB with Global Secondary Indexes.",
      "C": "Amazon Aurora with JSON support.",
      "D": "Amazon DocumentDB."
    },
    "explanation": "Amazon Neptune is designed for graph data with optimized traversal performance. DynamoDB and DocumentDB are not graph-native. Aurora JSON isn\u2019t optimized for graph queries."
  },
  {
    "id": "211bc2ac2e81ae04a3999b936443d49de906558bda1233a30a65d3f38ea1db2a",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.3",
    "stem": "A financial trading application on Amazon RDS MySQL consistently requires 25,000 IOPS with sub-millisecond latency. Which storage configuration meets this requirement at the lowest cost?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "General Purpose SSD (gp3) with 25,000 provisioned IOPS.",
      "B": "Provisioned IOPS SSD (io1) volume with 25,000 IOPS.",
      "C": "General Purpose SSD (gp2) volume, relying on burst credits.",
      "D": "Standard magnetic (HDD) volume."
    },
    "explanation": "io1 volumes support provisioned IOPS above gp3\u2019s 16,000 IOPS limit. gp2 is not guaranteed at sustained 25,000 IOPS. Magnetic storage cannot meet latency or throughput requirements."
  },
  {
    "id": "a1020222b4b62ed298c1c75d44c7330e54881812670acbac3db64ee2c59f8aae",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.3",
    "stem": "A SaaS application has unpredictable and spiky workloads requiring a relational database to automatically scale compute and storage with minimal management overhead. Which AWS database solution should you choose?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Aurora Serverless v2.",
      "B": "Amazon RDS Multi-AZ with auto-scaling read replicas.",
      "C": "Amazon Aurora provisioned cluster with manual scaling.",
      "D": "Amazon DynamoDB on-demand capacity."
    },
    "explanation": "Aurora Serverless v2 transparently scales both compute and storage for relational workloads without manual provisioning. RDS Multi-AZ doesn\u2019t auto-scale. DynamoDB is non-relational."
  },
  {
    "id": "45e42a48f22004c8a5b0166bd0c5e98d2ea51dda310d3f66d6a9c3c3f98c90a9",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.3",
    "stem": "A travel booking system requires multi-master writes and low-latency reads in multiple Regions with eventual consistency for non-critical data. Which database solution should you choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon DynamoDB global tables.",
      "B": "Amazon Aurora Global Database.",
      "C": "Amazon RDS cross-Region read replicas with promotion.",
      "D": "Amazon DocumentDB cross-Region replicas."
    },
    "explanation": "DynamoDB global tables provide multi-master writes and regional read/write endpoints with eventual consistency. Aurora Global Database is single-writer. RDS replicas are asynchronous and not multi-master."
  },
  {
    "id": "480d98d56a7e02fffb24ba0b3b6c718dfe099af2ef5ec2d785580822ab3da461",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.3",
    "stem": "A content-rich application uses Amazon RDS for PostgreSQL. Metadata updates occur hourly, and metadata reads require sub-10 ms latency with consistency within one minute of updates. Which caching strategy satisfies these requirements?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use Amazon ElastiCache for Redis with a TTL of 60 seconds and write-through cache invalidation on updates.",
      "B": "Use DynamoDB Accelerator (DAX) with global tables.",
      "C": "Enable the pgBouncer extension on the RDS instance for caching.",
      "D": "Rely on Aurora buffer cache warming with automated refresh."
    },
    "explanation": "A Redis read-through cache with TTL matches the consistency window and latency targets. DAX is for DynamoDB. pgBouncer pools connections but does not cache data. Aurora buffer cache cannot guarantee sub-10 ms across RDS."
  },
  {
    "id": "ced89eb8f468f5dfcc30814c1f14d2d4404c6a2df4f5ed9cc8743cddea37d8b2",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.3",
    "stem": "A production Amazon Aurora MySQL cluster handles both transactional workloads and periodic complex analytical queries that degrade performance. How can you offload analytics with minimal latency impact?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create Aurora Replicas in the same cluster and direct analytics queries to them.",
      "B": "Create a cross-Region read replica and run analytics on it.",
      "C": "Use Amazon Redshift federated queries on the Aurora cluster.",
      "D": "Use AWS Database Migration Service to replicate to an external data warehouse."
    },
    "explanation": "Aurora Replicas in-cluster share the same storage and provide low-latency offload. Cross-Region adds latency. Redshift federation and DMS introduce complexity and higher latency."
  },
  {
    "id": "e1e8747e4785e814e4273d015c9c7d9e74b014ea28e9358ecde289eeda3d2dce",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.3",
    "stem": "An application uses Amazon Aurora PostgreSQL and experiences seasonal read workloads requiring up to 100,000 TPS. You need an in-memory caching solution that transparently scales out/in. Which solution is most appropriate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon ElastiCache for Redis cluster-mode enabled with auto scaling of shards.",
      "B": "Amazon ElastiCache Memcached cluster with auto scaling.",
      "C": "Amazon Aurora Auto Scaling for read replicas.",
      "D": "Amazon DynamoDB Accelerator (DAX)."
    },
    "explanation": "Redis cluster-mode supports transparent shard scaling. Memcached auto scaling is manual. Aurora read replicas scale compute, not in-memory cache. DAX is for DynamoDB."
  },
  {
    "id": "43e0b3a0e1e68280b18fcb47a3c65505d9d08e572a25e6dff6914afc9af28e72",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.3",
    "stem": "An OLTP application stores critical data in Amazon RDS for PostgreSQL. The Recovery Point Objective (RPO) must be zero in the event of an Availability Zone failure. Which configuration ensures this requirement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable Multi-AZ deployment with synchronous replication.",
      "B": "Create a Read Replica in another AZ.",
      "C": "Use Amazon Aurora Global Database.",
      "D": "Schedule EBS snapshots every minute."
    },
    "explanation": "RDS Multi-AZ uses synchronous replication to ensure zero data loss on AZ failure. Read replicas are asynchronous. Aurora Global Database is for cross-Region. Frequent snapshots still allow data loss."
  },
  {
    "id": "8ab3ac5c73b2388003adddf1264d7f050b77994ad8cbe5417aee19671d519975",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.3",
    "stem": "An application stores user-generated JSON documents with dynamic schemas. It requires MongoDB API compatibility, multi-AZ durability, and high read performance. Which AWS service should you choose?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon DocumentDB.",
      "B": "Amazon DynamoDB.",
      "C": "Amazon Aurora PostgreSQL with JSONB.",
      "D": "Amazon S3 with Athena."
    },
    "explanation": "DocumentDB is MongoDB-compatible with multi-AZ durability. DynamoDB is key-value. Aurora JSONB supports JSON but not MongoDB API. S3+Athena is not a primary database."
  },
  {
    "id": "31013052432bb02b1ed20c4bfe7be79e897778aab1ec6c142f6fd8d9e4a5dcac",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.3",
    "stem": "A telemetry system ingests time-series sensor data at 1,000 writes per second, requiring millisecond write and read latency for the latest data points with automatic scaling. Data older than one week can be archived. Which database should you select?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Amazon DynamoDB with on-demand capacity and Time-to-Live (TTL).",
      "B": "Amazon RDS for MySQL with Provisioned IOPS.",
      "C": "Amazon DocumentDB.",
      "D": "Amazon Aurora Serverless v2."
    },
    "explanation": "DynamoDB on-demand offers seamless scaling and millisecond latency; TTL handles archiving. RDS and Aurora are not as elastic for unpredictable writes. DocumentDB is not optimized for time-series throughput."
  },
  {
    "id": "1eeff8a9e6aee04d1da454bcabe247447c715dffb10c911458687fdd736bb72a",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.4",
    "stem": "A company runs a global e-commerce platform with separate domains for static assets (JS, CSS, images) and dynamic content served by microservices behind ALBs in us-east-1 and eu-west-1. They want to minimize latency for dynamic API calls and static content, improve resilience to regional API failures, and maintain a single DNS name for each type of content with minimal operational overhead. Which network architecture meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy two CloudFront distributions (one for static assets, one for dynamic API), each with origin failover to both ALBs, and use geolocation routing in Route 53 to direct traffic to the distribution with the lowest latency.",
      "B": "Use AWS Global Accelerator with HTTP(S) listeners that route dynamic API calls to regional ALBs and use a separate CloudFront distribution for static assets.",
      "C": "Use AWS Global Accelerator with HTTP(S) listeners for both static and dynamic content, forwarding to the origin S3 bucket and ALBs.",
      "D": "Configure Route 53 latency-based routing for the domain names, pointing directly to the ALBs for dynamic content and S3 website endpoints for static assets."
    },
    "explanation": "Option B combines GA for dynamic traffic acceleration and failover with CloudFront for edge caching of static assets. A adds complexity with two distributions and Route 53 geolocation. C uses GA for caching, which is suboptimal. D lacks TCP-level optimizations and edge caching for static content."
  },
  {
    "id": "c8445638cad72541ee3ad2b87514c94a070eca989a6fe7ce88eb7c4b47a4a46f",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.4",
    "stem": "A financial services firm has 200 branch offices connected via low-bandwidth internet and a centralized data center in Frankfurt connected to three AWS VPCs (us-east-1, eu-west-1, ap-southeast-1) via AWS Direct Connect. Employees access cloud-hosted applications with unpredictable internet performance. They need consistent low-latency TCP connectivity from branch offices to all regions using a static IP address and automatic failover across regions. What network design should you recommend?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure Site-to-Site VPN connections from each branch office directly to each VPC.",
      "B": "Deploy AWS Global Accelerator with a static anycast IP and endpoint groups for Network Load Balancers in each region, and maintain existing Direct Connect for on-prem data center traffic.",
      "C": "Create an AWS Transit Gateway in each region with inter-region peering and reconfigure branch offices to use Direct Connect to the nearest VPC.",
      "D": "Use CloudFront with custom origin pointing to ALBs in each region."
    },
    "explanation": "Global Accelerator provides a single static IP, anycast routing for TCP, and health-based failover. VPNs (A) are operationally heavy. Transit Gateway peering (C) doesn\u2019t give a static global IP or improve Internet performance for branch offices. CloudFront (D) doesn\u2019t optimize generic TCP workloads."
  },
  {
    "id": "e2471b84f449efd4024e4be42cbcebef01225045270418cf28331106921116de",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.4",
    "stem": "A gaming company runs a multiplayer game that uses UDP for real-time interactions, currently deployed in us-east-1 behind a single Network Load Balancer. To reduce latency for APAC players and provide a single endpoint DNS name and static IP addresses for firewall rules, which network architecture should they implement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy another NLB in ap-southeast-1 and configure Route 53 latency-based routing.",
      "B": "Use AWS Global Accelerator with UDP listeners and add the us-east-1 and ap-southeast-1 NLBs as endpoints.",
      "C": "Create a CloudFront distribution with a custom origin group pointing to the NLBs.",
      "D": "Use an Application Load Balancer with static IP addresses via Elastic IP attachments."
    },
    "explanation": "Global Accelerator supports UDP and provides static anycast IPs with low-latency global routing. Adding another NLB with Route 53 (A) lacks static anycast IP. CloudFront (C) doesn\u2019t support UDP. ALB (D) doesn\u2019t support UDP or Elastic IPs."
  },
  {
    "id": "1df3295a565b111de35ef84fcff4ddac7230a4ad5b16fb766eac10b0e6c957eb",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.4",
    "stem": "Two AWS accounts: Account A hosts a microservice in a private subnet behind an NLB, Account B hosts a legacy application needing the service. They want to avoid public internet exposure, NAT gateways, and centrally manage service versions. How should you connect them?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Establish a VPC peering connection between the two VPCs.",
      "B": "Publish the NLB service via AWS PrivateLink to Account B.",
      "C": "Attach both VPCs to a shared Transit Gateway with route propagation.",
      "D": "Create a Site-to-Site VPN connection between the two VPCs."
    },
    "explanation": "AWS PrivateLink exposes the NLB as an interface endpoint in the consumer account without internet. VPC peering (A) can\u2019t expose NLB privately across accounts. Transit Gateway (C) is more complex and broader in scope. VPN (D) uses the internet."
  },
  {
    "id": "4e827b7626bea1e20840f390b8088fc97947f0a0c2c284b005cceac67360e789",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.4",
    "stem": "A chat application maintains WebSocket (wss) connections to an Application Load Balancer in us-east-1 and eu-west-1. They require a fixed set of IP addresses for client whitelisting, global failover, and seamless scaling. Which network design provides this with minimal changes?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Place the ALBs behind an AWS Global Accelerator with HTTP(S) listeners and ALB endpoints.",
      "B": "Replace the ALBs with Network Load Balancers and use Global Accelerator with TCP listeners.",
      "C": "Put CloudFront in front of the ALBs to obtain static IP prefixes.",
      "D": "Configure Route 53 weighted policies pointing to the ALB DNS names."
    },
    "explanation": "GA HTTP(S) listeners support WebSocket upgrades and provide static anycast IPs with health-based failover. Using NLB (B) would break WebSocket framing. CloudFront (C) doesn\u2019t support WebSockets. Route 53 (D) lacks static IPs and rapid failover."
  },
  {
    "id": "689194012e4144cd57fbcc50a02864235bfd7b48e160fcbff632d4821be42da3",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.4",
    "stem": "A media streaming company uses TCP for control signals and UDP for video payload. They host endpoints in us-east-1 and eu-central-1. They need global ingress with sub-100 ms failover on unhealthy endpoints and static client-side IPs. Which network architecture meets these?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "Configure two CloudFront distributions (TCP on HTTPS and UDP via media store origin) with origin failover.",
      "B": "Deploy separate AWS Global Accelerators: one with TCP listener, one with UDP listener, each pointing to NLB endpoints in both regions.",
      "C": "Use Route 53 weighted DNS with health checks and two NLBs for control and streaming.",
      "D": "Use an Application Load Balancer for TCP control and a Network Load Balancer with client IP preservation for UDP."
    },
    "explanation": "Global Accelerator supports both TCP and UDP with static IPs and rapid health-based failover. CloudFront (A) doesn\u2019t support UDP. DNS (C) can be slow to update and doesn\u2019t support UDP acceleration. ALB/NLB only (D) lack static global IPs and automatic cross-region failover."
  },
  {
    "id": "859a964216b228ebb90eaba2f6bfe94d233336cd92f1eef2687b5bb7b090034a",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.4",
    "stem": "A real-time bidding ad platform receives HTTPS requests globally on port 443. They require DDoS protection, centralized WAF policy enforcement, SSL termination close to users, and dynamic content customization at the edge. Which network architecture is most suitable?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Global Accelerator with TLS listeners and attach regional ALBs, integrate AWS WAF at ALB.",
      "B": "Deploy CloudFront with an HTTPS listener, enable AWS WAF, and origin request policies to route to regional ALBs.",
      "C": "Configure Route 53 latency-based records to ALBs and apply WAF at each ALB.",
      "D": "Use API Gateway edge-optimized endpoints with AWS WAF for SSL termination and routing."
    },
    "explanation": "CloudFront provides DDoS mitigation, WAF at edge, SSL termination, and Lambda@Edge for dynamic customization. GA (A) doesn\u2019t integrate WAF. Route 53 (C) applies WAF only at ALB, not edge. API Gateway (D) adds latency and cost."
  },
  {
    "id": "1264ceb801675a1722273915e3e59df85fba58df5ed91cc3b7400612530311da",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.4",
    "stem": "A B2B API uses mutual TLS (mTLS) and is deployed behind ALBs in us-east-1 and ap-southeast-1. Clients require a single DNS name, static IP addresses for firewall rules, mTLS authentication at the edge, and >99.99% availability. Which network architecture should you choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Put the ALBs behind an AWS Global Accelerator with TLS listeners and ALB endpoints.",
      "B": "Replace ALBs with NLBs (TLS) for mTLS termination and front them with Global Accelerator.",
      "C": "Place a CloudFront distribution with custom origin (ALB) and configure mTLS at CloudFront.",
      "D": "Use Route 53 multi-value answer routing to ALB endpoints with health checks."
    },
    "explanation": "GA TLS listeners with ALB endpoints provide static IPs, health-based failover, and preserve ALB mTLS termination. NLB (B) can\u2019t handle mTLS properly. CloudFront (C) doesn\u2019t support mTLS. Route 53 (D) lacks static IPs and fast failover."
  },
  {
    "id": "f3d1dad8f07d4791589bcd15f6618976bf7995e990386c64cc3bfe8225549698",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.4",
    "stem": "A global enterprise has a single Direct Connect connection in us-east-1 attached to a Direct Connect gateway. They run a subsidiary application in eu-west-1 using Transit Gateway. They want to reach the eu-west-1 Transit Gateway across the same Direct Connect link without deploying a second connection. How can they architect this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create a new Direct Connect connection in eu-west-1.",
      "B": "Attach the us-east-1 Transit Gateway to the Direct Connect gateway via a Transit VPC.",
      "C": "Use Transit Gateway inter-region peering from the us-east-1 TG attached to the DX gateway to the eu-west-1 TG.",
      "D": "Establish a VPN connection from eu-west-1 VPC to the on-premises network."
    },
    "explanation": "Transit Gateway inter-region peering allows routing across regions using the existing Direct Connect link. Adding a new connection (A) increases cost. Transit VPC (B) is an outdated pattern. VPN (D) falls back to internet and lacks DX performance."
  },
  {
    "id": "54464beba5d6b41260048c42e7137bc97f14db46489d6b483241b51f33ae320f",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.4",
    "stem": "A dual-stack (IPv4/IPv6) web service deployed across multiple AZs needs to present static IPv6 addresses to clients and scale under unpredictable load. Which AWS load-balancing component should be used?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Application Load Balancer (dual-stack) with fixed Elastic IPv6 addresses.",
      "B": "Network Load Balancer (dual-stack) with Elastic IP pairs for IPv6.",
      "C": "Network Load Balancer (dual-stack) automatically allocates static IPv6 addresses per subnet.",
      "D": "CloudFront distribution with IPv6 enabled and origin failover."
    },
    "explanation": "A dual-stack NLB automatically allocates static IPv6 addresses in each subnet. ALB (A) doesn\u2019t support attaching Elastic IPv6 addresses. CloudFront (D) IP ranges aren\u2019t static for whitelisting."
  },
  {
    "id": "e6b88932a2a0dee8cd5612da89a6e428a3d400a0dfac9a95b42006315e79787d",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.4",
    "stem": "An organization wants all traffic to Amazon S3 from EC2 instances and on-premises to traverse the AWS private network without using public internet or NAT gateways, and enforce access controls on buckets. Which network architecture meets this requirement?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Deploy interface VPC endpoints (AWS PrivateLink) for S3 in each VPC.",
      "B": "Create gateway VPC endpoints for S3 in each VPC and configure endpoint policies.",
      "C": "Configure NAT gateways and use S3 public endpoints with bucket policies limiting source IP.",
      "D": "Use CloudFront origin access identity for S3 buckets."
    },
    "explanation": "Gateway endpoints for S3 route traffic over the AWS network and support endpoint policies. Interface endpoints (A) aren\u2019t supported for S3. NAT gateways (C) still traverse the internet. CloudFront OAI (D) only secures CloudFront access."
  },
  {
    "id": "fb827097f211dfe2b441fd95b330794a7b7787e0643816546abd950f322f8e8b",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.4",
    "stem": "A global API suite exposed via Amazon API Gateway edge-optimized endpoints in us-east-1 experiences ~200 ms latency for clients in Asia. They need to reduce latency to <100 ms and add DDoS protection without modifying their APIs. What network architecture change should they implement?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Switch to regional API Gateway endpoints fronted by AWS Global Accelerator.",
      "B": "Create a CloudFront distribution in front of the edge-optimized API Gateway.",
      "C": "Use Route 53 latency-based DNS to direct Asian clients to a regional API Gateway endpoint.",
      "D": "Migrate APIs to Application Load Balancers behind CloudFront."
    },
    "explanation": "Global Accelerator fronting regional API Gateway provides TCP optimizations, static IPs, and improved latency. Edge-optimized (B) already uses CloudFront. DNS-only (C) has caching delays and no TCP acceleration. Migrating (D) is unnecessary."
  },
  {
    "id": "dc3baed8f4590136d29ae39320077df2a7c43a637399552ca8137b66c8177d59",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.4",
    "stem": "A microservices architecture on Amazon ECS uses path-based routing via ALBs. They want to improve global performance, enforce WAF at the edge, and preserve the original Host header for routing. Which network pattern should they use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure AWS Global Accelerator with HTTP(S) listeners and ECS service ALBs as endpoints.",
      "B": "Deploy a CloudFront distribution with custom origin set to the ALBs, forward Host header, and attach AWS WAF.",
      "C": "Use Route 53 geoproximity routing to ALB DNS names and apply WAF at each ALB.",
      "D": "Replace ALBs with edge-optimized API Gateway HTTP APIs."
    },
    "explanation": "CloudFront forwards headers, integrates WAF at edge, and preserves Host for path-based routing. GA (A) doesn\u2019t forward all headers. Route 53 (C) applies WAF at ALB only, not edge. API Gateway (D) requires refactoring."
  },
  {
    "id": "84ef39bd45cb10bab4815004efb66fbf7cf67ea911322c104819db03056672b6",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.4",
    "stem": "A backup application running on-premises transfers several terabytes of data daily to Amazon S3 over the public internet. They need to maximize throughput and minimize transfer duration without requiring additional physical appliances. Which solution should they use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure an S3 Transfer Acceleration-enabled bucket and require clients to use its alias endpoint.",
      "B": "Deploy AWS DataSync agents on-premises and configure tasks to sync to S3.",
      "C": "Use an AWS Direct Connect public VIF to S3 without encryption.",
      "D": "Use AWS Storage Gateway with cached volumes to upload data to S3."
    },
    "explanation": "DataSync optimizes parallelism and protocol efficiency for large data transfers over the internet. S3 Transfer Acceleration (A) helps some use cases but has variable throughput for large datasets. Direct Connect (C) requires new connectivity. Storage Gateway (D) is optimized for block or file workloads, not bulk transfer."
  },
  {
    "id": "1b85d101ee7739ba97ae519a6dc74283c10dcc425dc918d2070fcb2e8d9da07d",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.4",
    "stem": "A web application deployed in us-east-1 and eu-west-1 needs a single global DNS name that fails over within 30 seconds on regional outage, with minimal administrative overhead. Which network architecture should you recommend?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Use Amazon Route 53 failover record sets with a 30-second TTL.",
      "B": "Implement AWS Global Accelerator with two endpoint groups for the ALBs in each region.",
      "C": "Deploy a CloudFront distribution with origin failover to the secondary region.",
      "D": "Leverage Route 53 latency-based routing with health checks."
    },
    "explanation": "Global Accelerator provides health-based failover in ~10 seconds and a static anycast IP. Route 53 TTLs (A) may not guarantee failover <30 s. CloudFront failover (C) suits static content poorly and has slower failover. Latency-based routing (D) has DNS caching delays."
  },
  {
    "id": "154bdb71a144e251d5c650141490d6b26079bf3a95ef0e38e840fd640e6a9434",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.2",
    "stem": "A media company needs to build a low-operational-overhead, near real-time log analytics pipeline processing 5,000 JSON events per second with schema evolution (new fields added over time) and sub-minute end-to-end latency. Which ingestion and transformation architecture best meets these requirements?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Kinesis Data Firehose writes raw JSON to S3, and a nightly AWS Glue ETL job converts to Parquet.",
      "B": "Amazon Kinesis Data Streams \u2192 AWS Lambda \u2192 S3 in Parquet format.",
      "C": "Amazon Kinesis Data Firehose with Lambda transformation to Parquet \u2192 S3.",
      "D": "Amazon Kinesis Data Streams \u2192 Amazon Kinesis Data Analytics for in-stream transformation \u2192 Kinesis Data Firehose delivery to S3 in Parquet format"
    },
    "explanation": "Option D uses KDA for continuous, in-stream transformations with built-in schema registry support and Firehose delivery, minimizing Lambda management and keeping latency under a minute."
  },
  {
    "id": "b222b2ae73ebb504d09db84ba4539332b798194f119c926f38dc7c65c3f75b36",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "2.1",
    "stem": "An on-premises file server hosts 5 TB of daily updated files. The security team requires incremental change detection, metadata preservation, and encryption in transit when moving data to Amazon S3 each night. Which solution most efficiently meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy an SMB File Gateway; copy files overnight via NFS mount to S3.",
      "B": "Use AWS DataSync with agent on-premises, enable incremental transfers and metadata preservation, and use TLS.",
      "C": "Use AWS Storage Gateway Volume Gateway to snapshot and upload to S3 as EBS snapshots.",
      "D": "Use Amazon S3 Transfer Acceleration over HTTPS to copy files with AWS CLI."
    },
    "explanation": "AWS DataSync supports incremental change detection, metadata preservation, and encryption in transit with minimal setup and bandwidth optimization, making it the best fit."
  },
  {
    "id": "6469e5ff7d2d7e5475e494d8e9e2316cbedc8669078fd4af98a08daf99b63f4a",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.3",
    "stem": "A financial services team must restrict sensitive column access in a central data lake to only authorized users, enforce encryption at rest and in transit, and manage row-level permissions. Which combination of AWS services meets these requirements?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "S3 bucket policies with IAM conditions for encryption and AWS KMS CMKs to enforce access policies.",
      "B": "AWS IAM policies granting access to S3 prefixes and AWS KMS key policies for encryption.",
      "C": "AWS Lake Formation with column and row-level permissions, Lake Formation-managed CMKs, and S3 bucket encryption.",
      "D": "AWS Glue Data Catalog with IAM policies and AWS KMS envelope encryption in Glue jobs."
    },
    "explanation": "Lake Formation natively supports fine-grained row- and column-level access controls, integrates with Lake Formation CMKs, and enforces encryption at rest/in transit for a secure data lake."
  },
  {
    "id": "379309d96e6a1266b1d306bbf5c5f800c3605038f096f4058cebd39d7c6e869f",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.1",
    "stem": "Your analytics team queries a 50 TB Parquet dataset partitioned by year/month/day in Amazon S3 using Athena. Queries for the last 7 days are slow due to partitions. You need to optimize performance and reduce metadata overhead. What should you implement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use an AWS Glue crawler daily to discover new partitions before querying.",
      "B": "Enable Athena partition projection and convert data to ORC with Snappy compression via a CTAS job.",
      "C": "Run a nightly Athena MSCK REPAIR TABLE to rebuild partitions and use GZIP compressed Parquet.",
      "D": "Use Amazon Redshift Spectrum instead of Athena to query the S3 data lake."
    },
    "explanation": "Partition projection eliminates crawler metadata calls, and using ORC with Snappy improves query performance over large datasets and leverages columnar optimizations."
  },
  {
    "id": "2a1c66c0b98a9140fc25f2e024d86540e287442039fb7bd137d700baab63a090",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.1",
    "stem": "A software vendor streams JSON application logs to Amazon S3 and wants on-the-fly conversion to compressed Parquet with minimal custom code. Which solution satisfies this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure Kinesis Data Firehose with record format conversion to Parquet using AWS Glue Data Catalog and compression settings.",
      "B": "Use Kinesis Data Streams with an AWS Lambda consumer that writes Parquet to S3.",
      "C": "Deploy an AWS Glue streaming ETL job reading from S3, converting to Parquet, and writing back.",
      "D": "Send logs to Amazon MSK and use a Kafka Connect Parquet sink to S3."
    },
    "explanation": "Firehose's built-in record format conversion integrates with the Glue Data Catalog to output Parquet, eliminating custom Lambda code and simplifying management."
  },
  {
    "id": "6c0e0c624c926a7547963af00c5c45acabf130908babbde92bd4ae3d9dabcde9",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.1",
    "stem": "Your team runs a nightly 10 TB ETL job on Amazon EMR. They want to minimize costs, auto-terminate when idle, and scale based on workload. Which EMR configuration achieves this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "EMR on On-Demand EC2 instances with a fixed 10-node cluster and YARN capacity scheduler.",
      "B": "EMR Serverless application with fixed memory and vCPU allocation.",
      "C": "EMR on EC2 Spot Instances with Managed Scaling enabled and Auto-Termination set to zero idle minutes.",
      "D": "AWS Glue ETL job using 2.0 G.1X worker type with Auto-Scaling enabled."
    },
    "explanation": "Using Spot instances with EMR Managed Scaling automatically adjusts cluster size to workload and auto-terminates idle clusters immediately, optimizing cost and performance."
  },
  {
    "id": "f89304104d60347acf0d381fc1a8356326f0c188e9b961000446abc09ac204fe",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "2.1",
    "stem": "An IoT application emits 10,000 events/sec. You must store the data in S3 batches every 5 minutes, and trigger real-time alerts on temperature anomalies within 1 minute. Which architecture meets both requirements?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Kinesis Data Firehose with 5 min buffer interval \u2192 S3; Lambda on Firehose failures for alerts.",
      "B": "AWS IoT Core rule to S3; AWS IoT Events for anomaly detection.",
      "C": "Amazon Kinesis Data Streams \u2192 Kinesis Data Analytics for anomaly detection \u2192 Kinesis Data Firehose with 5 min buffer \u2192 S3.",
      "D": "Amazon MSK \u2192 AWS Lambda consumer writes to S3; Lambda for anomaly alerts."
    },
    "explanation": "Kinesis Data Streams provides durable ingest, Data Analytics performs sub-minute anomaly detection, and Firehose buffers 5 min batches to S3, satisfying both latency and batch requirements."
  },
  {
    "id": "0aad39696f61650d28e4cb96edbe9c954d770df63eb97d4453ff236d8edadb39",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.4",
    "stem": "A security requirement mandates that only traffic from your VPC can upload data into your S3 ingestion bucket used by AWS DataSync. How do you enforce this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use S3 bucket policy with aws:SourceIp matching the NAT gateway IP.",
      "B": "Use security groups on the S3 bucket to allow only VPC access.",
      "C": "Configure AWS WAF on the S3 bucket endpoint to allow VPC CIDR.",
      "D": "Create a S3 Gateway VPC Endpoint and an S3 bucket policy that allows PutObject only if aws:sourceVpce equals the endpoint ID."
    },
    "explanation": "A Gateway VPC Endpoint provides private S3 access from your VPC, and the bucket policy condition on aws:sourceVpce enforces that only endpoint traffic can upload."
  },
  {
    "id": "f4cc3886a5c6d9f8bbb5107c6c986c19397de8e0959c16f1a029c75fef7a76d4",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.4",
    "stem": "Multiple AWS accounts generate application logs that must be centralized into a single S3 bucket in a reporting account. Which ingestion pattern securely automates this cross-account data transfer?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use S3 Cross-Region Replication from each account\u2019s logging bucket to the central bucket.",
      "B": "Deploy a Kinesis Data Firehose delivery stream in each source account with an IAM role granting PutObject to the central bucket, and configure the central bucket policy to allow only those roles.",
      "C": "Run an AWS Glue crawler in each account to catalog logs, then use AWS Glue ETL jobs to copy into the central bucket.",
      "D": "Use Amazon CloudWatch cross-account subscription filters to forward logs directly into the central S3 bucket."
    },
    "explanation": "Firehose supports cross-account S3 delivery by assuming a role with PutObject permissions; the central bucket policy restricts writes to only those roles, providing a secure, automated pipeline."
  },
  {
    "id": "2a557c2208de52edabffd8339754cee34fa173a14dd6ed9398ff5ee3e7b8a145",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.1",
    "stem": "Your team needs to convert daily landing zone CSV files in S3 to partitioned Parquet automatically, ensuring that only new files are processed and duplicates are avoided. Which approach is optimal?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use AWS Glue ETL jobs with job bookmarks enabled, triggered daily by an EventBridge schedule, writing Parquet into date-based partitions.",
      "B": "Create an Amazon EMR cluster with a cron-triggered step to convert all CSV files to Parquet each night.",
      "C": "Configure Athena CTAS queries via Lambda triggered on every S3 PUT event.",
      "D": "Use Kinesis Data Analytics to batch read from S3 and output Parquet back to S3."
    },
    "explanation": "Glue job bookmarks track processed files, avoiding duplicates. Scheduled daily triggers and partitioned writes to Parquet meet performance and deduplication requirements."
  },
  {
    "id": "bdc1bd737b033ca112df835c846573542b48c4cef6d67e010e4dd1584325b6e1",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.1",
    "stem": "A business intelligence team requires a QuickSight dashboard that refreshes within 5 minutes of new Parquet data landing in S3. QuickSight SPICE minimum schedule is 15 minutes. How do you meet the 5-minute SLA?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Use QuickSight Direct Query against Amazon Athena for real-time data access.",
      "B": "Create an Amazon EventBridge rule to trigger an Athena CTAS every 5 minutes and use that as the data source.",
      "C": "Schedule SPICE ingestion every 15 minutes and accept eventual consistency.",
      "D": "Trigger a Lambda function on S3 object creation that calls the QuickSight Create Ingestion API to update the SPICE dataset."
    },
    "explanation": "Using S3 event notifications and Lambda to invoke the QuickSight ingestion API achieves sub-15 minute SPICE updates, meeting the 5-minute SLA."
  },
  {
    "id": "9aa7d96bb35fa7b1e9a9eadaaa1c49eb8f66e4f8fb9248993778f05a6c30e1f6",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.1",
    "stem": "An NFS share contains millions of small image metadata files that must be moved to S3 with preserved timestamps and user metadata as fast as possible. Which DataSync settings optimize throughput?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Increase DataSync\u2019s task parallelism and enable POSIX metadata mode on the agent, transferring via Direct Connect.",
      "B": "Use Storage Gateway File Gateway with 16 GB cache and default settings.",
      "C": "Use AWS CLI with aws s3 sync and the --metadata-directive option.",
      "D": "Configure AWS Transfer Family with SSH-FS mounted to the NFS server and multithread uploads."
    },
    "explanation": "DataSync supports high-performance, parallel transfers of small files and preserves POSIX metadata when configured for parallelism and metadata mode, making it the fastest and most reliable option."
  },
  {
    "id": "0da507a728dd5be65ae2dcc5be8c41db2f53ea75d5476caef17f94e58d28be48",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.5",
    "stem": "A remote site collects 20 TB of media files daily and must perform format conversion on-site before uploading to AWS. On-site connectivity is limited. Which solution fits?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS DataSync over VPN with an EC2 proxy for local transformation.",
      "B": "Deploy AWS Storage Gateway Tape Gateway and convert files on-premise then archive.",
      "C": "Use AWS Snowball Edge with embedded Lambda functions to perform format conversion and then ship to AWS.",
      "D": "Use AWS Snowmobile for bulk transfer and convert files in AWS Batch."
    },
    "explanation": "Snowball Edge allows on-device Lambda functions for edge transformation, handling large volumes with limited connectivity before shipping back to AWS."
  },
  {
    "id": "1ec1277620bf2bf5405b4ebde8133d6d598976c6edf7ee49b3d3e1dd9b9978e3",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.2",
    "stem": "Thousands of 1 MB JSON event files per hour arrive in S3. You need to convert them to Parquet immediately but avoid Lambda cold-start overhead and scaling limits. What is the best design?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Configure S3 event notifications to invoke a Glue ETL job per object.",
      "B": "Use S3 event notifications to publish to an Amazon SQS queue, batch messages for one minute, then trigger a single Lambda to process the batch to Parquet.",
      "C": "Use Athena CTAS in response to each S3 event via AWS Batch jobs.",
      "D": "Deploy an always-on EMR cluster with a Spark Streaming job consuming S3 events."
    },
    "explanation": "Batching S3 events in SQS reduces the number of Lambda invocations, avoiding cold-start and concurrency limits while still providing near-real-time transformation."
  },
  {
    "id": "cfc1bf04bb505eb5b0c657216961cd8680d8eb53e43b4e6de25fc7779e101b7e",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.4",
    "stem": "Your organization plans to enforce a strict, evolving schema for streaming data and prevent unauthorized schema changes. Which service and feature combination should you use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Kinesis Data Streams with custom schema validation in AWS Lambda.",
      "B": "Amazon S3 object metadata with policy checks on PutObject.",
      "C": "AWS Glue Schema Registry integrated with Kinesis Data Firehose record format conversion and AWS IAM policies.",
      "D": "AWS Glue Data Catalog crawlers with JSON schema auto-inference and Athena enforcement."
    },
    "explanation": "Glue Schema Registry provides a centralized schema repository, enforces schema evolution rules, and integrates with Firehose to reject unauthorized changes automatically."
  },
  {
    "id": "940ddc93212abcf2065ffa41fbe0503e47bd19688476b68f9ee249454abe90f2",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.1",
    "stem": "A gaming company streams 100 MB of log data every hour from 200 EC2 instances. Logs must be retained for one year, accessed fewer than twice per quarter, and require sub-second retrieval when needed. The solution must minimize storage costs. Which storage tier and configuration should you choose?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Store logs in S3 Glacier Instant Retrieval with a lifecycle policy to transition objects immediately after upload.",
      "B": "Store logs in S3 Standard-Infrequent Access (Standard-IA) and transition objects after 30 days.",
      "C": "Use S3 Intelligent-Tiering to automatically move objects between frequent and infrequent access tiers.",
      "D": "Store logs in S3 One Zone-Infrequent Access (One Zone-IA) and transition objects after 7 days."
    },
    "explanation": "S3 Glacier Instant Retrieval offers the lowest storage cost for data accessed infrequently yet requiring millisecond retrieval. Standard-IA is more expensive per GB-month; Intelligent-Tiering adds monitoring charges; One Zone-IA reduces durability and risks data loss."
  },
  {
    "id": "c9106de32d37188b8e18e8127af9ee19277619b8a02c0a06bce0f892cb9f70db",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.1",
    "stem": "Your data analytics team runs EMR clusters for one week every month to process a 50 TB dataset. The data remains unchanged between runs and must be persistently stored. During processing, the cluster needs high throughput, but no access outside of EMR. Which storage solution minimizes cost while meeting throughput needs?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Store the dataset in S3, configure EMR to use S3 as input with S3-optimized connectors, and delete intermediate HDFS data after processing.",
      "B": "Attach gp3 EBS volumes to EMR cluster nodes and keep volumes provisioned month-round.",
      "C": "Attach st1 EBS volumes to EMR cluster nodes and snapshot monthly.",
      "D": "Use Amazon FSx for Lustre linked to S3 and keep the file system always provisioned."
    },
    "explanation": "Storing raw data in S3 and using EMR\u2019s S3-optimized connector eliminates continuous EBS provisioning costs. gp3 and st1 incur charges even when clusters idle; FSx for Lustre provisioned capacity is costly."
  },
  {
    "id": "10da4ce8a1cef615fda58d6942d792086438bce8f851884c53374e922ae2ac0b",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.1",
    "stem": "A biotechnology firm must store 10 TB of genomic data that is rarely accessed but must be archived for compliance for 7 years. Retrieval is expected once or twice per year and can tolerate up to 12 hours of latency. The solution should minimize long-term storage costs. Which service and tier should be used?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "S3 Glacier Instant Retrieval with a 7-year lifecycle policy.",
      "B": "S3 Standard-IA with a 30-day transition to S3 Glacier Instant Retrieval.",
      "C": "S3 Glacier Deep Archive with a lifecycle policy transitioning objects immediately to Glacier Deep Archive.",
      "D": "S3 One Zone-IA with a 7-year retention bucket lock."
    },
    "explanation": "Glacier Deep Archive is the lowest-cost tier for data rarely accessed with retrieval times up to 12 hours. Glacier Instant Retrieval incurs higher storage costs; Standard-IA is more expensive; One Zone-IA lacks multi-AZ durability needed for compliance."
  },
  {
    "id": "73447555154f63432eee2a7e9bbd5799586eacadfd5c322afe3d0582e0cd96a0",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.1",
    "stem": "Your application generates small (<128 KB) files continuously and writes directly to S3. You observe high PUT request costs. Which design change will most effectively reduce storage and request costs without affecting data accessibility?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Enable S3 Intelligent-Tiering to reduce request costs automatically.",
      "B": "Aggregate files into larger archives (e.g., daily tar files) before uploading to S3.",
      "C": "Switch to S3 Standard-IA to reduce request costs.",
      "D": "Use S3 Transfer Acceleration to batch PUT requests."
    },
    "explanation": "Aggregating small files reduces number of PUT requests and overall request cost. Intelligent-Tiering doesn\u2019t reduce PUT requests; Standard-IA has same request cost; Transfer Acceleration reduces latency, not number of requests."
  },
  {
    "id": "1f553da13e6ba468506ab785b9925359f5312d15bbeef6f973d51d8a56658801",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.1",
    "stem": "A video streaming service stores user-generated videos in S3. Storage costs are high, but frequently-watched videos must be instantaneously available. Infrequently-watched content can tolerate retrieval times of seconds. How should you design your S3 storage tiering?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Store all videos in S3 Standard and rely on CloudFront for caching.",
      "B": "Store all videos in S3 Intelligent-Tiering without lifecycle policies.",
      "C": "Store new uploads in S3 Glacier Instant Retrieval and restore watched videos to S3 Standard.",
      "D": "Store new uploads in S3 Standard, apply lifecycle rules to transition videos to S3 Intelligent-Tiering IA after 30 days."
    },
    "explanation": "Using Standard for new uploads ensures instant access; Intelligent-Tiering IA tier automatically moves infrequently accessed objects, reducing storage cost while keeping retrieval latency within milliseconds. Glacier requires manual restores."
  },
  {
    "id": "67bc1adb63ae6d8518e84fa6a3a80d0eeed554e926f5930e4232deb02d3fc696",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.1",
    "stem": "A legacy on-premises file share with 500 TB of infrequently accessed data is migrating to AWS. The data must remain accessible via NFS, but you must minimize storage costs. Which AWS service should you deploy?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon EFS Standard with lifecycle policy to Infrequent Access after 30 days.",
      "B": "Amazon FSx for Windows File Server with Standard HDD deployment.",
      "C": "AWS Storage Gateway File Gateway backed by S3 with lifecycle policies.",
      "D": "Amazon EFS One Zone with no lifecycle policies."
    },
    "explanation": "File Gateway presents NFS backed by S3 and supports lifecycle rules to reduce cost. EFS Standard and FSx incur high persistent costs; EFS One Zone doesn\u2019t integrate S3 lifecycle policies as effectively."
  },
  {
    "id": "48ea73d34501481993d37d8f27bbb2b027e300adff1ddd967e064605238a40f3",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.1",
    "stem": "A financial services company uses EBS volumes for database snapshots that accumulate to 200 TB monthly. Snapshots are incremental but growing. They require 30-day retention. To reduce snapshot storage costs, which strategy is best?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch snapshots from incremental to full backups every 30 days.",
      "B": "Use data lifecycle manager to automate incremental snapshots and prune older backups beyond retention.",
      "C": "Export snapshots to S3 Standard-IA and delete original snapshots.",
      "D": "Copy snapshots to another region and back to leverage cross-region deduplication."
    },
    "explanation": "Data Lifecycle Manager automates incremental snapshots and pruning reduces storage usage. Full backups increase cost; exporting to S3 Standard-IA isn\u2019t supported; cross-region copy doubles storage cost."
  },
  {
    "id": "6f4dc7f14429b98ab70c711cbd8681149d22dfc51932ea7ff8e52c9caae87c5d",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.1",
    "stem": "An IoT application writes large batch files to S3 via Storage Gateway Volume Gateway. You need to minimize storage costs for volumes that are mostly cached. Which caching configuration is most cost-effective?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set cache storage equal to the entire dataset size.",
      "B": "Use File Gateway instead of Volume Gateway.",
      "C": "Configure Volume Gateway in cached mode with default cache size.",
      "D": "Configure Volume Gateway in cached mode with minimal cache size covering working set only."
    },
    "explanation": "In cached mode, only the working set requires on-premises cache; bulk data resides in S3 billed at lower rates. Over-provisioning cache wastes on-premises resources; File Gateway replaces iSCSI interface, not needed."
  },
  {
    "id": "6a0a2ec4ee33520f480f863cc1e77fc84eb1eb0fcd4fd8d655ee3fb5a7bf4d58",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.1",
    "stem": "You maintain an archive of compliance documents in S3. Direct Connect data transfer charges are significant when restoring large archives to your on-premises system. Which option reduces egress costs for large restores?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Use S3 Transfer Acceleration over the internet.",
      "B": "Enable S3 Cross-Region Replication to a nearer region.",
      "C": "Request an AWS Snowball device, import data locally, and return the device.",
      "D": "Use a VPC endpoint for S3 to eliminate data transfer charges."
    },
    "explanation": "Snowball import avoids egress charges by physically shipping data. Transfer Acceleration increases AWS charges; cross-region replication doesn't reduce on-prem egress; VPC endpoints only apply to VPC traffic."
  },
  {
    "id": "507f46305f6e82fc73c087affbe02d180fba710809890fc4ac20bbde555e1c7a",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.1",
    "stem": "A backup solution writes daily database snapshots to EBS and then exports them to S3. Snapshot size varies unpredictably. You need to minimize storage cost and ensure backups older than 90 days are deleted automatically. Which design achieves this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use an S3 bucket with a lifecycle policy transitioning objects to Glacier after 30 days and Expiry after 90 days.",
      "B": "Export snapshots to S3 Standard using EBS direct APIs and apply a lifecycle policy to expire objects after 90 days.",
      "C": "Use AWS Backup with a 90-day backup plan for EBS and transition to Cold Storage tier after 30 days.",
      "D": "Schedule Lambda to delete EBS snapshots older than 90 days and rely on S3 versioning."
    },
    "explanation": "AWS Backup supports Cold Storage (cost-optimized) and automated retention. Exporting manually to S3 requires custom lifecycle but lacks Cold Storage; Lambda deletes snapshots not S3 exports; S3 lifecycle alone ignores Cold Storage tier."
  },
  {
    "id": "f5c24ba5a2198ecf09e8dbef1baaa05791f2cb22d2f45cba151aff3c92ab8c5f",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.1",
    "stem": "An archival database uses Amazon RDS with automated backups storing 10 TB of backups. You want long-term retention (1 year) and to minimize cost. Which combination is most cost-effective?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Keep automated RDS snapshots for 35 days and export to S3 Standard-IA for older backups via AWS Backup.",
      "B": "Increase automated retention to 365 days and rely on RDS automated backups.",
      "C": "Use AWS Backup to copy RDS snapshots to its Cold Storage vault with a one-year retention policy and set automated backup retention to default.",
      "D": "Manually snapshot RDS monthly and store snapshots in S3 Glacier Deep Archive."
    },
    "explanation": "AWS Backup\u2019s Cold Storage vault offers lowest cost for long retention and automates lifecycle. RDS automated backup retention beyond 35 days isn\u2019t supported; manual snapshots and export to Glacier requires custom automation."
  },
  {
    "id": "1e3b88ede1536225b59490b3f48185d95e410849cbe897a2f9b305ac17b74e89",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.1",
    "stem": "A web application stores user uploads in S3. Uploads are read-heavy for one week, then rarely accessed. To reduce cost after week one, what lifecycle policy should you implement?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Transition to S3 Standard-IA after seven days and to Glacier Flexible Retrieval after thirty days.",
      "B": "Transition to S3 One Zone-IA after seven days and delete after sixty days.",
      "C": "Transition to Glacier Instant Retrieval after seven days and to Deep Archive after ninety days.",
      "D": "Transition to S3 Standard-IA after seven days and to S3 Glacier Deep Archive after ninety days."
    },
    "explanation": "Standard-IA after seven days balances retrieval performance and cost; Deep Archive after ninety days yields lowest long-term cost. One Zone-IA risks data loss; Glacier Instant Retrieval storage cost is higher than Standard-IA."
  },
  {
    "id": "eb9474fa91fb9ea534e05a0d7a68abab205dcfc4f8be5a302514659cf69b11f6",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.1",
    "stem": "A media company needs to transfer 50 TB of raw footage monthly from on-premises to AWS. Network bandwidth is limited, and they need the lowest data transfer cost. Which solution meets the requirement?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "Use AWS DataSync over VPN.",
      "B": "Ship data monthly using AWS Snowball Edge Storage Optimized.",
      "C": "Transfer via S3 Transfer Acceleration.",
      "D": "Set up a permanent Direct Connect link."
    },
    "explanation": "Snowball Edge eliminates VPN data transfer costs for large monthly shipments. DataSync incurs egress and VPN costs; Transfer Acceleration adds charges without solving bandwidth limits; Direct Connect is expensive to provision for low-frequency transfer."
  },
  {
    "id": "1384577fd4257d4d328953b8d197eacd323be8d4d649b4ee4a4fddcec3b2625e",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.1",
    "stem": "Your application stores temporary files on EBS gp3 volumes that are deleted after processing. You observe idle volumes incurring costs. How can you redesign to lower storage costs?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use instance store volumes for temporary data instead of EBS.",
      "B": "Automate deletion of EBS volumes after processing.",
      "C": "Switch gp3 volumes to st1 volumes.",
      "D": "Use EFS with lifecycle policy to IA for all temporary files."
    },
    "explanation": "Instance store is ephemeral and incurs no storage charges. While deleting EBS volumes helps, instance store is simpler and avoids snapshot management. st1 and EFS incur storage charges."
  },
  {
    "id": "e4d475b988c0914dbbf2425d89b9f357c0f4272e3e9b19e3921b188dbbf28e1f",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.1",
    "stem": "A scientific project needs to move petabytes of sensor data daily into AWS for analysis. Bandwidth spikes cause high egress charges using DataSync. How to minimize transfer cost while ensuring daily data availability in S3?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set up Direct Connect with a dedicated circuit for consistent transfer.",
      "B": "Use S3 Transfer Acceleration to optimize network usage.",
      "C": "Deploy multiple Snowball devices in rotation\u2014ship one while filling the next.",
      "D": "Use AWS Storage Gateway File Gateway for synchronous uploads."
    },
    "explanation": "Rotating Snowball devices avoids internet egress charges for large daily volumes. Direct Connect is costly; Transfer Acceleration adds charges; File Gateway still uses internet bandwidth."
  },
  {
    "id": "003dfaf61a9731dfbab84d88f3a9265860942c776d84d1bc1344843ef8d10c65",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.2",
    "stem": "A media\u2010processing pipeline runs continuously, requiring exactly 16 vCPUs and 64 GiB RAM. The team can commit to one year of usage. Which reservation or Savings Plan option provides the largest cost savings compared to On\u2010Demand, assuming instance family and region usage remains constant?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "One-year No Upfront Standard Reserved Instance (M5.4xlarge)",
      "B": "One-year All Upfront Standard Reserved Instance (M5.4xlarge)",
      "C": "One-year All Upfront Convertible Reserved Instance (C5.4xlarge)",
      "D": "One-year Compute Savings Plan with full utilization"
    },
    "explanation": "For a fixed-family, fixed-region, steady workload, a Standard RI All Upfront yields the highest discount (~72%) versus Convertible RI (slightly lower) or Compute Savings Plan (~66%)."
  },
  {
    "id": "a966fc49d5d4e2747bb947f5ccd9192464b357d084232751f052682ddffe8b6f",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.2",
    "stem": "A development cluster uses GPU instances for short experiments but must preserve GPU\u2010local data after each run. They run jobs Mon\u2013Fri 08:00\u201318:00; outside those hours the cluster is idle. What is the most cost-optimized design to preserve data and minimize billing?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Switch to Spot Instances and rely on AWS saving of ephemeral storage",
      "B": "Use EBS-backed G4dn instances and stop/start outside hours",
      "C": "Convert GPU instances to EBS-backed roots, enable stop/start schedule outside hours, and snapshot EBS by Lambda upon stop",
      "D": "Enable EC2 hibernation for G4dn instances"
    },
    "explanation": "GPU hibernation isn\u2019t supported. Stopping EBS-backed instances stops compute charges; a Lambda snapshot preserves data. Spot would lose local data."
  },
  {
    "id": "827cb65142e3ea3bc1aaee04b9b54beac533ad27199feb43d8fc519cf88c6d16",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.2",
    "stem": "A remote construction site with no reliable network for 3 months needs on-site GPU inference for safety cameras. There is no AWS Region nearby and no year-long commitment. Which compute option is most cost-effective?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "AWS Snowball Edge (Compute optimized with GPU) deployed for 3 months",
      "B": "AWS Outposts rack with GPU for 3-month contract",
      "C": "On-Demand EC2 G4dn instances in nearest Region accessed over VPN",
      "D": "Lambda@Edge with GPU extension"
    },
    "explanation": "Snowball Edge can be rented for short term without long-term commitment. Outposts requires a one-year minimum. On-Demand EC2 over unreliable network is impractical; Lambda@Edge doesn\u2019t offer GPU."
  },
  {
    "id": "4fb18c7e5288d8c31d89edf3b94f0e80e11c688e682711121ddd160b1b3b2c93",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.2",
    "stem": "A microservice processes unpredictable bursts of transactions requiring <100 ms CPU. The team needs lowest cost for spiky traffic with minimal idle overhead. Which compute service should they choose?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Lambda on-demand invocations",
      "B": "AWS Fargate with a baseline cluster",
      "C": "EC2 Auto Scaling Group of t3 instances",
      "D": "Reserved EC2 instances with custom baselines"
    },
    "explanation": "For unpredictable, short-lived CPU bursts, Lambda\u2019s per-invocation billing with no idle cost is most economical. Fargate and EC2 incur baseline idle charges."
  },
  {
    "id": "e7c56b336f7c1b6c6d71a928703c672f9707027fdfac6d3cc687baf16e6aba60",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.2",
    "stem": "A TLS-encrypted web application serves low, steady throughput (100 requests/minute). Which load balancer minimizes cost while satisfying encryption termination?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Network Load Balancer with TCP passthrough",
      "B": "Classic Load Balancer with SSL termination",
      "C": "Application Load Balancer with HTTPS listener",
      "D": "Gateway Load Balancer with TLS inspection"
    },
    "explanation": "At low, steady LCU usage, ALB\u2019s per-LCU cost (~$0.008/hour) is lower than NLB (~$0.0225/hour) and CLB (~$0.025/hour). GLB is unrelated."
  },
  {
    "id": "008f72da0a9116ed5c519cf108ab6d18d5cd86fa3bde6d74383f9d4ea96fbf3e",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.2",
    "stem": "A batch job with thousands of short-lived tasks can tolerate interruptions. The goal is lowest compute cost. Which EC2 purchasing and allocation strategy should you apply?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "On-Demand Spot Fleet without allocation strategy",
      "B": "Spot Fleet with capacity-optimized allocation strategy",
      "C": "Reserved Instances for worst-case capacity",
      "D": "Compute Savings Plan applied to On-Demand instances"
    },
    "explanation": "Spot with capacity-optimized allocation reduces interruptions and is cheapest for interruptible batch. Savings Plans or RIs aren\u2019t as low as Spot for batch."
  },
  {
    "id": "b693f12269be561e8588fec2535e75830b4583ce64ca10e31228ce4c984314f8",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.2",
    "stem": "A latency-sensitive network application needs high packet rate and low jitter. Which EC2 instance family minimizes cost while meeting enhanced networking requirements?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "c5.large with default networking",
      "B": "t3.large burstable instance",
      "C": "c5n.large with ENA",
      "D": "m5.large general-purpose"
    },
    "explanation": "c5n provides high packet rate and ENA at low cost. c5 lacks the network performance, t3 is bursty, m5 is general-purpose and more costly."
  },
  {
    "id": "98b12d3a74d7f77a5f6371ec76c27cc4c4dd902b7a1bdaf5b35b2a997d557fd5",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.2",
    "stem": "A global application has a pilot-light DR in a secondary Region that must incur minimal cost when idle but scale quickly on failure. Which design is most cost-optimized?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Warm standby with minimum Fargate tasks in DR Region",
      "B": "Active-active across Regions with weighted endpoints",
      "C": "On-Demand EC2 fleet in DR Region kept running at small size",
      "D": "Pilot-light: single micro EC2 instance plus RDS snapshot in DR Region"
    },
    "explanation": "Pilot-light keeps minimal EC2 running and snapshots stored, minimizing compute cost. Warm standby and active-active incur more running cost."
  },
  {
    "id": "634deef13493337f2c8879f48a630473ed1998047cfa5cf034fd46adbb54cc5e",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.2",
    "stem": "A containerized service runs 24\u00d77 at steady throughput. Which deployment yields the lowest monthly compute cost?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Fargate with On-Demand tasks",
      "B": "Amazon ECS on EC2 with one-year Standard RIs for cluster instances",
      "C": "AWS Lambda with Provisioned Concurrency",
      "D": "EKS on Fargate with Savings Plan"
    },
    "explanation": "Steady, long-running containers on EC2 with Standard RIs gives highest discounts. Fargate, Lambda Provisioned Concurrency, and Fargate Savings Plan are more costly at constant load."
  },
  {
    "id": "80d7bbab13e17b56a3d6fea964250292c5e178597aec00cfd659b896893edf3f",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.2",
    "stem": "Your architecture allows switching instance families but not Regions. Which Savings Plan gives the best discount while permitting family and size changes within a Region?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Compute Savings Plan",
      "B": "EC2 Instance Savings Plan",
      "C": "Standard Reserved Instance",
      "D": "Convertible Reserved Instance"
    },
    "explanation": "Compute Savings Plan offers highest flexibility across families/sizes/OS within Region. Instance SP only covers one family; RIs are less flexible."
  },
  {
    "id": "6309d212642953fdee0f6c972561dc8a4565aba9e3c2323d57b6db6868fc7aaf",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.2",
    "stem": "A GPU cluster experiment runs ten hours per day, five days per week, and must persist local GPUs\u2019 state between runs. Hibernation isn\u2019t supported on GPU. How do you minimize cost and preserve data?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Convert to Spot GPU instances and snapshot afterward",
      "B": "Use AWS Outposts GPU equipment with pay-per-use",
      "C": "Switch to a Lambda GPU layer",
      "D": "EBS-back GPU instances, schedule stop/start outside hours, and daily EBS snapshots via Lambda"
    },
    "explanation": "GPU hibernation not supported. Stopping EBS-backed instances stops compute charges; snapshots preserve data. Spot risks data loss; Outposts and Lambda GPU aren\u2019t viable."
  },
  {
    "id": "fb24c88e3eb6ae9f977e87df3869802304ea4c1d8948a0af7bffb4b81d012ccd",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.2",
    "stem": "A licensing partner requires bare-metal EC2 with host-level isolation 24\u00d77. You need to minimize long-term cost. Which strategy is most cost-optimized?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "On-Demand Dedicated Hosts",
      "B": "EC2 Dedicated Instances with Convertible RIs",
      "C": "EC2 Dedicated Hosts with three-year Standard Reserved Hosts",
      "D": "Dedicated Hosts on a Compute Savings Plan"
    },
    "explanation": "Reserved Dedicated Hosts for three years yield the deepest discount. On-Demand is costly; Dedicated Instances lack host affinity; Savings Plans don\u2019t apply to hosts."
  },
  {
    "id": "42b0fcfdd3ce8a21fc36ad9663d50fc889012bc9a9be4cd9e32ee73f4774b59d",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.2",
    "stem": "A compliance workload must preserve in-memory state across nightly restarts. You enable EC2 hibernation every night for 12 hours. Which resource continues to incur charges during hibernation?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "vCPU and RAM usage",
      "B": "EBS volume storage equal to RAM+root volume",
      "C": "ENI networking allocation",
      "D": "Load balancer hourly charges"
    },
    "explanation": "During hibernation, EC2 compute stops billing, but EBS storage (RAM snapshot + root) is charged. ENI and LB charges are separate but not part of hibernation."
  },
  {
    "id": "6d248da0320c2fa18ace07b99cbbcb8aea10b73545894bdc17880070b8bc0d3a",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.2",
    "stem": "A large-scale ML training runs sporadically and tolerates interruption. You deploy a Spot Fleet across three AZs. Which allocation strategy minimizes cost while maximizing likelihood of uninterrupted capacity?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "lowest-price allocation strategy",
      "B": "capacity-optimized allocation strategy",
      "C": "diversified allocation strategy",
      "D": "balanced allocation strategy"
    },
    "explanation": "Capacity-optimized minimizes interruptions by favoring capacity pools with spare capacity. Lowest-price risks frequent interruptions; diversified and balanced spread risk but not minimize cost interruptions."
  },
  {
    "id": "58b6e256055a69e3579ac1820ce2c6059d4c8d3e24c7a6afe66758ce2dbacc0a",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.2",
    "stem": "A video encoding job leverages Lambda versus Fargate and requires p99 latency <200 ms under steady load of 200 concurrent tasks. The development team finds Lambda cold starts impact SLAs. Which option is most cost-optimized while meeting latency?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Fargate with fixed 200 task concurrency",
      "B": "Lambda with Provisioned Concurrency maintained at 200",
      "C": "EC2 Auto Scaling group of t3.large instances",
      "D": "API Gateway direct integration to ECS tasks"
    },
    "explanation": "Fargate avoids Lambda provisioned concurrency\u2019s idle cost and cold starts, while meeting latency. Lambda PC at 200 concurrency incurs large idle billing; EC2 ASG has management overhead."
  },
  {
    "id": "690f6761405f22eb2b3e317c80dede32cc9312c2f80413aa2b04a7c199f31043",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.3",
    "stem": "A company runs an Aurora MySQL provisioned cluster that experiences unpredictable variable workloads and requires consistent performance. The cluster is idle 70% of the time but sees sporadic high concurrency requiring up to 64 Aurora Capacity Units (ACUs). The business wants to minimize cost without sacrificing performance. Which solution provides the most cost-efficient architecture?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Migrate the cluster to Aurora Serverless v2 with a minimum capacity of 2 ACUs and a maximum of 64 ACUs.",
      "B": "Continue using the provisioned cluster but implement scheduled scaling to adjust instance classes at night and during business hours.",
      "C": "Migrate to Aurora Serverless v1 because it can scale down to zero during idle periods and scale up on demand.",
      "D": "Replace the cluster with a provisioned writer (r5.large) and add multiple reader replicas to handle concurrency spikes."
    },
    "explanation": "Aurora Serverless v2 provides fine-grained, nearly instant scaling between 2 and 64 ACUs, avoiding 24\u00d77 provisioning costs. Serverless v1 has cold-start latency and only scales by whole ACUs. Scheduled scaling cannot handle unpredictable spikes, and adding replicas increases cost when idle."
  },
  {
    "id": "07cb284c72cae03bdda07b7ea20341873abcdfe3cc71c21f3185ad6700525871",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.3",
    "stem": "A finance application uses DynamoDB for transactional data. It has predictable daily throughput of 500 Write Capacity Units (WCUs) and 1,000 Read Capacity Units (RCUs) except for occasional trading spikes that double traffic for 5 minutes each day. The team wants the lowest monthly cost while ensuring performance. Which capacity configuration should they choose?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Provisioned capacity with Application Auto Scaling: minimum 500 WCU/1,000 RCU and maximum 2,000 WCU/2,000 RCU.",
      "B": "On-demand capacity mode to handle unpredictable spikes without manual provisioning.",
      "C": "Provisioned static capacity of 1,000 WCU/2,000 RCU to cover peak usage.",
      "D": "Provisioned capacity with minimum 500 WCU/1,000 RCU and maximum 800 WCU/1,200 RCU auto-scaling."
    },
    "explanation": "Auto Scaling with a maximum of 2,000 WCU/2,000 RCU covers spikes at only the moments needed. On-demand is more expensive for predictable workloads, static provisioning wastes capacity, and too low a max will still throttle during spikes."
  },
  {
    "id": "24dd6fdb71152ce3044b63435193405e7f00fbfe68f5e2b3038ecf67ddb94128",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.3",
    "stem": "A startup has dev/test Oracle workloads on Amazon RDS running multi-AZ 24\u00d77 with 100 GB of gp2 storage. They are only used during business hours. The team wants to cut costs for non-production without affecting dev/test performance. Which solution is the most cost-optimized?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch to single-AZ RDS Oracle instances, schedule stop/start outside business hours, and migrate storage to gp3 volumes.",
      "B": "Maintain multi-AZ, downgrade engine version, and migrate to gp3 storage.",
      "C": "Migrate dev/test to Amazon Aurora PostgreSQL Serverless v1.",
      "D": "Use AWS DMS to migrate dev/test workloads to DynamoDB on-demand."
    },
    "explanation": "Stopping single-AZ instances outside business hours eliminates compute charges when idle, and gp3 offers lower storage cost. Multi-AZ isn\u2019t needed for dev/test, and migrating to Aurora or DynamoDB adds migration effort and may not support Oracle features."
  },
  {
    "id": "3a8e4d7946817dd65c0652a701d36f18408664600185338acaa34b7bb62d191d",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.3",
    "stem": "A company requires automated incremental backups of an Aurora PostgreSQL cluster to a secondary region for disaster recovery with an RPO of 24 hours and 30-day retention. They currently use custom cross-region snapshot scripts and pay high storage costs. How can they optimize cost while meeting requirements?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Backup to automatically copy backups cross-region with a lifecycle policy to archive to S3 Glacier after 7 days.",
      "B": "Continue using snapshot scripts but reduce snapshot retention in the secondary region to 7 days.",
      "C": "Enable Aurora Global Database for replication to the secondary region.",
      "D": "Use AWS CLI to copy automated snapshots and delete snapshots older than 30 days."
    },
    "explanation": "AWS Backup\u2019s cross-region copy plus built-in lifecycle to Glacier minimizes storage cost. Reducing retention to 7 days breaks the 30-day requirement, Aurora Global Database replicates data continuously but is more expensive, and manual CLI copying still stores snapshots in RDS pricing tiers."
  },
  {
    "id": "27858ffbe7044ae78f48fddee2baf72d4e6c576f51092406767782be5ae923f9",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.3",
    "stem": "An application uses DynamoDB provisioned capacity with Application Auto Scaling to handle read-heavy traffic. During sudden spikes, auto-scaling takes up to 15 minutes to adjust, causing throttling. On-demand mode is prohibitively expensive. Which approach minimizes throttling and cost?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Keep provisioned mode but increase the maximum throughput further and rely on burst credits.",
      "B": "Switch entirely to on-demand mode despite higher cost.",
      "C": "Use provisioned mode with a shorter scale-out cooldown period.",
      "D": "Implement DynamoDB Accelerator (DAX) to cache reads and reduce read capacity consumption."
    },
    "explanation": "DAX offloads read requests to an in-memory cache, reducing RCU usage and avoiding throttling during spikes. Increasing max throughput or cooldowns still incurs higher idle cost or slow scaling; on-demand is too costly."
  },
  {
    "id": "9396c6b2d6df3976f61146591aaaaeb3a5aeeae919a1b32c13b9cf1af8f53879",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.3",
    "stem": "A global retailer uses DynamoDB global tables for active-active replication between the US and EU regions. They only need write operations in the US and mostly read operations in the EU. They want to reduce write and storage costs in the EU. What should they do?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Replace global tables with a single US primary table and use DynamoDB Streams plus a Lambda function to replicate changes to a separate EU table.",
      "B": "Configure global tables to one-way replication from the US to EU.",
      "C": "Keep global tables but set EU read capacity units (RCUs) to the minimum.",
      "D": "Deploy a DAX cluster in the EU to cache reads from the US table."
    },
    "explanation": "Using Streams and Lambda to populate an EU table allows one-way replication, incurring write costs only once in the US and write charges once per record in the EU, rather than full global tables which double all writes. One-way global tables aren\u2019t supported, and DAX reads still pay inter-region data transfer."
  },
  {
    "id": "dba9324c8281c7130e66c318df5d7589ae1a9cc276999457c590f961725c86e7",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.3",
    "stem": "A production RDS PostgreSQL instance runs at a steady 80% CPU utilization and is expected to continue for the next 3 years. Which purchasing option yields the lowest total cost for this workload?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "One-year Standard Reserved Instance with no upfront payment.",
      "B": "Three-year Standard Reserved Instance with all upfront payment.",
      "C": "Three-year Convertible Reserved Instance with partial upfront payment.",
      "D": "A three-year Compute Savings Plan covering RDS usage."
    },
    "explanation": "A three-year Standard RI with all upfront offers the highest discount level for steady, predictable workloads. Convertible RIs have lower discounts, and Savings Plans provide slightly less discount than Standard RIs for RDS."
  },
  {
    "id": "12d280dd8bcc5f0b078df19dfff4dd74733ba4fa168f1613445e9dfaf7c7bbdf",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.3",
    "stem": "An RDS SQL Server instance uses 100 GB of gp2 storage and experiences moderate I/O requirements. The team wants to cut storage costs by 25% while maintaining 99% of current IOPS performance. Which storage configuration should they choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch to gp3 storage and provision 4,000 IOPS while reducing storage to the necessary capacity.",
      "B": "Switch to standard magnetic (magnetic) storage to save costs.",
      "C": "Reduce gp2 storage to 50 GB to take advantage of burst credits.",
      "D": "Enable gp2 storage autoscaling to upsize only when needed."
    },
    "explanation": "gp3 lets you decouple IOPS from storage size, so you can provision 4,000 IOPS at a lower per-GB cost and choose the exact storage you need. Magnetic storage can\u2019t meet performance; gp2 burst credits are intermittent; gp2 autoscaling only increases storage."
  },
  {
    "id": "820c109d0ee4fced03e25c4705cfb029cd8c80e3436b066404a24f7bf54f3108",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.3",
    "stem": "A development team uses an Aurora MySQL Serverless v2 cluster for intermittent testing workloads. They notice they still incur ACU charges during idle periods because Serverless v2 cannot pause. How can they reduce cost to near zero when idle?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Convert the cluster to Aurora Serverless v1, configure minimum capacity of 1 ACU, and set auto-pause after 5 minutes of inactivity.",
      "B": "Lower the Serverless v2 minimum capacity to 0.5 ACU and rely on capacity scaling.",
      "C": "Continue with Serverless v2 and stop the cluster manually when tests are complete.",
      "D": "Switch to provisioned RDS instances and schedule stop/start outside business hours."
    },
    "explanation": "Aurora Serverless v1 supports auto-pause, stopping charges when idle. Serverless v2 does not support pause. Manual stop on v2 isn\u2019t supported, and switching to provisioned instances loses the serverless benefit."
  },
  {
    "id": "a0f879affdd6983560f748ffbb29639ca49b0a91abf5a9226dc49440e3607a72",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.3",
    "stem": "A company retains automated backups for 35 days on an RDS MySQL instance but only needs 7 days for operational recovery. They also occasionally create manual snapshots for compliance. What is the most cost-effective way to meet these requirements?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Use AWS Backup with a 7-day lifecycle and create manual AWS Backup vault copies for compliance.",
      "B": "Set the automated backup retention period on RDS to 7 days and continue creating manual snapshots as needed.",
      "C": "Keep automated backups at 35 days and export older backups to S3 Glacier.",
      "D": "Use AWS CLI to copy and delete automated snapshots older than 7 days."
    },
    "explanation": "Reducing the RDS automated backup retention period to 7 days minimizes storage cost and still allows manual snapshots for compliance. AWS Backup or manual scripts add complexity or extra storage."
  },
  {
    "id": "a4c0d9957dbefe69228f02650bd5f09d45665849d64eac1b9366a4a14e878427",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.3",
    "stem": "An RDS SQL Server instance was initially provisioned with 200 GB of storage, and storage autoscaling was enabled up to 500 GB. After deleting large tables, the allocated storage remains at 200 GB and does not shrink. The team wants to reduce storage cost. Which action is the most cost-effective?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Take a snapshot of the database and restore it to a new RDS instance with the desired lower storage allocation.",
      "B": "Lower the minimum storage threshold in the autoscaling settings to shrink allocated storage.",
      "C": "Reboot the RDS instance to trigger an automatic shrink of the storage volume.",
      "D": "Switch from gp2 to gp3 storage, which automatically reduces the allocated size."
    },
    "explanation": "RDS storage only scales up; it never shrinks. Restoring from a snapshot to a new instance with smaller storage reclaims unused space. Autoscaling settings don\u2019t shrink, reboots don\u2019t affect storage size, and switching to gp3 retains the same allocation."
  },
  {
    "id": "40934a72f14322a27979bf5755e2ab95b22a8a4552803d9927aff830937feecc",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.3",
    "stem": "A microservices application uses Amazon RDS for PostgreSQL with hundreds of short-lived connections, causing high connection churn. The team considers Amazon RDS Proxy. How will RDS Proxy reduce database costs?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "By pooling and reusing connections, RDS Proxy reduces the per-connection overhead on the database, allowing a smaller instance class.",
      "B": "By caching query results in the proxy layer, RDS Proxy offloads read traffic from the database.",
      "C": "By offloading write operations to the proxy, reducing I/O on the database storage.",
      "D": "By consolidating automated backups and snapshots through the proxy, saving storage costs."
    },
    "explanation": "RDS Proxy pools connections and reuses them, reducing CPU and memory overhead on the database, so you can choose a smaller instance. It does not cache queries or writes, nor does it handle backups."
  },
  {
    "id": "d28a9a4a63b3585e2b3acde403630cd958228be0852967dc1cebb3a1432fb3c3",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.3",
    "stem": "A read-heavy Amazon Aurora MySQL application experiences occasional spikes between 20 ACUs and 100 ACUs for less than 30 minutes per day. Which configuration is most cost-effective?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Provision a cluster with one writer and four reader instances sized for the 100 ACU peak.",
      "B": "Use Aurora Serverless v2 with capacity scaling between 20 and 100 ACUs.",
      "C": "Provision a cluster for 20 ACUs and rely on Application Auto Scaling of reader instances.",
      "D": "Use provisioned capacity for 20 ACUs and add a read replica in another AZ for peak periods."
    },
    "explanation": "Serverless v2 scales automatically and charges only for used ACUs, avoiding paying for 100 ACUs 24\u00d77. Provisioned instances sized for the peak incur full-time cost, and reader replicas with scaling lag or AZ-only don\u2019t address spikes efficiently."
  },
  {
    "id": "ed180186d92d0ee563979457e28460a658c844908b97405ff03331afddc0a24c",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.3",
    "stem": "An application uses Amazon DocumentDB (with MongoDB compatibility) for testing, but development only occurs Monday through Friday. DocumentDB cannot be paused. What is the most cost-optimized approach to minimize charges on weekends?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Take a cluster snapshot on Friday evening, delete the cluster, then restore it from the snapshot on Monday morning.",
      "B": "Configure DocumentDB auto-scaling to zero instances on weekends.",
      "C": "Convert the cluster to a serverless engine to enable pause functionality.",
      "D": "Purchase Reserved Instances for DocumentDB to amortize weekend charges."
    },
    "explanation": "DocumentDB has no pause feature. Taking a snapshot and deleting the cluster stops compute costs; restoring on Monday resumes operations. Auto-scaling to zero and serverless conversion aren\u2019t supported, and Reserved Instances don\u2019t eliminate weekend charges."
  },
  {
    "id": "4083fa6624ab2fd5744007e24693302ead9b0395d53e202bcda85a505a9b1d9f",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.3",
    "stem": "A company uses AWS DMS to migrate a 5 TB on-premises Oracle database to Amazon RDS MySQL with minimal downtime. They want to optimize migration cost and meet an RTO of 1 hour. Which migration strategy is most cost-effective?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use DMS one-time full load followed by ongoing replication (CDC) with a t3.small replication instance, then cut over.",
      "B": "Use DMS full load only and take the application offline to import data for the fastest completion.",
      "C": "Use AWS Snowball Edge to transfer a physical copy and then use DMS CDC to catch up.",
      "D": "Use AWS Database Migration Hub to orchestrate multi-engine migrations offline."
    },
    "explanation": "A t3.small DMS instance for full load plus CDC meets the 1-hour RTO at minimal instance cost. Full load only requires more downtime; Snowball adds device costs and logistics, and Migration Hub orchestrates tasks but doesn\u2019t reduce compute costs."
  },
  {
    "id": "f64c8332ec1434e356f8ff1219b943fcff747386de9e7107ec399cafa81e84d6",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.3",
    "stem": "A company retains expired items in a DynamoDB table to comply with an audit, using TTL to delete items older than 30 days. They observe that storage costs remain high even after items expire. Why is this occurring?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "TTL-expired items are deleted asynchronously and storage space is reclaimed only during periodic compaction, which can take up to 48 hours.",
      "B": "DynamoDB on-demand tables do not automatically delete TTL-expired items.",
      "C": "TTL only marks items as expired; a separate process must delete them manually.",
      "D": "Deleted items are retained in DynamoDB for 7 days for potential recovery and still incur storage charges."
    },
    "explanation": "DynamoDB TTL deletes items asynchronously during table compaction, which may lag by up to 48 hours, so expired items can continue to consume storage briefly. The other statements are incorrect regarding TTL behavior."
  },
  {
    "id": "914bf309270f0093a631d0e6abbc401c1870d46bbc8f2a7975de47ffa0f18155",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.4",
    "stem": "A multinational e-commerce company has two AWS accounts, each hosting a VPC in us-east-1 and eu-west-1. The applications in each VPC need to communicate bi-directionally with high throughput and low latency, and data transfer costs between regions must be minimized. Which networking design will achieve the lowest inter-region data transfer cost while meeting the requirements?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Establish an AWS Transit Gateway in each region and peer the Transit Gateways across regions.",
      "B": "Set up a Site-to-Site VPN over the internet between the two VPCs in different regions.",
      "C": "Create VPC Peering connections between the us-east-1 and eu-west-1 VPCs in each account.",
      "D": "Use AWS Direct Connect public VIFs from each VPC to a co-located router for region-to-region connectivity."
    },
    "explanation": "VPC Peering provides direct, private connectivity between VPCs across regions at the lowest per-gigabyte rate for inter-region AZ traffic. Transit Gateway peering and VPN over internet are more expensive per GB. Direct Connect public VIFs incur data transfer out charges and cross-region data transfer, so peering is most cost-efficient."
  },
  {
    "id": "2ee50511c2423164787a2f9c2c8113e78d94f8b32fc97e47ff8b7e62a063fc93",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.4",
    "stem": "A fintech startup processes sensitive data that must never traverse the public internet. They need to connect their on-premises data center to a VPC in us-west-2. They require predictable bandwidth of at least 5 Gbps and need to minimize ongoing network costs. Which option should the solutions architect recommend?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Establish an AWS Site-to-Site VPN connection over the internet with a single tunnel for 5 Gbps throughput.",
      "B": "Provision an AWS Direct Connect private connection at 5 Gbps to a Direct Connect location in the nearest region, then create a private VIF to the VPC.",
      "C": "Use AWS Transit Gateway with VPN attachments from on-premises and peer with the VPC in us-west-2.",
      "D": "Deploy a high-throughput NAT instance fleet with VPN tunnels for on-premises routing."
    },
    "explanation": "Direct Connect private connection provides dedicated 5 Gbps bandwidth over a private line, avoids internet, and has lower per-Gbps cost at scale. A Site-to-Site VPN cannot guarantee 5 Gbps. Transit Gateway adds complexity and charges. NAT instances are not appropriate for private lines."
  },
  {
    "id": "1f2d2967ab2fa984e6b60834727a184c9868572bd86fdc6c615f709a4ba43057",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.4",
    "stem": "A media company transfers large files daily from S3 in one VPC to EC2 instances in another VPC in the same region. They currently use a NAT gateway and internet gateway for S3 access. Which change will reduce monthly network costs?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch from the NAT gateway to a highly available pair of NAT instances.",
      "B": "Enable S3 Transfer Acceleration and transfer over the accelerated endpoint.",
      "C": "Route traffic through a VPC peering connection to the S3 bucket.",
      "D": "Create a VPC Gateway Endpoint for S3 in the consuming VPC and update the route table."
    },
    "explanation": "An S3 Gateway Endpoint routes traffic over the AWS network without NAT or IGW, eliminating NAT gateway data processing and egress charges. NAT instances may reduce cost slightly but add management. Transfer Acceleration is more expensive. VPC peering cannot reach S3."
  },
  {
    "id": "77f70bb6517507a6c012dfbe43ca97f1e2b273860cae841fff03eae628ed1138",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.1",
    "stem": "An analytics workload queries data across multiple AWS accounts in the same region. Queries retrieve data from two DynamoDB tables. The solutions architect must minimize data transfer costs between accounts. How should the architecture be designed?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure DynamoDB global tables with replication in each account\u2019s VPC and read locally.",
      "B": "Establish VPC peering between the accounts and query the remote table directly over the peering link.",
      "C": "Use AWS Data Pipeline to copy data nightly to a central account and query locally.",
      "D": "Create a Transit Gateway shared by both accounts and configure routing for DynamoDB endpoints."
    },
    "explanation": "DynamoDB global tables replicate data between accounts at DynamoDB internal rates, eliminating cross-account VPC data transfer. VPC peering and Transit Gateway still incur inter-AZ/region data charges. Data Pipeline introduces latency and additional ETL costs."
  },
  {
    "id": "0c050ace2548a4b0381d9da49ee4226654fb42b6fa6f51e4b071a6700f25463e",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.4",
    "stem": "A multinational enterprise runs an application in eu-central-1 and wants to distribute static assets globally with minimal cost. They require HTTPS support, DDoS protection, and want to cache objects at edge locations. Which AWS service configuration optimizes cost and meets requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy an Application Load Balancer in each region and register regional S3 buckets as targets.",
      "B": "Use Amazon CloudFront with S3 origin, standard edge caches, and AWS Shield Standard (default).",
      "C": "Use Amazon Global Accelerator with Application Load Balancer endpoints in each region.",
      "D": "Configure an AWS WAF on the regional Application Load Balancer fronted by AWS Shield Advanced."
    },
    "explanation": "CloudFront with S3 origin provides global caching, HTTPS, and Shield Standard at no extra cost beyond data transfer and requests. Global Accelerator accelerates dynamic TCP/UDP traffic and adds fixed fees. ALB+WAF+Shield Advanced has higher operational cost."
  },
  {
    "id": "408cf6e48bdbf0bd49c8760538e64ae07cf9ee61e1c72bdf37a03419304b6955",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.4",
    "stem": "An enterprise is charged high costs for cross-AZ traffic between EC2 instances behind an ALB with cross-zone load balancing enabled. They have two AZs in us-east-1. How can they minimize these costs without reducing availability?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Disable cross-zone load balancing on the ALB.",
      "B": "Migrate to an NLB which doesn\u2019t charge for cross-AZ traffic.",
      "C": "Keep cross-zone load balancing and colocate backend instances in a single AZ within us-east-1.",
      "D": "Configure the ALB to use private IP addressing and traffic mirroring."
    },
    "explanation": "Colocating backend instances in the same AZ means inter-AZ traffic doesn\u2019t occur and charged cross-zone traffic is eliminated. Disabling cross-zone LB impacts availability. NLB still charges cross-AZ if targets are across AZs. Private IP and mirroring don\u2019t reduce LB traffic costs."
  },
  {
    "id": "0702cb0c25bb95616928293233f3df048e8a522307542c2a49f460ab5ffcf5b9",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.4",
    "stem": "A SaaS provider with multiple customers on separate VPCs wants to centralize DNS resolution for internal hosts while minimizing inter-VPC data transfer costs. Which solution meets the requirement?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy a Route 53 public hosted zone shared across accounts with VPC association.",
      "B": "Use a centralized DNS server on EC2 in one VPC and forward queries through VPC peering.",
      "C": "Configure each VPC\u2019s DHCP options set to point to the centralized server\u2019s private IP.",
      "D": "Create a Route 53 private hosted zone in a shared services account and associate it with each VPC."
    },
    "explanation": "Route 53 private hosted zones can be associated with multiple VPCs at no data transfer cost for DNS queries. EC2-based DNS forwarding over peering incurs data charges and management overhead. Public hosted zones expose records publicly."
  },
  {
    "id": "d26fa69215b6281b61f46e5a77805984990301fc05ae901c1ee3cb95514f7ba7",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.3",
    "stem": "A company\u2019s on-premises data center sends 10 TB/month to an S3 bucket in us-west-2 over the internet via Site-to-Site VPN. Egress charges are high. The company wants to optimize cost but maintain encryption in transit. What should the architect propose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set up an AWS Direct Connect public VIF to S3 with MACsec encryption to reduce egress costs.",
      "B": "Use AWS Transfer Family over SSH to push files directly into S3.",
      "C": "Use S3 Transfer Acceleration to improve throughput and lower egress costs.",
      "D": "Enable VPC Gateway Endpoint and copy data via a NAT gateway to S3."
    },
    "explanation": "Direct Connect public VIF provides lower per-GB egress rates, and MACsec provides encryption on physical link. Transfer Acceleration has a surcharge. Transfer Family is for managed protocols, not cost-effective at scale. Gateway endpoint doesn\u2019t reduce on-premises internet egress."
  },
  {
    "id": "c0b6226d92b2083f78a9079426fe3ecf07f21bfd5706c9d0a0f1c7132cc34184",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.2",
    "stem": "A SaaS application hosted in us-east-1 frequently pulls data from a partner API in another AWS account in us-west-2 over the public internet through an internet gateway and NAT gateway. The solution architect must reduce data transfer cost and improve security. Which change should be made?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "Terminate the partner\u2019s VPN in us-east-1 and route traffic to the NAT gateway.",
      "B": "Establish a cross-account VPC peering connection and route API traffic over private IPs.",
      "C": "Configure an AWS Transit Gateway with VPN attachments from both VPCs.",
      "D": "Use AWS PrivateLink with the partner\u2019s service behind an NLB exposed in us-east-1."
    },
    "explanation": "VPC peering across accounts within the same region routes traffic over private AWS backbone with no NAT gateway charges. Transit Gateway adds per-hour and per-GB cost. PrivateLink is regional and charges endpoint and data processing fees. VPN over IGW/NAT still uses public internet."
  },
  {
    "id": "da9d7c486b06ba5f4b5bf8f6e36e61d9da8f6736d2783ee0209220d00d550983",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.1",
    "stem": "A streaming service uses accelerated mobile pages (AMP) from regional edge caches. They experience frequent cache misses due to short default TTLs, leading to high origin fetch costs for their S3-backed content. How should the solutions architect reduce these costs while ensuring content freshness?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Increase CloudFront default TTL and configure cache invalidation for updates.",
      "B": "Implement Lambda@Edge to dynamically set longer Cache-Control headers.",
      "C": "Switch to AWS Global Accelerator for edge caching and static origin.",
      "D": "Use S3 Transfer Acceleration to reduce origin fetch costs."
    },
    "explanation": "Increasing TTL reduces origin fetch frequency and costs; invalidations ensure freshness only when updates occur, avoiding high egress. Lambda@Edge adds compute costs. Global Accelerator doesn\u2019t cache objects. Transfer Acceleration is for uploads, not caching."
  },
  {
    "id": "9d5e402987a0a5573c1f111ee294232234358b7279a8d8f3f253b98d31caf688",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.3",
    "stem": "An application in us-east-1 writes logs to CloudWatch Logs in us-west-2 for central analysis. They currently use cross-region VPC endpoints over a NAT gateway. Which design minimizes cost and complexity?<newline>",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Send logs via a Transit Gateway peering link between regions.",
      "B": "Publish logs to Kinesis Data Firehose in us-east-1 and replicate to us-west-2.",
      "C": "Configure a CloudWatch Logs cross-region subscription to deliver logs to an S3 bucket in us-west-2.",
      "D": "Deploy a centralized ingest application in us-west-2 that pulls logs via the CloudWatch Logs API."
    },
    "explanation": "Cross-region subscriptions to S3 push data directly within AWS network at standard S3 inter-region replication rates. Using Firehose adds service costs. Transit Gateway peering is expensive. Polling via API adds API call and data transfer costs plus complexity."
  },
  {
    "id": "6db818a5dfa5ed7bd456ab58823db3f95f406c96c537c1332fd60bdee17c7f4f",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "2.1",
    "stem": "A global gaming company wants to reduce latency and data transfer costs for dynamic game sessions between players in Asia and Europe. Players connect to game servers in regionally deployed VPCs. Which service should the solutions architect use to optimize routing and cost?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provision AWS Direct Connect in each region with private VIFs connected to each VPC.",
      "B": "Use Amazon CloudFront for TCP traffic acceleration between regions.",
      "C": "Configure a VPC peering mesh across all game regions.",
      "D": "Deploy AWS Global Accelerator with regional ALB endpoints for the game traffic."
    },
    "explanation": "Global Accelerator provides optimal routing over the AWS backbone for dynamic TCP/UDP traffic, reducing latency and egress charges vs public internet. CloudFront is optimized for HTTP. Direct Connect is impractical for many regions. VPC peering mesh incurs higher inter-region transfer costs."
  },
  {
    "id": "a4b5f38b9280aa61aa2439be749e5745ab2f246fba9129a604a0b2c8eb102b69",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "4.2",
    "stem": "A healthcare application stores patient images in an S3 bucket and processes them on EC2 instances across multiple AZs. Data ingress into S3 is free, but egress to EC2 for processing in a different AZ incurs charges. How can the architect reduce this cross-AZ cost?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable S3 Transfer Acceleration to improve throughput and reduce data transfer time.",
      "B": "Deploy EC2 instances in the same AZ as the S3 bucket and schedule jobs accordingly.",
      "C": "Use a VPC endpoint inside each AZ to access S3, eliminating inter-AZ data transfer costs.",
      "D": "Configure S3 Cross-Region Replication to replicate data to all AZs."
    },
    "explanation": "By running EC2 in the same AZ where the S3 bucket resides, cross-AZ data transfer costs are avoided. S3 Gateway endpoints do not eliminate same-region cross-AZ traffic costs if instances are outside the bucket\u2019s AZ. Transfer Acceleration and replication do not address AZ-level transfer charges."
  },
  {
    "id": "1aa5b1eb1d24d7d75075ded0d62d3ed689cc8e88ea5fe981721e59c66122a7d5",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "1.1",
    "stem": "A financial services firm uses inter-account VPC peering for high-throughput data sharing across multiple VPCs. Their peering topology has become complex and incurs high management overhead. They want to simplify and reduce cost per GB for peered traffic. What should they implement?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Migrate peering to a centralized AWS Transit Gateway in the same region and attach each VPC.",
      "B": "Use AWS PrivateLink endpoints between each pair of VPCs.",
      "C": "Replace peering with VPC Endpoints for S3 and DynamoDB.",
      "D": "Convert peering connections to VPN tunnels to route through a hub EC2 router."
    },
    "explanation": "A regional Transit Gateway centralizes connectivity, simplifies topology, and has a lower per-GB data transfer rate than VPC peering when many VPCs are attached. PrivateLink has per-GB and per-endpoint charges. VPC endpoints only work for APIs. VPN tunnels over EC2 add complexity and cost."
  },
  {
    "id": "5f5f886030f0b661078b30a015a5157fb5ba7d8314ff3a5fb75b468fb7b000fd",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.5",
    "stem": "A financial trading firm generates approximately 10,000 JSON-formatted events per second on-premises. They require near real-time ingestion of these events into Amazon S3, transformation into Parquet, and partitioning by date and hour so that data is queryable in Amazon Athena within two minutes of arrival. A solution architect initially recommends Kinesis Data Streams with a consumer application running on EC2 instances to perform the transformation and delivery. Why is this solution suboptimal, and which architecture best meets the requirements with minimal operational overhead?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "Use Amazon Kinesis Data Streams with a fleet of EC2 instances running custom transformation code to convert JSON to Parquet and write to S3.",
      "B": "Deploy Amazon Kinesis Data Firehose with a Lambda data transformation function to convert JSON records to Parquet, configure Firehose buffering and S3 date/hour prefixes, and query directly via Athena.",
      "C": "Use Amazon Kinesis Data Streams with AWS Lambda triggers on shards to convert records to Parquet and write to S3 with date/hour prefixes.",
      "D": "Build an AWS Data Pipeline that launches an EMR cluster every hour to pull JSON files from Kinesis Data Streams, convert to Parquet, and write to S3."
    },
    "explanation": "Kinesis Data Firehose natively handles ingestion to S3, buffering, partitioning via prefix templates, and integrates with Lambda for record transformation, minimizing operational overhead compared to managing EC2 or EMR clusters. Lambda-on-Streams (option C) can meet latency but requires custom shard management and scaling. EMR and custom EC2 add unnecessary complexity."
  },
  {
    "id": "c502651a5d64181889a8d154f22598f41acdbf60290c780a6536a8dc61eeb846",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.5",
    "stem": "Your organization is migrating a petabyte-scale on-premises file repository into a data lake on AWS. The requirements are:\n1. Perform an initial bulk transfer of all existing data.\n2. Continuously replicate daily delta changes.\n3. Convert incoming CSV files into ORC format and store them partitioned by year and month.\n4. Minimize operational overhead and handle schema evolution.\nWhich combination of AWS services best addresses these requirements?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Snowball Edge for the initial bulk transfer, AWS DataSync for daily deltas, and AWS Glue Python shell jobs to convert CSV to ORC and write partitioned data to S3.",
      "B": "Use AWS Snowball for the initial bulk transfer, deploy a custom rsync solution on EC2 for daily deltas, and run an Amazon EMR Hadoop cluster to convert CSV to ORC.",
      "C": "Use AWS DataSync for both the initial bulk transfer and daily deltas, and configure an AWS Glue Spark ETL job with schema inference and auto-scaling to convert CSV to partitioned ORC.",
      "D": "Use AWS Direct Connect for the initial bulk transfer, AWS DataSync for deltas, and an EC2-based custom application to convert CSV to ORC and upload to S3."
    },
    "explanation": "AWS DataSync supports high-throughput bulk and incremental transfers with minimal setup. AWS Glue Spark ETL provides serverless, auto-scaling processing, built-in schema inference, and partitioning. Snowball adds overhead, and custom EC2/EMR solutions increase operational complexity."
  },
  {
    "id": "5a7bf635dd1415bf9814d91f497f99dc2f17bec22a83a0d807c498f28c307bf0",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.5",
    "stem": "A SaaS provider must onboard sensitive CSV datasets (up to 10 TB/day) from multiple third-party partners using SFTP. Data must land encrypted in Amazon S3, then be converted automatically to Parquet format and cataloged for analytics. The solution should minimize operational management and meet strict security controls (encryption at rest/in transit, access auditing). Which solution meets these requirements?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy AWS Transfer Family SFTP endpoints backed by Amazon S3 (with SSE-KMS). Configure S3 event notifications to trigger AWS Lambda, which submits an AWS Glue ETL job to convert CSV to Parquet and update the AWS Glue Data Catalog.",
      "B": "Launch an EC2-based SFTP server writing to Amazon EFS (encrypted). Use AWS DataSync to move files to S3, then schedule an EMR cluster daily to convert CSV to Parquet.",
      "C": "Use AWS Transfer Family FTPS to an S3 bucket with SSE-S3. Schedule AWS Batch jobs on EC2 Spot instances to run custom conversion scripts for Parquet.",
      "D": "Build an API Gateway REST API invoking a custom Fargate task to receive CSV uploads, encrypt and store in S3, then trigger AWS Glue Studio pipelines for conversion."
    },
    "explanation": "AWS Transfer Family natively supports SFTP, integrates with S3 (SSE-KMS), and offloads server management. S3 events and Lambda/Glue ETL automate Parquet conversion and catalog updates. Other options introduce more components and operational overhead."
  },
  {
    "id": "0267dad1878c1e76795ca886ac96c9545fc09b81bbfc264496f27890748cd085",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.5",
    "stem": "An ad-tech company needs to ingest 50,000 clickstream events per second, enrich them with geolocation metadata, store both raw and enriched data in Amazon S3, and make enriched data immediately available for Redshift Spectrum queries. The end-to-end processing latency must be under 5 seconds. Which architecture best fulfills these requirements with the least operational complexity?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon Kinesis Data Streams to ingest events, invoke AWS Lambda for enrichment, then push both raw and enriched records into two separate Kinesis Data Firehose delivery streams writing to S3, and query with Redshift Spectrum.",
      "B": "Use Amazon Kinesis Data Firehose with a Lambda data transformation function for geolocation enrichment, configure Firehose to deliver raw data to one S3 prefix and enriched data to another via copy-to-S3 feature, and use Redshift Spectrum on the enriched prefix.",
      "C": "Deploy Amazon MSK to ingest events, run an EMR streaming Spark job for enrichment and writes to S3, and query via Redshift Spectrum.",
      "D": "Set up Amazon API Gateway to receive events, process enrichment in Fargate tasks, write results to S3, and use AWS Glue Catalyst for cataloging and Redshift Spectrum."
    },
    "explanation": "Kinesis Data Firehose with Lambda transformations provides managed ingestion, enrichment, and dual-prefix delivery to S3 with <5 s latency and minimal operational overhead. Managing MSK/EMR or API Gateway/Fargate adds complexity and potential latency."
  },
  {
    "id": "3f51ae374afc9b35918554c0540ee808e7b74ec9780ac0a5dabdfc19c4e59451",
    "exam": "AWS Certified Solutions Architect - Associate (SAA-C03)",
    "taskStatement": "3.5",
    "stem": "A manufacturing company receives 500 GB per day of IoT sensor data files deposited into Amazon S3. They need to convert these files from CSV to Parquet, partition by sensor type, and make the data available in Amazon Athena and QuickSight. They require elastic scaling, built-in schema evolution handling, and minimal operational administration. Which compute option is most appropriate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Schedule an AWS Glue Spark ETL job with auto-scaling DPUs, schema inference enabled, and dynamic partitioning on sensor type to convert CSV to Parquet.",
      "B": "Provision an Amazon EMR cluster with auto-scaling and run a Spark job scheduled via Oozie to process and partition the data.",
      "C": "Trigger an AWS Lambda function for each file to convert CSV to Parquet and write partitioned output to S3.",
      "D": "Develop a Python shell AWS Glue job on a single DPU to iterate files, convert to Parquet, and handle partitioning."
    },
    "explanation": "AWS Glue Spark ETL jobs provide serverless, auto-scaling compute, built-in schema evolution, and dynamic partitioning, minimizing admin overhead. EMR adds cluster management, Lambda cannot handle large files efficiently, and Python shell lacks scalability."
  },
  {
    "id": "24182800169c878ab88030df3ee1de70f18679acc44b1127072581fceb6eb0ac",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.2",
    "stem": "A financial services company wants to automatically provision infrastructure changes and be able to revert changes instantly when issues occur. They enforce \u201cinfrastructure as code\u201d and \u201cautomated rollback\u201d capabilities. Which pillar of the AWS Well-Architected Framework most directly applies?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Operational excellence",
      "B": "Security",
      "C": "Reliability",
      "D": "Cost optimization"
    },
    "explanation": "Operational Excellence includes the principles of performing operations as code and making reversible changes through automation. While Reliability addresses recovery, rollback is primarily an operations practice."
  },
  {
    "id": "00f859723b3293dab67f7ba70f328d1b4594712d6d6976d51c6bc033447af35b",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.2",
    "stem": "A global media company designs a streaming platform that must automatically fail over to alternate Regions when an Availability Zone becomes unhealthy, and automatically test the recovery process periodically. Which design principle and pillar are they following?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Test recovery procedures\u2009\u2013\u2009Reliability",
      "B": "Automate to make architectural experiments\u2009\u2013\u2009Performance efficiency",
      "C": "Use immutable infrastructure\u2009\u2013\u2009Operational excellence",
      "D": "Apply security at all layers\u2009\u2013\u2009Security"
    },
    "explanation": "\u2018Test recovery procedures\u2019 is a principle under the Reliability pillar, ensuring that failover and recovery processes are validated regularly."
  },
  {
    "id": "d2caa7d068e59b42ce1dccce6f2bb8981369a9fe35d1c5733e2e5c5e42149763",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.2",
    "stem": "A startup adopts AWS Lambda and container services with pay-per-use billing to eliminate idle server costs and accelerate deployments. Which Well-Architected Framework principle does this best exemplify?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Adopt a consumption model\u2009\u2013\u2009Cost optimization",
      "B": "Optimize for cost\u2009\u2013\u2009Performance efficiency",
      "C": "Use managed services\u2009\u2013\u2009Reliability",
      "D": "Design for failure\u2009\u2013\u2009Reliability"
    },
    "explanation": "\u2018Adopt a consumption model\u2019 under Cost Optimization encourages paying only for what you use to minimize idle resource costs."
  },
  {
    "id": "43346b45634ccc1f231b7304f6394799525065fcc328f5482c87e7df61d25402",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.2",
    "stem": "During a security review, an AWS Solutions Architect suggests enabling automatic key rotation, enforcing multi-factor authentication at the IAM user level, and modeling traceability of changes. Which pillar\u2019s design principles are these examples of?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Security",
      "B": "Operational excellence",
      "C": "Reliability",
      "D": "Performance efficiency"
    },
    "explanation": "Security pillar principles include enabling traceability, implementing a strong identity foundation, and centralized key management (automatic rotation)."
  },
  {
    "id": "e210f0c846240b67c4882162012b0575dada3a275560043d4d906f2ffe1f7558",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.2",
    "stem": "A biotech firm automates resource provisioning and teardown using AWS CloudFormation with parameterized templates for daily test runs, ensuring environments can be decommissioned when not in use. Which design principle aligns with this approach?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Perform operations as code\u2009\u2013\u2009Operational excellence",
      "B": "Stop guessing capacity\u2009\u2013\u2009Performance efficiency",
      "C": "Economies of scale\u2009\u2013\u2009Cost optimization",
      "D": "Test recovery procedures\u2009\u2013\u2009Reliability"
    },
    "explanation": "\u2018Perform operations as code\u2019 under Operational Excellence advocates automating procedures and deploying resources through repeatable scripts."
  },
  {
    "id": "2d8465fcccb09882b52a03be99fccdaf3802db7d3f763075501fc1b93d1d0f06",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.2",
    "stem": "A social gaming company routes player traffic to the nearest edge location for content caching to minimize latency and reduce backend load. Which AWS Well-Architected Framework pillar and principle does this illustrate?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Performance efficiency\u2009\u2013\u2009Use edge locations",
      "B": "Reliability\u2009\u2013\u2009Geographically distribute",
      "C": "Cost optimization\u2009\u2013\u2009Use horizontal scaling",
      "D": "Security\u2009\u2013\u2009Protect data in transit"
    },
    "explanation": "\u2018Use edge locations\u2019 is a Performance Efficiency principle that reduces latency by caching content closer to users."
  },
  {
    "id": "1fae18b27186f618d60e8822c1f9f2c6aa8ef7740c1b40d271a11b50c9e24449",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.2",
    "stem": "A regulated enterprise must ensure all configuration changes are approved via an audit trail, and all operational events are logged centrally for forensic analysis. Which pillar\u2019s design principle does this requirement reflect?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Security\u2009\u2013\u2009Enable traceability",
      "B": "Reliability\u2009\u2013\u2009Test recovery procedures",
      "C": "Operational excellence\u2009\u2013\u2009Perform operations as code",
      "D": "Cost optimization\u2009\u2013\u2009Implement usage monitoring"
    },
    "explanation": "\u2018Enable traceability\u2019 under the Security pillar requires logging and monitoring to track changes and for post-event analysis."
  },
  {
    "id": "f65947c10bff5f82c9c9f304962bf3b75e4360cd4829054ec9b87652a0e0f5f4",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.2",
    "stem": "A retail application experiences unpredictable demand spikes. The architecture uses Amazon EC2 Auto Scaling with predictive policies and DynamoDB on-demand capacity mode. Which pillar and principle does this embody?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Performance efficiency\u2009\u2013\u2009Use scalable and elastic resources",
      "B": "Reliability\u2009\u2013\u2009Implement elasticity",
      "C": "Cost optimization\u2009\u2013\u2009Right-size resources",
      "D": "Operational excellence\u2009\u2013\u2009Make operations observable"
    },
    "explanation": "\u2018Use scalable and elastic resources\u2019 is a Performance Efficiency principle that ensures resources automatically adjust to load."
  },
  {
    "id": "9ca7af329bedcc801606a326ce138b6862380271d6a317506987cc8a01ef9bb5",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.2",
    "stem": "A financial trading system implements Amazon ElastiCache for in-memory caching and multi-AZ replication to ensure sub-millisecond latency and resilience. Which pair of Well-Architected principles are demonstrated under Performance Efficiency and Reliability, respectively?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Performance Efficiency\u2009\u2013\u2009Use caching; Reliability\u2009\u2013\u2009Automatically recover from failure",
      "B": "Performance Efficiency\u2009\u2013\u2009Experiment more often; Reliability\u2009\u2013\u2009Test recovery procedures",
      "C": "Performance Efficiency\u2009\u2013\u2009Implement best-fit resources; Reliability\u2009\u2013\u2009Scale vertically",
      "D": "Performance Efficiency\u2009\u2013\u2009Use managed services; Reliability\u2009\u2013\u2009Shield workloads"
    },
    "explanation": "Caching is a Performance Efficiency principle; automated failover/replication is a Reliability principle to recover from failures."
  },
  {
    "id": "cac9efb14085eda3c3d37b9197c9753c92f96b44014e6e8dbb006ea1550b7730",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.2",
    "stem": "A healthcare provider tests infrastructure changes in a sandbox environment and integrates AWS CloudTrail logs into AWS Config for drift detection before production deployment. Which two Well-Architected design principles are they applying?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Operational excellence\u2009\u2013\u2009Perform operations as code; Security\u2009\u2013\u2009Enable traceability",
      "B": "Reliability\u2009\u2013\u2009Test recovery procedures; Performance efficiency\u2009\u2013\u2009Use edge locations",
      "C": "Cost optimization\u2009\u2013\u2009Adopt a consumption model; Security\u2009\u2013\u2009Implement a strong identity foundation",
      "D": "Operational excellence\u2009\u2013\u2009Evolve procedures with feedback; Reliability\u2009\u2013\u2009Automatically recover from failure"
    },
    "explanation": "Automating sandbox provisioning is \u2018Perform operations as code\u2019 (Operational Excellence) and capturing/configuring logs is \u2018Enable traceability\u2019 (Security)."
  },
  {
    "id": "1e59b86f03c09da55c0ff82b2a2650a113f33a2950b862dbe5dd8fa28c6a37ff",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.3",
    "stem": "A large enterprise runs a monolithic .NET application on-premises and wants to migrate it to AWS. The application only requires minor performance tuning and a managed runtime environment, with minimal code changes. The team also wants to reduce their operational overhead by leveraging AWS-managed services. According to the AWS Cloud Adoption Framework migration strategies, which approach should they choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Refactor \u2013 rewrite key modules to use AWS-native microservices.",
      "B": "Replatform \u2013 lift, shift, and optimize to a managed .NET platform (for example, AWS Elastic Beanstalk).",
      "C": "Rehost \u2013 perform a lift-and-shift of virtual machines without platform changes.",
      "D": "Retain \u2013 keep the application on-premises and migrate later."
    },
    "explanation": "Replatform involves making minimal changes to take advantage of managed services like Elastic Beanstalk. Refactor requires significant code changes. Rehost does not leverage managed runtimes. Retain leaves the app on-premises, failing the requirement."
  },
  {
    "id": "e6292df833a3be28b1a0adefcc5d5c6278e5a1406ac5413999302911d604e20c",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.3",
    "stem": "An organization wants to standardize and automate the provisioning of its AWS infrastructure using templates, continuous integration, and repeatable processes. In which AWS Cloud Adoption Framework (CAF) perspective would these concerns primarily fall?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Platform Perspective",
      "B": "Operations Perspective",
      "C": "Governance Perspective",
      "D": "Security Perspective"
    },
    "explanation": "The Platform Perspective addresses infrastructure provisioning, automation, and DevOps practices. Operations focuses on day-to-day support. Governance covers policies and compliance. Security covers controls and risk management."
  },
  {
    "id": "d583876776f054a42e0c867cbda69925e0c615ac64d75fbc099634dfec2a0dab",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.3",
    "stem": "During the planning phase of a migration, a company needs to create a detailed application dependency map and gather performance metrics from its on-premises servers. Which AWS service provides the deep discovery data required to plan migration waves and resource sizing?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Migration Hub",
      "B": "AWS Config",
      "C": "AWS Application Discovery Service",
      "D": "Amazon CloudWatch Agent"
    },
    "explanation": "AWS Application Discovery Service collects detailed inventory, performance, and dependency data. Migration Hub provides a tracking dashboard but relies on discovery sources. Config tracks AWS resource configurations, not on-premise. CloudWatch Agent collects metrics but not dependency mapping."
  },
  {
    "id": "2baf34d09f3566664e0123e69ed7d626d924c9a69962280fcc2d0d5986225952",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.3",
    "stem": "A financial institution running an on-premises Oracle database with near-zero acceptable data loss (RPO of seconds) needs to migrate to AWS with minimal downtime. Which combination of migration strategy and AWS tool best meets the requirement?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Replatform with AWS Snowball Edge for bulk data transfer.",
      "B": "Refactor using AWS Schema Conversion Tool to move to a managed service.",
      "C": "Rehost using AWS Database Migration Service with change data capture enabled.",
      "D": "Repurchase by subscribing to Amazon RDS for Oracle and exporting full backups manually."
    },
    "explanation": "Rehosting with DMS and CDC supports ongoing replication and minimal downtime. Snowball Edge is for offline bulk transfer only. Refactoring with SCT requires schema conversion and code changes. Repurchasing RDS and manual exports cannot ensure near-zero RPO."
  },
  {
    "id": "be6655f36b5d13dc6630eb8af0043b115bc09b774120faf6f9c2032d4fb46979",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.3",
    "stem": "A mid-sized company wants a prescriptive, comprehensive migration plan that includes estimated cost savings, best practices, reference architectures, and change management guidance. Which AWS offering should they engage?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Prescriptive Guidance",
      "B": "AWS Migration Acceleration Program (MAP)",
      "C": "AWS Quick Start reference deployments",
      "D": "AWS Migration Hub"
    },
    "explanation": "MAP provides funded, end-to-end migration planning, TCO analysis, prescriptive best practices, and organizational change management. Prescriptive Guidance offers best practices but not funded planning or TCO estimates. Quick Starts deploy patterns. Migration Hub tracks progress but does not guide strategy."
  },
  {
    "id": "12c880bfb67dd2b8024aab23bf3f0cb923aa738485f2a1c0b36905e6512b4fc4",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.3",
    "stem": "An enterprise realizes its workforce lacks cloud skills and needs to define training, role changes, and staffing requirements as part of its cloud adoption. Which AWS CAF perspective addresses this need?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Business Perspective",
      "B": "Governance Perspective",
      "C": "People Perspective",
      "D": "Security Perspective"
    },
    "explanation": "The People Perspective focuses on organizational change management, skills development, and role definitions. Business covers strategy and business case. Governance covers policies. Security covers controls."
  },
  {
    "id": "e3d7a61f6fccdb50066503915c117957449f3d385584c40b6defa76fae39f80d",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.3",
    "stem": "A retailer plans to move a Java-based web application to AWS. They require automatic scaling and environment management but prefer no major code refactoring. They also want to minimize DevOps overhead. Which migration strategy is most appropriate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Rehost on EC2 with Auto Scaling groups.",
      "B": "Replatform using AWS Elastic Beanstalk for Java.",
      "C": "Refactor into microservices on EKS.",
      "D": "Repurchase by adopting a SaaS ecommerce platform."
    },
    "explanation": "Replatform with Elastic Beanstalk minimizes code changes while providing managed environment and scaling. Rehost requires more DevOps work. Refactor is complex. Repurchase changes the application entirely."
  },
  {
    "id": "db6e305241f98e646e9c8a20f025803212b428337ad4e0531b896687d5753e21",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.3",
    "stem": "When coordinating a complex migration involving multiple AWS accounts, various application teams, and heterogeneous workloads, which AWS service offers a centralized dashboard to track each application\u2019s migration status, health, and dependencies?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Migration Hub",
      "B": "AWS Control Tower",
      "C": "AWS Systems Manager",
      "D": "AWS Organizations"
    },
    "explanation": "AWS Migration Hub provides a single pane to monitor migration status, collect per-application metrics, and view dependencies. Control Tower sets up accounts. Systems Manager manages instances post-migration. Organizations manages billing and accounts."
  },
  {
    "id": "ae5382713d9509c761dc35e0ac80a36aaa86844ecefd08315b82cbf7116bea31",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.3",
    "stem": "A company wants to perform a lift-and-shift migration of its on-premises VMs to AWS, run test instances to validate performance and configurations, and then cut over with minimal downtime. Which migration strategy and AWS service combination meet these requirements?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Rehost using AWS Application Migration Service",
      "B": "Replatform using AWS Database Migration Service",
      "C": "Refactor using AWS Server Migration Service",
      "D": "Repurchase by migrating data with AWS DataSync"
    },
    "explanation": "Rehosting via AWS Application Migration Service continuously replicates VMs for testing and supports minimal-downtime cutover. DMS is for databases only. Server Migration Service is deprecated. DataSync transfers files, not full VM environments."
  },
  {
    "id": "70d1e7bf2ecaca1e10453accd85d3789c10d339232ef77296ff4a31039e7c92e",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.3",
    "stem": "A CRM application is nearing end-of-life, and the maintenance cost exceeds any benefit of migration. The organization decides not to move this workload to AWS. According to AWS migration strategies, which approach should they take?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Replatform by adopting a different managed CRM service.",
      "B": "Refactor into a cloud-native application.",
      "C": "Repurchase an equivalent SaaS CRM.",
      "D": "Retire the application and decommission it."
    },
    "explanation": "Retire means decommissioning workloads that are no longer valuable. Replatform, refactor, or repurchase involve moving or replacing the application, which contradicts the intent to decommission."
  },
  {
    "id": "4c43c313d17e0fcef3907e7c01b18a2f064b77f7ca349f1a04ade4ac264a4fa6",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.4",
    "stem": "A retail company currently spends $120,000 per month on on-premises datacenter hardware, network equipment, and facilities (a fixed cost), plus an additional $0.08 per CPU-hour in power and cooling (a variable cost). After migrating to AWS, they plan to run equivalent workloads on On-Demand EC2 instances at $0.06 per CPU-hour with no upfront fees. Which cost component demonstrates the shift from capital expense (CapEx) to operating expense (OpEx) when moving to AWS?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "The $120,000 monthly datacenter facility lease, because it becomes part of the AWS support plan subscription.",
      "B": "The $0.08 per CPU-hour power and cooling expense, because AWS charges an hourly energy surcharge.",
      "C": "The EC2 On-Demand instance rate of $0.06 per CPU-hour, because compute costs are now paid only as consumed instead of via upfront hardware purchases.",
      "D": "The AWS data transfer charges, because networking becomes a monthly fee instead of an upfront network equipment purchase."
    },
    "explanation": "In an on-premises model, purchasing servers is a CapEx (fixed) cost. On AWS, compute is a usage-based (On-Demand) cost, which is OpEx. The hourly EC2 rate replaces the need for upfront hardware investments, illustrating the CapEx\u2192OpEx shift."
  },
  {
    "id": "5d58a047c7b86f805e57b5284374bfe04a52ba74813166c28030e9c3230d57fd",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.4",
    "stem": "An analytics workload shows a baseline of 5,000 CPU-hours per month with monthly peaks up to 10,000 CPU-hours. Which of the following purchasing strategies will minimize total monthly compute costs while still meeting peak demand?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Purchase Reserved Instances for 10,000 CPU-hours and rely exclusively on them throughout the month.",
      "B": "Purchase Reserved Instances for 5,000 CPU-hours and use On-Demand instances for the additional peak usage.",
      "C": "Use Spot Instances for all 10,000 CPU-hours, since Spot pricing is always lower than Reserved and On-Demand pricing.",
      "D": "Use On-Demand Instances for the baseline and Spot Instances for all peak usage above 5,000 CPU-hours."
    },
    "explanation": "Reserving the baseline workload captures the largest, predictable portion at a discount. On-Demand covers peaks. Relying solely on Spot risks interruptions; reserving the entire 10,000 incurs extra unused capacity expense."
  },
  {
    "id": "3807442c0325ff5fdcaccb16e8cadde452a169b9296967e3cd194ccf9611150a",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.4",
    "stem": "A company with a perpetual on-premises license for a commercial database moves to AWS and wants to minimize licensing costs. They can license via \u201clicense included\u201d model on RDS for $0.25 per hour or bring their existing license (BYOL) with Software Assurance at $0.15 per hour. They expect 24\u00d77 usage. Which licensing strategy yields the lowest annual cost, and why?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Use RDS \u201clicense included\u201d because it covers automated backups and patching.",
      "B": "Use the existing license on EC2, since virtualization rights allow unlimited cores.",
      "C": "Continue running on-premises to avoid AWS licensing fees.",
      "D": "Use RDS with BYOL at $0.15 per hour, since continuous usage multiplies into the lowest total cost over the year."
    },
    "explanation": "At 24\u00d77, 365 days: 0.15\u00d724\u00d7365 = ~$1,314 vs 0.25\u00d724\u00d7365 = ~$2,190. BYOL on RDS is cheapest for continuous use; automated features are available in both models."
  },
  {
    "id": "fdf95fd06987c6f38fccb3464b59a2cb1faf6742875c32032f42e0a1bc5d5c1f",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.4",
    "stem": "After migration, a team observes that CPU utilization across EC2 instances averages only 8% over a month, but they are running m5.4xlarge instances. Which cost-optimization action yields the greatest variable savings?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Downsize to a smaller instance family or size so that utilization aligns with instance capacity.",
      "B": "Purchase a 3-year All-Upfront Reserved Instance for the existing m5.4xlarge fleet.",
      "C": "Convert all workloads to Spot Instances on the same m5.4xlarge type.",
      "D": "Enable EBS volume auto-scaling to reduce storage costs."
    },
    "explanation": "Rightsizing to match demand is the most direct way to reduce compute spend. Reserved or Spot on an over-sized instance still results in idle capacity charges."
  },
  {
    "id": "9a59db27844082f33f9dee7d9703909495bee08bcf55af925ce6107edf9671ec",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.4",
    "stem": "A fintech startup needs to run end-of-month batch jobs for 400 hours total and remains idle for the rest of the month. Which cost model combination best minimizes total cost while ensuring the batch jobs complete?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use On-Demand instances for all 400 hours to avoid any upfront commitment.",
      "B": "Use Spot Instances for 350 hours and On-Demand for the remaining 50 hours in case Spot capacity is reclaimed.",
      "C": "Purchase a 1-year No-Upfront Reserved Instance even though usage is only 400 hours per month.",
      "D": "Use Savings Plans for a fixed 400 hours per month workload."
    },
    "explanation": "Spot pricing reduces cost but carries interruption risk. Covering just 50 hours with On-Demand ensures job completion. A 1-year reservation or Savings Plan is not cost-effective for a mostly idle workload."
  },
  {
    "id": "2b33914fa8c189f6bfe6b527993406826cd654a3e92b9c135026ae9fc7f85225",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.4",
    "stem": "Which of the following is best described as a variable cost in an AWS deployment?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "The annual fee for an AWS Enterprise Support plan.",
      "B": "The upfront payment for a 3-year Reserved Instance reservation.",
      "C": "The data egress charges incurred when end users download media from S3.",
      "D": "The cost of purchasing EC2 Dedicated Hosts scheduled monthly."
    },
    "explanation": "Data egress charges vary directly with the volume of data transferred out. Support plan fees and Reserved Instance upfronts are fixed; Dedicated Host scheduling incurs fixed fees per host."
  },
  {
    "id": "11bd12a63ec72a5046e810dc5de1ffb280e744bca6f022f1fe3fb26d262c1eea",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.4",
    "stem": "A media company currently pays $0.023 per GB-month for on-premises storage but incurs high operational staffing costs for scaling and maintenance. On S3 Standard they pay $0.025 per GB-month. Which economic factor most justifies the slightly higher storage unit cost?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Elimination of maintenance labor and hardware replacement overhead, lowering total cost of ownership.",
      "B": "Increased latency of S3 Standard compared to on-premises storage.",
      "C": "Higher data retrieval costs for frequently accessed objects in S3 compared to local disks.",
      "D": "The lack of volume-based tiered pricing in S3 Standard."
    },
    "explanation": "Although per-GB cost is marginally higher, AWS handles hardware maintenance, scaling, and staffing overhead, yielding overall lower TCO. The other options are drawbacks, not cost justifications."
  },
  {
    "id": "c0db0bff848eaa660968d65774bdf2e93f7f5b7766f3abf53ce461322acf4fe6",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.4",
    "stem": "A SaaS provider experiences unpredictable traffic spikes and is evaluating savings plans versus on-demand pricing. Which statement accurately describes an advantage of Compute Savings Plans over Reserved Instances?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Compute Savings Plans provide deeper discounts but require instance type and region locking like Reserved Instances.",
      "B": "Compute Savings Plans automatically apply discounts to any EC2 instance family, size, region, or OS, providing maximum flexibility.",
      "C": "Compute Savings Plans eliminate the need to commit to any hourly usage baseline.",
      "D": "Compute Savings Plans can be applied to on-premises VMware workloads without additional licensing."
    },
    "explanation": "Compute Savings Plans give up to 66% discount and apply to any EC2 compute usage regardless of instance attributes. Reserved Instances require tighter scope commitments."
  },
  {
    "id": "fd47e1a15aac850637b5f636db1ce436d62619f6ea8cd2ea2fe6b0132db4391a",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "1.4",
    "stem": "A company automates its nightly backup process using AWS Backup instead of manual scripts that run 8 hours per night. The automation service charges a flat $100 per month. If the manual process required two administrators at $50/hour each, how much monthly savings does automation yield, and which cost concept does this illustrate?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "$0; it\u2019s a like-for-like swap of staff time for a flat fee, illustrating fixed cost neutrality.",
      "B": "$200; illustrating economies of scale as automation spreads cost across processes.",
      "C": "$700; illustrating variable cost reduction by eliminating per-hour labor.",
      "D": "$1,100; illustrating fixed cost reduction by converting high labor variable costs into a predictable fixed subscription fee."
    },
    "explanation": "Manual cost: 2 admins \u00d7 $50/hr \u00d7 8 hr \u00d7 30 days = $24,000/month. Automation fee $100; savings = $23,900. This converts unpredictable labor costs into a predictable fee, reducing both variable cost and TCO."
  },
  {
    "id": "c9c684c70d6c16b929567282d820f0559669e8b6eb9224cb0ad76f3906fe1473",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.4",
    "stem": "2.4: Your security operations team wants to aggregate and prioritize security findings from AWS services such as AWS Config, Amazon GuardDuty, and Amazon Inspector, as well as integrate third-party security products purchased from AWS Marketplace. Which AWS service provides a single pane of glass for security alerts and is natively integrated with AWS services and Marketplace partners?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Security Hub",
      "B": "AWS Config Aggregator",
      "C": "AWS Systems Manager",
      "D": "AWS CloudTrail Lake"
    },
    "explanation": "AWS Security Hub centralizes and prioritizes findings from AWS security services and integrated third-party solutions in one dashboard."
  },
  {
    "id": "90587bd68280d8740a4703ef24eca9d1428f5f74544a53b3e0c4c8fcf57c4837",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.4",
    "stem": "2.4: Your compliance team needs to retrieve on-demand, up-to-date third-party audit and compliance reports (e.g., SOC, ISO) for AWS services used by your organization. Which AWS component should they use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Artifact",
      "B": "AWS Audit Manager",
      "C": "AWS Support Center",
      "D": "AWS Organizations"
    },
    "explanation": "AWS Artifact is the AWS portal for downloading official AWS compliance and audit reports."
  },
  {
    "id": "abbf480b65e0f725ce6c576071876ad51fec9e3b3c38ef0ce2ede2333e692bef",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.4",
    "stem": "2.4: You are tasked with routinely identifying known software vulnerabilities and unintended network accessibility issues on your fleet of Amazon EC2 instances. Which AWS service is purpose-built for performing host-level security assessments and vulnerability scans on EC2 instances?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Inspector",
      "B": "Amazon Macie",
      "C": "AWS Config",
      "D": "AWS WAF"
    },
    "explanation": "Amazon Inspector is designed to perform automated vulnerability and configuration assessments on EC2 instances."
  },
  {
    "id": "d6aec6203de41c812832392bc16f4c59b39eede5eae738154bdc6e175ba84554",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.4",
    "stem": "2.4: Your data security team needs to automatically discover, classify, and protect sensitive personally identifiable information (PII) stored in Amazon S3 buckets across multiple AWS accounts. Which AWS service should they deploy?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Macie",
      "B": "AWS Shield",
      "C": "AWS Key Management Service (KMS)",
      "D": "AWS CloudTrail"
    },
    "explanation": "Amazon Macie uses machine learning to discover and classify sensitive data in S3, including PII."
  },
  {
    "id": "e1bfdac3560af12663433be8606724316244bb4702ac8e2fbcc809ef98872edc",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.4",
    "stem": "2.4: You need to detect suspicious activity related to unauthorized API calls, credential compromises, and anomalous behavior in your AWS accounts without deploying additional agents. Which AWS service provides threat detection capabilities based on continuously analyzing AWS CloudTrail event logs, VPC Flow Logs, and DNS logs?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon GuardDuty",
      "B": "AWS Config",
      "C": "AWS CloudTrail",
      "D": "Amazon CloudWatch"
    },
    "explanation": "Amazon GuardDuty continuously analyzes CloudTrail, VPC Flow Logs, and DNS logs to identify potential threats."
  },
  {
    "id": "998e53c9cf4e218adf07189bdc9fd88ecc00281eaf9f907ac686b8cc25dff7e0",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.4",
    "stem": "2.4: To enforce that no Amazon S3 buckets in your organization allow public write access and automatically revert any configuration drift, which AWS service and feature combination should you implement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Config with a managed rule and automated remediation",
      "B": "AWS Security Hub with custom action workflows",
      "C": "Amazon CloudWatch Alarms with Lambda auto-remediation",
      "D": "Amazon GuardDuty with Findings Action Streams"
    },
    "explanation": "AWS Config managed rules can detect non-compliant bucket policies and trigger automated remediation actions via AWS Systems Manager."
  },
  {
    "id": "e22e33fabe9e11b0ee406383ece8944b5dc94ae485bb5678c78d5930a3465996",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.4",
    "stem": "2.4: A security analyst needs to stay informed about newly disclosed vulnerabilities, AWS security advisories, and mitigations directly from AWS. Which AWS resource is the most authoritative and continuously updated source for official AWS security bulletins?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Security Bulletins page",
      "B": "AWS News Blog",
      "C": "AWS Marketplace Security Hub Integrations page",
      "D": "AWS Support Health Dashboard"
    },
    "explanation": "The AWS Security Bulletins page publishes official advisories and mitigations as soon as they are released."
  },
  {
    "id": "02b4335c12022d551412938241c7cfb618decf9454b0cf99f6f76686e25b29eb",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.4",
    "stem": "2.4: Your network operations team wants to capture detailed information about IP traffic going to and from network interfaces in your VPC for performance and security analysis. Which AWS feature should they enable?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "VPC Flow Logs",
      "B": "Amazon GuardDuty",
      "C": "AWS CloudTrail",
      "D": "AWS Network Firewall"
    },
    "explanation": "VPC Flow Logs record IP traffic flow data at the network interface level for analysis and logging."
  },
  {
    "id": "deb6090a97c0a621fd8f3d9a3d4574f6a5e71d54daf22229712289e2572f213b",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.4",
    "stem": "2.4: The security team must ensure that all IAM users with console access have at least one active MFA device enabled. Which AWS Config managed rule should they enable?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "iam-mfa-enabled-for-iam-console-access",
      "B": "root-account-mfa-enabled",
      "C": "iam-password-policy",
      "D": "iam-user-unused-credentials-check"
    },
    "explanation": "The iam-mfa-enabled-for-iam-console-access rule checks that each IAM user with console access has MFA enabled."
  },
  {
    "id": "68a1854d219eca3a2dd771646201a8cdd2ca5e651fcafd5e3574505540b7d6b2",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "2.4",
    "stem": "2.4: To centralize API activity logs from all member accounts of your AWS Organization into a single S3 bucket for analysis and long-term retention, which AWS service and feature should you configure?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS CloudTrail organization trail",
      "B": "AWS Config aggregator",
      "C": "AWS Organizations consolidated logging",
      "D": "Amazon Athena cross-account federated queries"
    },
    "explanation": "An AWS CloudTrail organization trail automatically captures API events from all member accounts and delivers them to a central S3 bucket."
  },
  {
    "id": "f98e2504dd4da9cc0c120272595e33eebc7eb683df39ff0c445362e69f8876a5",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.5",
    "stem": "A company operates two VPCs with overlapping CIDR ranges and requires full bidirectional communication between them. VPC peering cannot be used due to overlapping IP spaces. Which AWS network service should they use to achieve this requirement with minimal administrative overhead?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create a Site-to-Site VPN connection between the two VPCs using virtual private gateways",
      "B": "Deploy an AWS Transit Gateway, attach both VPCs, and enable route table propagation",
      "C": "Use AWS Direct Connect gateway to connect both VPCs",
      "D": "Establish an AWS PrivateLink interface endpoint for each VPC to the other"
    },
    "explanation": "A Transit Gateway supports overlapping CIDRs by using separate attachments and route tables, and centralizes connectivity without IP renumbering. VPN with VGW won\u2019t handle overlapping ranges, PrivateLink only exposes specific services, and Direct Connect gateway is for on-premises connectivity."
  },
  {
    "id": "a8b017e43d14cd95c5e37135d4f4e602e650a66de67efb5b5781331f8d731626",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.5",
    "stem": "An application running in private subnets across three Availability Zones needs to download files from an Amazon S3 bucket in the same region without using a NAT gateway or internet gateway. Which combination of steps will satisfy this requirement while adhering to least privilege?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Deploy a NAT gateway in one AZ, update route tables, and restrict its security group",
      "B": "Create an interface VPC endpoint for S3 in each subnet and add bucket policy allowing the endpoint",
      "C": "Create a gateway VPC endpoint for S3, update each subnet\u2019s route table with the S3 prefix list, and apply an endpoint policy restricting to the specific bucket",
      "D": "Configure AWS PrivateLink for S3 and attach it to each subnet\u2019s route table"
    },
    "explanation": "A gateway endpoint for S3 is free, supports prefix lists, and can be restricted via endpoint policy. Interface endpoints aren\u2019t available for S3, PrivateLink isn\u2019t used for S3, and NAT gateways incur cost."
  },
  {
    "id": "0ae9e59cfc9c3e312c10591277e63889fa5835a3d679c3a7056dae3e7e99a074",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.5",
    "stem": "Your ECS tasks in private subnets cannot pull container images from Amazon ECR. You have created an interface VPC endpoint for com.amazonaws.us-east-1.ecr.dkr but still see pull failures. What additional configuration is required?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a NAT gateway to the private subnets and update route tables",
      "B": "Configure VPC peering between the VPC and the ECR service account",
      "C": "Enable VPC endpoint for AWS Secrets Manager",
      "D": "Create interface endpoints for com.amazonaws.us-east-1.ecr.api and a gateway endpoint for S3"
    },
    "explanation": "ECR image pulls use both the ECR API and ECR DKR interface endpoints, and require access to S3 via a gateway endpoint. A NAT gateway isn\u2019t needed if endpoints are configured correctly."
  },
  {
    "id": "a1aacb1efb5cefde46cc2dd6d8dc07d7495468643e3e76afea4a8058fb5ff5da",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.5",
    "stem": "A developer created an interface VPC endpoint for Amazon DynamoDB and S3 in a VPC, but DNS resolution still points to the public service endpoints. What must be enabled to ensure private DNS names resolve to the VPC endpoint?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Enable the \u201cPrivate DNS Name\u201d option when creating the interface endpoint",
      "B": "Associate an Elastic IP with the endpoint network interface",
      "C": "Add a custom host zone in Amazon Route 53 and map the service domain",
      "D": "Configure a forwarding rule in the VPC DHCP options set"
    },
    "explanation": "Interface endpoints support private DNS name integration when the \u201cPrivate DNS Name\u201d option is enabled. This overrides public DNS resolution. The other options don\u2019t enforce AWS service DNS hijacking."
  },
  {
    "id": "e189dd70553f5da00619a6f1e9f61c63398565ce175946f5157aafbf1806a754",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.5",
    "stem": "A global e-commerce application in two regions wants to route users to the lowest latency endpoint. DNS-based routing must automatically fail over if an endpoint becomes unhealthy. Which Amazon Route 53 routing policy should be used?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Weighted routing policy with health checks",
      "B": "Geolocation routing policy",
      "C": "Latency routing policy with health checks",
      "D": "Multivalue answer routing policy without health checks"
    },
    "explanation": "Latency routing directs users to the lowest latency region and health checks enable automatic failover. Weighted routing balances by weight, geolocation directs by user location, and multivalue doesn\u2019t guarantee latency."
  },
  {
    "id": "33e59061f80b76ddd982c95a02bc61bb9bfd060dd384ee4a66a14bb87b138ed4",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.5",
    "stem": "A SaaS provider in Account A wants to expose a custom API to multiple customer VPCs without exposing their VPC to the public internet. Which configuration achieves this with AWS PrivateLink?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Create a NAT gateway and share it via AWS Resource Access Manager",
      "B": "Publish an endpoint service in Account A, enable acceptance, then customers create interface endpoints to that service",
      "C": "Peer each customer VPC to the provider VPC and share security groups",
      "D": "Use AWS Transit Gateway and attach all customer VPCs to it"
    },
    "explanation": "With PrivateLink, the provider publishes an endpoint service, accepts connections, and customers create interface endpoints to access it privately. Transit Gateway and peering expose the entire VPC, and NAT Gateway is irrelevant."
  },
  {
    "id": "62449d151941a549c6a2111d0aca1affdf075c700d5a74e9755289dff3a661f2",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.5",
    "stem": "Your on-premises data center requires private, low\u2010latency connectivity to an Amazon VPC and AWS public services (S3, DynamoDB) in the same region. You already have a Direct Connect connection. Which Direct Connect configuration meets these requirements?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Create a private virtual interface for the VPC and a public virtual interface for AWS public services",
      "B": "Use only a private virtual interface and route public service traffic through the VPC endpoint",
      "C": "Establish two private virtual interfaces to two separate Direct Connect gateways",
      "D": "Leverage a Transit Gateway attachment with a single private virtual interface"
    },
    "explanation": "A private VIF connects to the VPC; a public VIF accesses AWS public services directly. Routing public service traffic via the VPC endpoint over the private VIF isn\u2019t supported, and Transit Gateway doesn\u2019t replace public VIF."
  },
  {
    "id": "94ac2c4fa9738399220307689efb4c428363b728cab1de01aa52853bf3fa3fc4",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.5",
    "stem": "You have a VPC with multiple subnets and enable a gateway endpoint for S3. You notice traffic to S3 is still routing to the internet gateway. Which route table configuration change fixes this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a /0 route pointing to the S3 gateway endpoint",
      "B": "Add a route for the S3 prefix list (pl-xxxx) pointing to the gateway endpoint",
      "C": "Associate the gateway endpoint\u2019s subnet IDs with the main route table",
      "D": "Modify the internet gateway to forward S3 traffic to the endpoint"
    },
    "explanation": "Gateway endpoints use predefined prefix lists (pl-*) that must be added to the subnet route tables pointing to the endpoint. A /0 route would misroute all traffic, and internet gateways can\u2019t forward to endpoints."
  },
  {
    "id": "c539656111542fd3642ee58582e2ae2366048241c8bdc870c122de9b77a20e2b",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.5",
    "stem": "An application must inspect all east-west traffic between private subnets for anomalies. Which AWS network service should you deploy, and how is it integrated?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Network Firewall deployed in its own VPC with VPC peering",
      "B": "AWS WAF attached to an Application Load Balancer",
      "C": "AWS Shield Advanced with custom anomaly detection rules in Transit Gateway",
      "D": "AWS Network Firewall in a centralized inspection VPC, routed via Transit Gateway with firewall route tables"
    },
    "explanation": "Deploying AWS Network Firewall in a centralized inspection VPC with Transit Gateway allows inspection of VPC-to-VPC traffic. WAF and Shield Advanced inspect only perimeter traffic, and peering isn\u2019t scalable."
  },
  {
    "id": "fc753c0e15c4364fb22f8890621c0ae31d047c483478946ae9e9267cf4cacaea",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.6",
    "stem": "A company must store 10 PB of compliance data for 7 years in AWS. Data is accessed no more than twice per year, and retrieval times of up to 12 hours are acceptable. The solution needs the lowest possible storage cost and durability of 99.999999999%. Which S3 storage class should you choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon S3 Standard-Infrequent Access",
      "B": "Amazon S3 Glacier Instant Retrieval",
      "C": "Amazon S3 Glacier Deep Archive",
      "D": "Amazon S3 One Zone-Infrequent Access"
    },
    "explanation": "S3 Glacier Deep Archive offers the lowest storage cost and 11-9s durability, with retrieval within 12 hours. Glacier Instant Retrieval is more expensive and designed for milliseconds retrieval. Standard-IA and One Zone-IA have higher storage costs and are less optimized for year-long archives."
  },
  {
    "id": "e13c0f9f8ec0bf6db568b4473770c6aee57e040f5683cf51556ec24e70ec7775",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.6",
    "stem": "A media production team needs a multi-AZ, POSIX-compliant file system to share large Linux home directories across hundreds of EC2 instances. Throughput must scale with workload, and administrative overhead should be minimal. Which storage service should you deploy?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon EFS Standard with bursting throughput",
      "B": "Amazon FSx for Lustre linked to S3",
      "C": "Amazon S3 with S3FS mounting",
      "D": "Amazon EBS gp3 volumes in each instance"
    },
    "explanation": "Amazon EFS Standard provides managed, elastic, multi-AZ NFS file shares with bursting throughput and no capacity planning. FSx for Lustre requires managing scratch vs persistent mode and isn\u2019t multi-AZ by default. S3FS is not natively POSIX-compliant and adds complexity. EBS volumes are not shareable across instances."
  },
  {
    "id": "4b666bf63a6125702cba931d9d227427a905e9953b19e917cc0ba453c11d5d69",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.6",
    "stem": "A Windows-based application needs a fully managed, multi-AZ SMB file share integrated with Active Directory. Data must be encrypted at rest, and automated daily backups are required. Which service meets these needs with minimal operational effort?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon EFS with AWS Backup for Windows",
      "B": "Amazon FSx for Windows File Server",
      "C": "Amazon S3 with FSx File Gateways",
      "D": "EBS volumes attached to a Windows file server EC2 cluster"
    },
    "explanation": "Amazon FSx for Windows File Server provides a fully managed, multi-AZ SMB share, supports AD integration, encryption at rest, and built-in daily backups. EFS does not support SMB or AD. S3 is object storage and not SMB. DIY EC2 file servers require high operational overhead."
  },
  {
    "id": "6dba58d2f843d20f112802adec86cbc9149bff220eaacb6f503514cde2804ae1",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.6",
    "stem": "A computational chemistry workload requires high-performance, POSIX-compliant file access with sub-millisecond latencies and throughput exceeding 1 GB/s. The data set is stored in S3, and files must be cached locally during processing. Which storage solution is most appropriate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon EFS One Zone Infrequent Access",
      "B": "Amazon FSx for Windows File Server",
      "C": "Amazon FSx for Lustre linked to the S3 data repository",
      "D": "Amazon EBS io2 volumes mounted on each compute node"
    },
    "explanation": "Amazon FSx for Lustre can be linked to an S3 repository, providing a cached POSIX file system with sub-millisecond latencies and high throughput. EFS cannot reach >1 GB/s per client. EBS io2 volumes are block storage and not automatically integrated or cached from S3. FSx Windows uses SMB, not POSIX."
  },
  {
    "id": "8b97278ae305f46c0b85f670ceec0cd60a242da03303bbc42794bc9c297783e8",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.6",
    "stem": "Your security team requires that S3 objects older than 90 days move to a lower-cost tier and after one year move to Glacier Flexible Retrieval. Which feature should you configure?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "An S3 lifecycle policy with two transition rules",
      "B": "AWS Backup plan targeting the S3 bucket",
      "C": "Amazon Data Lifecycle Manager policy",
      "D": "Enable S3 Intelligent-Tiering on the bucket"
    },
    "explanation": "S3 lifecycle policies support multiple transitions, such as 90 days \u2192 Standard-IA and 365 days \u2192 Glacier Flexible Retrieval. AWS Backup does not transition S3 objects. Data Lifecycle Manager manages EBS snapshots, not S3. Intelligent-Tiering automates tiering but doesn\u2019t transition to Glacier Flexible Retrieval."
  },
  {
    "id": "ffc88e500a871f7033336fc009097ef0224fbdbcb9b452d43335251a6580598b",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.6",
    "stem": "An operations team needs automated, cross-region backup and restore of EBS volumes. Snapshots must be encrypted using a customer-managed KMS key. Which service provides this capability with minimal custom scripting?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Data Lifecycle Manager for cross-region snapshot copy",
      "B": "Configure EBS snapshot lifecycle policies in AWS CLI",
      "C": "Enable EBS fast snapshot restore with cross-region copy",
      "D": "Create an AWS Backup vault with cross-region copy enabled"
    },
    "explanation": "AWS Backup integrates EBS volume backup, using customer-managed KMS keys, and supports automated cross-region copy. Data Lifecycle Manager can copy snapshots but requires separate policies per region and lacks the centralized vault abstraction. Fast snapshot restore is for performance, not automated DR."
  },
  {
    "id": "98f885696e3eaa4b72e40456205ef5f561016ca1f9a4e66fd95347cd72ae1332",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.6",
    "stem": "A database team needs 5 000 guaranteed IOPS and 250 MB/s throughput for a critical transactional workload on a 1 TB block volume. They also want to minimize monthly cost. Which EBS volume type should they choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "gp2 volumes sized at 1 TB",
      "B": "gp3 volumes with provisioned IOPS and throughput",
      "C": "io1 volumes for guaranteed performance",
      "D": "st1 volumes for cost-effective throughput"
    },
    "explanation": "gp3 lets you provision IOPS and throughput independently at a lower cost than io1, meeting 5 000 IOPS and 250 MB/s. gp2 provides only 3 000 baseline IOPS on 1 TB and relies on bursting. io1 is more expensive and designed for >16 000 IOPS. st1 is throughput-optimized HDD, not suitable for IOPS requirements."
  },
  {
    "id": "cea6943c10e68d9165f30593cb38e6eabcf303c3d268f718b19968833b0602e2",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.6",
    "stem": "A web analytics team generates daily log files (~100 KB each). They want to minimize storage cost and only need immediate retrieval for logs younger than 30 days, after which retrieval latency of a few minutes is acceptable. Which S3 storage class configuration should they use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Store all logs in Glacier Flexible Retrieval",
      "B": "Store in Standard and apply a lifecycle to Intelligent-Tiering",
      "C": "Use S3 One Zone-IA for all logs",
      "D": "Use a lifecycle that transitions logs to S3 Standard-IA after 30 days and to Glacier Instant Retrieval after 365 days"
    },
    "explanation": "Keeping logs in Standard for the first 30 days ensures immediate access, then transitioning to Standard-IA provides low-cost storage with millisecond retrieval for the next period. Glacier Instant Retrieval is only needed when logs become very cold. Storing all in Glacier or One Zone-IA offers lower availability or slower first-30-day access. Intelligent-Tiering does not transition to Glacier Instant Retrieval."
  },
  {
    "id": "9fffe041c7f1e8f15b2f9ef52de52be4da759fc4a2bcfb377db870f06c771345",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.6",
    "stem": "A global engineering team needs a shared, POSIX-compliant, low-latency file system that can burst to high throughput and supports regional failover. The solution must integrate with AWS Backup. Which combination meets all requirements?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon EFS Standard with AWS Backup",
      "B": "Amazon FSx for Lustre scratch file system",
      "C": "Amazon FSx for Windows File Server with AWS Backup",
      "D": "EBS volumes in each AZ with cross-AZ attestations"
    },
    "explanation": "Amazon EFS Standard is POSIX-compliant, bursts throughput, is accessible across AZs, supports regional failover using DNS failover, and is fully supported by AWS Backup. FSx scratch Lustre doesn\u2019t integrate with AWS Backup and isn\u2019t multi-AZ by default. FSx Windows uses SMB, not POSIX. EBS volumes are not shareable across instances or AZs."
  },
  {
    "id": "c3b6091f145636063198dfdac41b0a0b4f8658d20b4b909fe1223faa1bc07cc8",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.6",
    "stem": "A file share used by critical applications must be encrypted at rest, automatically backed up daily, and copied to a second region for disaster recovery. Which storage service and configuration should you choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon EFS with a cross-region lifecycle policy",
      "B": "Amazon FSx for Windows File Server with AWS Backup cross-region vault",
      "C": "Amazon S3 with cross-region replication and lifecycle to Glacier",
      "D": "Amazon FSx for Lustre with Data Lifecycle Manager"
    },
    "explanation": "Amazon FSx for Windows File Server supports encryption at rest, automated daily backups, and AWS Backup can copy snapshots to another region. EFS does not support cross-region backup natively. S3 and replication are object-based, not SMB shares for applications. FSx Lustre scratch doesn\u2019t integrate with Data Lifecycle Manager or AWS Backup for cross-region DR."
  },
  {
    "id": "1aec815de059dcf724d9260df1ee176428a65d911374858df0671670a2342d6b",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.2",
    "stem": "You manage an AWS Organization with one management account and three member accounts. The CFO requires an email alert when any single member account's monthly spending exceeds $3,000. Which configuration accomplishes this with the fewest resources and correct isolation of alerts per account?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "In each member account, create a monthly cost budget targeting the total cost and configure an SNS email notification to the CFO.",
      "B": "In the management account, create one cost budget with a LinkedAccount filter for each member account combined (comma-separated) and set the threshold at $3,000; configure a single SNS notification.",
      "C": "In the management account, create three separate cost budgets, each filtered by one member account\u2019s LinkedAccount ID with a $3,000 threshold; configure SNS email notification for each.",
      "D": "In the management account, enable CloudWatch billing alarms for each member account\u2019s estimatedCharges metric with a $3,000 threshold; configure email actions."
    },
    "explanation": "AWS Budgets are created in the payer (management) account and can filter by LinkedAccount. To isolate alerts per account, you must create one budget per member account in the management account. CloudWatch billing alarms only track the payer\u2019s total charges. Option B would alert on combined spending. Option A budgets in member accounts are not visible centrally to the CFO. Option D uses billing alarms which cannot be scoped per member account."
  },
  {
    "id": "0f0793d230f7e9544e4500fab0f5796d79a03218b2ffc3b9d2130a6ce5bc1aa9",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.2",
    "stem": "You activate two user-defined cost allocation tags in the Billing Console for your organization. One tag was created yesterday and applied to existing resources; the other created today. You immediately query Cost Explorer and see that the tag created today is available but the one created yesterday has no cost data associated. Why?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Cost allocation tags apply only to usage after activation and require up to 24 hours to appear in Cost Explorer; existing resources tagged before activation will only have cost data after activation.",
      "B": "AWS automatically applies only the most recently activated user-defined tag in Cost Explorer; older tags are deprioritized and archived after 24 hours.",
      "C": "You must enable cost allocation tags in the AWS Organizations Service Control Policy before they propagate across linked accounts; tags not created under Organizations won't show up.",
      "D": "Cost Explorer only supports up to 50 user-defined tags; the tag created yesterday exceeded the limit and was dropped."
    },
    "explanation": "User-defined cost allocation tags are applied prospectively\u2014only to usage after activation\u2014and can take up to 24 hours to propagate into Cost Explorer. They are not applied retroactively. Options B, C, and D describe incorrect behaviors."
  },
  {
    "id": "daaf0e02f212c406af59ca3ceab0b63867f5cb1484f531308882bd5e82fe9fce",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.2",
    "stem": "You need to produce a monthly report of costs attributed to ProjectCode tags across all accounts in your AWS Organization. The report must group costs by ProjectCode and allow flexible mapping of project codes to department names. Which AWS cost management feature provides this functionality?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Organizations with Service Control Policies to enforce ProjectCode, then export AWS Config resource inventory.",
      "B": "Create Cost Categories in the Billing Console mapping ProjectCode values to departments, then use Cost Explorer with Cost Categories grouping.",
      "C": "Use AWS Budgets to create a budget for each ProjectCode tag, then export budget reports.",
      "D": "Configure Cost Allocation Tags for ProjectCode, activate them, and then use AWS Pricing Calculator to group by tag."
    },
    "explanation": "Cost Categories allow you to define custom grouping rules (for example mapping ProjectCode values to department names) and then view or filter costs in Cost Explorer by those categories. Budgets cannot provide flexible grouping, Pricing Calculator is for planning, and AWS Config is unrelated to cost reporting."
  },
  {
    "id": "272e3ca428c7934873baded8e23b5d2607645a1f091ce4c53490eeedf2450766",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.2",
    "stem": "You are designing a solution to estimate the monthly cost of a new application before deployment. The application will provision 10 m5.large EC2 instances for 720 hours each per month, store 5 TB in S3 Standard, and transfer 1 TB of data out per month. Which tool should you use to generate the most accurate cost estimate?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Budgets to forecast costs based on current usage trends.",
      "B": "AWS Cost Explorer to analyze similar historical usage months.",
      "C": "AWS Pricing Calculator, entering the usage details for each service.",
      "D": "Create a Cost and Usage Report and modify it to include the new services."
    },
    "explanation": "The AWS Pricing Calculator is designed to model and estimate costs for planned or new workloads by specifying service usage details. Budgets and Cost Explorer rely on historical usage; Cost and Usage Reports are for detailed billing export, not upfront costing."
  },
  {
    "id": "38a837a7763527f21c2ceee40a060d3e1310c0ce0e7cc3b4ab2d248d7fc01db6",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.2",
    "stem": "Your security team wants to automatically stop underutilized EC2 instances when monthly costs for a development account exceed $1,000. Which combination of AWS features meets this requirement with the least operational overhead?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create a Cost Budget with a $1,000 threshold and configure the budget action to STOP_EC2_INSTANCES on the development account.",
      "B": "Create a Cost Budget with a $1,000 threshold and configure an AWS Chatbot Slack notification to the security team, then have a Lambda function stop instances when notified.",
      "C": "Create a Cost Budget with a $1,000 threshold, configure an SNS topic for notification, subscribe a Lambda function that calls the StopInstances API for tagged development instances.",
      "D": "Use CloudWatch billing alarms to trigger an Auto Scaling policy reducing instance count to zero when estimated charges exceed $1,000."
    },
    "explanation": "AWS Budgets cannot directly perform EC2 actions; you must configure a notification via SNS and then use a Lambda subscriber to stop instances. AWS Chatbot is not required. CloudWatch billing alarms cannot integrate with Auto Scaling for this purpose."
  },
  {
    "id": "9ea301a9d220b68a67595d86fca9e9b67212d0d59a5feeba866948320640d71e",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.2",
    "stem": "A finance team uses Savings Plans across your organization and needs to view how the Savings Plans discount is allocated to each member account. Which tool and configuration should they use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "In Cost Explorer, view the Savings Plans Utilization report and group by LinkedAccount.",
      "B": "In AWS Budgets, create a Savings Plans budget and filter by account.",
      "C": "Use AWS Pricing Calculator\u2019s Savings Plans tab to simulate discount allocations per account.",
      "D": "Enable a Savings Plan cost allocation tag and filter it in a Cost and Usage Report."
    },
    "explanation": "AWS Cost Explorer provides a Savings Plans Utilization report that can be grouped by LinkedAccount to see how discounts are applied per account. Budgets cannot track Savings Plans, Pricing Calculator is for modeling, and Savings Plans cannot be tagged for cost allocation."
  },
  {
    "id": "8c9e3ef74577dbfecc2792e2ec7beadad1ae9e02a90064970884687154e0a987",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.2",
    "stem": "Your team needs hourly-granularity cost data delivered to an S3 bucket with all resource-level details for integration into a third-party BI tool. Which AWS service provides this level of detail?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Cost Explorer API with hourly granularity.",
      "B": "AWS Budgets with hourly report export functionality.",
      "C": "AWS Cost and Usage Report (CUR) configured for hourly delivery to S3.",
      "D": "AWS Pricing Calculator exporting hourly usage predictions."
    },
    "explanation": "The AWS Cost and Usage Report (CUR) can be configured to deliver detailed, hourly-granularity billing data to an S3 bucket. Cost Explorer cannot export data directly to S3 at hourly granularity, Budgets is for notifications, and Pricing Calculator is for estimates."
  },
  {
    "id": "4577e99f3e67a84c4a26fc83247ba1963e5bf0d2f0e9caa3690eb1f7c5ea5d40",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.2",
    "stem": "You have purchased Reserved Instances (RIs) for various EC2 instance types and tagged them with \u201cEnvironment:Production\u201d. Which tool allows you to view the RI utilization percentage specifically for resources tagged as Production?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Cost Explorer RI Utilization report filtered by tag \u201cEnvironment:Production\u201d.",
      "B": "AWS Budgets usage budget filtered by tag \u201cEnvironment:Production\u201d to alert on RI utilization.",
      "C": "AWS Pricing Calculator savings plan comparison for the tag.",
      "D": "AWS Config conformance pack for checking RI tag compliance and utilization."
    },
    "explanation": "AWS Cost Explorer\u2019s RI Utilization report can be filtered by user-defined tags such as \u201cEnvironment:Production.\u201d AWS Budgets cannot measure RI utilization, Pricing Calculator is for modeling, and AWS Config is for compliance, not utilization metrics."
  },
  {
    "id": "35e06d43cd9f187272ccda6bada4e7a7b9cd77cd2c37363bf8a94400e097657f",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.2",
    "stem": "An IAM user in your payer account has the BillingView permission but cannot see cost allocation tags in Cost Explorer for your Organization\u2019s accounts. What must you do to allow the user to view billing information including tags?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Grant the user the aws-portal:ViewBilling permission in the Billing console settings.",
      "B": "Enable IAM Access to Billing in the root account\u2019s Account Settings, then ensure the user has BillingView permissions.",
      "C": "Add the user to a group with CloudWatch billing read-only policy.",
      "D": "Attach the AWS managed policy ReadOnlyAccess to the user."
    },
    "explanation": "Even with BillingView permissions, IAM users must first have IAM Access to Billing enabled in the root account\u2019s Account Settings to view billing information, including cost allocation tags. Granting aws-portal:ViewBilling is part of that enablement. ReadOnlyAccess or CloudWatch policies are insufficient if IAM billing access is disabled."
  },
  {
    "id": "7ec4ed470d88d4fea4975af96631789ddd4bfa6f1739b763114dbc9e9e684079",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "4.2",
    "stem": "The DevOps team wants a report that alerts them when EC2 On-Demand instance hours exceed 5,000 hours in a month across all accounts. Which AWS Budgets configuration achieves this?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Create a cost budget with filter Service = Amazon EC2 and threshold set using the estimated cost of 5,000 hours.",
      "B": "Create a usage budget of type EC2-Service-Usage with threshold of 5,000 instance hours, filtered by Service = Amazon EC2.",
      "C": "Create a reservation budget tracking EC2 Reserved Instances usage with threshold of 5,000 hours.",
      "D": "Create a Savings Plans budget filtered for EC2 to track usage beyond 5,000 hours."
    },
    "explanation": "Usage budgets track usage units (such as EC2 instance hours) and can be filtered by service. Cost budgets track dollar amounts, Reservation budgets track RI usage, and Savings Plans budgets track coverage of Savings Plans."
  },
  {
    "id": "ee2eeaac33ffbd7bfd3559fe83637d108ef8a67d9391184950c2f574673cd369",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.4",
    "stem": "A financial services company runs a critical on-premises Oracle database and wants to minimize operational overhead, automate backups, and achieve built-in high availability without major schema changes. Which AWS database service should they choose?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon RDS for Oracle",
      "B": "Self-managed Oracle on Amazon EC2",
      "C": "Amazon Aurora (MySQL-compatible)",
      "D": "Amazon DynamoDB"
    },
    "explanation": "Amazon RDS for Oracle provides a managed Oracle environment with automated backups, patching, and Multi-AZ high availability. Running Oracle on EC2 requires manual administration. Aurora MySQL does not support Oracle workloads, and DynamoDB is a NoSQL service not suited for Oracle schemas."
  },
  {
    "id": "b300b497f2cac4ceca8a0377803562b8de7c4c628666afcb27a835d5a88d3110",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.4",
    "stem": "An e-commerce application stores session data that must be read and written with sub-millisecond latency and supports replication across AZs. Which AWS database service or feature meets this requirement most cost-effectively?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Amazon ElastiCache for Redis",
      "B": "Amazon DynamoDB Accelerator (DAX)",
      "C": "Amazon RDS Proxy",
      "D": "Amazon S3 with Transfer Acceleration"
    },
    "explanation": "ElastiCache for Redis provides sub-millisecond latency with in-memory performance and offers Multi-AZ replication. DAX only accelerates DynamoDB queries, RDS Proxy is for RDS, and S3 cannot provide sub-millisecond read/write latency."
  },
  {
    "id": "89488a55a9e7c10d29e60d41a9dd49f3fd843f53f1c2a91ba211a2fedba1f1ce",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.4",
    "stem": "A social network needs to model and query complex, highly connected data such as user relationships and recommendations. Which AWS database service is optimized for this use case?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon DynamoDB",
      "B": "Amazon Neptune",
      "C": "Amazon Neptune",
      "D": "Amazon DocumentDB"
    },
    "explanation": "Amazon Neptune is a purpose-built graph database for highly connected datasets and supports Gremlin and SPARQL queries. DynamoDB is a key-value/NoSQL store, DocumentDB is for document models, and only Neptune is optimized for graph workloads."
  },
  {
    "id": "a18ebb13e44665cde91a1c84c8cdc4188eb83a022a610c0864271220c2ddd66c",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.4",
    "stem": "Your team requires a multi-master relational database within a single AWS Region to support writes across multiple Availability Zones with minimal conflict detection. Which AWS service or configuration should you select?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon RDS Multi-AZ deployment",
      "B": "Amazon Aurora Multi-Master DB cluster",
      "C": "Amazon DynamoDB global tables",
      "D": "Amazon ElastiCache for Redis replication"
    },
    "explanation": "Aurora Multi-Master allows writes on multiple nodes across AZs within a single Region. RDS Multi-AZ is standby-only (no writes on standby), DynamoDB global tables span Regions and are NoSQL, and ElastiCache is an in-memory cache, not a primary relational database."
  },
  {
    "id": "f35b914040102019d00df9ba2651076b30f22b75fd27c6248e5173d8ceb3b30b",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.4",
    "stem": "A startup wants to migrate its MySQL database to AWS with minimal downtime and continuous data replication. Which combination of services should they use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Database Migration Service (DMS) alone",
      "B": "AWS Schema Conversion Tool (SCT) alone",
      "C": "Amazon Aurora Serverless",
      "D": "AWS SCT for schema conversion and AWS DMS for ongoing replication"
    },
    "explanation": "SCT converts database schema objects, while DMS handles data migration and ongoing replication. DMS alone cannot convert incompatible schema, and Aurora Serverless is an endpoint configuration, not a migration tool."
  },
  {
    "id": "bd88fb029ff132bf8cc7db9095ca761835ddec48705129a0a171c78c8c3b6fa3",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.4",
    "stem": "A gaming company needs a serverless, key-value database that scales automatically to handle unpredictable traffic spikes, and provides single-digit millisecond latency for player profiles. Which service should they choose?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon RDS for MySQL",
      "B": "Amazon Aurora Global Database",
      "C": "Amazon DynamoDB",
      "D": "Amazon ElastiCache for Memcached"
    },
    "explanation": "DynamoDB is a serverless NoSQL key-value store that auto-scales and delivers single-digit millisecond latency. RDS and Aurora require instance management, and ElastiCache is an in-memory cache, not a durable serverless database."
  },
  {
    "id": "bfb3615be8ff2a196ba0c498bc37e563869ad00b5d8b1e9d4ccf47e0c690cfc3",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.4",
    "stem": "An application uses MongoDB drivers and requires a managed service compatible with MongoDB APIs, automated backups, and patching. Which AWS database service is the best fit?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon DynamoDB",
      "B": "Amazon DocumentDB",
      "C": "Amazon RDS for PostgreSQL",
      "D": "Amazon DocumentDB"
    },
    "explanation": "Amazon DocumentDB is compatible with MongoDB APIs and offers managed backups and patching. DynamoDB is a proprietary key-value store, RDS for PostgreSQL is not MongoDB compatible."
  },
  {
    "id": "76e55c8076230b9b3b7deff28c61d229306e725e9a4f59f2eab6a37d614f12ab",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.4",
    "stem": "A retail analytics team wants to accelerate frequent read queries on a hot DynamoDB table without modifying application code. Which AWS feature should they implement?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Create a read replica in Amazon RDS",
      "B": "Enable DynamoDB DAX",
      "C": "Front with Amazon CloudFront",
      "D": "Move data to Amazon S3"
    },
    "explanation": "DAX provides an in-memory cache for DynamoDB with minimal code changes. RDS replicas are for relational databases, CloudFront is for content delivery, and S3 is object storage unsuitable for low-latency DB queries."
  },
  {
    "id": "5ebe664716b9d578456ed5ac0e9994759f499301d00ebfd425e346002e331ab7",
    "exam": "AWS Certified Cloud Practitioner (CLF-C02)",
    "taskStatement": "3.4",
    "stem": "A global content provider needs a read-heavy relational database replicated across multiple AWS Regions for low-latency reads and disaster recovery. Which service should they choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Aurora Global Database",
      "B": "Amazon RDS Read Replicas",
      "C": "Amazon DynamoDB global tables",
      "D": "Amazon ElastiCache for Redis global replication"
    },
    "explanation": "Aurora Global Database replicates data in under a second across Regions for low-latency global reads. RDS Read Replicas are within a Region, DynamoDB global tables are NoSQL, and ElastiCache is an in-memory cache, not a primary relational database."
  }
]