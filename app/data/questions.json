[
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A company processes IoT sensor data streams at 200 MB/s for ML feature engineering. The data arrives in JSON format and must be ingested into SageMaker Data Wrangler in near-real time. The ingestion solution must minimize operational overhead and storage costs while enabling efficient downstream transformations. Which solution meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy Amazon Kinesis Data Streams to buffer JSON records. Configure an AWS Lambda function to batch and convert records into Parquet, then write to S3. Use Data Wrangler to read from S3.",
      "B": "Use Amazon Kinesis Data Firehose to ingest JSON data directly into an S3 bucket with Parquet conversion and GZIP compression. Configure Data Wrangler to read from that S3 bucket.",
      "C": "Set up an Amazon Managed Streaming for Apache Kafka cluster. Use Kafka Connect to stream data into an Amazon Redshift table. Use Data Wrangler to query Redshift.",
      "D": "Ingest streams into an AWS Glue Streaming ETL job. Write output to Amazon DynamoDB. Use Data Wrangler to fetch items from DynamoDB."
    },
    "explanation": "Kinesis Data Firehose natively converts JSON to Parquet with compression and writes to S3, minimizing operational overhead and cost. The other options add complexity or incur higher cost/management burden."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A dataset consists of structured CSV transaction logs, semi-structured JSON user event logs, and tens of millions of small binary image files. The ML pipeline uses Spark on EMR for preprocessing and SageMaker for training. The dataset has grown to multi-terabyte scale and ingestion performance is suffering. Which storage configuration should the ML engineer choose to optimize throughput, cost, and simplicity?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Store CSV and JSON in S3. Store images in Amazon FSx for Lustre mounted to EMR. Use Spark on EMR to read both sources.",
      "B": "Import all data into Amazon Redshift with Spectrum. Store CSV/JSON in tables, images as BLOBs. UNLOAD data for SageMaker training.",
      "C": "Store all data in Amazon S3 as partitioned, SNAPPY-compressed Parquet files. Configure EMR to read via the S3A connector and SageMaker to use S3 input mode.",
      "D": "Use Amazon EFS to store all raw files. Mount EFS to EMR clusters and SageMaker training jobs as a unified data source."
    },
    "explanation": "Partitioned Parquet on S3 offers cost-effective, high-throughput storage, simple management, and native support in Spark and SageMaker. FSx adds cost/ops, Redshift BLOBs and Spectrum incur complexity, and EFS suffers lower throughput and higher cost for large-scale data."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An ML engineer runs a SageMaker processing job to merge and feature-engineer 50 million small JSON files in Amazon S3. The job fails due to S3 request throttling from excessive LIST and GET operations. Which solution will reduce operational overhead and ensure reliable job completion?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure a VPC endpoint for S3 in the SageMaker processing job to reduce throttling. Increase parallelism to maximize throughput.",
      "B": "Enable S3 Transfer Acceleration on the bucket to speed up GET requests. Use the Transfer Acceleration endpoint in the processing job.",
      "C": "Use AWS Glue to run a job that aggregates the small JSON files into larger SNAPPY-compressed Parquet files in S3. Update the SageMaker processing job to read the Parquet files.",
      "D": "Copy all JSON files to an Amazon EFS file system using AWS DataSync. Configure the SageMaker processing job to read directly from EFS."
    },
    "explanation": "Aggregating small files into larger Parquet files reduces S3 request count and improves throughput with minimal ongoing operations. VPC endpoints and Transfer Acceleration do not change S3 request rate limits; EFS adds complexity and cost."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "An ML engineer needs to encode a high-cardinality categorical feature (10 000 unique values) for a tree-based model. They must minimize memory footprint, avoid one-hot explosion, and preserve ordering where meaningful. Which approach and AWS tool should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use one-hot encoding in SageMaker Data Wrangler to produce sparse binary columns.",
      "B": "Use label encoding in AWS Glue DataBrew to assign integer codes to categories.",
      "C": "Use binary encoding in SageMaker Data Wrangler to compress categories into \u2308log\u2082(10000)\u2309 binary features.",
      "D": "Use frequency encoding in an AWS Glue ETL job to replace each category with its occurrence count."
    },
    "explanation": "Binary encoding (also called bit hashing) reduces dimensionality to \u2308log\u2082(n)\u2309 columns and preserves some ordinal information. SageMaker Data Wrangler supports a built-in binary encoding transform, minimizing memory and avoiding one-hot explosion."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A data scientist is building a cleaning pipeline in SageMaker Data Wrangler for a numerical dataset with extreme outliers and missing values. They need to: (1) cap outliers at the 1st and 99th percentiles, (2) impute remaining missing values with the column median, and (3) standardize features to zero mean and unit variance. Which sequence of Wrangler nodes achieves this with minimal custom code?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Standardize \u2192 percentile clip \u2192 median impute",
      "B": "Median impute \u2192 percentile clip \u2192 standardize",
      "C": "Percentile clip \u2192 median impute \u2192 standardize",
      "D": "Median impute \u2192 standardize \u2192 percentile clip"
    },
    "explanation": "Clipping outliers first avoids imputing capped values. Next, median imputation handles any remaining missing data. Finally, standardization yields zero mean/unit variance on the cleaned data."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A real-time recommendation engine requires streaming user events from Kinesis Data Streams to be transformed (JSON flattening, type conversion) and ingested into SageMaker Feature Store with under-second latency. Which solution minimizes operational overhead and meets latency SLAs?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Implement an AWS Lambda function triggered by Kinesis Data Streams that transforms each record and calls the SageMaker Feature Store PutRecord API.",
      "B": "Run an AWS Glue streaming ETL job on Kinesis Data Streams with PySpark to transform and write directly to Feature Store.",
      "C": "Deploy an Apache Flink application on Amazon EMR to consume Kinesis, perform transformations, write to S3, then batch-load into Feature Store.",
      "D": "Use Amazon Kinesis Data Analytics (Flink SQL) to transform the stream and send to a SageMaker Batch Transform job for periodic ingestion."
    },
    "explanation": "A Lambda function on Kinesis provides sub-second processing with minimal infrastructure to manage. It can call PutRecord directly. Glue streaming and EMR/Flink introduce greater complexity and latency; Batch Transform cannot meet real-time SLAs."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A health-tech startup has a dataset in Amazon S3 with PII fields (names, SSNs) and binary labels. They must prepare the data for SageMaker training by removing or masking PII, validating data quality, computing pre-training bias metrics (class imbalance, difference in proportions), and mitigating identified bias before training. Which sequence of AWS services and actions fulfills these requirements with minimal operational overhead?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon Macie to discover PII \u2192 apply client-side KMS encryption \u2192 run a SageMaker Processing job with a custom script to compute bias metrics.",
      "B": "Use AWS Glue Data Quality to profile dataset \u2192 use AWS Glue ETL to anonymize PII \u2192 run SageMaker Model Monitor to compute pre-training bias metrics.",
      "C": "Use SageMaker Ground Truth to label PII columns \u2192 use SageMaker Data Wrangler to transform data \u2192 use SageMaker Clarify ModelBias to compute post-training bias.",
      "D": "Use SageMaker Data Wrangler to detect and mask PII with built-in transforms \u2192 launch a SageMaker Clarify DataBias job to calculate pre-training CI and DPL metrics \u2192 apply synthetic oversampling for the minority class via Data Wrangler or a SageMaker Processing job."
    },
    "explanation": "Option D leverages Data Wrangler\u2019s built-in PII masking, uses Clarify\u2019s DataBias monitoring to compute pre-training bias metrics, and then mitigates imbalance via a native transform or Processing job, minimizing custom code and meeting compliance and bias-mitigation requirements."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "During preprocessing, an ML engineer discovers a significant class imbalance: one protected class comprises only 1% of the training data. They need to generate additional synthetic samples for that class without introducing label noise and then validate the augmented dataset\u2019s quality. Which approach aligns best with AWS-recommended bias mitigation and data validation practices?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Clarify\u2019s synthetic data generator to produce minority-class examples and merge with original data.",
      "B": "Launch a SageMaker Processing job that uses scikit-learn\u2019s SMOTE to create synthetic minority-class samples, then run AWS Glue Data Quality jobs to validate data integrity.",
      "C": "Use AWS Glue DataBrew\u2019s \u201cGenerate rows\u201d transform to duplicate minority-class records until balance is achieved.",
      "D": "Ingest data into SageMaker Feature Store and use record augmentation in Feature Store to rebalance classes."
    },
    "explanation": "Option B applies SMOTE in a managed Processing job to generate realistic synthetic samples for the minority class, then validates the augmented dataset with Glue Data Quality, aligning with best practices for bias mitigation and data integrity."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "An ML engineer must deliver preprocessed, bias-mitigated training data to a SageMaker training job. The data must be encrypted at rest and in transit, and the training dataset split must be stratified by a protected attribute to avoid introducing bias. How should the engineer configure the data pipeline to meet these requirements?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Store the CSVs in S3 with SSE-S3 encryption, use File input mode in SageMaker training, and rely on built-in random shuffle for splits.",
      "B": "Provision an encrypted FSx for Lustre file system, copy preprocessed files there, and configure the training job with File mode and a JSON split manifest.",
      "C": "Mount an encrypted Amazon EFS volume in the training container, copy data via a Processing job that shuffles and splits, then train using File mode.",
      "D": "Store preprocessed data in S3 with SSE-KMS encryption, use Pipe input mode for the training job, and implement a stratified shuffle-and-split routine inside the training container to ensure splits respect the protected attribute."
    },
    "explanation": "Option D ensures end-to-end encryption with SSE-KMS, leverages Pipe mode to stream data efficiently, and uses a custom stratified shuffle-and-split routine in the container to maintain fairness across protected groups."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A fintech company is building a credit-scoring model to classify loan applicants as high or low risk. The dataset consists of 1 million rows of structured tabular data with a mix of numerical and categorical features of moderate cardinality. Regulatory requirements mandate that the model be highly interpretable for audit, and the application demands low-latency inference. Data scientists want to minimize manual feature engineering. Which SageMaker built-in algorithm should the ML engineer choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use the XGBoost built-in algorithm with default hyperparameters.",
      "B": "Use the Factorization Machines built-in algorithm to capture feature interactions.",
      "C": "Use the Linear Learner built-in algorithm configured for logistic regression.",
      "D": "Use the K-Nearest Neighbors built-in algorithm for classification."
    },
    "explanation": "Linear Learner in logistic regression mode provides a fully interpretable model with low inference latency and minimal feature engineering. XGBoost offers higher accuracy but less interpretability. Factorization Machines capture interactions but are harder to audit. KNN has high latency and is not suited for large datasets."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A legal analytics startup needs to automate abstractive summarization of large contract documents. The team lacks expertise in training sequence-to-sequence models and requires a fully managed, high-quality solution that supports few-shot prompting. Which modeling approach should the ML engineer select?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon Comprehend\u2019s extractive summarization API on the contracts.",
      "B": "Fine-tune a T5 sequence-to-sequence model on SageMaker using the Hugging Face framework from JumpStart.",
      "C": "Invoke a foundation model via Amazon Bedrock (for example, Titan) with a prompt template for abstractive summarization.",
      "D": "Build a custom TensorFlow sequence-to-sequence model from scratch on SageMaker training instances."
    },
    "explanation": "Amazon Bedrock provides managed foundation models that excel at few-shot abstractive summarization with minimal ML expertise. Comprehend only supports extractive summarization. Fine-tuning on SageMaker or building from scratch incurs significant development and infrastructure overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "An industrial IoT provider collects streaming time-series sensor data from thousands of devices and needs to detect anomalies in near real time. The provider wants a fully managed service requiring minimal ML development and configuration. Which AWS service or approach should the ML engineer choose?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Train and deploy a Random Cut Forest (RCF) model on SageMaker for anomaly detection.",
      "B": "Develop and train a custom LSTM-based anomaly detector on SageMaker using TensorFlow.",
      "C": "Use Amazon Lookout for Metrics to automatically detect anomalies in the time-series data.",
      "D": "Use the anomaly-detection recipe in SageMaker Studio JumpStart and deploy the generated pipeline."
    },
    "explanation": "Amazon Lookout for Metrics is a fully managed anomaly-detection service that automatically tracks time-series metrics with minimal configuration. Training an RCF or LSTM on SageMaker requires substantial ML development and ongoing maintenance. JumpStart recipes simplify development but still require managing model training and pipelines."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An ML engineer previously ran a hyperparameter tuning job on a deep learning model and wants to optimize a new tuning job by reusing the results of that earlier job. Additionally, to reduce overall training cost and time, the engineer needs to automatically stop underperforming training jobs during tuning. Which configuration will meet these requirements?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run a new hyperparameter tuning job using BayesianOptimization search, with MaxNumberOfTrainingJobs set to 100, and leave EarlyStoppingType unset.",
      "B": "Run a warm start tuning job with type TRANSFER_LEARNING and disable early stopping to leverage prior results only.",
      "C": "Run a warm start hyperparameter tuning job with type IDENTICAL_DATA_AND_ALGORITHM and set EarlyStoppingType to Auto.",
      "D": "Run a random search tuning job with MaxNumberOfTrainingJobs set to 50 and enable EarlyStoppingType to OfflineStopping."
    },
    "explanation": "A warm start of type IDENTICAL_DATA_AND_ALGORITHM reuses prior tuning results for the same algorithm and search space, and setting EarlyStoppingType to Auto applies SageMaker\u2019s median stopping rule to halt underperforming jobs early."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A data scientist has fine-tuned a 2 GB PyTorch BERT model for text classification on a GPU instance. The model must now be deployed to a CPU-based inference endpoint with a memory footprint below 500 MB while maintaining at least 95% of its original accuracy. Which approach best meets these requirements with the least development effort?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Export the model to ONNX, write custom quantization code to convert weights to int8, and then deploy the ONNX model on the CPU endpoint.",
      "B": "Compile and optimize the trained PyTorch model using Amazon SageMaker Neo to perform graph optimizations and int8 quantization, then deploy the Neo-compiled model to the CPU endpoint.",
      "C": "Rewrite the model architecture to reduce hidden layer sizes by 50%, retrain from scratch on CPU, and deploy the smaller model.",
      "D": "Use AWS Lambda with a custom container to dynamically load and prune model weights at inference time to reduce memory usage."
    },
    "explanation": "SageMaker Neo automates model graph optimization and precision quantization (e.g., int8), reducing size and improving CPU performance with minimal code changes while preserving accuracy."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A research team has trained a custom XGBoost model locally and now wants to integrate it into Amazon SageMaker for managed hosting, hyperparameter tuning, and CI/CD pipelines without rewriting the training code. What is the most appropriate way to import and serve this externally trained model in SageMaker?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Package the model artifact into a model.tar.gz, build a custom inference Docker container (BYOC) that loads the artifact, push it to Amazon ECR, and create a SageMaker Model referencing that container and S3 artifact.",
      "B": "Translate the local training code into a SageMaker Script Mode training script, run it in a built-in XGBoost container, and register the new model.",
      "C": "Use the SageMaker SDK to call CreateTrainingJob with LocalMode enabled to import and train the model artifact directly.",
      "D": "Upload the model artifact to Amazon SageMaker Model Registry without a container and deploy it to a serverless endpoint."
    },
    "explanation": "Bringing an externally trained model into SageMaker requires a Bring-Your-Own-Container (BYOC) that contains the inference logic; SageMaker Model Registry alone cannot host artifacts without a container."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "A financial services company is building a fraud detection model. The base fraud rate in production is 1%. On a validation set of 10,000 transactions (100 frauds, 9,900 non-frauds), two candidate models yield the following metrics:\n\nModel A: precision = 0.667, recall = 0.8\nModel B: precision = 0.8, recall = 0.6\n\nThe business cost of a false positive (FP) is $1 and the cost of a false negative (FN) is $10. Which model has the lower expected cost per 10,000 transactions, and which should the company choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Choose Model A: expected cost = (FP=40\u00d7$1)+(FN=20\u00d7$10) = $240",
      "B": "Choose Model B: expected cost = (FP=15\u00d7$1)+(FN=40\u00d7$10) = $415",
      "C": "Both models have the same cost",
      "D": "Cannot decide without additional metrics"
    },
    "explanation": "Compute TP, FP, FN for each: Model A: TP=0.8\u00d7100=80, FP=80\u00d7(1/0.667\u22121)=40, FN=20 \u2192 cost=40+200=240. Model B: TP=60, FP=60\u00d7(1/0.8\u22121)=15, FN=40 \u2192 cost=15+400=415. Model A yields lower cost."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "An ML engineer needs to detect if a production classification model begins to rely on different features over time due to data drift. Which SageMaker Clarify monitor class should the engineer configure to track changes in feature contributions (for example, SHAP values) between a baseline and production data?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ModelBiasMonitor",
      "B": "ModelExplainabilityMonitor",
      "C": "ModelQualityMonitor",
      "D": "DataQualityMonitor"
    },
    "explanation": "ModelExplainabilityMonitor tracks explainability metrics such as SHAP feature attributions over time. The other monitors handle bias metrics (ModelBiasMonitor), overall prediction quality (ModelQualityMonitor), or input data distributions (DataQualityMonitor)."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "A deep neural network trained on SageMaker displays stagnating training loss and high validation loss. The engineer enabled SageMaker Debugger\u2019s default rules and then enabled the vanishing gradient rule, which flagged anomalies in the earliest layers. Which action best addresses the vanishing gradient problem in these layers?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase the global learning rate to accelerate gradient propagation",
      "B": "Replace sigmoid/tanh activations with ReLU (and use He initialization)",
      "C": "Add L2 weight regularization to penalize large weights",
      "D": "Reduce batch size to increase gradient noise and variability"
    },
    "explanation": "Vanishing gradients in early layers are mitigated by using activation functions with constant gradients (ReLU) and appropriate weight initializations (He). Other options (higher learning rate, L2 penalty, smaller batches) do not directly solve vanishing gradient issues."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A fintech firm has developed a credit-scoring ML model (<50 MB) that must serve real-time API requests with no more than 20 ms 95th-percentile latency. Traffic patterns are unpredictable, ranging from 5 to 500 requests per minute. The firm wants to minimize infrastructure management overhead and pay only for the compute capacity it uses, while ensuring consistent low-latency performance. Which SageMaker deployment infrastructure should the ML engineer select?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Deploy the model to a SageMaker serverless endpoint with default concurrency limits.",
      "B": "Deploy the model to a multi-model real-time SageMaker endpoint on a single ml.m5.large instance with auto scaling.",
      "C": "Deploy the model to a SageMaker real-time endpoint on ml.c5.large instances with a provisioned concurrency configuration and target-tracking auto scaling.",
      "D": "Deploy the model to a SageMaker asynchronous inference endpoint with default VCPU provisioning."
    },
    "explanation": "A serverless endpoint can cold-start and breach latency requirements. A single-instance multi-model endpoint cannot guarantee sub-20 ms under unpredictable load. Asynchronous inference is batch-oriented. A real-time endpoint with provisioned concurrency keeps containers warm and uses target-tracking scaling to elastically adjust capacity while maintaining low latency."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An industrial IoT company needs to deploy a vision ML model to 1 000 ARM-based edge cameras with intermittent connectivity. The model must run local inference with latency <50 ms and accept periodic updates without manual intervention. Which deployment infrastructure should the ML engineer choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Containerize the model and deploy it to AWS Lambda functions on AWS Greengrass Core.",
      "B": "Use SageMaker Edge Manager to package and deploy the model directly to AWS IoT devices.",
      "C": "Compile and optimize the model with SageMaker Neo for the target ARM architecture and deploy it to devices using AWS IoT Greengrass.",
      "D": "Deploy the model to a SageMaker real-time endpoint and configure the devices to call the endpoint when connected."
    },
    "explanation": "Lambda on Greengrass incurs container startup and may not meet <50 ms. Edge Manager provides monitoring but still requires a runtime-optimized model. A real-time endpoint requires connectivity. SageMaker Neo compiles and optimizes the model for ARM, and AWS IoT Greengrass handles offline deployment and periodic updates with minimal ops overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A retail company requires an automated, end-to-end CI/CD pipeline for their ML workflow: data preprocessing, training with hyperparameter tuning, model registration and approval, and blue/green deployment to production endpoints. The pipeline must integrate with a Git repository for version control, provide step-level observability, and support easy rollback. Which orchestrator should the ML engineer select to meet these requirements with minimal custom infrastructure?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS CodePipeline with custom AWS Lambda functions for each ML workflow step.",
      "B": "Amazon Managed Workflows for Apache Airflow (MWAA) with DAGs defining each stage.",
      "C": "AWS Step Functions orchestrating AWS Batch jobs for training and deploying Step Functions tasks.",
      "D": "Amazon SageMaker Pipelines with integrated steps for data processing, training, model registry, approval, and deployment."
    },
    "explanation": "CodePipeline requires significant custom Lambda code for ML steps. MWAA is general-purpose and needs custom operators. Step Functions plus Batch require building and managing additional compute and monitoring. SageMaker Pipelines natively integrates preprocessing, training, tuning, registry, approval, and deployment, supports Git integration, lineage, monitoring, and rollback with minimal custom infrastructure."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "A machine learning team must deploy a SageMaker real-time multi-model endpoint in private subnets with no internet access. The models are packaged as custom Docker images stored in Amazon ECR. The team wants to automate provisioning of all networking, ECR, and SageMaker resources using infrastructure as code (IaC) with minimal operational overhead, and to enforce coding best practices and unit tests. Which approach best meets these requirements?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "Write an AWS CloudFormation template that defines the VPC, private subnets, ECR repository, SageMaker model, endpoint configuration, and endpoint resources.",
      "B": "Develop an AWS CDK application (in a supported language) that defines the VPC with interface endpoints, ECR repository, SageMaker model, endpoint configuration, and auto scaling policies, and run unit tests against the CDK constructs.",
      "C": "Use an AWS SAM template to define a Lambda function that creates the VPC and provisions the SageMaker endpoint when invoked.",
      "D": "Author a Terraform module to provision the VPC, ECR repository, and SageMaker resources, and manage state in an S3 backend."
    },
    "explanation": "AWS CDK provides a programmable IaC framework with built-in support for unit testing of constructs, reduces YAML/JSON boilerplate compared to raw CloudFormation, and automates resource provisioning including VPC interface endpoints and SageMaker auto scaling policies with minimal operational overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "A company\u2019s SageMaker real-time endpoint experiences sudden traffic spikes, causing increased latency. The current target-tracking auto scaling policy uses CPUUtilization. The operations team wants to trigger scale-out more quickly on incoming inference requests while avoiding over-provisioning during lulls. Which metric should they use in the target-tracking policy?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CPUUtilization",
      "B": "ModelLatency",
      "C": "InvocationsPerInstance",
      "D": "MemoryUtilization"
    },
    "explanation": "InvocationsPerInstance measures the number of inference requests handled per instance, providing a direct signal of request load and enabling faster scaling on bursts, whereas CPUUtilization and MemoryUtilization lag and ModelLatency may not correlate directly with load volume."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "An ML engineer needs an automated, end-to-end pipeline that builds custom inference container images on code changes, pushes them to ECR, and updates a SageMaker endpoint to use the new image\u2014all defined as code. Which design has the least operational overhead?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use EventBridge to trigger a CodeBuild project on Git push; CodeBuild builds and pushes the image to ECR; a CloudFormation stack update is triggered manually to point the endpoint to the new image.",
      "B": "Define a CodePipeline pipeline (in AWS CDK) that uses CodeCommit, a CodeBuild action to build/push the image, and a CloudFormation action to update the SageMaker endpoint; deploy the pipeline via the CDK app.",
      "C": "Install AWS CLI scripts on an EC2 instance to poll CodeCommit for changes, build and push the container, then run AWS CLI to update the endpoint.",
      "D": "Use CodePipeline with CodeCommit and a Lambda step to build the container image, then invoke an AWS SAM deployment to update the endpoint."
    },
    "explanation": "A CDK-defined CodePipeline with CodeCommit, CodeBuild, and CloudFormation actions provides a fully managed, declarative CI/CD pipeline with minimal custom code, built-in integration, and automated endpoint updates, reducing operational overhead compared to scripting or Lambda-based workarounds."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "An ML engineer is tasked with building a fully automated CI/CD pipeline for an Amazon SageMaker\u2013based model. The pipeline must: 1) pull code and configuration from a Git repository, 2) run unit tests and data validation, 3) execute data transformation in a SageMaker Processing job, 4) train the model, 5) register the trained model in the SageMaker Model Registry, and 6) perform a canary deployment to production with traffic shifting. The solution must minimize custom Lambda code and leverage native AWS services. Which pipeline architecture best meets these requirements?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS CodePipeline with stages: Source (GitHub), CodeBuild (run tests and data validation), CloudFormation (provision Processing and Training jobs and register model), Manual approval, CloudFormation (update endpoint).",
      "B": "Use AWS CodePipeline with stages: Source (GitHub), CodeBuild (run tests, invoke direct SDK calls to Processing and Training, register model), Manual approval, CodeBuild (use AWS CLI to update endpoint).",
      "C": "Implement a SageMaker Pipeline that includes Processing, Training, Evaluation, and Model Registry steps, and trigger it via EventBridge on git push. Use SageMaker Pipeline\u2019s built-in endpoint update step with canary configuration.",
      "D": "Use AWS CodePipeline with stages: Source (GitHub), CodeBuild (invoke a SageMaker Pipeline via AWS CLI to run Processing, Training, Evaluation, and register the model), Approval, CodeDeploy (configured for SageMaker endpoint canary deployment and traffic shifting)."
    },
    "explanation": "Option D minimizes custom code by delegating ML workflow orchestration to SageMaker Pipelines and uses native CodePipeline stages and CodeDeploy for canary traffic shifting. It cleanly separates CI (CodeBuild) from CD (CodeDeploy) without custom Lambdas."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A new CodePipeline contains a CodeBuild action that uses the AWS CLI to start SageMaker Training and Model Deployment jobs. The CodeBuild project\u2019s IAM role has permissions for sagemaker:CreateTrainingJob, sagemaker:CreateModel, and sagemaker:CreateEndpoint, but the build fails with an AccessDenied error stating that the role cannot be passed. What is the LEAST-privilege IAM change required to allow the CodeBuild stage to succeed?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add iam:PassRole permission for the SageMaker execution role to the CodeBuild project's IAM role policy.",
      "B": "Update the SageMaker execution role trust policy to allow sts:AssumeRole from codepipeline.amazonaws.com.",
      "C": "Add iam:PassRole permission for the CodeBuild project role to the CodePipeline service role policy.",
      "D": "Add sts:AssumeRole permission for the SageMaker execution role in the CodeBuild project's trust policy."
    },
    "explanation": "When CodeBuild calls SageMaker, it must pass the SageMaker service execution role. Granting iam:PassRole on that role in the CodeBuild project's IAM role policy satisfies the least-privilege requirement."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A financial services team uses SageMaker Model Monitor to emit CloudWatch metrics when data drift exceeds thresholds. They need to automatically retrain the model end-to-end (processing, training, evaluation, registry) and roll out the new model with a linear traffic shift as soon as drift is detected. Which combination of AWS services and configurations will satisfy these requirements with minimal custom code?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create a CloudWatch alarm on the Model Monitor drift metric and an EventBridge rule that triggers an AWS CodePipeline pipeline. In CodePipeline, invoke a SageMaker Pipeline (processing, training, evaluation, model registration) and add a CodeDeploy stage configured for SageMaker endpoint linear traffic shifting.",
      "B": "Subscribe an SNS topic to the Model Monitor violation notification and use an AWS Lambda function to start a SageMaker Pipeline and update the endpoint via SDK with gradual traffic shifting.",
      "C": "Configure Model Monitor to directly invoke a SageMaker Pipeline on drift and include a built-in traffic shifting step in the SageMaker Pipeline definition.",
      "D": "Schedule a daily CodeBuild job to query Model Monitor metrics, and if drift is detected, run a training job and invoke a CloudFormation change set to update the endpoint."
    },
    "explanation": "Option A uses native integration: CloudWatch\u2192EventBridge to trigger CodePipeline, SageMaker Pipelines for retraining, and CodeDeploy for linear traffic shifting. This minimizes custom code and leverages managed services end-to-end."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An e-commerce company deployed a recommendation model on SageMaker and needs to monitor both input data drift and model bias in production. They have a baseline training dataset for drift detection and fairness metrics for bias detection. To minimize operational overhead and leverage fully managed monitoring, which solution meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a single SageMaker Model Monitor job with default settings to detect both drift and bias; schedule hourly.",
      "B": "Configure SageMaker Model Monitor (DefaultModelMonitor) to run a data quality monitoring job with baseline statistics and constraints for input features, and SageMaker Clarify ModelBiasMonitor to run a bias monitoring job with baseline fairness metrics; configure both on a daily schedule.",
      "C": "Use SageMaker Clarify's ModelExplainabilityMonitor to detect both data drift and bias by specifying a SHAP baseline, and schedule it hourly.",
      "D": "Use AWS Lambda triggered every hour to run custom Python scripts that compute drift metrics and bias metrics against baselines; send alerts via Amazon SNS."
    },
    "explanation": "SageMaker Model Monitor (DefaultModelMonitor) is optimized for data drift and quality monitoring using baseline statistics and constraints. SageMaker Clarify\u2019s ModelBiasMonitor is designed to monitor bias against fairness baselines. Combining these two fully managed monitors meets both drift and bias requirements with minimal custom code and operational overhead. Options A and C misuse or over-simplify the services, and D introduces unnecessary custom code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A SaaS provider hosts a real-time image classification endpoint on Amazon SageMaker. They have observed intermittent spikes in 5XX invocation errors and increased inference latency impacting user experience. The operations team needs a solution with the least operational overhead to automatically detect and alert on these issues in near real time. Which approach should the team implement?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Enable Amazon CloudWatch Logs for the endpoint; configure a metric filter to count '5XX' errors; create CloudWatch alarms for '5XX' error count and for 'ModelLatency' metric.",
      "B": "Configure SageMaker Model Monitor to capture inference requests and responses; schedule a data quality monitoring job every 5 minutes with a custom script to check for errors and latency; publish custom metrics to CloudWatch.",
      "C": "Configure DataCaptureConfig on the endpoint to capture all invocations to S3; create an AWS Lambda function triggered by S3 events that calculates error rates and latency; publish custom CloudWatch metrics and alarms.",
      "D": "Use SageMaker Clarify ModelExplainabilityMonitor to detect anomalies in output embeddings, which will indirectly detect errors and latency issues; schedule it on an hourly basis."
    },
    "explanation": "SageMaker endpoints emit built-in CloudWatch metrics such as Invocation5XXErrors and ModelLatency. Creating CloudWatch alarms on these metrics provides near real-time monitoring with zero custom code and minimal overhead. Model Monitor (B) and Lambda/S3 solutions (C) introduce unnecessary complexity, and Clarify (D) is not designed for error or latency monitoring."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A financial analytics team deployed a credit-scoring model on SageMaker real-time endpoints. They want to detect univariate feature drift in critical numeric input features (such as \u2018annual_income\u2019 and \u2018debt_ratio\u2019) and alert if the distribution has shifted beyond acceptable thresholds. They already have training data to serve as a baseline. Which SageMaker feature and configuration should the team use to meet this requirement with minimal custom coding?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure SageMaker Clarify ModelBiasMonitor with the training dataset as a baseline and set the 'drift_detection' parameter for numeric features; schedule hourly.",
      "B": "Use SageMaker Model Monitor's DefaultModelMonitor to create a MonitorSchedule with a DataQualityJobDefinition that specifies baseline_statistics and baseline_constraints created from the training dataset; set the monitoring schedule to run hourly.",
      "C": "Deploy a Python-based Lambda function that loads the training dataset baseline, fetches the latest batch of inference requests, computes Kolmogorov\u2013Smirnov tests for each numeric feature, and sends alerts via Amazon SNS.",
      "D": "Use Amazon CloudWatch Metrics Insight to query the 'InputDataMean' metric for each feature and configure alarms when mean deviates by more than a threshold from the training data mean."
    },
    "explanation": "SageMaker Model Monitor\u2019s DefaultModelMonitor supports univariate feature drift detection by automatically generating baseline_statistics and baseline_constraints from training data, and scheduling the monitoring job. This fully managed solution minimizes custom coding. Clarify\u2019s ModelBiasMonitor (A) is for fairness, not general drift, and options C and D require significant custom development or rely on unsupported metric calculations."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A company hosts a SageMaker real-time inference endpoint that experiences predictable daily traffic spikes at 9 AM and 6 PM, plus unpredictable fluctuations throughout the day. The ML engineer needs to minimize costs during off-peak hours while ensuring the endpoint maintains a p95 latency below 100 ms. The solution must use built-in AWS services and require minimal custom code. Which configuration should the engineer implement?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure a step scaling policy in Amazon CloudWatch that scales out when CPU Utilization > 70% and scales in when CPU Utilization < 30%; set MinCapacity=1 and MaxCapacity=10.",
      "B": "Deploy an AWS Lambda function triggered by Amazon EventBridge schedule rules at 8:50 AM and 5:50 PM to call UpdateEndpointWeightsAndCapacities, and use a target tracking policy for unpredictable loads.",
      "C": "Define an Application Auto Scaling target tracking policy on the SageMaker endpoint based on p95 latency with MinCapacity=1, MaxCapacity=10, and add a scheduled scaling action to increase capacity to 8 at 8:50 AM and 5:50 PM.",
      "D": "Use Spot Instances by converting the endpoint to asynchronous inference, configure Spot for inference to de-provision overnight, and rely on target tracking for bursts."
    },
    "explanation": "Option C leverages built-in Application Auto Scaling for SageMaker to handle both scheduled and dynamic scaling in a single service with minimal custom code. Step and Lambda solutions are more complex and less precise; asynchronous/Spot inference cannot guarantee real-time p95 latency."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "An ML engineer needs to optimize cost and performance of multiple SageMaker real-time inference endpoints. Each endpoint has different traffic patterns and latency requirements. The engineer wants to identify the most cost-effective instance type for each endpoint, based on sample inference payloads, throughput, and latency SLOs. Which tool or workflow should the engineer use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Compute Optimizer to generate EC2 instance recommendations and manually map those to SageMaker endpoints.",
      "B": "Use SageMaker Inference Recommender to run profiling jobs with sample payloads and choose the instance types with the lowest cost per inference that meet the SLOs.",
      "C": "Use AWS Trusted Advisor to forecast inference endpoint costs and recommend Reserved Instances.",
      "D": "Use AWS Cost Explorer\u2019s RI purchase recommendations to apply savings to SageMaker endpoint instance types."
    },
    "explanation": "SageMaker Inference Recommender is designed to profile real-world inference workloads and recommend the optimal instance types for cost and performance. Compute Optimizer and Trusted Advisor do not profile SageMaker workloads at the application level."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A global team uses SageMaker across 10 AWS accounts under consolidated billing. They must enforce that every SageMaker training job, endpoint, and pipeline carries the tags Project and CostCenter, automatically deny creations without them, and generate a centralized monthly cost dashboard per project. Which solution meets these requirements with the least operational overhead?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy AWS Config rules in each account to audit SageMaker resource tags, use AWS Service Catalog with TagOptions to solicit the tags, and build QuickSight dashboards from Cost Explorer filtered by tags.",
      "B": "Implement AWS Organizations Tag Policies to require Project and CostCenter tags on SageMaker resources, enable AWS Config for compliance evaluation, and use Cost Explorer/AWS Budgets with tag filters for centralized dashboards.",
      "C": "Create Service Control Policies to forbid creation of untagged SageMaker resources, use CloudTrail + Lambda to remediate missing tags, and aggregate costs via Cost Explorer.",
      "D": "Enforce all SageMaker deployments through AWS CloudFormation StackSets with mandatory tags in the templates, and view costs in the AWS Billing console."
    },
    "explanation": "AWS Organizations Tag Policies provide native, centralized enforcement of required tags across accounts with minimal operational effort. Combined with AWS Config compliance checks and Cost Explorer/AWS Budgets, this delivers automated enforcement and centralized cost reporting. Other options require custom code or more operational overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "An enterprise mandates that all SageMaker training and inference workloads must run within a secured VPC without any public internet access. However, these workloads need to read training data from Amazon S3, pull container images from Amazon ECR, write logs to Amazon CloudWatch Logs, and use a customer-managed AWS KMS key. The security team requires the solution to minimize ongoing maintenance overhead while enforcing least-privilege network connectivity. Which architectural configuration meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable SageMaker network isolation and launch jobs outside any VPC; rely on network isolation to prevent internet access.",
      "B": "Launch SageMaker jobs inside a VPC with no internet gateway, configure interface VPC endpoints for S3, ECR (both API and Docker), CloudWatch Logs, and KMS, and remove any NAT gateways.",
      "C": "Launch SageMaker jobs in the default VPC and attach a security group that blocks 0.0.0.0/0; use public endpoints for AWS services.",
      "D": "Use a network ACL on the private subnet to deny all outbound traffic; allow S3 and ECR access by whitelisting their public IP ranges."
    },
    "explanation": "Enabling interface VPC endpoints for S3, ECR, CloudWatch Logs, and KMS in a VPC without an Internet Gateway ensures workloads have private, least-privilege access to required AWS services and keys, while preventing any public internet access. Network isolation alone (A) doesn\u2019t provide VPC-private access to AWS services. Security groups cannot filter based on AWS service endpoints (C). NACLs cannot reliably allow AWS service traffic by IP (D)."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A company\u2019s sensitive training data is stored in an S3 bucket encrypted with a customer-managed KMS key (DataKey). They must run SageMaker training jobs that decrypt the data and write model artifacts to a separate S3 bucket encrypted with another KMS key (OutputKey). The SageMaker execution role must follow the principle of least privilege. Which combination of IAM role policy statements and KMS key policy statements satisfies this requirement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "IAM role: Grant s3:GetObject and s3:PutObject on both buckets and kms:Decrypt, kms:Encrypt on both keys. KMS key policies: Trust the entire account principal for all operations.",
      "B": "IAM role: Grant s3:GetObject on the data bucket; s3:PutObject on the artifact bucket; kms:Decrypt and kms:GenerateDataKey on DataKey; kms:Encrypt and kms:GenerateDataKey on OutputKey. KMS key policies: Specify the SageMaker execution role as the only principal allowed to use each key.",
      "C": "IAM role: Grant s3:* on both buckets and kms:* on both keys. KMS key policies: No changes needed (account admins manage keys).",
      "D": "Use S3 bucket policies to grant SageMaker service principal full access, and KMS grants to allow any IAM principal in account to decrypt keys."
    },
    "explanation": "Least-privilege requires the IAM role only have s3:GetObject on the data bucket, s3:PutObject on the output bucket, and only the specific KMS operations. The key policies must explicitly allow the SageMaker execution role to use each key. Over-permissive (A, C) or relying on bucket policies alone (D) violates least-privilege."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A security team manages multiple AWS accounts using AWS Organizations. They want to ensure that data scientists in member accounts can only create, update, or delete SageMaker endpoints within their own accounts, and prevent any SageMaker endpoint operations in accounts outside their OU. Which mechanism should be implemented at the organization level to enforce this restriction?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use an AWS Organizations service control policy (SCP) at the OU level to Deny all sagemaker:CreateEndpoint, UpdateEndpoint, and DeleteEndpoint actions when the resource\u2019s account ID does not match the requesting account.",
      "B": "Apply IAM permission boundaries to the data scientists\u2019 roles in each member account that restrict sagemaker:* to specific endpoint ARNs.",
      "C": "Attach a resource-based policy to each SageMaker endpoint specifying allowed IAM principals from the same account.",
      "D": "Use the SageMaker service-linked role policy to restrict endpoint operations to certain accounts."
    },
    "explanation": "An AWS Organizations SCP can centrally deny SageMaker endpoint management actions across accounts outside the OU, enforcing the requirement. IAM permission boundaries (B) must be set per role and cannot prevent actions outside the OU. SageMaker endpoints do not support resource-based policies for management operations (C). Service-linked roles cannot enforce cross-account restrictions (D)."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A fintech company needs to ingest 1 million financial transaction events per second into Amazon SageMaker Feature Store for real-time fraud detection. The solution must support low latency, horizontal scalability, and minimal data loss. Which ingestion architecture meets these requirements?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "Use Amazon Kinesis Data Firehose to deliver events to S3 and batch import into SageMaker Feature Store Offline Store.",
      "B": "Use Amazon Kinesis Data Streams with enhanced fan-out consumers and AWS Lambda functions invoking the PutRecord API to ingest into the SageMaker Feature Store Online Store.",
      "C": "Use Amazon MSK with brokers backed by EBS volumes and use a Kafka consumer to push records into the Feature Store offline bucket.",
      "D": "Use AWS IoT Core to route transaction events to SageMaker Feature Store via a custom AWS Lambda integration."
    },
    "explanation": "Real-time, low-latency ingest into the Feature Store Online Store requires Kinesis Data Streams (for horizontal scaling and enhanced fan-out) combined with Lambda invoking the PutRecord API. Firehose and batch imports do not meet real-time SLAs; MSK and IoT Core approaches add unnecessary complexity and latency."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An analytics team has 200 GiB of nested JSON log files in Amazon S3. They require interactive profiling and transformation in SageMaker Data Wrangler with minimal cost and latency. Which file format should they convert the data into before ingestion?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CSV files (flattening nested structures into columns).",
      "B": "Apache Parquet with nested column support and predicate pushdown.",
      "C": "Avro files with JSON serialization.",
      "D": "RecordIO binary format."
    },
    "explanation": "Parquet is a columnar storage format with efficient compression, nested column support, and predicate pushdown, which significantly reduces I/O and speeds interactive profiling in Data Wrangler. CSV lacks nested support, Avro is row-oriented, and RecordIO is optimized for deep learning, not interactive ETL."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An ML engineer must merge hourly transaction updates from Amazon RDS and real-time clickstream events from DynamoDB Streams into a single dataset for training in SageMaker Data Wrangler, while automatically handling schema evolution in both sources. Which ingestion solution should the engineer choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Schedule an AWS Glue ETL job to export RDS to S3 and join with Lambda-pushed DynamoDB data once per hour.",
      "B": "Use SageMaker Data Wrangler\u2019s built-in connectors for Amazon RDS and for DynamoDB streams to import and join data in a single flow.",
      "C": "Provision an EMR Spark cluster to read from both sources nightly and write merged Parquet to S3.",
      "D": "Use AWS DMS to replicate both RDS and DynamoDB to Redshift and join via Redshift Spectrum in Data Wrangler."
    },
    "explanation": "Data Wrangler connectors natively support incremental read and schema inference for RDS and DynamoDB streams, simplifying merges and automatically handling schema changes. Glue jobs or EMR require more operational overhead, and DMS replication to Redshift adds extra cost and latency."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A genomics research team requires sub-millisecond file I/O and high throughput for a 500 GiB NFS-based dataset during preprocessing before model training. Which AWS storage service should they provision to meet performance and POSIX compatibility requirements?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "Amazon EFS One Zone SSD backed by NFS.",
      "B": "Amazon FSx for Lustre file system with link to S3.",
      "C": "Amazon FSx for NetApp ONTAP with SSD volumes.",
      "D": "Amazon S3 bucket mounted via VPC endpoint."
    },
    "explanation": "FSx for Lustre delivers the lowest POSIX read-latencies (sub-millisecond) and highest throughput for large NFS-based datasets. EFS has higher latencies; FSx ONTAP adds data management features but lower throughput; S3 is object storage, not POSIX."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An e-commerce company experiences high startup latency for SageMaker training jobs reading hundreds of small CSV files (~5\u201310 MiB each) from S3. Without modifying the training code, which solution will most reduce startup time?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable Amazon S3 Transfer Acceleration on the bucket.",
      "B": "Use AWS Glue to consolidate small CSV files into larger Parquet files and update the S3 prefix.",
      "C": "Upgrade to a SageMaker instance with higher network throughput.",
      "D": "Change the training input mode to Pipe mode."
    },
    "explanation": "Consolidating small files into fewer, larger Parquet files minimizes S3 list and open calls, dramatically reducing startup latency without code changes. Transfer Acceleration and network upgrades do not address metadata overhead; Pipe mode still requires metadata enumeration."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A media streaming platform requires exactly-once processing of real-time events into its ML ingestion pipeline for feature computation. Which combination of services and configurations provides this guarantee?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Amazon Kinesis Data Firehose with a retry buffer.",
      "B": "Amazon Kinesis Data Streams with Enhanced Fan-Out (guarantees at-least-once).",
      "C": "Amazon MSK (Kafka) configured with idempotent producers and transactional writes.",
      "D": "DynamoDB Streams with AWS Lambda and conditional writes to downstream store."
    },
    "explanation": "Kafka with idempotent producers and transactional writes supports exactly-once semantics end-to-end. Kinesis Data Streams and Firehose only guarantee at-least-once; DynamoDB Streams plus Lambda may still result in duplicates without complex idempotency logic."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An ML pipeline ingests unvalidated CSV files into Amazon S3, but inconsistent schemas and missing columns cause downstream failures. Which AWS service can enforce schema validation during ingestion and alert on violations?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Glue Schema Registry with schema-validation-enabled data producers.",
      "B": "AWS Config rules for S3 object schemas.",
      "C": "Amazon Macie classification jobs.",
      "D": "AWS Lake Formation schema enforcement."
    },
    "explanation": "AWS Glue Schema Registry can define, register and enforce schemas at ingestion for streaming and batch, and can reject or alert on malformed records. Lake Formation handles data access control, not row-level schema validation."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A large oil company must migrate 100 TiB of POSIX file-system data to Amazon S3 within 48 hours, with incremental file changes tracked and minimal management overhead. Which service meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Snowball Edge device shipment.",
      "B": "AWS DataSync configured for incremental sync to S3.",
      "C": "Amazon S3 Transfer Acceleration over the internet.",
      "D": "AWS Storage Gateway file gateway."
    },
    "explanation": "AWS DataSync optimizes incremental transfer of POSIX files to S3 with built-in scheduling and encryption, suitable for large, continuously changing data. Snowball has shipment delays; Transfer Acceleration and Storage Gateway don\u2019t handle incremental POSIX sync as efficiently."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An ML engineer wants to profile, clean, and transform streaming IoT telemetry data directly in SageMaker Data Wrangler without intermediate storage. Which ingestion method should the engineer use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use the Amazon Kinesis Data Streams connector in Data Wrangler to ingest data in real time.",
      "B": "Upload data to S3 and use the S3 connector for batch profiling.",
      "C": "Route data through AWS IoT Analytics to an S3 channel and ingest.",
      "D": "Crawl raw data with AWS Glue and import the Glue table."
    },
    "explanation": "The Kinesis Data Streams connector in Data Wrangler enables real-time profiling and transformation of streaming data. Other options introduce batch windows or extra services, increasing latency and operational overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "When mounting an Amazon EFS file system to a SageMaker Studio notebook in a VPC, which configuration step ensures secure, low-latency access?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Mount the EFS file system over the public internet using TLS.",
      "B": "Create EFS mount targets in the same VPC subnets as the SageMaker notebooks and attach security groups allowing NFS traffic (TCP/2049).",
      "C": "Use AWS Direct Connect to mount the EFS file system from on-premises.",
      "D": "Peer the notebook VPC to another VPC containing EFS and mount through a transit gateway."
    },
    "explanation": "To achieve secure, low-latency NFS access from Studio in the same VPC, you must create EFS mount targets in the same subnets and open NFS ports in security groups. Public mounts or cross-VPC peering add latency and complexity."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A cross-account pipeline must ingest data from an S3 bucket in Account A into SageMaker Studio in Account B. To minimize privileges and management, which approach should be used?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a bucket ACL granting Account B the s3:GetObject permission.",
      "B": "Create an IAM role in Account B with a trust policy for Account A\u2019s principal, and update the bucket policy to allow s3:GetObject for that role.",
      "C": "Use an AWS Organizations SCP to allow cross-account access.",
      "D": "Set up VPC peering between the two accounts and access the bucket over the interface endpoint."
    },
    "explanation": "Creating a cross-account IAM role in Account B with trust from Account A and granting that role permission in the bucket policy provides least-privilege, auditable access. Bucket ACLs are legacy, and Organizations SCP/VPC peering do not directly enable S3 access control."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A data ingestion job reading from S3 into SageMaker notebooks is intermittently throttled with HTTP 503 SlowDown errors. Which approach will most effectively reduce these errors while maintaining high throughput?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Implement exponential backoff with jitter in the client\u2019s S3 request retry logic.",
      "B": "Enable S3 Transfer Acceleration to optimize network path.",
      "C": "Configure the bucket as requester-pays.",
      "D": "Increase the IAM request quota for the S3 service."
    },
    "explanation": "503 SlowDown errors indicate S3 throttling; the recommended mitigation is implementing exponential backoff with jitter to retry requests gracefully. Transfer Acceleration and requester-pays do not address throttling, and IAM quotas are unrelated."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An Amazon SageMaker training job mounts an Amazon FSx for Lustre file system backed by an 8 TiB S3 data repository. After heavy parallel reads, the engineer observes metadata lookup latencies >100 ms for POSIX operations. How can the engineer optimize metadata performance?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase the Lustre file system\u2019s throughput capacity to provision more metadata IOPS.",
      "B": "Switch the workload to Amazon EFS for metadata caching.",
      "C": "Directly mount the underlying S3 bucket instead of Lustre.",
      "D": "Implement a local cache on the training instance\u2019s EBS volume."
    },
    "explanation": "FSx for Lustre provides scalable metadata throughput proportionate to the configured throughput capacity. Increasing capacity raises available metadata IOPS and reduces latency. EFS and direct S3 mounts do not meet the sub-millisecond metadata requirements."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A data science team must join millions of small S3 objects with a 50 GiB Redshift table for feature engineering before training. The join must complete in the lowest end-to-end latency. Which ingestion and compute architecture should they choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run an AWS Glue ETL job to merge the data into a Parquet file and reload into SageMaker.",
      "B": "Use Redshift Spectrum to query the S3 objects in place and join directly with the local Redshift table.",
      "C": "Provision an EMR Spark cluster to read both sources and write joined output to S3.",
      "D": "Use Athena CTAS to precompute the join and then read from S3 in SageMaker."
    },
    "explanation": "Redshift Spectrum can directly query external S3 data and join with internal tables, minimizing data movement and achieving the lowest latency. Glue, EMR, and Athena introduce additional data shuffles or write phases, adding latency."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "When performing offline ingestion into SageMaker Feature Store, which file format and partitioning scheme will maximize batch import throughput and query efficiency?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A single unpartitioned CSV file containing all records.",
      "B": "Many small gzipped JSON files partitioned by feature group name.",
      "C": "Parquet files partitioned by ingestion date (yyyy/mm/dd) and hour.",
      "D": "Avro files with no partitions."
    },
    "explanation": "Partitioning Parquet files by date and hour enables parallel reads by the batch import process and efficient predicate pushdown, maximizing throughput and query efficiency. Single large CSVs or unpartitioned formats become bottlenecks; Avro lacks columnar benefits."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You have a numeric feature in your dataset that is strongly right-skewed. You plan to preprocess the data in SageMaker Data Wrangler and then train a linear regression model. Which sequence of transformations in Data Wrangler will best reduce skewness and satisfy the assumptions of linear regression?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Standardize the feature first, then apply a log transform",
      "B": "Apply a log transform first, then standardize the resulting values",
      "C": "Min\u2013max scale the feature first, then apply a log transform",
      "D": "One-hot encode the feature, then normalize the encoded columns"
    },
    "explanation": "A log transform first reduces skew, and subsequent standardization yields zero mean and unit variance, aligning with linear regression assumptions."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "Your streaming sensor pipeline ingests data via Kinesis Data Streams. You need to filter out invalid readings and impute missing values in real time with sub-second latency. Which solution provides the lowest operational overhead and meets performance requirements?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Kinesis Data Analytics SQL to filter and invoke Lambda for imputation",
      "B": "Use a Lambda function to filter and then call SageMaker Data Wrangler batch job for imputation",
      "C": "Use Kinesis Data Analytics for Apache Flink to filter and impute within the streaming application",
      "D": "Use AWS Glue streaming ETL job to filter and impute before writing to S3"
    },
    "explanation": "Kinesis Data Analytics for Apache Flink supports stateful, low-latency streaming transforms including filter and impute within the service, minimizing custom infrastructure."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A categorical feature has 5,000 unique values. You intend to use a tree-based algorithm and want to avoid excessive dimensionality. Which encoding technique should you apply using SageMaker Data Wrangler?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "One-hot encoding of all categories",
      "B": "Label encoding (assigning integer codes)",
      "C": "Frequency encoding (replace category with its occurrence frequency)",
      "D": "Hashing trick with a large hash space"
    },
    "explanation": "Frequency encoding reduces cardinality to a single numeric value per category, retaining information without high-dimensional expansion."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You need to remove duplicate records from two large S3 datasets in preparation for feature engineering. Which AWS Glue Spark ETL approach ensures schema enforcement and minimal data loss?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Glue DynamicFrame.drop_duplicates on the combined DynamicFrame",
      "B": "Convert to Spark DataFrame and call DataFrame.distinct()",
      "C": "Use a SageMaker Processing job with custom dedupe code",
      "D": "Use a Glue DataBrew recipe step to drop duplicates"
    },
    "explanation": "Glue DynamicFrame.drop_duplicates preserves schema and metadata, integrates with the Glue catalog, and scales automatically."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You need to detect and remove outliers beyond 3 standard deviations for a numeric column using AWS Glue DataBrew. Which recipe step accomplishes this with minimal effort?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Add a Filter rows step with condition > mean + 3*stddev",
      "B": "Use the built-in Remove outliers step and set the z-score threshold to 3",
      "C": "Add a Cluster rows step and drop the smallest cluster",
      "D": "Use Impute missing step to replace values beyond 3 standard deviations"
    },
    "explanation": "The Remove outliers step in DataBrew natively handles z-score thresholds, detecting and dropping extreme values automatically."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "Your team needs to implement a custom feature transformation in a visual pipeline that includes Python code (e.g., complex binning logic). Which SageMaker tool supports this requirement and collaborative workflows?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Glue DataBrew",
      "B": "SageMaker Data Wrangler",
      "C": "AWS Glue Python shell job",
      "D": "Spark on Amazon EMR notebook"
    },
    "explanation": "Data Wrangler provides a visual flow with built-in support for custom Python steps, versioning, and collaboration in Studio."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You need to compute a 7-day rolling mean on a time-series feature for millions of records. Which approach in an AWS managed service offers the highest throughput with minimal code?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run a SageMaker Processing job with PySpark window functions",
      "B": "Write a Glue Spark ETL job using Spark SQL window functions",
      "C": "Use SageMaker Data Wrangler\u2019s rolling statistics transform",
      "D": "Use Lambda functions with batched DynamoDB lookups"
    },
    "explanation": "Data Wrangler\u2019s built-in rolling statistic transform executes efficiently in a managed environment without custom code or cluster management."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You have a free-text column that needs tokenization into word tokens and conversion into integer IDs for embedding. Which Data Wrangler transformation sequence should you apply?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use the Tokenize step (word level) and then Encode step (label encoding)",
      "B": "Use TextVectorization step with TF-IDF output",
      "C": "One-hot encode the text column directly",
      "D": "Write a custom PySpark transform in SageMaker Processing"
    },
    "explanation": "The Tokenize step splits text into words, and Label Encoding maps each token to a unique integer ID, preparing for embeddings."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "Before training, you must enforce schema constraints (data types, value ranges) programmatically. Which AWS feature will you use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Glue Data Quality rules leveraging Deequ",
      "B": "SageMaker Data Wrangler profile job",
      "C": "DataBrew column profiling summary",
      "D": "Glue Data Catalog crawler validation"
    },
    "explanation": "Glue Data Quality uses Deequ to define and evaluate table constraints and thresholds in code, enforcing schema at scale."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You plan to store engineered features for real-time inference in SageMaker Feature Store. Which Data Wrangler export option supports batching data into a Feature Group?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Use a SageMaker Processing job to call PutRecord API",
      "B": "Use a Lambda function to batch and call PutRecordBatch",
      "C": "Export directly from Data Wrangler to SageMaker Feature Store",
      "D": "Upload CSV to S3 and configure offline store only"
    },
    "explanation": "Data Wrangler has a direct export feature to batch-write transformed data into a Feature Store Feature Group."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You need to reduce a numeric feature\u2019s cardinality to 100 bins based on equal-frequency intervals using a no-code solution. Which DataBrew step do you choose?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Group by step with custom quantile aggregation",
      "B": "Bin numeric values step with equal-frequency option",
      "C": "Cluster rows step with K-means clustering",
      "D": "Custom recipe with Python UDF"
    },
    "explanation": "The Bin numeric values step with equal-frequency creates quantile-based bins automatically, reducing cardinality."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "To impute missing age values by country group using DataBrew, which step sequence is correct?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use Partition by country, then Impute missing step with median strategy",
      "B": "Aggregate to compute medians, then perform a join back",
      "C": "Apply global median imputation for the age column",
      "D": "Export data and impute in SageMaker Processing"
    },
    "explanation": "Partitioning by country followed by median imputation uses group-wise statistics directly in DataBrew with minimal extra steps."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "Two 1 TB CSV files in S3 must be inner-joined on a composite key before feature engineering. You want a serverless, managed service that handles scaling and scripting. Which approach is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run an AWS Glue ETL job using DynamicFrame.join",
      "B": "Launch an EMR cluster with custom PySpark code",
      "C": "Use a SageMaker Processing PySpark job",
      "D": "Perform a CTAS join in Amazon Athena"
    },
    "explanation": "AWS Glue ETL with DynamicFrame.join provides serverless scaling, schema enforcement, and integration with the Glue Data Catalog."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You need to anonymize email addresses in real time within a Kinesis Data Stream, ensuring <200 ms processing per record. Which architecture meets this requirement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Glue streaming ETL job with built-in masking",
      "B": "Kinesis Data Analytics for Apache Flink with a UDF to hash emails",
      "C": "AWS Lambda subscribed to the stream invoking a hashing library",
      "D": "SageMaker Data Wrangler in streaming mode"
    },
    "explanation": "Kinesis Data Analytics for Flink executes in-stream UDFs with low, consistent latency, meeting sub-200 ms requirements without cold starts."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "Duplicate events may occur within a 2-minute window in your Kinesis stream. You need to drop duplicates in real time before storing to S3. Which solution is most appropriate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Kinesis Data Analytics for Flink, keyBy event ID and apply a 2-minute deduplication window",
      "B": "Configure an AWS Glue streaming job with dedupe enabled",
      "C": "Trigger a Lambda function per record and check DynamoDB for prior IDs",
      "D": "Run an EMR streaming job with custom dedupe code"
    },
    "explanation": "Kinesis Data Analytics for Flink supports stateful windowed deduplication on event keys with minimal infrastructure overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "An ML engineer needs to set up automated, scheduled data quality checks on an Amazon S3 dataset to validate completeness, uniqueness, and detect numeric outliers over time. Which solution meets these requirements with the LEAST operational overhead?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Use AWS Glue DataBrew to author a recipe with transformations and schedule it as a DataBrew job.",
      "B": "Use AWS Glue Data Quality to define data quality rules and configure a schedule to run them.",
      "C": "Use SageMaker Clarify DataQualityCheckConfig in a SageMaker Processing job triggered by EventBridge.",
      "D": "Use Amazon Athena SQL queries inside a Lambda function scheduled by EventBridge."
    },
    "explanation": "AWS Glue Data Quality is designed for automated, rule-based validation and scheduling of data quality checks with minimal operational overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A data scientist must detect pre-training label disparity between male and female groups in a binary classification dataset. Which SageMaker Clarify metric should be used?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "pre_training_bias:CI (class imbalance)",
      "B": "pre_training_bias:DPL (difference in proportions of labels)",
      "C": "post_training_bias:CI",
      "D": "post_training_bias:DPL"
    },
    "explanation": "Difference in proportions of labels (DPL) is the appropriate pre-training metric to measure label distribution disparity between protected groups."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A dataset for fraud detection has severe class imbalance (0.5% positives). The engineer wants synthetic minority oversampling before training. Which AWS service/plugin should be used to implement SMOTE?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Specify class_weight in the training script and rely on model loss adjustment.",
      "B": "Use AWS Glue DataBrew sampling transform to oversample the minority class.",
      "C": "Use a SageMaker Processing job that implements imbalanced-learn\u2019s SMOTE algorithm.",
      "D": "Use SageMaker Clarify to automatically generate synthetic samples."
    },
    "explanation": "A SageMaker Processing job allows you to run custom code (e.g., imbalanced-learn) to generate SMOTE synthetic samples before training."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A tabular dataset containing PII must be anonymized before model training. The engineer wants to mask names, emails, and SSNs in CSV files with minimal coding. Which solution should be implemented?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use AWS Glue DataBrew recipe steps with the built-in PII Mask transform on the relevant columns.",
      "B": "Write a Lambda function to scan each file in S3, mask PII, and write back to S3.",
      "C": "Use SageMaker Clarify\u2019s bias analysis and misapply it for PII detection.",
      "D": "Use AWS KMS custom encryption to encrypt only the PII columns at rest."
    },
    "explanation": "AWS Glue DataBrew provides built-in PII Mask transforms that can mask specified columns with minimal code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "An ML engineer uses an Amazon EFS file system to store training data. To meet compliance, data must be encrypted at rest and in transit between the SageMaker training instance and EFS. Which configuration meets these requirements?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable only SSE-KMS encryption on the EFS file system.",
      "B": "Configure the EFS mount with NFSv4 and no encryption.",
      "C": "Use Amazon FSx for Lustre with default settings.",
      "D": "Enable EFS encryption at rest (SSE-KMS) and mount using EFS mount options with encryption in transit (TLS)."
    },
    "explanation": "To meet both requirements, enable SSE-KMS on EFS and use the EFS mount option to enforce TLS encryption in transit."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A dataset contains multiple records per user_id. To prevent data leakage, how should an ML engineer split the dataset into train and test subsets?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Randomly split 80/20 at the record level.",
      "B": "Use stratified split to preserve label proportions.",
      "C": "Use a group-based split (e.g., scikit-learn\u2019s GroupShuffleSplit) with user_id as the group key.",
      "D": "Split based on time, taking the latest 20% of records as test."
    },
    "explanation": "A group-based split ensures that all records for a given user_id are either in train or test, preventing leakage."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A text dataset with PII must be anonymized before training. The engineer wants automatic detection of names and SSNs. Which AWS service should be used in a SageMaker Processing job?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Glue DataBrew PII Masking",
      "B": "Amazon Comprehend PII detection API",
      "C": "SageMaker Clarify bias detection",
      "D": "AWS Lake Formation data labeling"
    },
    "explanation": "Amazon Comprehend\u2019s PII detection API can be called from a Processing job to automatically identify and anonymize PII in text."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "To detect statistical parity in a numeric target variable across demographic groups before training, which SageMaker Clarify component should be used?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "DataQualityCheckConfig in SageMaker Clarify",
      "B": "PreTrainingBiasCheckConfig in a SageMaker Clarify Processing job",
      "C": "ModelBiasMonitor",
      "D": "BatchTransform with a custom script"
    },
    "explanation": "PreTrainingBiasCheckConfig in a Clarify Processing job enables computation of pre-training bias metrics on the dataset."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "Before ingesting S3 data into SageMaker, an engineer needs to automatically discover and classify PII fields at scale. Which AWS service should be used?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Config",
      "B": "Amazon Athena",
      "C": "Amazon Macie",
      "D": "AWS CloudTrail"
    },
    "explanation": "Amazon Macie automatically discovers and classifies PII in S3 objects at scale."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A SageMaker training job must encrypt all EBS volumes with a customer-managed KMS key. How should the engineer configure the training job?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Specify the KmsKeyId parameter in the CreateTrainingJob API.",
      "B": "Configure the InputDataConfig to include an EncryptionKeyId.",
      "C": "Enable SSE-KMS on the S3 bucket only.",
      "D": "Use a SageMaker Pipeline with a default KMS key."
    },
    "explanation": "The CreateTrainingJob API\u2019s KmsKeyId parameter applies the specified customer-managed KMS key to encrypt all training EBS volumes."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A company must enforce fine-grained access control on PII columns in its Data Catalog tables. Which AWS capability enables this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "S3 bucket policies on the underlying data files.",
      "B": "AWS Lake Formation LF-Tags attached to Data Catalog columns.",
      "C": "IAM identity-based policies only.",
      "D": "AWS KMS key policies on the Data Catalog."
    },
    "explanation": "Lake Formation LF-Tags on Data Catalog columns provide column-level access control for sensitive data."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "An engineer needs to ensure that a string column contains only unique values before training. Which AWS service and rule type should be used?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Glue DataBrew with a completeness rule.",
      "B": "AWS Glue Data Quality with a passing ratio rule.",
      "C": "SageMaker Clarify DataQualityCheckConfig uniqueness check.",
      "D": "AWS Glue Data Quality with a uniqueness rule."
    },
    "explanation": "AWS Glue Data Quality supports a rule to check column uniqueness to enforce that values are unique."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "To prevent order-based bias in large S3 datasets during training, how can the engineer shuffle data in a SageMaker training job?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use File input mode (default shuffling).",
      "B": "Use RecordIO input mode (shuffles automatically).",
      "C": "Use Pipe input mode with ShuffleConfig set to an appropriate buffer size.",
      "D": "Download and shuffle in the training script only."
    },
    "explanation": "Pipe input mode with ShuffleConfig allows SageMaker to shuffle streaming records before training to avoid order bias."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A dataset contains missing numeric values that must be imputed with the median before feature engineering. Which tool provides a built-in transform for this?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Glue DataBrew recipe step \u201cFill missing values\u201d with median.",
      "B": "SageMaker Debugger preprocessing hook.",
      "C": "SageMaker Feature Store ingestion transform.",
      "D": "AWS Glue Data Quality imputation rule."
    },
    "explanation": "AWS Glue DataBrew includes a \u201cFill missing values\u201d transform that can impute missing entries with statistics like median."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "An engineer wants to use a consistent, repeatable set of cleaned and validated features for both offline training and real-time inference. Which AWS capability should be used?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon S3 with versioned CSV files.",
      "B": "Amazon DynamoDB table with precomputed features.",
      "C": "SageMaker Feature Store to store and retrieve features.",
      "D": "Amazon Redshift external table."
    },
    "explanation": "SageMaker Feature Store provides a centralized store for features that ensures consistency between offline training and online inference."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A credit risk team needs a highly interpretable, low-latency regression model for predicting applicant default probability using 50 numerical and categorical features. They require clear feature coefficients for regulatory reporting. Which SageMaker built-in algorithm should they choose?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "XGBoost built-in algorithm with SHAP explanations",
      "B": "Linear Learner built-in algorithm in regression mode",
      "C": "Factorization Machines built-in algorithm",
      "D": "DeepAR forecasting algorithm"
    },
    "explanation": "Linear Learner in regression mode provides direct feature weights for interpretability and low latency. XGBoost can require SHAP post hoc analysis, and other algorithms aren\u2019t designed for regression interpretability."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "An online advertising platform must predict click-through rates with hundreds of categorical features of high cardinality. The team wants to capture pairwise interactions between sparse features efficiently. Which built-in SageMaker algorithm is most appropriate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "XGBoost built-in algorithm",
      "B": "Linear Learner built-in algorithm",
      "C": "Factorization Machines built-in algorithm",
      "D": "K-Means built-in clustering algorithm"
    },
    "explanation": "Factorization Machines are designed to model interactions among high-cardinality categorical features more efficiently than tree-based or linear models."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A retail company needs to forecast hourly product demand for 10,000 SKUs with seasonal patterns and occasional promotions. They require a probabilistic forecast of future demand. Which built-in SageMaker algorithm should they select?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "DeepAR forecasting algorithm",
      "B": "XGBoost regression algorithm",
      "C": "Linear Learner regression algorithm",
      "D": "K-Nearest Neighbors built-in algorithm"
    },
    "explanation": "DeepAR provides probabilistic time series forecasts at scale and handles seasonality and promotions. Other algorithms do not directly support probabilistic forecasting."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "An operations team needs to detect anomalies in streaming server CPU metrics. They require a one-class unsupervised method that adapts to drift. Which SageMaker built-in algorithm meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Isolation Forest built-in algorithm",
      "B": "Random Cut Forest built-in algorithm",
      "C": "One-Class SVM via script mode",
      "D": "K-Means built-in clustering algorithm"
    },
    "explanation": "Random Cut Forest is an unsupervised, streaming-friendly anomaly detection algorithm that adapts to drift; it\u2019s a SageMaker built-in algorithm."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A global publisher must classify news articles in 10 languages with minimal training data per language. They need an AWS managed NLP service with built-in support. Which service should they choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Custom BERT model on SageMaker",
      "B": "Custom TF-based text classifier in script mode",
      "C": "Amazon Comprehend DetectDominantLanguage and Custom Classification APIs",
      "D": "Amazon Translate to English plus custom classifier"
    },
    "explanation": "Amazon Comprehend offers multi-language classification with minimal training data and managed infrastructure. Custom models require more data and maintenance."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A biotech startup wants to identify cell types in microscopy images with minimal ML expertise. They prefer a no-code AWS solution that adapts to new classes. Which service should they use?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Custom CNN built in TensorFlow on SageMaker script mode",
      "B": "Deploy a PyTorch model on SageMaker endpoint",
      "C": "Use SageMaker AutoML via built-in algorithms",
      "D": "SageMaker Canvas image classification with Amazon Rekognition Custom Labels integration"
    },
    "explanation": "SageMaker Canvas provides a no-code interface, and integration with Rekognition Custom Labels supports labeling and incremental class adaptation without deep ML expertise."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A call-center analytics team needs real-time transcription and sentiment analysis of customer calls. They require a managed service with low operations overhead. Which combination should they use?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Custom DeepSpeech model on SageMaker",
      "B": "Amazon Transcribe for transcription and Amazon Comprehend for sentiment",
      "C": "Deploy open-source Kaldi on SageMaker",
      "D": "Amazon Lex chatbot for transcription and sentiment"
    },
    "explanation": "Amazon Transcribe and Comprehend are managed services for ASR and sentiment analysis, respectively, minimizing operational overhead compared to custom models."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A financial modeling team needs to generate synthetic financial reports using a foundation model. They require a generative text service with minimal training. Which AWS service best fits?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Bedrock with a foundation LLM",
      "B": "Custom GPT-2 on SageMaker",
      "C": "Use TensorFlow Sequence-to-Sequence on SageMaker",
      "D": "Amazon Translate with custom glossary"
    },
    "explanation": "Amazon Bedrock provides foundation LLMs for text generation, reducing heavy training and tuning efforts versus custom Seq2Seq models."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "An edge computing use case requires running an image classifier on IoT cameras with limited compute. They have a trained ResNet model. Which SageMaker feature should they use to optimize the model?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker built-in Image Classification algorithm",
      "B": "SageMaker Script Mode with TensorFlow",
      "C": "SageMaker Neo compilation to target edge architecture",
      "D": "Containerize model for SageMaker endpoint"
    },
    "explanation": "SageMaker Neo compiles and optimizes trained models for edge devices, reducing latency and resource usage on IoT cameras."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A team needs to quickly prototype a multi-label text classification for internal documents with zero code. They want to experiment with pre-trained models and fine-tune them. Which SageMaker capability should they use?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "SageMaker Clarify for data bias analysis",
      "B": "SageMaker Model Monitor for drift alerts",
      "C": "Notebook instance with pre-built scripts",
      "D": "SageMaker JumpStart solution templates for multi-label text classification"
    },
    "explanation": "SageMaker JumpStart provides pre-trained model solutions and templates for multi-label classification that can be fine-tuned with minimal code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A startup has a small tabular dataset (5,000 rows) and needs a quick binary classifier with built-in regularization and automated hyperparameter tuning. Which SageMaker built-in algorithm and mode is most appropriate?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Linear Learner in binary classification mode",
      "B": "XGBoost regression mode",
      "C": "K-Means clustering mode",
      "D": "DeepAR forecasting mode"
    },
    "explanation": "Linear Learner supports binary classification with built-in regularization and integrates with SageMaker Automatic Model Tuning for small datasets efficiently."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A fraud detection team requires an algorithm that handles class imbalance and produces probabilistic scores for each transaction. They prefer a tree-based approach. Which built-in algorithm should they select?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Linear Learner with L1 regularization",
      "B": "XGBoost built-in algorithm",
      "C": "Random Cut Forest",
      "D": "K-Nearest Neighbors"
    },
    "explanation": "XGBoost produces probabilistic outputs, handles class imbalance via objective weighting, and is a high-performance tree-based algorithm."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A medical imaging project needs semantic segmentation on X-ray images. The team has no pre-built algorithm in SageMaker. They want minimal development overhead. Which approach should they take?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker built-in Object Detection algorithm",
      "B": "Use Rekognition Custom Labels",
      "C": "Use JumpStart semantic segmentation pre-trained model in script mode",
      "D": "Develop a U-Net from scratch in a notebook"
    },
    "explanation": "JumpStart offers pre-trained semantic segmentation models that can be fine-tuned in script mode, reducing overhead compared to building models from scratch."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A business needs to extract key phrases from customer reviews in Spanish. They want a managed solution that supports custom phrase detection. Which service should they choose?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Custom spaCy model on SageMaker",
      "B": "Custom PyTorch NLP in script mode",
      "C": "Amazon Translate to English plus Comprehend key phrases",
      "D": "Amazon Comprehend with custom entity recognizer for Spanish"
    },
    "explanation": "Amazon Comprehend supports custom entity detection in multiple languages including Spanish, avoiding translation pipelines or heavy custom models."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A team must choose between deploying a SageMaker built-in algorithm versus a pre-packaged deep learning framework in script mode. They prioritize faster training on CPU-only instances for a moderate-sized tabular dataset. Which should they choose?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "SageMaker built-in XGBoost algorithm",
      "B": "TensorFlow in script mode",
      "C": "PyTorch in script mode",
      "D": "Bring-Your-Own container with Scikit-learn"
    },
    "explanation": "XGBoost built-in algorithm is optimized for CPU training on tabular data and will train faster with minimal configuration compared to deep learning frameworks."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A data scientist has an existing SageMaker automatic model tuning (AMT) job that produced optimal hyperparameter values for a regression model. New data arrives and the scientist wants to reuse the previous tuning results as a starting point for a new hyperparameter tuning job to save compute time. Which warm start configuration type should the scientist choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a warm start tuning job with type IDENTICAL_DATA_AND_ALGORITHM.",
      "B": "Use a warm start tuning job with type TRANSFER_LEARNING.",
      "C": "Use a warm start tuning job with type CURRENT_BEST.",
      "D": "Use a regular hyperparameter tuning job without warm start."
    },
    "explanation": "TRANSFER_LEARNING reuses previous tuning job results to inform new search on similar data or algorithm. IDENTICAL_DATA_AND_ALGORITHM reruns the same search space exactly. CURRENT_BEST and non\u2013warm-start jobs don\u2019t utilize prior results to reduce compute."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An ML engineer wants to reduce wasted compute and stop unpromising training jobs early during an AMT hyperparameter tuning job. Which configuration change will achieve this with minimal code changes?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set EarlyStoppingType to AUTO in the hyperparameter tuning job configuration.",
      "B": "Set MaxRuntimePerTrainingJob to a lower value.",
      "C": "Add a CloudWatch alarm to terminate long-running training jobs.",
      "D": "Wrap each training job in a custom script that kills the process if validation loss stops improving."
    },
    "explanation": "Setting EarlyStoppingType='Auto' enables SageMaker to automatically terminate unpromising training jobs. Lowering MaxRuntime limits total time but doesn\u2019t target unpromising jobs. CloudWatch alarms and custom scripts add operational overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A team trains a large PyTorch model on 8 GPU instances, but training is slow and network communication is a bottleneck. Which SageMaker feature will provide an efficient out-of-the-box distributed training solution to accelerate this job?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure Horovod for distributed training.",
      "B": "Enable smdistributed.dataparallel in Script Mode.",
      "C": "Use the built-in PyTorch Multi-GPU Estimator.",
      "D": "Use a parameter server architecture implemented in the training script."
    },
    "explanation": "smdistributed DataParallel integrates with Script Mode and optimizes gradient synchronization. Horovod requires extra setup, custom parameter server needs code changes, and there is no separate built-in PyTorch multi-GPU Estimator."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "During training of a convolutional neural network, an ML engineer observes overfitting: training accuracy far exceeds validation accuracy. Which single change to the training job will increase generalization with the least complexity?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase the dropout rate from 0.25 to 0.75.",
      "B": "Reduce the initial learning rate by a factor of ten.",
      "C": "Add L2 weight-decay regularization to the optimizer.",
      "D": "Switch to early stopping and set patience to 3 epochs."
    },
    "explanation": "L2 weight decay penalizes large weights and reduces overfitting without requiring new callbacks. Excessive dropout may underfit; reducing learning rate doesn\u2019t directly address overfitting; early stopping adds extra tuning and monitoring."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An ML engineer has trained both a random forest and an XGBoost model for the same classification task and wants to combine them to improve accuracy. Which ensemble approach should the engineer implement to learn an optimal combination of predictions?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a meta-learner on the model outputs (stacking).",
      "B": "Average the predictions from both models (bagging).",
      "C": "Train the XGBoost model to correct errors of the random forest (boosting).",
      "D": "Select the model with the higher validation accuracy for each input dynamically (voting)."
    },
    "explanation": "Stacking uses a meta\u00ad-model trained on base-model outputs to learn optimal weights. Bagging pools multiple instances of one algorithm. Boosting uses sequential training, not two distinct models. Voting is a simple unweighted ensemble."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A financial services firm needs to track, promote, and audit multiple versions of their ML models in SageMaker. Which feature should they use to centrally manage model versions and stages (e.g., Approved, Pending)?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon S3 object versioning on the model artifacts bucket.",
      "B": "SageMaker Model Registry.",
      "C": "SageMaker Experiments.",
      "D": "AWS CodeCommit repository."
    },
    "explanation": "SageMaker Model Registry is designed to store, version, and annotate model artifacts and their approval lifecycle. S3 versioning tracks raw objects but lacks metadata. Experiments track experiments, not production stages. CodeCommit is source control."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "When using smdistributed.dataparallel for PyTorch training, the network bandwidth is still limiting training throughput. Which additional configuration can reduce communication overhead?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch to Horovod with NCCL backend.",
      "B": "Enable gradient compression (fp16) in smdistributed.dataparallel.",
      "C": "Use MPI and ring-allreduce instead of reduce-scatter.",
      "D": "Increase batch size to amortize communication."
    },
    "explanation": "smdistributed DataParallel supports fp16 gradient compression to reduce bandwidth. Horovod setup is heavier, using MPI doesn\u2019t enable compression, and larger batches may help but don\u2019t directly reduce communication volume."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An ML engineer is constrained by budget and can run at most 50 training jobs for hyperparameter tuning. Which search strategy in SageMaker AMT will likely find the best solution within the least number of jobs?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Grid search.",
      "B": "Random search.",
      "C": "Bayesian optimization.",
      "D": "Genetic algorithm search."
    },
    "explanation": "Bayesian optimization balances exploration and exploitation, converging faster. Grid search is exhaustive, random search is less efficient, and genetic algorithms aren\u2019t natively supported in SageMaker AMT."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A company must deploy a deep learning model to edge devices that have strict memory limits. Which approach using SageMaker will produce the smallest model artifact with minimal manual optimization?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Apply manual weight pruning in the training script.",
      "B": "Use SageMaker Neo to compile and quantize the model for the target device.",
      "C": "Enable mixed-precision training and save fp16 weights.",
      "D": "Use model distillation to train a smaller student network."
    },
    "explanation": "SageMaker Neo automates compilation and quantization for the target hardware, reducing model size. Manual pruning requires custom code; mixed precision lowers memory at runtime but not artifact size; distillation requires additional training."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An ML engineer wants to fine-tune a large pre-trained Hugging Face transformer using SageMaker. Which method requires the least boilerplate code?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Bring your own container with transformers installed.",
      "B": "Use the SageMaker Hugging Face estimator in Script Mode.",
      "C": "Translate the model code to MXNet and use the built-in MXNet Estimator.",
      "D": "Use Amazon SageMaker built-in text classification algorithm."
    },
    "explanation": "The SageMaker Hugging Face estimator simplifies fine-tuning with minimal code. BYOC adds container management; translating to MXNet is error-prone; no built-in algorithm for large transformer fine-tuning exists."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A data scientist configures an AMT tuning job but forgot to configure early stopping. Which change should they make to the tuning configuration to enable automated early stopping of poor performing jobs?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set EarlyStoppingType='Auto' and choose an appropriate WaitInterval.",
      "B": "Lower the MaxJobs parameter to force quicker completion.",
      "C": "Define a CloudWatch rule to terminate jobs with low metrics.",
      "D": "Wrap the training script in a debugger rule to kill unresponsive jobs."
    },
    "explanation": "EarlyStoppingType='Auto' with WaitInterval allows SageMaker to stop jobs whose objective does not improve. Lowering MaxJobs only limits total count. CloudWatch rules and debugger scripts are more operationally complex."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A neural network training job shows high training accuracy but validation accuracy plateaus early. The engineer wants an automated mechanism to detect this during training. Which SageMaker Debugger rule should they enable?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "VanishingGradientDetector.",
      "B": "LossNotDecreasing.",
      "C": "OverfitDetector.",
      "D": "WeightUpdateVerifier."
    },
    "explanation": "OverfitDetector monitors training vs. validation metrics to detect early signs of overfitting. VanishingGradientDetector and LossNotDecreasing focus on gradients and loss. WeightUpdateVerifier checks parameter updates."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A team wants to track parameters, metrics, and artifacts across multiple training jobs and compare runs in SageMaker. Which feature should they use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Model Monitor.",
      "B": "SageMaker Experiments.",
      "C": "Amazon CloudWatch metrics.",
      "D": "AWS X-Ray."
    },
    "explanation": "SageMaker Experiments is built to organize, track, and compare training runs. Model Monitor observes deployed models. CloudWatch and X-Ray are for logs and traces, not training experiment management."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A neural network trained on tabular data shows many small nonzero weights and generalizes poorly. Which regularization change will encourage sparsity and reduce overfitting?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase the L2 (weight\u2010decay) coefficient.",
      "B": "Increase dropout to 0.8.",
      "C": "Switch to L1 regularization on weights.",
      "D": "Use batch normalization after every layer."
    },
    "explanation": "L1 regularization (Lasso) encourages many weights to become exactly zero, producing sparse models. L2 shrinks but doesn\u2019t enforce sparsity. Dropout and batch normalization address generalization but not weight sparsity."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You have trained a binary classifier on a dataset where the positive class constitutes only 1% of the data. On a held-out test set, the model achieves an AUC-ROC of 0.90, but its precision at low recall is poor. Which evaluation metric should you use to better assess the model\u2019s ability to identify the minority positive class?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Accuracy",
      "B": "AUC-ROC",
      "C": "Area Under the Precision-Recall Curve (AUC-PR)",
      "D": "Log Loss"
    },
    "explanation": "With extreme class imbalance, the precision-recall curve (AUC-PR) focuses on performance for the positive class and is more informative than AUC-ROC or accuracy."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "A regression model predicts house prices. Stakeholders are more concerned about a few very large prediction errors than many small ones. Which metric should you minimize?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Mean Absolute Error (MAE)",
      "B": "Root Mean Square Error (RMSE)",
      "C": "R\u00b2 (Coefficient of Determination)",
      "D": "Mean Absolute Percentage Error (MAPE)"
    },
    "explanation": "RMSE penalizes large errors more heavily than MAE, making it appropriate when large deviations are particularly undesirable."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "An ML engineer wants to run reproducible training experiments that track hyperparameters, metrics, and artifacts across multiple training jobs in Amazon SageMaker. Which AWS service should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon CloudWatch Logs",
      "B": "AWS CodePipeline",
      "C": "Amazon SageMaker Experiments",
      "D": "AWS Config"
    },
    "explanation": "SageMaker Experiments provides managed tracking of experiments, trials, hyperparameters, metrics, and artifacts to ensure reproducibility."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "During training, you notice that your model\u2019s loss on the training set stops decreasing early and plateaus. Which built-in SageMaker Debugger rule should you enable to detect this convergence issue?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "OverfitDetector",
      "B": "LossNotDecreasing",
      "C": "LearningRateFinder",
      "D": "WeightNormMonitor"
    },
    "explanation": "The LossNotDecreasing rule monitors training loss and raises an alert if it does not decrease sufficiently, indicating convergence problems."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You want to generate local feature\u2010level explanations (SHAP values) for your production model using SageMaker Clarify. Which configuration should you use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "DataBiasConfig in ClarifyProcessingJob",
      "B": "ModelBiasConfig in ClarifyProcessingJob",
      "C": "SHAPConfig in ClarifyProcessingJob",
      "D": "DriftConfig in ClarifyProcessingJob"
    },
    "explanation": "SHAPConfig enables Clarify to compute SHAP feature attributions for local explainability of model predictions."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "To select an optimal classification threshold that maximizes F1 score, what procedure should you perform on your validation data?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Evaluate F1 at various probability thresholds and choose the threshold yielding the highest F1",
      "B": "Select the threshold where ROC curve slope equals 1",
      "C": "Use the threshold where precision equals recall",
      "D": "Choose the threshold that balances true positive and false positive rates"
    },
    "explanation": "Maximizing F1 requires evaluating F1 at multiple thresholds on validation data and selecting the threshold with the highest F1."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your deep neural network exhibits vanishing gradient issues. Which SageMaker Debugger configuration helps you inspect gradient distributions during training?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable the base and gradient_histogram collections",
      "B": "Use the TorchLrFinder rule",
      "C": "Use the OverfitDetector rule",
      "D": "Configure only the losses collection"
    },
    "explanation": "The gradient_histogram collection captures gradient distributions so you can detect vanishing or exploding gradients."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You need to perform A/B testing between two model variants in production with minimal overhead. Which SageMaker feature supports traffic splitting between variants?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Deploy two separate endpoints and use Route 53 weighted routing",
      "B": "Configure two production variants in a single endpoint and set variant weights",
      "C": "Use an AWS Lambda function to proxy and split traffic",
      "D": "Use AWS CodePipeline for traffic routing"
    },
    "explanation": "SageMaker endpoints support multiple production variants with configurable weights for built-in traffic splitting (A/B testing)."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "For a multi-class classification problem with imbalanced classes, which evaluation metric gives equal importance to each class regardless of its frequency?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Micro-averaged F1 score",
      "B": "Macro-averaged F1 score",
      "C": "Overall accuracy",
      "D": "Weighted precision"
    },
    "explanation": "Macro-averaged F1 computes the F1 score for each class and averages them equally, treating all classes with equal importance."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You compare two regression models: Model A (RMSE = 5.0), Model B (RMSE = 4.8). How can you determine whether the observed difference is statistically significant?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Compare the RMSE values directly",
      "B": "Compute bootstrapped confidence intervals for the RMSE difference",
      "C": "Use one-way ANOVA on prediction residuals",
      "D": "Compare R\u00b2 values"
    },
    "explanation": "Bootstrapping provides confidence intervals on the RMSE difference to determine if the improvement is statistically significant."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Which SageMaker Clarify metric measures the difference in the proportion of positive outcomes between two demographic groups?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Class imbalance (CI)",
      "B": "Difference in Proportions of Labels (DPL)",
      "C": "Kolmogorov\u2013Smirnov (KS) statistic",
      "D": "Confusion matrix parity"
    },
    "explanation": "DPL quantifies the difference in label rates (positive outcome proportions) between a sensitive group and the baseline group."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your model\u2019s training loss decreases steadily while validation loss increases after a point. Which SageMaker Debugger rule can detect this behavior automatically?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "OverfitDetector",
      "B": "LossNotDecreasing",
      "C": "LearningRateFinder",
      "D": "GradientHistogram"
    },
    "explanation": "OverfitDetector monitors divergence between training and validation losses to detect overfitting during training."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You want to identify a suitable initial learning rate for faster convergence. Which SageMaker Debugger rule should you run?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "LearningRateFinder",
      "B": "LossNotDecreasing",
      "C": "OverfitDetector",
      "D": "EarlyStoppingRule"
    },
    "explanation": "LearningRateFinder systematically varies the learning rate to help identify an optimal learning rate for training convergence."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You need to compute evaluation metrics at multiple classification thresholds using SageMaker Clarify. Which configuration option enables this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Specify multiple thresholds in the ClarifyProcessor\u2019s ModelPredictionsConfig",
      "B": "Use SageMaker Model Monitor\u2019s default job",
      "C": "Run a SageMaker batch transform and manually compute metrics",
      "D": "Use SageMaker Automatic Model Tuning"
    },
    "explanation": "You can configure multiple thresholds in ModelPredictionsConfig for Clarify processing jobs to compute metrics (precision, recall, F1) at those thresholds."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "During training you observe that both training and validation losses are high and nearly identical, and the model fails to improve. What issue does this indicate?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Overfitting",
      "B": "Underfitting",
      "C": "Data leakage",
      "D": "Gradient explosion"
    },
    "explanation": "High and similar training/validation losses indicate underfitting, meaning the model is too simplistic to capture the underlying data patterns."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A startup has developed an ML model for an internal analytics tool that is invoked fewer than 100 times per day. Each inference must return within 200 ms, and total monthly cost must be minimized. Which SageMaker deployment option best meets these requirements?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Provision a real-time SageMaker endpoint with minimal instance capacity and auto scaling.",
      "B": "Deploy the model as a SageMaker serverless endpoint.",
      "C": "Use a SageMaker asynchronous endpoint with low concurrency settings.",
      "D": "Schedule a SageMaker batch transform job once per day."
    },
    "explanation": "A serverless endpoint incurs no idle instance cost, supports <200 ms latency for light workloads, and is optimal for very low invocation volume."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A machine learning team must host 500 small models (<50 MB each) behind a single API. All models share identical inference logic but have different weights. Which deployment infrastructure should be used to minimize cost and management overhead?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A multi-model SageMaker endpoint.",
      "B": "500 separate real-time SageMaker endpoints.",
      "C": "A SageMaker batch transform job per model.",
      "D": "Amazon ECS on Fargate with mounted S3 weights."
    },
    "explanation": "Multi-model endpoints load models on demand into a shared container, reducing cost and simplifying management compared to separate endpoints."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An automotive OEM wants to run an object-detection model on in-vehicle devices with limited compute and no internet connectivity. The model must start in <50 ms after request. Which deployment approach meets these requirements?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Host the model on a SageMaker real-time endpoint and stream data from the vehicle.",
      "B": "Use a SageMaker asynchronous endpoint with cached results on the vehicle gateway.",
      "C": "Compile the model with SageMaker Neo and deploy it to the edge container on the device.",
      "D": "Package the model in a Lambda@Edge function and invoke it from the vehicle."
    },
    "explanation": "SageMaker Neo compiles and optimizes models for specific hardware and enables sub-50 ms local inference without connectivity."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A document-processing model accepts 8 MB JSON payloads and may run up to 10 minutes per request. Customers must receive a response per request. Which SageMaker endpoint type is the most cost-effective?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Asynchronous SageMaker endpoint.",
      "B": "Real-time SageMaker endpoint.",
      "C": "Serverless SageMaker endpoint.",
      "D": "SageMaker batch transform job."
    },
    "explanation": "Asynchronous endpoints support large payloads, long processing durations, and return one response per request, avoiding persistent instance costs of real-time endpoints."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An e-commerce site needs <100 ms inference latency for its image-classification model at peak traffic. Daily traffic patterns vary widely. Which compute environment should the ML engineer choose?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CPU-based serverless SageMaker endpoint.",
      "B": "GPU-based serverless SageMaker endpoint.",
      "C": "CPU-based real-time SageMaker endpoint with fixed instance count.",
      "D": "GPU-based real-time SageMaker endpoint with auto scaling."
    },
    "explanation": "GPU real-time endpoints deliver required low latency, and auto scaling adjusts capacity to traffic variation."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A custom TensorFlow model requires specific OS libraries and drivers that are not available in SageMaker built-in containers. What is the least operationally intensive way to deploy the model?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Translate the model code to one compatible with a built-in container.",
      "B": "Run a batch transform job instead of hosting a real-time endpoint.",
      "C": "Build a custom Docker container, push to ECR, and use it in a SageMaker endpoint.",
      "D": "Use a Lambda function with layers containing the required libraries."
    },
    "explanation": "Custom containers in SageMaker allow full control over dependencies and integrate seamlessly with endpoint deployment."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A regulated enterprise requires all inference traffic to traverse a secured, private network with no public internet access. Which configuration satisfies this requirement with minimal changes?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy a public SageMaker endpoint and block internet via security groups.",
      "B": "Deploy a SageMaker real-time endpoint in the customer VPC with private subnets and no NAT gateway.",
      "C": "Use a serverless SageMaker endpoint and disable outbound access.",
      "D": "Host the model in EKS and restrict Internet at cluster level."
    },
    "explanation": "Launching the endpoint in private subnets of the customer VPC ensures no public internet access without additional NAT configuration."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An ML engineer must deploy a new model version and gradually shift 20% of production traffic to it for canary testing, with automatic rollback on errors. Which SageMaker feature accomplishes this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy a new real-time endpoint and use a Route 53 weighted record.",
      "B": "Create a second endpoint and manually switch after validation.",
      "C": "Use a SageMaker batch transform job for the canary.",
      "D": "Use SageMaker endpoint\u2010update with traffic shifting via endpoint configurations."
    },
    "explanation": "SageMaker\u2019s endpoint configuration supports traffic weights for A/B and canary deployments and automatic rollback on alarms."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "Your team uses Amazon Managed Workflows for Apache Airflow (MWAA) but wants tighter integration with SageMaker model deployments and lineage tracking. Which orchestrator should you choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Continue with MWAA and use custom operators.",
      "B": "SageMaker Pipelines.",
      "C": "AWS Step Functions directly.",
      "D": "AWS CodePipeline."
    },
    "explanation": "SageMaker Pipelines provides built-in integration for training, model registry, and deployment with lineage tracking."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A model requires a preprocessing container and a separate inference container on each request. How should you deploy this in SageMaker?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Multi-container real-time endpoint with both containers in the same inference pipeline.",
      "B": "Two separate endpoints chained via Lambda.",
      "C": "Batch transform job with a custom script calling both containers.",
      "D": "ECS service with multiple containers per task."
    },
    "explanation": "SageMaker multi-container endpoints allow a preprocessor and model container to run sequentially in the same endpoint."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A lightweight NLP model (<10 MB) must be invoked by other AWS services via API and handle occasional spikes of up to 50 requests per second. You want to avoid managing servers. Which deployment target is best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Serverless SageMaker endpoint.",
      "B": "Real-time SageMaker endpoint with a single instance.",
      "C": "Amazon Lambda container image with the model packaged and hosted behind an API Gateway.",
      "D": "ECS Fargate service."
    },
    "explanation": "A Lambda container image offers serverless scaling to handle spikes and exposes a simple API gateway endpoint without EC2 management."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "Your team needs to optimize inference performance on ARM-based instances in AWS. The model currently runs on x86 CPU. Which deployment approach will yield the greatest performance gain?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch to a serverless endpoint on ARM.",
      "B": "Compile the model with SageMaker Neo for ARM and host on ARM EC2 instances.",
      "C": "Use a GPU-based real-time endpoint on x86.",
      "D": "Deploy in ECS with ARM container images."
    },
    "explanation": "Neo compiles and optimizes for ARM hardware, delivering significant inference speedups compared to generic CPU execution."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A hybrid environment uses on-premises Kubernetes and AWS. You need to deploy an ML model so that it's available to both environments with unified CI/CD. Which deployment infrastructure is most appropriate?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker serverless endpoint with VPC peering.",
      "B": "SageMaker real-time endpoint in a public subnet.",
      "C": "ECS with Fargate in a public VPC.",
      "D": "Amazon EKS with GPU node group and a shared Terraform pipeline."
    },
    "explanation": "EKS provides Kubernetes compatibility on-prem and in AWS and integrates with existing CI/CD pipelines."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A data-science team must process nightly batches of 10 million records through an ML model. Sub-second latency is not required, but overall runtime must complete within 2 hours. Which deployment strategy is optimal?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High-capacity real-time SageMaker endpoint with multiple GPU instances.",
      "B": "Serverless SageMaker endpoint with high concurrency.",
      "C": "SageMaker batch transform job with GPU instances.",
      "D": "SageMaker asynchronous endpoint with high timeouts."
    },
    "explanation": "Batch transform jobs are optimized for high-throughput offline inference and can leverage large GPU fleets to meet deadlines without request-based endpoint costs."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "Your organization uses Kubernetes microservices on Amazon EKS. You must deploy an ML model as a microservice in this architecture with GPU acceleration. Which approach do you choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a SageMaker real-time endpoint and call it from EKS pods.",
      "B": "Deploy the model in a container on EKS with a GPU instance profile.",
      "C": "Package the model in a Lambda function behind an Application Load Balancer.",
      "D": "Use SageMaker multi-model endpoint within the VPC and proxy through EKS."
    },
    "explanation": "Hosting directly on EKS with GPU nodes integrates with existing microservices, avoids network hops, and provides GPU acceleration under Kubernetes control."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "You have two CloudFormation stacks: one provisioning a VPC with subnets and security groups, and another provisioning a SageMaker real-time endpoint. The endpoint stack needs to reference the VPC subnets and security group ARNs created by the VPC stack. What is the most maintainable way to share these values?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "In the VPC stack, export the subnet IDs and security group ARNs using Outputs with Export names. In the endpoint stack, import them with Fn::ImportValue.",
      "B": "Store the subnet IDs and security group ARNs in an S3 object in the VPC stack and download them in CloudFormation custom resources in the endpoint stack.",
      "C": "Pass the subnet IDs and security group ARNs as parameters manually each time you deploy the endpoint stack.",
      "D": "Use a Lambda-backed custom resource in the endpoint stack to call DescribeStacks on the VPC stack and parse the JSON output."
    },
    "explanation": "Cross-stack references via Outputs and Fn::ImportValue is the recommended, maintainable approach for sharing values between CloudFormation stacks."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "You need to configure application autoscaling for a SageMaker real-time endpoint variant using CloudFormation. Which CloudFormation resource type must you declare to register the endpoint variant with AWS Application Auto Scaling?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS::SageMaker::Endpoint",
      "B": "AWS::ApplicationAutoScaling::ScalableTarget",
      "C": "AWS::SageMaker::EndpointConfiguration",
      "D": "AWS::ApplicationAutoScaling::ScheduledAction"
    },
    "explanation": "AWS::ApplicationAutoScaling::ScalableTarget is required to register the SageMaker endpoint variant with Application Auto Scaling before you attach scaling policies."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "Your SageMaker endpoint experiences unpredictable spikes in traffic throughout the day. You want to configure auto scaling to trigger when each instance receives more than 100 requests per minute. Which scaling metric should you choose in your scaling policy?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CPUUtilization",
      "B": "ModelLatency",
      "C": "Invocation4xxErrors",
      "D": "InvocationsPerInstance"
    },
    "explanation": "InvocationsPerInstance is the supported Application Auto Scaling metric for SageMaker endpoints to scale based on request volume per instance."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "You deploy a SageMaker endpoint inside a VPC without a NAT gateway to save cost. The endpoint fails to pull the container from ECR and the model artifacts from S3. Which minimum set of VPC endpoints must you script to restore functionality?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "An interface endpoint for com.amazonaws.<region>.s3 only",
      "B": "Interface endpoints for com.amazonaws.<region>.ecr.api and com.amazonaws.<region>.ecr.dkr only",
      "C": "Interface endpoints for com.amazonaws.<region>.ecr.api, com.amazonaws.<region>.ecr.dkr, and a gateway endpoint for com.amazonaws.<region>.s3",
      "D": "Interface endpoints for com.amazonaws.<region>.sagemaker.api and com.amazonaws.<region>.sagemaker.runtime"
    },
    "explanation": "SageMaker endpoints in a private VPC need ECR API and DKR interface endpoints plus the S3 gateway endpoint to pull container images and model artifacts."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "You want to deploy a multi-model endpoint using CloudFormation to host multiple model artifacts in a single container. Which property must you use when defining AWS::SageMaker::Model to support this use case?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "PrimaryContainer",
      "B": "Containers",
      "C": "InferenceExecutionConfig",
      "D": "ModelPackageName"
    },
    "explanation": "The Containers property (a list) is required to define a multi-model endpoint in CloudFormation, whereas PrimaryContainer supports only a single container."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "Your inference container is stored in Amazon ECR and your SageMaker model execution role must pull the image. When scripting the IAM role in CloudFormation, which managed policy should you attach to grant the minimum required permissions?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "AmazonSageMakerFullAccess",
      "B": "AmazonEC2ContainerServiceforEC2Role",
      "C": "AmazonEC2ContainerRegistryReadOnly",
      "D": "AmazonS3ReadOnlyAccess"
    },
    "explanation": "AmazonEC2ContainerRegistryReadOnly grants the least-privilege permissions necessary for SageMaker to pull container images from ECR."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "Your organization hit the export limit for CloudFormation cross-stack references. You still need to share subnet IDs and security group IDs across stacks. What alternative approach should you use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use nested stacks instead of separate stacks.",
      "B": "Store the values in SSM Parameter Store and reference them with dynamic references.",
      "C": "Batch all values into a single export by concatenating comma-delimited strings.",
      "D": "Use AWS Organizations to share parameters between accounts."
    },
    "explanation": "Storing shared configuration in SSM Parameter Store avoids export limits and allows multiple stacks to reference values via dynamic references."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "You maintain several CloudFormation templates that define similar network resources for different teams. You want to reuse and standardize these resource definitions. Which CloudFormation feature is best suited for this purpose?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Custom resources",
      "B": "StackSets",
      "C": "Macros",
      "D": "Nested stacks"
    },
    "explanation": "Nested stacks allow you to factor out common resource definitions into reusable templates and include them in multiple parent stacks."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "When writing AWS CDK code for your SageMaker endpoint, you need to import an existing VPC by its tags so your endpoint can deploy into it. Which CDK method is most appropriate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ec2.Vpc.fromVpcAttributes()",
      "B": "ec2.Vpc.import()",
      "C": "ec2.Vpc.fromLookup()",
      "D": "ec2.Vpc.fromEnv()"
    },
    "explanation": "ec2.Vpc.fromLookup() performs a context lookup by tags or name at synthesis time to import existing VPCs into CDK apps."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "You are using AWS CDK to build and push a custom Docker container for SageMaker inference. Which CDK construct should you use to define and publish the image asset?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ecr.Repository",
      "B": "DockerImageAsset",
      "C": "ContainerImage.fromAsset",
      "D": "EcrDockerImage"
    },
    "explanation": "DockerImageAsset (from aws-cdk-lib/aws-ecr-assets) builds a Docker image from a local directory and publishes it to ECR automatically."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "You want to reduce training costs by using managed Spot Instances for a SageMaker training job defined in CloudFormation. Which property must you add to the AWS::SageMaker::TrainingJob resource?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "EnableManagedSpotTraining: true",
      "B": "UseSpotInstances: true",
      "C": "RuntimeSpotMode: Managed",
      "D": "SpotConfiguration: ManagedSpot"
    },
    "explanation": "EnableManagedSpotTraining set to true enables SageMaker managed Spot training for cost savings."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "You need to deploy a Lambda function in the same VPC as your SageMaker endpoint so it can invoke the endpoint privately. Which CloudFormation property must you include in the AWS::Lambda::Function resource?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "NetworkConfiguration",
      "B": "SecurityGroupIds",
      "C": "SubnetIds",
      "D": "VpcConfig"
    },
    "explanation": "VpcConfig ({ SubnetIds, SecurityGroupIds }) on AWS::Lambda::Function places the Lambda inside the specified VPC."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "You configured Application Auto Scaling for your SageMaker endpoint to use CPUUtilization, but you observe that scaling never occurs. Which explanation is correct?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "You must enable detailed monitoring on the endpoint to expose CPU metrics.",
      "B": "SageMaker endpoints only support InvocationsPerInstance as a built-in scaling metric.",
      "C": "CPUUtilization is supported only for asynchronous endpoints.",
      "D": "You must configure a CloudWatch alarm for CPUUtilization even when using Application Auto Scaling."
    },
    "explanation": "Application Auto Scaling for SageMaker real-time endpoints supports only the InvocationsPerInstance metric; CPUUtilization is not supported directly."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "When you register your SageMaker endpoint variant with AWS Application Auto Scaling in CDK, what is the correct format of the resourceId property?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "endpoint/YourEndpointName/variant/AllTraffic",
      "B": "endpoint/YourEndpointName/variantName/AllTraffic",
      "C": "sagemaker:endpoint:YourEndpointName:variant:AllTraffic",
      "D": "YourEndpointName/AllTraffic"
    },
    "explanation": "The resourceId for a real-time SageMaker endpoint variant must be specified as \"endpoint/<endpointName>/variant/<variantName>\"."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "You are deploying a new version of your VpcConfig for a SageMaker endpoint in CloudFormation. You modify the Subnets list in the template and redeploy, but the change is not applied. Why?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CloudFormation cannot update VpcConfig on an existing EndpointConfiguration; you must modify another property.",
      "B": "You must delete the EndpointConfiguration resource manually before CloudFormation can apply changes.",
      "C": "CloudFormation only creates a new AWS::SageMaker::EndpointConfiguration when the Endpoint resource\u2019s EndpointConfigName property is updated; you haven\u2019t changed it.",
      "D": "VpcConfig changes require an update to AWS::SageMaker::Model, not EndpointConfiguration."
    },
    "explanation": "Modifying VpcConfig in the EndpointConfiguration has no effect until you update the Endpoint resource\u2019s EndpointConfigName to reference the new configuration."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "You have an existing CloudFormation template defining your SageMaker model and endpoint. You decide to adopt AWS CDK but want to reuse the existing template without rewriting it. Which CDK construct allows you to embed and extend the existing template?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CfnModel",
      "B": "TemplatePart",
      "C": "IncludeTemplate",
      "D": "CfnInclude"
    },
    "explanation": "CfnInclude (from aws-cdk-lib/cloudformation-include) lets you import an existing CloudFormation template into a CDK app for extension."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A data science team needs to implement a CI/CD pipeline that automates model training, testing, and deployment for a SageMaker ML model. The pipeline should enforce a manual approval before deploying to production and use AWS CodePipeline with minimal custom code. Which pipeline configuration meets these requirements?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a manual approval action between the CodeBuild action that trains the model and the CloudFormation Deploy action that updates the production SageMaker endpoint.",
      "B": "Use a Lambda function as a CodePipeline action to pause for approval between the training and deployment stages.",
      "C": "Configure the SageMaker Model Training action to require manual confirmation before executing the SageMaker Model Deploy action.",
      "D": "Use a CloudWatch Events rule to trigger a manual deployment after the training stage finishes."
    },
    "explanation": "CodePipeline supports manual approval actions natively; inserting an Approval action between the training (CodeBuild) stage and the deployment (CloudFormation Deploy endpoint) stage provides a built-in manual gate with minimal custom code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "An ML engineer must trigger a CI/CD pipeline whenever new training data arrives in an S3 bucket. The pipeline uses CodePipeline and CodeBuild to preprocess data and train a SageMaker model. Which configuration best accomplishes this requirement with event-driven automation?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure the S3 bucket as the Source stage in CodePipeline and enable change detection polling.",
      "B": "Create a CloudWatch Events rule for s3:ObjectCreated:* that targets the CodePipeline API to start a pipeline execution.",
      "C": "Use an S3 Event Notification to invoke a Lambda function that calls StartPipelineExecution for the CodePipeline.",
      "D": "Configure EventBridge to listen for S3 notifications and invoke the CodeBuild project directly."
    },
    "explanation": "Using an S3 event notification to invoke a Lambda function that calls StartPipelineExecution provides event-driven triggering with fine-grained control and low latency."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A team wants to deploy updated SageMaker endpoints in a blue/green deployment fashion as part of their CI/CD pipeline. They need to shift traffic gradually to the new model and enable easy rollback if issues occur. Which approach using AWS native services meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a SageMaker CreateModel action in CodePipeline with the 'Blue/Green' deployment type and specify the traffic shifting percentage.",
      "B": "Use CloudFormation Deploy action in CodePipeline configured with CodeDeploy SafeMode for CloudFormation change sets to perform traffic shifting between endpoints.",
      "C": "Invoke the SageMakerUpdateEndpoint API in CodeBuild and script traffic weights in the buildspec.",
      "D": "Configure a Lambda function in a pipeline action that calls UpdateEndpointWeightsAndCapacities for traffic shifting."
    },
    "explanation": "Using a CloudFormation Deploy action with CodeDeploy-managed change sets enables native blue/green deployments and traffic shifting with rollback capabilities without custom scripts."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A CodeBuild project in a CI/CD pipeline needs to access resources inside a VPC (private RDS and SageMaker endpoint). Builds are failing because the project cannot reach VPC-only endpoints. How should you modify the CodeBuild configuration to allow access while maintaining least privilege?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add the CodeBuild project to the same security group as the RDS instance and SageMaker endpoint.",
      "B": "Configure the CodeBuild project with VPC configuration specifying the subnets and security groups that allow access to the private endpoints.",
      "C": "Create a NAT gateway in the VPC and enable public access for the CodeBuild project.",
      "D": "Enable CodeBuild network isolation and whitelist the VPC CIDR block."
    },
    "explanation": "Configuring the CodeBuild project with the correct VPC, subnets, and security groups allows it to access private resources securely and maintain least privilege."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "You need to add automated testing stages to your ML CI/CD pipeline: unit tests for preprocessing code, integration tests against a development SageMaker endpoint, and performance validation of the new model. Which pipeline actions should you include to satisfy these requirements?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Three CodeBuild actions: one running pytest for code, one invoking the dev endpoint via AWS CLI, and one running a custom performance test script.",
      "B": "One CodeBuild action with sequential buildspec phases for unit, integration, and performance tests.",
      "C": "Unit tests as a CodeBuild action, integration tests as a Lambda invoke action, and performance tests as a SageMaker Batch Transform action.",
      "D": "Integration tests first, then unit tests, then performance validation, all in a single CodeBuild action."
    },
    "explanation": "Separating testing into distinct CodeBuild actions for unit, integration, and performance ensures isolation and clear visibility of failures, and uses the native build environment to invoke tests."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "An ML engineer has a SageMaker Pipeline defined for data preprocessing, model training, and evaluation. To include this SageMaker Pipeline in a larger CodePipeline CI/CD workflow, which native CodePipeline action type should they use to start and monitor the SageMaker Pipeline execution?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS CloudFormation action with a custom resource to invoke the SageMaker Pipeline.",
      "B": "AWS Lambda invoke action that calls StartPipelineExecution.",
      "C": "SageMakerPipeline action type provided by AWS CodePipeline.",
      "D": "CodeBuild action using AWS CLI to start the SageMaker Pipeline."
    },
    "explanation": "CodePipeline provides the SageMakerPipeline action to natively integrate SageMaker Pipelines as a stage, handling execution and status monitoring without custom scripts."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your ML CI/CD pipeline uses CodeBuild to package and push Docker images to a private ECR repository encrypted with a customer-managed KMS key. Builds are failing when CodeBuild attempts to push the image. What minimum IAM policy change should you apply to the CodeBuild service role to resolve this issue?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Add kms:Decrypt permission on the KMS key used to encrypt the ECR repository.",
      "B": "Add kms:GenerateDataKey permission on the KMS key.",
      "C": "Add kms:Encrypt and kms:Decrypt permissions on the KMS key.",
      "D": "Add kms:DescribeKey permission on the KMS key."
    },
    "explanation": "CodeBuild needs kms:GenerateDataKey to encrypt the image layers before pushing to ECR. Decrypt is not required for push operations."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A team adopts GitFlow branching for ML code and infrastructure definitions. How should branches map to CodePipeline stages to implement dev, test, and prod environments with automated promotion?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Map the 'develop' branch as the Source for the dev pipeline, 'release' for test, and 'master' for prod, each with its own CodePipeline using source triggers.",
      "B": "Use 'feature' branches for dev, 'develop' for test, and 'release' for prod, all in a single pipeline with multiple source actions.",
      "C": "Use 'master' branch for all environments and control deployments with manual approval actions.",
      "D": "Use 'hotfix' branches to promote code directly to production pipeline."
    },
    "explanation": "GitFlow maps develop \u2192 dev pipeline, release \u2192 test pipeline, and master \u2192 production pipeline, enabling automated promotions based on branch."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "You configured SageMaker Model Monitor to detect data drift for a production endpoint. You want your CI/CD pipeline to automatically retrain the model when drift is detected. Which event pattern and target configuration should you use in EventBridge to integrate drift detection with your pipeline?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Event pattern: SageMaker Model Monitor DataQualityCheckNotification; Target: CodePipeline StartPipelineExecution action.",
      "B": "Event pattern: CloudWatch Alarm for drift metric; Target: Lambda that updates the SageMaker endpoint.",
      "C": "Event pattern: SageMaker TrainingJobStateChange; Target: EventBridge rule that triggers retraining.",
      "D": "Event pattern: S3:ObjectCreated for captured data; Target: CodeBuild to start training."
    },
    "explanation": "Model Monitor emits DataQualityCheckNotification events; capturing these in an EventBridge rule targeting StartPipelineExecution automates retraining when drift is detected."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "You want to define your entire CI/CD pipeline in AWS CDK, including stages for building, testing, and deploying a SageMaker model. Which CDK construct and patterns should you use to best represent stages and actions?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use pipelines.CodePipeline construct with pipelines.CodeBuildStep and pipelines.ShellStep.",
      "B": "Use aws_codepipeline.Pipeline with aws_codepipeline_actions.CodeBuildAction and aws_codepipeline_actions.CloudFormationCreateUpdateStackAction.",
      "C": "Use aws_sagemaker.CfnPipeline and embed CodePipeline definitions as metadata.",
      "D": "Use a single CodeBuild project in CDK and run all steps in buildspec."
    },
    "explanation": "Using aws_codepipeline.Pipeline with native CodeBuild and CloudFormation actions allows explicit definition of CI/CD stages and is the recommended pattern in CDK for pipelines."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "During a CodePipeline execution, the CodeBuild step that builds the Docker container for a custom SageMaker algorithm fails due to insufficient privileges for Docker. Which setting in the CodeBuild project should you enable to allow Docker builds?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set privileged mode to true in the CodeBuild project settings.",
      "B": "Assign the CodeBuild role to the DockerUsers group in IAM.",
      "C": "Enable inbound and outbound network access in CodeBuild.",
      "D": "Grant CodeBuild service role permissions for sagemaker:CreateAlgorithm."
    },
    "explanation": "Privileged mode allows CodeBuild to run Docker commands needed for building and pushing container images."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "You need to validate model performance metrics generated during training before deploying the model in the CI/CD pipeline. Which CodePipeline action can you use to automatically compare the new metrics against a baseline and fail the pipeline if the model does not meet thresholds?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a Lambda invoke action that reads metrics from S3 and throws an error if thresholds are not met.",
      "B": "Use a CloudWatch Alarm action in CodePipeline to evaluate metrics.",
      "C": "Use the built-in SageMaker Model Quality check action in CodePipeline.",
      "D": "Use a CodeBuild action with a buildspec that runs a custom validation script."
    },
    "explanation": "CodePipeline does not have a native metric gating action; invoking a Lambda to assert metric thresholds provides a serverless, automated gate."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "In your pipeline, the output of the model training stage is a model artifact location in S3. You need to pass this dynamic S3 path to the SageMaker CreateModel deployment stage in CodePipeline. How should you configure the deployment action to consume this artifact location?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use CodePipeline artifact variables and parameter overrides in the SageMaker CreateModel action configuration.",
      "B": "Hardcode the S3 path in the CreateModel action ARN.",
      "C": "Write the path to Systems Manager Parameter Store and reference it in the deployment stage.",
      "D": "Use a Lambda function to retrieve the location and call CreateModel."
    },
    "explanation": "Artifact variables allow CodePipeline stages to access outputs from previous stages and dynamically pass them to action parameters without custom code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your team is evaluating whether to use SageMaker Pipelines or AWS CodePipeline for end-to-end ML workflows. They need automated data preprocessing, hyperparameter tuning, model evaluation, and deployment gates. Which is the most appropriate orchestration tool?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Pipelines for ML steps and embed it as a stage in AWS CodePipeline for approvals and multi-account deployments.",
      "B": "Use AWS CodePipeline exclusively for all ML and deployment tasks.",
      "C": "Use AWS Step Functions to orchestrate both ML and deployment steps.",
      "D": "Use Amazon Managed Workflows for Apache Airflow (MWAA) to orchestrate SageMaker training and deploy."
    },
    "explanation": "SageMaker Pipelines provides first-class support for ML workflows; embedding it in CodePipeline adds enterprise-grade CI/CD features like approvals and cross-account deployments."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "To implement automated rollback on a SageMaker endpoint if post-deployment smoke tests fail, which deployment pattern should you configure in CodePipeline and CodeDeploy?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "All-at-once deployment with a Lambda test hook in CodeDeploy that triggers rollback on failure.",
      "B": "Canary deployment type in CloudFormation Deploy action with pre-traffic and post-traffic validation Lambda hooks.",
      "C": "Linear deployment with time-based rollout and manual approval for rollback.",
      "D": "In-place deployment with CodeBuild test stage preceding deployment."
    },
    "explanation": "Using a canary deployment with pre- and post-traffic validation hooks in CodeDeploy allows automated testing and rollback if smoke tests fail."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An ML engineer has deployed a real-time SageMaker inference endpoint and needs to detect both input feature distribution drift and prediction quality degradation in production with minimal custom code. Which combination of SageMaker services and configurations should the engineer use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure DataCaptureConfig on the endpoint, deploy a DefaultModelMonitor job to detect data drift, and schedule a ModelQualityMonitor job with ground truth to detect prediction drift.",
      "B": "Configure DataCaptureConfig on the endpoint and schedule a Clarify DataBiasMonitor job to detect both data and prediction drift.",
      "C": "Stream inference events to Kinesis Data Firehose and use AWS Glue to analyze drift for both features and predictions.",
      "D": "Use SageMaker Clarify ModelExplainabilityMonitor to detect distribution shifts and model accuracy degradation."
    },
    "explanation": "Use DataCaptureConfig + DefaultModelMonitor for input drift and ModelQualityMonitor with ground truth for prediction/concept drift; other options do not cover both aspects or require more custom work."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "Before enabling continuous data drift detection on a production SageMaker endpoint, an ML engineer must establish baseline statistics and constraints. Which approach should the engineer use to generate these baselines automatically?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run a SageMaker Processing job using the DefaultModelMonitor container on a representative historical dataset in S3.",
      "B": "Use SageMaker Clarify DataBiasMonitor to create baseline drift constraints on the training data.",
      "C": "Write custom code to compute statistics and manually author the constraint JSON file.",
      "D": "Use AWS Glue DataBrew to profile the data and export a constraint file to S3."
    },
    "explanation": "The DefaultModelMonitor Processing container provides built-in functionality to compute baseline statistics and constraints; other options either don\u2019t generate constraints in the required format or don\u2019t support SageMaker\u2019s drift detectors."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A financial services company must monitor inference data while excluding PII fields from retention. Which configuration in SageMaker Model Monitor allows the engineer to capture inference payloads but filter out PII attributes?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set DataCaptureConfig sample_percentage to 0 and use encryption to prevent PII capture.",
      "B": "Use Clarify ModelBiasMonitor with an exclude_columns parameter for PII fields.",
      "C": "Configure DataCaptureConfig with a CaptureFilter to exclude PII JSON paths before writing to S3.",
      "D": "Enable a processing container script to mask PII after the monitor job runs."
    },
    "explanation": "DataCaptureConfig\u2019s CaptureFilter lets you specify JSONPath or CSV column filters so only non-PII fields are captured; other methods either capture PII or require post-processing."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A Model Monitor violation report shows a numeric feature\u2019s distribution exceeding baseline constraints for variance only. What is the most appropriate next step?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Adjust the variance threshold in the baseline constraint to suppress false positives.",
      "B": "Investigate the production data distribution change, update training data or retrain the model if the shift reflects new valid patterns.",
      "C": "Disable monitoring for that feature to reduce alert noise.",
      "D": "Regenerate the baseline constraints on the current production data without retraining."
    },
    "explanation": "A constraint violation indicates real drift; you should investigate and retrain or augment training data if needed. Changing thresholds or disabling the monitor risks missing true drift."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An operations team requires near real-time alerts whenever model input data drift is detected. Which integration provides automated notifications upon Model Monitor constraint violations?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Have a Lambda function poll the S3 violation reports folder every minute.",
      "B": "Create an Amazon EventBridge rule for SageMaker MonitoringExecutionStatus change events and target an SNS topic.",
      "C": "Subscribe an SNS notification directly to the Model Monitor S3 bucket.",
      "D": "Use CloudWatch Logs Insights to search for violations and trigger alarms."
    },
    "explanation": "EventBridge can capture SageMaker monitoring execution status change events and forward them to SNS; polling or log-based solutions introduce delay or complexity."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A team wants to detect anomalies in inference latency, error rates, and invoke rates on a SageMaker real-time endpoint. Which approach will meet these requirements with the least operational overhead?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use Amazon CloudWatch built-in metrics (Latency, Invocations, 4xx/5xx error counts) and configure CloudWatch Alarms.",
      "B": "Use SageMaker Model Monitor to detect infrastructure anomalies.",
      "C": "Use SageMaker Clarify to monitor model performance metrics.",
      "D": "Enable AWS CloudTrail on the endpoint and analyze logs in S3."
    },
    "explanation": "CloudWatch automatically captures endpoint performance and error metrics; Model Monitor focuses on data quality, and Clarify focuses on bias/explainability."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "Which statement best describes concept drift in a deployed ML model?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A change in the input feature distribution compared to baseline data.",
      "B": "A change in the statistical relationship between input features and target labels over time.",
      "C": "An imbalance in class label frequencies in production data.",
      "D": "A bias introduced during model training that only affects edge cases."
    },
    "explanation": "Concept drift refers to changes in P(Y|X), i.e., the relationship between features and labels; data drift is about P(X) changes."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An ML engineer needs to gradually shift traffic to a canary variant of a SageMaker endpoint and automatically roll back if error rates exceed a threshold. Which deployment mechanism should they use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker multi\u2010variant endpoints with AWS CodeDeploy canary traffic\u2010shifting and configure a CloudWatch Alarm on 5xx error rate.",
      "B": "Deploy two separate endpoints and manually switch DNS when errors are low.",
      "C": "Use SageMaker asynchronous endpoints with weighted routing.",
      "D": "Use SageMaker batch transform jobs scheduled hourly and compare error rates."
    },
    "explanation": "SageMaker multi\u2010variant endpoints integrated with CodeDeploy support canary traffic shifts and automatic rollback via CloudWatch Alarms."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "To detect shifts in feature attributions of a production model over time, which SageMaker monitoring job should an ML engineer schedule?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A Clarify ModelExplainabilityMonitor job with SHAP baseline and monitoring configuration.",
      "B": "A Clarify DataBiasMonitor job on the inference data.",
      "C": "A DefaultModelMonitor job focusing on data quality constraints.",
      "D": "A ModelQualityMonitor job comparing accuracy against ground truth."
    },
    "explanation": "ModelExplainabilityMonitor (SHAP) jobs track how feature attributions change; data bias or quality monitors don\u2019t measure attribution shifts."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "What is the minimum supported frequency for scheduling a SageMaker MonitoringSchedule to detect production data drift?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Every 5 minutes",
      "B": "Every 1 minute",
      "C": "Every hour",
      "D": "Every 24 hours"
    },
    "explanation": "SageMaker MonitoringSchedule supports a minimum interval of 5 minutes; shorter intervals are not allowed."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An engineering team uses batch transform jobs for asynchronous inference and needs to monitor prediction quality drift against offline ground truth. Which solution requires the least operational overhead?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable DataCaptureConfig on the batch transform endpoint and run DefaultModelMonitor.",
      "B": "Implement a custom Lambda to compare S3 outputs to ground truth and publish metrics.",
      "C": "Schedule a ModelQualityMonitor ProcessingJob using the batch transform output folder and S3 ground truth labels.",
      "D": "Use Clarify DataBiasMonitor on the batch transform results."
    },
    "explanation": "Scheduling a ModelQualityMonitor ProcessingJob directly on transform outputs and labels uses built-in monitoring; other options require custom polling or inappropriate monitors."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A production endpoint\u2019s data quality monitor runs but model accuracy has degraded without any data drift reported. What additional monitoring configuration is required?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a Clarify DataBiasMonitor to detect label shifts.",
      "B": "Increase the sampling percentage in DataCaptureConfig.",
      "C": "Enable multivariant monitoring on the same monitor.",
      "D": "Configure and schedule a ModelQualityMonitor job with ground truth labels to detect accuracy degradation."
    },
    "explanation": "DataQuality monitors P(X) shifts, not P(Y|X); ModelQualityMonitor with ground truth is required to detect concept or accuracy drift."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A company must monitor production inference outputs for fairness drift (e.g., change in demographic parity) over time. Which SageMaker Clarify monitor should they schedule?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Clarify DataBiasMonitor",
      "B": "Clarify ModelBiasMonitor",
      "C": "ModelExplainabilityMonitor",
      "D": "DefaultModelMonitor"
    },
    "explanation": "ModelBiasMonitor detects fairness metrics (demographic parity, equalized odds) on inference data; DataBiasMonitor analyzes training set biases."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An ML engineer must filter out sensitive fields from inference data before it reaches the Model Monitor processing container. Which component should they customize?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provide a custom preprocessing script in the MonitoringSchedule\u2019s ProcessingJobConfig.",
      "B": "Write a post-monitoring ETL job to remove sensitive columns from violation reports.",
      "C": "Modify the DefaultModelMonitor container image to drop PII.",
      "D": "Configure DataCaptureConfig to drop features via sample_percentage."
    },
    "explanation": "A custom preprocessing script in MonitoringSchedule\u2019s ProcessingJobConfig lets you transform or filter data before constraint evaluation; other methods occur after capture or require image modification."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An ML engineer wants to integrate drift detection into a SageMaker Pipelines workflow and automatically trigger retraining when drift is detected. Which pipeline step should they include to evaluate drift violations before invoking the retraining branch?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a MonitoringStep with fail_on_violation=True to stop the pipeline on any drift.",
      "B": "Use a ConditionStep to check if the MonitoringStep status equals 'Failed'.",
      "C": "Use a RegisterModel step to register the model only if no drift is detected.",
      "D": "Use a ConditionStep to inspect the MonitoringStep output property 'BaselineViolations' > 0 and branch accordingly."
    },
    "explanation": "A ConditionStep can examine the MonitoringStep output (e.g., violated constraint count) and route to retraining; this avoids hard failures and enables branching logic."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "An ML team needs to right-size SageMaker inference endpoints to minimize cost while maintaining performance. They want to benchmark actual model inference performance (latency and under various loads) to choose optimal instance types. Which AWS service should they use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Compute Optimizer",
      "B": "AWS SageMaker Inference Recommender",
      "C": "AWS Cost Explorer",
      "D": "AWS Trusted Advisor"
    },
    "explanation": "SageMaker Inference Recommender runs performance benchmarks on your model to recommend optimal instance families and sizes. Compute Optimizer provides general EC2/EBS recommendations, not ML-specific inference benchmarks."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A financial organization observes unexpected monthly spikes in SageMaker inference costs due to seasonal usage. They need to receive near real-time alerts when cost anomalies occur in their AWS account. Which solution meets this requirement with minimal operational overhead?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create a CloudWatch alarm on the AWS/Billing namespace EstimatedCharges metric",
      "B": "Configure an AWS Budget with email notifications when forecasted spend exceeds a threshold",
      "C": "Enable AWS Cost Anomaly Detection and configure alerts for anomaly events",
      "D": "Schedule daily Cost Explorer reports and parse them with Lambda for thresholds"
    },
    "explanation": "AWS Cost Anomaly Detection uses machine learning to detect unusual cost spikes and can send near real-time alerts without manual report parsing or budget forecasting."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A deployed SageMaker endpoint is experiencing intermittent increases in tail latency. The ML engineer needs to trace requests through the network stack and service mesh to pinpoint network bottlenecks. Which AWS service or feature should they enable?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run CloudWatch Logs Insights on the endpoint logs",
      "B": "Monitor EC2 CPUUtilization metrics in CloudWatch",
      "C": "Enable AWS X-Ray integration with SageMaker",
      "D": "Enable VPC Flow Logs on the endpoint\u2019s ENIs"
    },
    "explanation": "AWS X-Ray provides distributed tracing across network and service calls, enabling end-to-end latency analysis. VPC Flow Logs show packet metadata but not distributed service traces."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "For compliance, the security team wants to audit all Amazon SageMaker CreateEndpoint API calls over the past 30 days and retain logs for 90 days. Which configuration achieves this with the LEAST administrative overhead and ensures immutability?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Enable CloudTrail management events for SageMaker and send to an S3 bucket with Object Lock enabled",
      "B": "Create a CloudWatch Events rule for CreateEndpoint and log events to CloudWatch Logs",
      "C": "Enable CloudTrail Data events on CreateEndpoint and stream to Kinesis Data Firehose",
      "D": "Enable CloudTrail Insights to capture anomalous CreateEndpoint activity"
    },
    "explanation": "CloudTrail management events capture all CreateEndpoint API calls; delivering them to an S3 bucket with Object Lock provides immutability. Data events and Insights are unnecessary for standard API logging."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A director wants a single dashboard that shows per-endpoint invocation count, average 95th percentile latency, and cost per hour for each SageMaker endpoint in the account. Which solution meets this requirement with minimal development effort?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Build a custom web app that calls CloudWatch metrics API and Cost Explorer API",
      "B": "Use Amazon QuickSight to visualize a dataset combining AWS Cost and Usage Reports and CloudWatch metrics",
      "C": "Use CloudWatch Dashboards with metric math combining performance and billing metrics",
      "D": "Use the AWS Pricing API to fetch rates and combine with CloudWatch metrics in dashboards"
    },
    "explanation": "QuickSight can natively ingest both Cost and Usage Report data (cost per resource) and CloudWatch metrics to build a unified dashboard with minimal custom code. CloudWatch Dashboards cannot display cost per resource granularity."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "To reduce off-peak costs, an ML engineer wants to automatically update SageMaker endpoint instance types to smaller ones every night and revert to original sizes each morning. Which solution requires the LEAST operational overhead?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Use SageMaker Pipelines with a scheduled pipeline to call UpdateEndpointConfig",
      "B": "Create EventBridge cron rules that trigger Lambda functions invoking UpdateEndpointConfig",
      "C": "Configure AWS Step Functions with Wait states and Lambda tasks to update endpoints",
      "D": "Use Systems Manager Automation documents scheduled via State Manager"
    },
    "explanation": "Using EventBridge with cron schedules triggering lightweight Lambda functions to call UpdateEndpointConfig is the simplest and lowest-overhead scheduling solution."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A SageMaker real-time endpoint is exhibiting memory pressure, but memory utilization is not visible in CloudWatch by default. To monitor container-level memory for this endpoint, what should the ML engineer do?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable SageMaker Debugger profiling to collect memory metrics",
      "B": "Deploy the endpoint on EC2 instances where memory metrics are published by default",
      "C": "Install and configure the CloudWatch agent via a container lifecycle configuration",
      "D": "Use CloudWatch metric filters on container logs to estimate memory usage"
    },
    "explanation": "The CloudWatch agent must be installed inside the container via a lifecycle configuration script to collect OS-level metrics such as memory usage. Debugger profiles model internals, not OS memory."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "An ML application experiences periodic bursts of inference requests that exceed the 70% CPU utilization threshold. The ML engineer wants to add step scaling to the existing auto scaling policy to provision two additional instances when InvocationsPerInstance exceeds 100 for 2 minutes, and remove one instance when it drops below 50 for 5 minutes. Which configuration meets these requirements?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Define two CloudWatch alarms on the SageMakerVariantInvocationsPerInstance metric with thresholds 100 (2-minute evaluation) and 50 (5-minute evaluation), and attach a step scaling policy with +2 and -1 adjustments",
      "B": "Use CPUUtilization alarms instead of InvocationsPerInstance and a target-tracking policy",
      "C": "Use a single target-tracking policy on InvocationsPerInstance with a target value of 75",
      "D": "Configure two AWS Budgets for usage and link them to auto scaling actions"
    },
    "explanation": "Step scaling requires separate CloudWatch alarms on the SageMakerVariantInvocationsPerInstance metric with specified evaluation periods and corresponding step adjustments (+2, -1). Using CPUUtilization or budgets would not meet the specified invocation-based requirements."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A financial ML endpoint must maintain 99th percentile latency under 200 ms. To automate scaling to achieve this SLO, which Application Auto Scaling policy should be configured?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Target tracking policy on CPUUtilization at 70%",
      "B": "Target tracking policy on SageMakerVariantInvocationLatency at the p99 200 ms target",
      "C": "Step scaling policy on InvocationsPerInstance thresholds",
      "D": "Step scaling policy on ModelLatencyAvg metric"
    },
    "explanation": "A target-tracking policy on the p99 lifecycle metric of SageMakerVariantInvocationLatency targeting a 200 ms threshold will automatically adjust capacity to maintain the latency SLO. CPUUtilization or average latency would be less precise for the p99 requirement."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "After deploying a model, the team wants to trigger a retraining SageMaker pipeline whenever a new training dataset file is uploaded to S3. They also want to log all such retraining triggers for audit. Which combination of AWS services should they use?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Configure an S3 EventBridge notification on object upload to start the pipeline, and rely on CloudTrail to log the pipeline StartPipelineExecution API call",
      "B": "Use a CloudWatch scheduled rule to scan S3 daily and start the pipeline if new files exist, logging via CloudWatch Logs",
      "C": "Deploy a Lambda function to poll S3 every hour, invoke the pipeline, and log to DynamoDB",
      "D": "Send S3 events to SNS and have the pipeline poll SNS, with audit in CloudWatch Metrics"
    },
    "explanation": "Using an S3 EventBridge notification provides immediate trigger of the pipeline on new data. CloudTrail automatically logs the StartPipelineExecution API call for auditing."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "Which AWS service provides generalized compute resource recommendations, including SageMaker endpoints and EC2 instances, based on historical utilization metrics and can be applied across accounts with minimal configuration?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Inference Recommender",
      "B": "AWS Compute Optimizer",
      "C": "AWS Cost Explorer rightsizing recommendations",
      "D": "AWS Trusted Advisor"
    },
    "explanation": "AWS Compute Optimizer analyzes historical utilization across EC2, SageMaker endpoints, and other resources to recommend optimal instance types, whereas Inference Recommender focuses specifically on ML inference benchmarking."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "An engineering team needs to break down monthly SageMaker spend by project. They require cost allocation by tagging endpoints and jobs, and reporting at tag granularity. Which steps should they take?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Tag SageMaker endpoints and training jobs with project identifiers, activate those tags in AWS Billing Cost Allocation Tags, and use Cost Explorer with tag filters",
      "B": "Create separate AWS accounts per project and use consolidated billing",
      "C": "Use CloudWatch metric dimensions to filter cost metrics",
      "D": "Group SageMaker resources into separate CloudFormation stacks and view stack costs"
    },
    "explanation": "Activating resource tags for cost allocation and filtering in Cost Explorer is the standard way to break down costs by project without needing separate accounts."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "To optimize inference costs for stable production workloads with predictable traffic, which purchasing option should an ML engineer choose?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "On-Demand SageMaker Instances",
      "B": "SageMaker Savings Plans",
      "C": "EC2 Reserved Instances attached to SageMaker endpoints",
      "D": "SageMaker Spot Instances"
    },
    "explanation": "SageMaker Savings Plans offer a commitment discount on SageMaker compute usage for stable workloads. Spot Instances are not supported for real-time endpoints and Reserved Instances apply only to EC2, not SageMaker directly."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A model training workflow uses Amazon FSx for Lustre as input storage. The ML engineer notices frequent I/O throttling. To monitor FSx performance and identify bottlenecks, which CloudWatch metrics should they add to their dashboard?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "BurstCreditBalance and DataReadIOBytes",
      "B": "FreeStorageCapacity and NumberOfConnections",
      "C": "DataWriteIOPS and MetadataOperations",
      "D": "PercentIOPSUtilization and NetworkThroughput"
    },
    "explanation": "BurstCreditBalance shows available throughput credits and DataReadIOBytes shows actual read throughput, key metrics for diagnosing FSx for Lustre I/O throttling issues."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "An ML engineer is troubleshooting intermittent 5XX errors from inference endpoints. The engineer has enabled data capture to S3. To quickly identify patterns in the errors, which approach provides the fastest operational insight?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure a CloudWatch Logs subscription filter to Lambda to parse S3 logs",
      "B": "Use CloudWatch Logs Insights with a query on the /aws/sagemaker/Endpoints log group to count HTTPStatus5XX occurrences",
      "C": "Download the S3 logs locally and run custom scripts",
      "D": "Use AWS X-Ray traces to calculate error percentages"
    },
    "explanation": "CloudWatch Logs Insights can rapidly query large volumes of log data in the /aws/sagemaker/Endpoints log group to identify and aggregate 5XX errors without moving data."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "An ML engineer has deployed a SageMaker inference endpoint that writes inference results and debug logs to an S3 bucket encrypted with a customer-managed KMS key. The security team requires that only this endpoint can write to the bucket and decrypt the objects. Which configuration meets these requirements with the least administrative effort?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Attach an IAM policy to the SageMaker execution role allowing s3:PutObject on the bucket and kms:Decrypt on the CMK.",
      "B": "Configure the S3 bucket policy to allow s3:PutObject only from the VPC endpoint used by SageMaker.",
      "C": "Configure the S3 bucket policy with a condition aws:SourceArn equal to the SageMaker endpoint ARN, and update the KMS key policy to grant only the SageMaker execution role decrypt permissions.",
      "D": "Create an S3 ACL that grants the SageMaker service principal write access and rely on the IAM role for decryption."
    },
    "explanation": "A resource-based S3 bucket policy using aws:SourceArn ensures only that SageMaker endpoint can write objects, and the CMK key policy must explicitly grant the SageMaker execution role kms:Decrypt. IAM identity policies alone or ACLs cannot enforce both write and decrypt at the resource level as simply."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A security audit finds that any IAM user in the account can create SageMaker Studio user profiles, but the security team wants only members of the IAM group ProdMLUsers to be allowed. Which approach enforces this requirement at the SageMaker domain level?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add an identity-based IAM policy to the ProdMLUsers group allowing sagemaker:CreateUserProfile.",
      "B": "Attach a Service Control Policy in AWS Organizations denying sagemaker:CreateUserProfile unless the user is in ProdMLUsers.",
      "C": "Attach a resource-based policy to the SageMaker Domain that allows CreateUserProfile only when aws:PrincipalIsInGroup equals ProdMLUsers.",
      "D": "Use an S3 bucket policy to block studio creation from principals not in the group."
    },
    "explanation": "A SageMaker Domain resource policy can restrict CreateUserProfile to principals in a specific IAM group via aws:PrincipalIsInGroup. SCPs or identity policies alone cannot enforce at the domain resource level."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "An organization wants to run SageMaker training jobs in a VPC with no internet egress, but the training data resides in S3. Which network configuration meets these requirements while minimizing cost and operational overhead?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provision a NAT Gateway in the private subnet to access S3.",
      "B": "Attach an Internet Gateway to the VPC and allow routing to S3.",
      "C": "Create a VPC Gateway endpoint for S3 and update the private route table to direct S3 traffic through it.",
      "D": "Create an Interface VPC endpoint for SageMaker in the VPC and rely on AWS private networking for S3 access."
    },
    "explanation": "A Gateway VPC endpoint for S3 allows private, cost-effective access to S3 from a VPC with no internet egress. An interface endpoint for SageMaker does not enable S3 data access, and a NAT Gateway adds cost."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A data science team wants to share a SageMaker Model Registry package with a partner AWS account. The model artifacts are encrypted by a customer-managed CMK. The partner must be able to deploy the model from their account. Which combination of steps is required?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "In the CMK console, add the partner account as a key administrator, and update the model package resource policy to grant DescribeModelPackage.",
      "B": "Add the partner AWS account principal to the CMK key policy with decrypt permissions, and attach a resource policy to the model package allowing that account DescribeModelPackage and CreateModel.",
      "C": "Enable cross-account sharing in SageMaker Model Registry settings and share the CMK ARN.",
      "D": "Create a cross-account IAM role in the partner account with sagemaker:CreateModel permission and trust the partner\u2019s account; no changes to CMK."
    },
    "explanation": "Both the CMK and the model package need resource policies. The key policy must allow the partner account to decrypt, and the model package policy must allow DescribeModelPackage and CreateModel so they can register and deploy it."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A centralized CodePipeline in Account A builds ML packages and needs to deploy a trained model to a SageMaker endpoint in Account B. The security team requires least privilege. How should the cross-account permissions be configured?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "In Account A, grant the CodePipeline role sagemaker:* across all resources in Account B and trust Account B\u2019s principals.",
      "B": "Use CloudFormation StackSets from Account A to provision SageMaker resources in Account B without IAM role assumption.",
      "C": "In Account B, create an IAM role with sagemaker:CreateEndpoint and related actions, trust policy allowing assumption by the CodePipeline role\u2019s ARN in Account A, and configure the pipeline to assume that role.",
      "D": "In Account B, add the CodePipeline service principal from Account A to an IAM group with full SageMaker privileges."
    },
    "explanation": "The pipeline in Account A should assume a dedicated IAM role in Account B that has only the permissions needed to deploy the SageMaker endpoint. This follows least-privilege and standard cross-account role assumption practices."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "The security team requires that all SageMaker data-plane API calls (for example, CreateModel, InvokeEndpoint) be logged in CloudTrail. By default, only management events are recorded. What must the ML engineer do to capture these data-plane operations?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable CloudTrail management events in all regions.",
      "B": "In the CloudTrail console, enable data events for Amazon SageMaker to record data-plane API calls.",
      "C": "Enable AWS Config recording for SageMaker resource types.",
      "D": "Create CloudWatch Logs metric filters for SageMaker API calls."
    },
    "explanation": "To log data-plane API operations such as CreateModel and InvokeEndpoint, you must enable CloudTrail data events specifically for the SageMaker service. Management events alone do not capture these."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "An enterprise wants to expose a private SageMaker inference endpoint to on-premises clients over their VPN without using the internet. Which architecture meets this requirement securely with minimal overhead?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy an Amazon NLB in front of the SageMaker endpoint and configure an Interface VPC endpoint (AWS PrivateLink) for the NLB; route on-prem VPN traffic to it.",
      "B": "Create an Internet-facing Application Load Balancer in front of the endpoint and restrict access via CIDR.",
      "C": "Peer the SageMaker VPC with the on-premises network and access the endpoint via peering.",
      "D": "Use AWS Transit Gateway to route to a public-facing endpoint with strong security groups."
    },
    "explanation": "Using a Network Load Balancer with a PrivateLink interface endpoint allows on-premises VPN clients to connect privately without exposing the endpoint to the internet, and with minimal additional components."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A security policy requires that every SageMaker training job include a CostCenter tag and use the execution role arn:aws:iam::123456789012:role/MLExecRole. Which feature can the ML engineer use to enforce both requirements at job creation?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use an IAM permissions boundary on ML users to restrict tag values.",
      "B": "Deploy an AWS Organizations SCP that denies sagemaker:CreateTrainingJob when tags or role do not match.",
      "C": "Enable an AWS Config rule for SageMaker jobs to require tags and roles.",
      "D": "Attach an IAM policy to users with a condition on sagemaker:RequestTag/CostCenter and sagemaker:ResourceTag/CostCenter and a condition requiring the UseServiceRole parameter equals MLExecRole."
    },
    "explanation": "An IAM identity policy with sagemaker:RequestTag and sagemaker:ResourceTag conditions can prevent job creation if the CostCenter tag is missing or if the specified ServiceRole is not MLExecRole. SCPs cannot inspect request tags."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A processing job running in SageMaker needs to mount an Amazon EFS file system for intermediate data. The security team requires encryption at rest and in transit for EFS, and only the processing job\u2019s subnets should be able to mount it. Which configuration meets these requirements?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create an unencrypted EFS, rely on encryption in transit only, and restrict mount via security group.",
      "B": "Create an encrypted EFS with SSE-KMS, mount it in the processing job with encryption in transit disabled.",
      "C": "Use FSx for Lustre with default encryption, and mount via a VPN.",
      "D": "Create an EFS file system encrypted at rest with a CMK, enable encryption in transit (TLS) on the mount target, and restrict the EFS security group to only allow mount traffic from the processing job\u2019s security group."
    },
    "explanation": "An Amazon EFS file system can be encrypted at rest using a CMK and can enforce TLS encryption in transit. Security groups can restrict mount access so that only SageMaker processing job instances can connect."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "The security team wants only members of the SecurityAudit role to read SageMaker logs in CloudWatch Logs. Other users must be denied. How can this be implemented?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Attach a resource-based policy to the specific CloudWatch Log Group that allows only the SecurityAudit role to GetLogEvents and FilterLogEvents.",
      "B": "Encrypt the log group with a CMK and grant decrypt only to the SecurityAudit role.",
      "C": "Use an IAM identity policy denying CloudWatch Logs actions unless aws:PrincipalArn equals the SecurityAudit role.",
      "D": "Move the log group to a different AWS account used by security."
    },
    "explanation": "CloudWatch Log Groups support resource-based policies that can explicitly allow only a principal to perform log-reading actions. This is the most direct way to restrict access at the log-group level."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A CI/CD pipeline in CodePipeline uses an ECR repository for custom container images. The security team requires that only this pipeline can pull images and no other IAM principals. Which configuration enforces this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Attach an IAM policy to the pipeline\u2019s role allowing ecr:BatchGetImage on the repo.",
      "B": "Add an ECR repository resource policy that allows ecr:BatchGetImage for the CodePipeline service role ARN and denies all others.",
      "C": "Use SCP to deny ecr:BatchGetImage globally for all principals except the pipeline.",
      "D": "Rely on IAM identity policies on all users to not allow ECR actions."
    },
    "explanation": "An ECR repository policy can directly specify which principals (the pipeline role) are allowed to pull images and deny everyone else, enforcing container image access at the resource level."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "The security team wants to prevent developers from creating any SageMaker notebook instances or Studio domains that are internet-facing. Which control can achieve this across the organization?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use an IAM permissions boundary for all developers disallowing sagemaker:CreateDomain.",
      "B": "Apply an AWS Organizations Service Control Policy that denies sagemaker:CreateDomain or CreateNotebookInstance when NetworkIsolation is false.",
      "C": "Attach an identity policy to each developer denying notebook creation.",
      "D": "Use AWS Config to detect and remediate non-VPC notebooks."
    },
    "explanation": "A Service Control Policy can centrally deny creation of SageMaker notebooks or domains unless the NetworkIsolation parameter is set to true, preventing internet access organization-wide."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "An automated retraining workflow triggers a Lambda function via EventBridge. The function needs only permissions to DescribeEndpoint and CreateTrainingJob for a specific endpoint and training job prefix. How should the IAM role be defined to follow least privilege?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Grant sagemaker:* on all resources in the account.",
      "B": "Grant sagemaker:DescribeEndpoint and sagemaker:CreateTrainingJob on all SageMaker ARNs.",
      "C": "Grant sagemaker:DescribeEndpoint on the specific endpoint ARN and sagemaker:CreateTrainingJob with a resource ARN pattern matching the training-job-prefix*, and no other actions.",
      "D": "Attach AWS managed SageMaker full-access policy."
    },
    "explanation": "Defining resource-level permissions for only the specific endpoint and a wildcard training job prefix ensures the Lambda role has the minimum permissions required for its function."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "Security requirements dictate that SageMaker notebook instances access internal services only on port 443 and cannot initiate outbound traffic to the internet. Which combination of controls enforces this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure the notebook instance\u2019s security group to allow outbound 443 to specific subnets and attach an Internet Gateway to the VPC.",
      "B": "Use a NAT Gateway with a route table restricting to internal CIDR.",
      "C": "Configure the SG to allow outbound TCP 443 to internal subnet CIDR, set the route table for the private subnet with no internet gateway, and use a VPC Endpoint for required AWS service calls.",
      "D": "Deploy a firewall appliance in the VPC to filter outbound traffic."
    },
    "explanation": "Removing an internet gateway and not configuring a NAT Gateway prevents internet egress. A security group can allow only TCP 443 to internal subnets. VPC endpoints enable needed AWS service access without the internet."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "An ML engineer needs to capture inference requests and responses in a SageMaker endpoint. The security team requires that the data capture archive in S3 be encrypted at rest with a CMK and that all data in transit use TLS. Which configuration achieves this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable S3 default encryption on the bucket and configure DataCaptureConfig without specifying KMS.",
      "B": "Specify a KMS key in the SageMaker ModelRegistry settings and enable DataCaptureConfig.",
      "C": "Use an S3 bucket policy to require server-side encryption and rely on endpoint defaults.",
      "D": "In the endpoint\u2019s DataCaptureConfig, set EnableCapture true and specify the CMK KmsKeyId, and ensure the endpoint uses HTTPS (TLS) invocations."
    },
    "explanation": "DataCaptureConfig allows specifying a customer-managed KMS key for server-side encryption of captured data. SageMaker endpoints always use HTTPS for traffic, ensuring TLS in transit."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A data engineering team must transfer 100 TB of historical log files from an on-premises data center to Amazon S3 within 48 hours. The network bandwidth is limited to 1 Gbps and is cost-constrained. Which ingestion method meets the requirement with the lowest operational overhead?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS DataSync over the 1 Gbps link with parallel transfers.",
      "B": "Use S3 Transfer Acceleration over the internet connection.",
      "C": "Ship multiple AWS Snowball Edge devices and import data.",
      "D": "Use a custom multi-threaded upload client over the VPN."
    },
    "explanation": "At 1 Gbps, 100 TB over the network exceeds 48 hours; Snowball Edge avoids bandwidth constraints and minimizes operational complexity."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An ML workflow requires high-throughput reading of a 200 GB dataset during training in SageMaker. The data is stored in CSV format in S3. Training jobs are currently I/O-bound. Which change yields the greatest reduction in I/O time?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable S3 Transfer Acceleration on the bucket.",
      "B": "Convert the dataset to Parquet with Snappy compression and use Apache Arrow for reading.",
      "C": "Increase the SageMaker instance EBS volume throughput.",
      "D": "Enable S3 request rate optimization on the bucket."
    },
    "explanation": "Parquet is a columnar, compressed format that reduces data transfer size and read time, outperforming acceleration or EBS tweaks."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A streaming ingestion pipeline uses Amazon Kinesis Data Streams with 10 shards. Downstream training jobs require data partitioned by customer ID. The engineer notices uneven data distribution and hot shard errors. Which solution evenly distributes load?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase to 20 shards without changing partition key.",
      "B": "Use a composite primary key combining customer ID and timestamp.",
      "C": "Switch to Amazon MSK and let Kafka partition dynamically.",
      "D": "Hash the customer ID with a random suffix as the partition key."
    },
    "explanation": "Appending a random suffix to the customer ID spreads records across shards evenly, avoiding hot shards while preserving grouping on average."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A SageMaker Data Wrangler flow reads multiple small JSON files (~100 MB each) from S3. The job startup and cataloging latency dominates time. How can the engineer minimize this overhead?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Consolidate the small files into larger partitioned Parquet files.",
      "B": "Increase the SageMaker Data Wrangler instance type.",
      "C": "Use S3 Select on each JSON file.",
      "D": "Enable S3 Requester Pays on the bucket."
    },
    "explanation": "Consolidating files into fewer larger Parquet partitions reduces per-object overhead and speeds up scanning and schema inference."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An application uses Amazon Kinesis Data Firehose to deliver JSON records to S3. The delivery latency spikes under peak load. Which Firehose configuration change will smooth delivery latency while controlling costs?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set buffer size to minimum and buffer interval to default.",
      "B": "Set buffer size to maximum and buffer interval to minimum.",
      "C": "Increase buffer size to max and buffer interval to 300 seconds.",
      "D": "Decrease buffer size to minimum and buffer interval to 300 seconds."
    },
    "explanation": "Larger buffer size with a longer interval results in fewer deliveries, smoothing spikes and reducing egress costs."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A team must ingest near-real-time IoT telemetry (5 KB messages, 5,000 msg/sec) into S3 for batch training. Which AWS service combination best meets throughput and minimal operational overhead?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS IoT Core + Lambda writing to S3.",
      "B": "Kinesis Data Streams + Kinesis Data Firehose to S3.",
      "C": "MSK cluster + custom consumer that writes to S3.",
      "D": "Directly upload from devices to S3 using the AWS SDK."
    },
    "explanation": "Kinesis Streams scales to handle throughput and Firehose auto-batches and delivers to S3 without infra management."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A data scientist merges customer profile data from Amazon RDS and clickstream logs stored in S3. They need an automated, serverless solution with minimal code. Which approach is optimal?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Deploy an EMR cluster with Spark to join RDS and S3 datasets.",
      "B": "Use AWS Glue ETL jobs on a scheduled basis.",
      "C": "Use SageMaker Processing with custom Pandas scripts.",
      "D": "Use AWS Glue DataBrew recipe to connect to RDS and S3 and join datasets."
    },
    "explanation": "DataBrew provides serverless, no-code recipes to join RDS and S3 datasets, minimizing code and infra."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A company requires secure ingestion of PII data into Amazon S3. Data must be encrypted in transit and at rest, and decryption only occurs in SageMaker. Which configuration satisfies these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable SSE-S3 on the bucket and HTTPS for uploads.",
      "B": "Configure HTTPS for uploads, enable SSE-KMS with a CMK granting only SageMaker decryption.",
      "C": "Use client-side encryption and store keys in AWS Secrets Manager.",
      "D": "Enable SSE-KMS and grant all users read access to the CMK."
    },
    "explanation": "SSE-KMS with CMK access restricted to SageMaker ensures encrypted transit and at-rest encryption, with decryption only in SageMaker."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An ML pipeline uses Amazon MSK to stream data into S3 via Kafka Connect. During peak, Kafka Connect tasks lag behind. Which change will improve ingestion throughput?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase Kafka Connect workers and partitions for the connector topic.",
      "B": "Switch connector batch.size to 1.",
      "C": "Use a single large Kafka Connect worker.",
      "D": "Decrease connector retries to zero."
    },
    "explanation": "More workers and partitions allow parallelism in Kafka Connect, increasing ingestion throughput and reducing lag."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A training job on SageMaker needs to read a dataset stored on Amazon FSx for Lustre for low-latency I/O. The dataset originates in S3. Which configuration provides the fastest initial data access?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Mount FSx for Lustre and copy data manually from S3.",
      "B": "Use AWS DataSync to mirror S3 to FSx for Lustre.",
      "C": "Use FSx\u2019s native S3 integration by specifying the S3 bucket as the data repository.",
      "D": "Use a SageMaker Processing job to prefetch data into EBS."
    },
    "explanation": "FSx for Lustre native S3 integration transparently caches and streams S3 data into the file system without manual copy."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A dataset stored in S3 is consumed by multiple SageMaker training instances concurrently. To minimize read latency and S3 request costs, what ingestion pattern should be used?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Each instance downloads the full dataset to its EBS volume.",
      "B": "Use S3 Select API within training scripts.",
      "C": "Use a shared EFS mount backed by S3.",
      "D": "Use S3 Replication with Cross-Region Replication disabled and parallel GETs with HTTP Keep-Alive."
    },
    "explanation": "Parallel GETs with HTTP Keep-Alive reuse connections, reducing latency and request overhead compared to duplicated EBS or EFS."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A company needs to ingest 10 TB of data daily from S3 into SageMaker Feature Store with minimal lag. Which approach scales ingestion while controlling costs?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use service API PutRecord sequentially for each feature.",
      "B": "Use the batch ingest offline feature with RecordIO protobuf files.",
      "C": "Use real-time endpoint calls for each record.",
      "D": "Use Glue Streaming to write directly to Feature Store."
    },
    "explanation": "Batch ingest with RecordIO is optimized for large-scale ingestion into Feature Store and is cost-effective compared to per-record API calls."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An engineer must ingest data from multiple AWS accounts into a central S3 data lake. Data producers may intermittently lose network connectivity. Which ingestion pattern ensures reliability and scalability?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use S3 Cross-Account replication with retry policies.",
      "B": "Use a central Firehose delivery stream with producers as PUT clients.",
      "C": "Use AWS DataSync agents in each account.",
      "D": "Use AWS Transfer Family SFTP endpoints in the central account."
    },
    "explanation": "Cross-Account replication handles intermittent producers with built-in retry and scales to multi-account sources without agents."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A SageMaker processing job fails intermittently reading from an S3 bucket due to 503 throttling errors. Which change reduces the throttling while optimizing cost?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable S3 Transfer Acceleration.",
      "B": "Increase the processing instance count.",
      "C": "Enable S3 Requester Pays and use signed requests with exponential backoff in the SDK.",
      "D": "Switch the bucket to use SSE-C encryption."
    },
    "explanation": "Signed requests with retry/backoff reduce throttling; Requester Pays can shift cost but throttling reduction via SDK backoff is key."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A compliance team requires that logs ingested into S3 by Kinesis Firehose are immutable and versioned. Which S3 bucket configuration achieves this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable MFA Delete and SSE-S3.",
      "B": "Enable Object Lock in Governance mode with versioning.",
      "C": "Enable default encryption with SSE-KMS.",
      "D": "Enable Lifecycle rules to transition logs to Glacier."
    },
    "explanation": "Object Lock with versioning set to Governance mode enforces immutability on ingested S3 objects."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "During a large dataset migration, objects are written to an S3 bucket in parallel. The team observes increased 503 errors. Which bucket-level configuration resolves this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable S3 Transfer Acceleration.",
      "B": "Enable default encryption with SSE-KMS.",
      "C": "Enable MFA Delete on the bucket.",
      "D": "Enable S3 request rate optimization on the bucket."
    },
    "explanation": "Request rate optimization (prefix deprecation) reduces 503 errors under high parallel write rates by distributing keys."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A pipeline uses AWS Glue to extract data from an Amazon Aurora MySQL database to S3. The Glue job times out during snapshot extraction. Which approach resolves the timeout while minimizing developer effort?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Increase the connection timeout in the Glue job script.",
      "B": "Use AWS Database Migration Service with full load to S3.",
      "C": "Split the Glue job into multiple smaller jobs.",
      "D": "Export Aurora snapshot to S3 using native export."
    },
    "explanation": "Using DMS for full load to S3 handles large data extracts reliably with minimal custom code versus Glue timeouts."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An ML engineer must ingest encrypted CSV files from S3 to SageMaker Processing. The files are encrypted with SSE-KMS using a custom CMK. The processing role lacks decrypted access. What adjustment grants the least privilege?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Attach an IAM policy to the processing role allowing kms:Decrypt on the CMK.",
      "B": "Re-encrypt the files with SSE-S3.",
      "C": "Use client-side encryption and share the key.",
      "D": "Grant the role AmazonS3FullAccess."
    },
    "explanation": "Granting kms:Decrypt on the CMK to the processing role grants only needed permissions rather than broad S3 access."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A data lake uses S3 with AWS Lake Formation. Data ingestion from streaming sources must respect table-level LF permissions. Which integration ensures permissions enforcement?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Kinesis Data Firehose direct to S3 outputs.",
      "B": "Use Lambda to write to S3 then catalog with Glue.",
      "C": "Use Lake Formation transactions with AWS Glue streaming jobs.",
      "D": "Use Athena to INSERT INTO Lake Formation tables."
    },
    "explanation": "Lake Formation transactions in Glue streaming enforce table-level LF permissions during ingestion."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An ML pipeline ingests files via S3 Event Notifications to Lambda. Under high throughput, some events are lost. Which change ensures all records are ingested?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase the Lambda concurrency limit.",
      "B": "Use SQS as the Event Notification target before invoking Lambda.",
      "C": "Switch to SNS Event Notifications.",
      "D": "Enable retry behavior in S3."
    },
    "explanation": "Using SQS between S3 and Lambda buffers events, ensuring delivery and retry rather than direct event loss."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A company plans to centralize logs from multiple regions into a single S3 bucket. Logs must remain in native partition folders. How should a cross-region replication rule be configured?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a replication rule with IncludePuts and prefix replication per region.",
      "B": "Use replication time control to replicate objects hourly.",
      "C": "Use batch Athena CTAS to copy logs across regions.",
      "D": "Use DataSync to mirror entire buckets."
    },
    "explanation": "A replication rule with region-specific prefixes replicates objects into same partition paths in central bucket."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A secure pipeline must ingest data from Amazon DynamoDB to S3 for ML training. The transfer must be encrypted, serverless, and cost-efficient. Which service accomplishes this?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Use EMR Spark job to export DynamoDB to S3.",
      "B": "Use AWS DataSync to copy table exports.",
      "C": "Use AWS Glue with DynamoDB connector.",
      "D": "Use DynamoDB Export to S3 with SSE-KMS encryption."
    },
    "explanation": "DynamoDB native Export to S3 is serverless, encrypted, and cost-efficient compared to EMR or Glue."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An ML pipeline needs to ingest image files from multiple SFTP servers into S3. The process must be fully AWS managed and scalable. Which solution meets these requirements?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Deploy EC2 instances running custom SFTP-to-S3 scripts.",
      "B": "Use AWS DataSync agents on EC2.",
      "C": "Use AWS Transfer Family SFTP endpoints with custom identity providers and S3 backing.",
      "D": "Use Glue FTP connector."
    },
    "explanation": "AWS Transfer Family provides managed SFTP endpoints that store files directly in S3, scaling without custom servers."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A pipeline needs to ingest sensor data at 100 MB/s into S3 for real-time analytics. Which combination of services and configurations meets throughput while controlling costs?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Kinesis Data Streams with 100 shards directly writing to S3.",
      "B": "MSK with 5 broker nodes, MirrorMaker to Firehose with gzip compression.",
      "C": "Direct Firehose delivery with default shard count.",
      "D": "Lambda triggered by S3 upload events."
    },
    "explanation": "MSK scales to high throughput; MirrorMaker populates Firehose which batches and compresses data to S3 cost-effectively."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "During ingest of nested JSON event data into S3, the team needs to infer schema and partition by event date. Which AWS Glue configuration fulfills this with minimal code?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Use a custom PySpark Glue job to parse JSON and write Parquet.",
      "B": "Use Athena CTAS to convert and partition data.",
      "C": "Use SageMaker Processing to flatten and write to S3.",
      "D": "Use AWS Glue JSON classifier with crawler partitioning on event_date field."
    },
    "explanation": "A Glue crawler with JSON classifier can infer schema and apply partitioning on a specified JSON field automatically."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A dataset must be ingested daily from an external partner to S3 via the internet. The partner cannot use AWS SDK. How should the company securely and reliably ingest files?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provision an AWS Transfer Family SFTP endpoint backed by the S3 bucket.",
      "B": "Provide S3 PUT URLs with public write ACL.",
      "C": "Use HTTP POST to S3 REST API with credentials.",
      "D": "Ask partner to email files and upload manually."
    },
    "explanation": "Transfer Family SFTP endpoint allows partners to upload over SFTP without SDK, delivering files directly and securely to S3."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "The team needs to ingest 500 GB of data daily from an on-prem Hadoop cluster to S3 under network constraints. Which approach provides incremental ingestion and optimizes bandwidth?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Snowball Edge weekly ingestion.",
      "B": "Deploy AWS DataSync agent with incremental sync enabled.",
      "C": "Use a Glue job with JDBC connector.",
      "D": "Use AWS Glue DataBrew to pull data."
    },
    "explanation": "DataSync incremental sync only transfers changed data daily, optimizing bandwidth and automating ingestion."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An ML pipeline reads an S3 directory of Avro files during training. The engineer wants to reduce startup latency when listing thousands of files. Which S3 configuration reduces LIST cost?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable SSE-KMS on the bucket.",
      "B": "Enable bucket versioning.",
      "C": "Use S3 Inventory to generate daily manifest and use the manifest for file listing.",
      "D": "Enable S3 Transfer Acceleration."
    },
    "explanation": "S3 Inventory provides a manifest file listing objects, eliminating costly API ListObjects calls during job startup."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A team needs to ingest HTTP logs into S3 for historical analysis. Logs are generated at 10 GB/hour. The solution must auto-scale and require no server management. Which ingestion architecture is best?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "EMR cluster with Flink reading logs.",
      "B": "Kinesis Data Firehose with Lambda data transformation to S3.",
      "C": "Custom ECS service tailing logs and writing to S3.",
      "D": "S3 multipart upload from application servers."
    },
    "explanation": "Firehose auto-scales ingestion and can deliver data to S3 with optional Lambda transformation, requiring no servers."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A multi-region application produces data that must land in a centralized S3 bucket in the same AWS Region as processing. Replication must preserve object metadata and ACLs. Which configuration meets this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use CloudTrail replication to central bucket.",
      "B": "Configure Firehose across regions.",
      "C": "Use DataSync cross-region replication.",
      "D": "Use cross-region replication with replicate object metadata and ACL."
    },
    "explanation": "S3 cross-region replication configured to replicate metadata and ACLs preserves all object attributes in central bucket."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A SageMaker training job must read millions of small JSON files from S3. The engineer notices high GET request costs. Which solution reduces GETs and maintains low latency?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Combine JSON files into larger S3 objects, e.g., Parquet, using a Glue job.",
      "B": "Enable S3 Intelligent-Tiering.",
      "C": "Use S3 Select for each JSON file.",
      "D": "Use Lifecycle rules to consolidate files."
    },
    "explanation": "Combining small files into larger Parquet objects reduces individual GET calls and lowers request costs."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "To ingest clickstream events into S3 with exactly-once semantics, an engineer uses Kinesis Data Streams and Lambda to write to S3. At times, duplicates occur. Which pattern ensures exactly-once writes?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use PutObject with versioning enabled.",
      "B": "Use S3 Multipart upload with unique upload IDs.",
      "C": "Use Kinesis Data Firehose with S3 delivery and deduplication via record ID.",
      "D": "Use DynamoDB streams and batch writes to S3."
    },
    "explanation": "Firehose supports exactly-once delivery semantics to S3 when configured with record IDs for deduplication."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A data lake's S3 bucket is configured with CloudWatch Events to start Glue crawlers upon new object creation. Under heavy load, some crawlers start late and miss events. How ensure timely ingestion?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase crawler concurrency in Glue.",
      "B": "Introduce a Step Functions state machine triggered by EventBridge to queue and throttle crawler invocations.",
      "C": "Use Lambda to start crawlers directly.",
      "D": "Enable S3 Intelligent-Tiering."
    },
    "explanation": "Using Step Functions to queue and throttle ensures orderly crawler invocations without overload compared to direct triggers."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An ML engineer is training a PyTorch model in SageMaker script mode on a single GPU instance. The training job often runs out of GPU memory when using a batch size of 256. To continue training with minimal code changes and without incurring additional infrastructure cost, which action should the engineer take?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable mixed precision training (automatic mixed precision) in the training script.",
      "B": "Switch to SageMaker built-in algorithm XGBoost with the same batch size.",
      "C": "Decrease the number of training epochs to reduce memory footprint.",
      "D": "Use a larger EC2 instance type with more GPU memory."
    },
    "explanation": "Mixed precision reduces memory usage with minimal code changes. Switching algorithms or decreasing epochs doesn\u2019t reduce per-batch memory; upgrading instance increases cost."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A data scientist uses SageMaker Automatic Model Tuning (AMT) with Bayesian optimization to tune a deep neural network. The search is converging too slowly. The search space contains 10 continuous and 5 categorical hyperparameters. What adjustment will most speed up convergence?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch from Bayesian optimization to random search.",
      "B": "Increase the maximum number of training jobs in the tuning job.",
      "C": "Reduce the hyperparameter search space by fixing low-impact parameters to default values.",
      "D": "Use an IDENTICAL_DATA_AND_ALGORITHM warm start tuning job type."
    },
    "explanation": "Reducing search space focuses Bayesian optimization on impactful parameters. Random search may be less efficient; warm start of type IDENTICAL_DATA_AND_ALGORITHM duplicates prior runs, not solving slow convergence."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An ML engineer trains a large transformer model using SageMaker script mode with TensorFlow. Training takes 48 hours. The engineer wants to halve training time with minimal hyperparameter changes. Which approach meets this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Double the batch size and halve the number of epochs.",
      "B": "Enable distributed data parallel training across two GPU instances.",
      "C": "Use L2 regularization with higher weight to speed up convergence.",
      "D": "Decrease learning rate and increase number of epochs."
    },
    "explanation": "Distributed training splits workload across GPUs, reducing wall-clock time. Changing regularization or learning rate affects training dynamics, not time directly; doubling batch/halving epochs may hurt convergence."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A tabular dataset shows early overfitting: validation loss increases after 5 epochs. Which combination of techniques best reduces overfitting while preserving model capacity?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase batch size and remove dropout layers.",
      "B": "Decrease learning rate and increase number of layers.",
      "C": "Add L2 regularization with high weight and increase epochs.",
      "D": "Implement dropout, add L2 weight decay, and enable early stopping after no validation improvement for 3 epochs."
    },
    "explanation": "Dropout+L2 regularization reduce overfitting; early stopping avoids unnecessary epochs. Increasing layers or removing dropout worsens overfitting."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A company uses a SageMaker built-in XGBoost algorithm to classify credit risk. The model size on disk is 1.5 GB, exceeding edge device storage limit of 500 MB. Which strategy best reduces model size without retraining from scratch?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Decrease max_depth and num_round in a new training job.",
      "B": "Use post-training model pruning and quantization using SageMaker Neo compilation.",
      "C": "Enable L1 regularization in the current model registry version.",
      "D": "Decrease the learning rate and increase the number of trees."
    },
    "explanation": "SageMaker Neo can prune and quantize a compiled model without retraining. Changing training hyperparameters requires retraining."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An ML engineer fine-tunes a pre-trained BERT model on SageMaker JumpStart. To prevent catastrophic forgetting of pre-trained weights, which configuration should they apply?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use lower learning rate for pre-trained layers and higher rate for newly added classification head.",
      "B": "Initialize all layers randomly before fine-tuning.",
      "C": "Freeze the classification head and only train transformer layers.",
      "D": "Increase batch size by fourfold to stabilize gradients."
    },
    "explanation": "Lower LR on pretrained layers preserves learned features; random init or freezing head loses benefits."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "During hyperparameter tuning with AMT, an engineer observes that many training jobs fail due to out-of-memory errors for large batch sizes. To optimize tuning efficiency, which AMT configuration change is best?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase the maximum parallel training jobs.",
      "B": "Switch from Bayesian to grid search.",
      "C": "Decrease the minimum number of training jobs.",
      "D": "Add a conditional statement in the training script to skip batch sizes causing OOM and return low objective metric."
    },
    "explanation": "Skipping invalid hyperparameter combinations via script invalidation avoids wasted jobs; changing search strategy or job counts doesn\u2019t prevent failures."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A deep CNN trained in SageMaker exhibits slow convergence and high variance among epoch losses. Which combination of techniques addresses both issues?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Remove data augmentation and add more layers.",
      "B": "Increase learning rate and decrease batch size.",
      "C": "Apply batch normalization, use learning rate scheduler, and add dropout layers.",
      "D": "Switch optimizer from Adam to stochastic gradient descent with fixed LR."
    },
    "explanation": "Batch norm and LR scheduling stabilize training; dropout reduces variance. Removing augmentation or switching optimizer without scheduler may worsen convergence."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An ML engineer needs to integrate a scikit-learn RandomForest model trained locally into SageMaker for batch transform inference. Which step is required?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Wrap the model in a SageMaker built-in algorithm container.",
      "B": "Create a custom inference script and container using SageMaker script mode to load the pickle file.",
      "C": "Use SageMaker Clarify to register the model directly from local filesystem.",
      "D": "Upload the model artifact to SageMaker Model Registry via console."
    },
    "explanation": "Local models require a custom container/script in script mode. Built-in algorithms can\u2019t import arbitrary pickles."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A large NLP model fine-tuned via Hugging Face in SageMaker shows slow throughput per epoch. Which SageMaker feature reduces training time without code changes?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable early stopping in AMT.",
      "B": "Use SageMaker Model Monitor to detect drift.",
      "C": "Enable managed spot training with distributed training.",
      "D": "Switch to a built-in algorithm for NLP tasks."
    },
    "explanation": "Managed distributed spot training uses multiple instances to speed training; early stopping and monitoring don\u2019t affect raw throughput."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An engineer uses SageMaker script mode to train a PyTorch model. The training time per epoch increases over time due to GPU memory fragmentation. How to mitigate this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable gradient checkpointing to reduce peak memory usage.",
      "B": "Increase batch size to better pack memory.",
      "C": "Disable data parallelism to reduce fragmentation.",
      "D": "Switch from PyTorch to TensorFlow."
    },
    "explanation": "Gradient checkpointing trades compute for memory, alleviating fragmentation. Increasing batch size worsens it; switching frameworks is heavy-handed."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A SageMaker AMT job tunes hyperparameters including learning_rate and optimizer choice. The team needs to incorporate prior tuning results to guide the new job. Which warm start type is appropriate?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "IDENTICAL_DATA_AND_ALGORITHM",
      "B": "TRANSFER_LEARNING",
      "C": "STANDARD",
      "D": "TRANSFER_LEARNING with previous job as parent"
    },
    "explanation": "TRANSFER_LEARNING warm start uses prior results from parent tuning jobs; IDENTICAL_DATA_AND_ALGORITHM reuses only matching configs."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An ML researcher wants repeated reproducible experiments in SageMaker. Which practice ensures identical results across training runs?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use random search in AMT.",
      "B": "Use spot instances to save costs.",
      "C": "Enable early stopping after fixed epochs.",
      "D": "Set and log explicit random seeds, fix training hyperparameters, use the Model Registry for versioning."
    },
    "explanation": "Explicit seed and consistent config plus registry ensure reproducibility; early stopping and spot instances introduce variability."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A team notices high variance between ensemble members when combining models. To improve ensemble performance, which strategy is best?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use identical architecture and data splits for each member.",
      "B": "Use diverse base learners and bootstrap sampling to increase diversity.",
      "C": "Increase depth of all tree-based models.",
      "D": "Use only the top-performing single model."
    },
    "explanation": "Ensemble benefits from diverse learners; identical architectures reduce benefit."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An engineer needs to fine-tune a large foundation model from Amazon Bedrock in SageMaker for a specialized downstream task. Which approach integrates seamlessly?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Export the model weights and import into a SageMaker built-in algorithm.",
      "B": "Download the model artifacts and retrain from scratch.",
      "C": "Use SageMaker JumpStart to fine-tune the model with custom dataset.",
      "D": "Convert Bedrock model to TensorFlow SavedModel and deploy."
    },
    "explanation": "JumpStart provides ready fine-tuning pipelines for Bedrock foundation models."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A deep learning model exhibits underfitting on training and validation sets. To improve fit, which hyperparameter adjustment is most effective?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Increase model capacity by adding layers or units.",
      "B": "Increase dropout rate.",
      "C": "Increase L2 regularization weight.",
      "D": "Decrease number of training epochs."
    },
    "explanation": "Underfitting requires more capacity; dropout and L2 worsen underfitting."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An automatic model tuning job uses AMT with a goal metric of validation error. The team now needs to optimize two metrics: inference latency and accuracy. Which solution supports this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run two separate AMT jobs and manually choose trade-off.",
      "B": "Use Bayesian optimization with a composite metric manually computed in script.",
      "C": "Switch search strategy to grid search for multi-objective coverage.",
      "D": "Use SageMaker multi-objective hyperparameter tuning-enabled job."
    },
    "explanation": "Multi-objective tuning in SageMaker supports simultaneous optimization of metrics."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A training job using Hugging Face Transformers in SageMaker uses a JSON Lines dataset on S3. The engineer observes slow data loading. Which training configuration change accelerates data throughput?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch input mode to Pipe mode.",
      "B": "Use File mode with Amazon FSx for Lustre linked to the S3 bucket.",
      "C": "Decrease number of worker processes in DataLoader.",
      "D": "Disable shuffling of the dataset."
    },
    "explanation": "FSx for Lustre caches data for high throughput; pipe mode streams raw S3, slower for many small records."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An ML engineer wants to prune a TensorFlow model during SageMaker training to reduce inference latency while preserving >98% of accuracy. Which training callback should be used?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "EarlyStopping on validation loss.",
      "B": "LearningRateScheduler.",
      "C": "TensorFlow Model Optimization pruning callback.",
      "D": "ReduceLROnPlateau."
    },
    "explanation": "TF Model Optimization pruning callback prunes weights during training; others adjust learning rate or stop early."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A model trained on SageMaker exhibits high training throughput but low GPU utilization. Which configuration change most directly improves GPU utilization?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase per-instance batch size.",
      "B": "Decrease the number of GPUs.",
      "C": "Enable early stopping.",
      "D": "Use a smaller instance type."
    },
    "explanation": "Batch size affects GPU utilization; larger batches fill GPU compute better."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A team uses AMT with max_jobs=50 and max_parallel_jobs=5. They need to reduce total run time but keep exploring same search space. Which change achieves this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase search space cardinality.",
      "B": "Increase max_parallel_jobs to 10.",
      "C": "Decrease max_jobs to 25.",
      "D": "Switch to random search strategy."
    },
    "explanation": "Increasing parallel jobs reduces wall-clock time; random search doesn\u2019t guarantee faster convergence."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An existing model version in SageMaker Model Registry is retrained with new hyperparameters. To automate promoting the version when validation accuracy improves by >1%, which approach is best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Manually compare metrics and click Promote in console.",
      "B": "Use SageMaker Experiments to tag and promote.",
      "C": "Define a SageMaker Pipeline with conditional ModelRegistry promotion step.",
      "D": "Write a Lambda triggered by CloudWatch alarm on metrics."
    },
    "explanation": "Pipelines support conditional promotion; Lambda approach requires custom code and integration."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "During distributed training on multiple GPU instances, training noise leads to divergence. What change reduces variance across replicas?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase initial learning rate.",
      "B": "Disable gradient clipping.",
      "C": "Use smaller batch size per replica.",
      "D": "Enable synchronized BatchNorm across replicas."
    },
    "explanation": "Synchronized BatchNorm ensures consistent statistics; other changes may worsen instability."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A SageMaker TensorFlow training job time per epoch is dominated by data preprocessing in training script. How to offload preprocessing to speed training?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Processing or Glue to preprocess and save TFRecords before training.",
      "B": "Enable profiler for training jobs.",
      "C": "Switch to script mode with Python SDK.",
      "D": "Use hyperparameter tuning to find optimal preprocessing parameters."
    },
    "explanation": "Preprocessing offline reduces per-epoch overhead; tuning and profiling don\u2019t offload work."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An engineer needs to train a model with imbalanced classes. To ensure balanced gradient updates per batch, which DataLoader or training config change is best?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Shuffle dataset each epoch.",
      "B": "Use a weighted random sampler to form balanced batches.",
      "C": "Increase number of epochs.",
      "D": "Add L1 regularization to loss."
    },
    "explanation": "Weighted sampler balances classes per batch; shuffling alone doesn\u2019t ensure balance."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A team wants to combine three diverse models into a final prediction in SageMaker. They need an ensemble pipeline that retrains all members and the meta-learner automatically. Which SageMaker feature should they use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Batch Transform.",
      "B": "SageMaker Clarify.",
      "C": "SageMaker Neo.",
      "D": "SageMaker Pipelines with Ensemble step implementations."
    },
    "explanation": "Pipelines can orchestrate multi-step ensemble training; other services unrelated."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You have a dataset with highly skewed numeric features and missing values partitioned across multiple S3 prefixes. You need a pipeline that imputes missing values with the median per feature, applies a log transform to reduce skew, and writes the cleaned data to Amazon SageMaker Feature Store. Which approach meets these requirements with the least custom code?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use an AWS Glue Spark ETL job to compute medians via AWS Glue Data Quality, apply a PySpark log transform, and write to Feature Store using the SageMaker SDK.",
      "B": "Use a SageMaker Data Wrangler flow: add a fill-missing recipe with median, add a built-in log transform step, then export directly to Feature Store.",
      "C": "Use AWS Glue DataBrew: create a project, add fill-missing and log transform recipes, and publish to S3 for later ingestion into Feature Store.",
      "D": "Run a SageMaker Processing job with a custom Scikit-Learn script that imputes medians, applies log transforms, and writes to Feature Store."
    },
    "explanation": "Data Wrangler provides built-in median imputation and log transform steps and can export directly to Feature Store, minimizing custom code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A streaming application ingests JSON records into Amazon Kinesis Data Streams. Each record contains nested user attributes and a categorical field with dozens of categories. You must extract the nested field, one-hot encode the categorical feature, and persist the output in S3 in Parquet format. Which solution is most operationally efficient?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Kinesis Data Analytics for Apache Flink with SQL to flatten JSON, then implement a user-defined function for one-hot encoding, and sink to S3.",
      "B": "Trigger an AWS Lambda function on Kinesis shards to parse JSON, use pandas.get_dummies for encoding, and write Parquet to S3.",
      "C": "Use SageMaker Data Wrangler with a streaming data source, apply transforms, and export to S3.",
      "D": "Use AWS Glue Streaming ETL with Apache Spark: apply JSON extract, use Spark ML OneHotEncoderEstimator, and write Parquet to S3."
    },
    "explanation": "AWS Glue Streaming ETL natively supports Spark transformations on streaming data, including JSON flattening and OneHotEncoder, with minimal custom code and built-in S3 sink."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You have a multi-class categorical feature with 1000 unique values, most of which appear in fewer than 5% of the records. You need to encode it for a tree-based model without creating high dimensionality. Which feature-engineering technique should you apply in SageMaker Data Wrangler?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use one-hot encoding with drop-last to reduce one dimension.",
      "B": "Apply label encoding directly to assign integer codes.",
      "C": "Group infrequent categories into an \"Other\" bucket, then apply one-hot encoding on the top categories.",
      "D": "Apply ordinal encoding based on average target probability per category."
    },
    "explanation": "Combining infrequent categories into \"Other\" reduces cardinality before one-hot encoding, balancing dimensionality and model interpretability."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A text column contains customer reviews that must be tokenized and vectorized for an NLP model. You need to integrate this into a SageMaker feature-engineering pipeline with minimal custom code. Which service and sequence accomplishes this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Glue DataBrew: add tokenization recipe, export to S3, and then train with a custom tokenizer.",
      "B": "Use a SageMaker Processing job with the Hugging Face tokenizer, then push token IDs to Feature Store.",
      "C": "Use SageMaker Data Wrangler: add a built-in text tokenizer transform, then export token counts to Feature Store.",
      "D": "Stream reviews through Lambda to call Amazon Comprehend for tokenization, then store results in S3."
    },
    "explanation": "A SageMaker Processing job with Hugging Face tokenizer integrates easily into pipelines and can write token IDs directly to Feature Store."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You need to discretize a continuous feature into bins of equal frequency and standardize another feature to zero mean and unit variance, using managed AWS services. Which combination is most appropriate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Data Wrangler recipe: add quantile binning for equal-frequency bins and Z-score normalization for standardization.",
      "B": "Use AWS Glue DataBrew: add equal-width binning recipe and MinMax scale recipe.",
      "C": "Run a SageMaker Processing job with a Scikit-Learn pipeline that uses KBinsDiscretizer and StandardScaler.",
      "D": "Use AWS Glue ETL: write PySpark code to bucketBy frequency and use VectorAssembler with StandardScaler."
    },
    "explanation": "Data Wrangler supports quantile binning (equal frequency) and Z-score normalization out of the box, minimizing custom code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "Your dataset contains duplicate records due to multiple ingestion systems. You need a low-code AWS solution to deduplicate by a composite key and write the result back to S3. Which approach is best?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Use SageMaker Data Wrangler: add a drop duplicates recipe on the composite key, then export to S3.",
      "B": "Run a Glue Spark ETL job with custom PySpark dropDuplicates call on the composite key.",
      "C": "Use AWS Glue DataBrew: create a project, add a dedup recipe on the composite key, and publish the output to S3.",
      "D": "Implement a SageMaker Processing job with Pandas drop_duplicates and write to S3."
    },
    "explanation": "DataBrew provides a built-in deduplication recipe that can drop duplicates on any key and export to S3 without custom code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You must engineer features that capture hour-of-day and day-of-week from a timestamp column for downstream modeling. The solution must integrate with SageMaker Pipelines and Feature Store. Which components do you use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A SageMaker ProcessingStep using a custom Pandas script to extract features and call PutRecord to Feature Store.",
      "B": "A SageMaker Data Wrangler step in the pipeline: use built-in ExtractTimestamp transforms for hour and weekday, and export to Feature Store.",
      "C": "An AWS Glue job to run Python code, write to S3, then a ProcessingStep to ingest into Feature Store.",
      "D": "Lambda functions behind EventBridge that trigger on S3 updates to compute features and write to Feature Store."
    },
    "explanation": "Data Wrangler can be used as a ProcessingStep in SageMaker Pipelines, with built-in timestamp extract transforms and direct export to Feature Store."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A dataset contains outliers in multiple numeric columns. You need to detect and cap them at the 1st and 99th percentiles before modeling. Which AWS Workflow is most efficient?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a SageMaker Processing job with custom code to compute percentiles and cap outliers.",
      "B": "Use SageMaker Data Wrangler: add an outlier detection transform to compute percentiles and use a value mapping recipe to cap values.",
      "C": "Write a Glue Spark ETL job using approxQuantile to find percentiles and withColumn to cap values.",
      "D": "Use AWS Glue DataBrew: add a percentile filter recipe then a replace recipe to cap extremes."
    },
    "explanation": "Data Wrangler outlier detection can compute percentiles and supports value mapping recipes to cap outliers without custom code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You have a categorical feature with high cardinality and need to generate target-guided encoding (mean target value per category) without leaking test data information. Which pipeline design prevents leakage?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a single SageMaker Processing job to compute global means on full dataset, apply to all splits.",
      "B": "Use Data Wrangler to compute target means on full dataset and join back to features.",
      "C": "Use Glue ETL to compute means on training set and apply same mapping to test set in the same job.",
      "D": "Define two ProcessingSteps in a SageMaker Pipeline: first compute category-target means on the training split only, store mapping; second step applies mapping to train and test splits separately."
    },
    "explanation": "Separating computation on training split from application to test split in different steps prevents target leakage."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A feature column contains text with inconsistent casing and punctuation. You need to normalize the text (lowercase, remove punctuation), tokenize, and compute TF-IDF vectors, using managed AWS services. Which sequence is correct?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker ProcessingStep: run a Scikit-Learn script for lowercasing, regex to remove punctuation, CountVectorizer with TF-IDF transformer.",
      "B": "Use Data Wrangler: add lowercase and remove-pattern transforms, then export tokens to S3 for external TF-IDF calculation.",
      "C": "Use Glue DataBrew recipes: add text lowercase and remove punctuation recipes, then a custom recipe for TF-IDF.",
      "D": "Use AWS Lambda triggered on S3: call Amazon Comprehend to normalize and tokenize, then compute TF-IDF in Lambda."
    },
    "explanation": "A ProcessingStep gives full control to use Scikit-Learn pipeline for normalization and TF-IDF vectorization directly in SageMaker Pipelines."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You need to enrich your dataset with rolling window features (7-day sum and average) on time-series data stored in S3. Which AWS service and pattern is most appropriate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Glue Streaming ETL with sliding window in Spark Structed Streaming to compute rolling features.",
      "B": "Use SageMaker Data Wrangler to define window aggregation transforms directly.",
      "C": "Schedule a SageMaker Processing job with PySpark code that reads S3 parquet, applies window functions, and writes enriched data back to S3.",
      "D": "Use AWS Lambda triggered by new data to update rolling aggregates in Amazon DynamoDB."
    },
    "explanation": "SageMaker Processing with PySpark supports Spark SQL window functions for rolling aggregates in a batch fashion, suitable for scheduled enrichment."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "Your dataset has a highly imbalanced binary label. You need to generate synthetic minority samples using SMOTE within a managed AWS pipeline. Which service and pattern achieves this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Data Wrangler\u2019s built-in synthetic data generation transforms.",
      "B": "Use a SageMaker Processing job with Imbalanced-Learn\u2019s SMOTE, then export to S3.",
      "C": "Use AWS Glue ETL with custom PySpark code to perform SMOTE.",
      "D": "Use AWS Glue DataBrew with a custom recipe for synthetic minority sampling."
    },
    "explanation": "Only a Processing job supports custom Python libraries like Imbalanced-Learn to run SMOTE within a managed pipeline."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You have feature interactions that must be generated (pairwise products) for a logistic regression model. You want to do this at scale on a large dataset in S3. What is the most serverless approach?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run a SageMaker Processing job on large ML instances with a custom script to generate interactions.",
      "B": "Use AWS Glue ETL on Spark with PySpark to compute pairwise products and write back to S3.",
      "C": "Use SageMaker Batch Transform with a script that outputs interactions.",
      "D": "Use AWS Glue DataBrew: create a recipe to generate new columns via formula for each interaction and publish to S3."
    },
    "explanation": "DataBrew scales serverlessly with partitions and can generate new columns via formulas for interactions without managing infrastructure."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You need to normalize a set of numeric features using robust scaling (subtract median and divide by IQR) within SageMaker Pipelines. Which component do you include?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A ProcessingStep that calls AWS Glue Data Quality for robust scaling.",
      "B": "A ProcessingStep that uses a DataBrew project for IQR scaling.",
      "C": "A ProcessingStep with a custom Scikit-Learn RobustScaler script.",
      "D": "A TransformStep using built-in SageMaker XGBoost with robust scaling parameters."
    },
    "explanation": "A ProcessingStep with a custom script is required for RobustScaler since no built-in recipe exists in Data Wrangler or DataBrew."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You must anonymize PII in free-text user comments by replacing names and emails, then vectorize text. Which AWS-managed solution minimizes custom code?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Processing with a custom regex script for PII removal and TF-IDF vectorization.",
      "B": "Use a SageMaker Processing job combining Amazon Comprehend PII entity detection API for redaction, then Scikit-Learn vectorizer.",
      "C": "Use AWS Glue DataBrew with PII detection recipes and export tokens to S3 for vectorization.",
      "D": "Use AWS Lambda to call Comprehend for redaction, store in S3, then run DataWrangler for vectorization."
    },
    "explanation": "A single Processing job can orchestrate Comprehend API calls and vectorization in one managed step."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You need to transform multi-value categorical fields (lists of tags per item) into indicator features and store them for low-latency inference. Which pipeline configuration should you choose?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "In SageMaker Data Wrangler, use the split multi-value transform to explode tags into rows, one-hot encode them, pivot back to wide format, and export to online Feature Store.",
      "B": "In Glue ETL, use PySpark explode then OneHotEncoderEstimator and write to S3 for offline Feature Store.",
      "C": "Use Lambda functions to parse tags into booleans and write JSON to DynamoDB.",
      "D": "Use SageMaker Processing with pandas to create indicator columns and write to S3."
    },
    "explanation": "Data Wrangler supports split multi-value transforms and direct export to online Feature Store for low-latency inference."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A dataset contains decimal representations of categorical codes. You need to convert these codes to binary bit-vector features (one bit per significant bit) for modeling. Which approach is most straightforward using Data Wrangler?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Use a ProcessingStep with custom Python bit-operation code.",
      "B": "Use Glue ETL with PySpark bit operations in withColumn.",
      "C": "Use SageMaker Data Wrangler\u2019s custom transform node: write a small Pandas UDF to convert codes to bit vectors.",
      "D": "Use DataBrew with multiple formula recipes to extract bits manually."
    },
    "explanation": "A custom transform node in Data Wrangler allows concise Pandas UDF bit extraction without full ETL management."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You want to join two large datasets in S3 by a composite key, perform feature computations (ratio of two columns), and write the result in parquet. Which solution scales most cost-effectively?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Data Wrangler to join and compute ratio then export.",
      "B": "Use AWS Glue ETL on Apache Spark with pushdown predicates and write to S3.",
      "C": "Use a SageMaker Processing job with PySpark on large instances.",
      "D": "Use AWS Glue DataBrew: import both datasets, join in project, and publish result."
    },
    "explanation": "AWS Glue ETL offers serverless Spark with pushdown and partition pruning, minimizing cost for large joins."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "Your feature store offline table has outdated records. You need to batch ingest new features while preserving historical records for audit. Which pattern do you follow?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Overwrite the offline store using Feature Group import mode FULL Refresh.",
      "B": "Use a Glue job to append to the offline table in the Feature Store\u2019s backing S3 bucket.",
      "C": "Use a ProcessingJob to write new features to the same S3 prefix without Feature Store APIs.",
      "D": "Use SageMaker Feature Store Batch PutRecord API to append new records, which preserves history if record identifier and event time increase."
    },
    "explanation": "Batch PutRecord appends new feature values and keeps older versions for audit based on eventTime."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You need to generate polynomial features (up to degree 3) on a subset of continuous variables for a regression model, using SageMaker Pipelines. Which step do you include?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A ProcessingStep that runs a Scikit-Learn PolynomialFeatures transformer on the selected columns.",
      "B": "A TransformStep using XGBoost\u2019s built-in polynomial feature parameters.",
      "C": "A Data Wrangler step with custom Python recipe for polynomial features.",
      "D": "A Glue ETL job scheduled separately and results stored in S3."
    },
    "explanation": "A ProcessingStep allows direct use of Scikit-Learn PolynomialFeatures within the pipeline."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "Your data contains timestamp strings in multiple formats. You must standardize to ISO8601 before downstream feature extraction. Which AWS service and method handle this with minimal code?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Processing with a custom regex parser to normalize formats.",
      "B": "Use Glue ETL with custom Python UDF in Spark to parse and format timestamps.",
      "C": "Use SageMaker Data Wrangler: add multiple parse-date recipes with input patterns, unify output to ISO8601, then export.",
      "D": "Use DataBrew: define multiple date parsing recipes and publish standardized data."
    },
    "explanation": "Data Wrangler supports multiple parse-date recipes and an output format selection, reducing custom code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You must encode a hierarchical categorical feature (e.g., Country > State > City) to capture both levels for a tree model. Which sequence in Data Wrangler best preserves hierarchy?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "One-hot encode City only.",
      "B": "Create combined category codes (Country_State_City) then one-hot encode top N combined categories and bucket rest as Other.",
      "C": "Label encode each level separately.",
      "D": "Use Hash encoding on City and ignore higher levels."
    },
    "explanation": "Creating combined codes preserves hierarchy and one-hot encoding top combinations captures joint distribution with controlled dimensionality."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A dataset includes text descriptions with HTML markup. You must remove tags, lowercase, and compute the length of each description. Which managed flow meets this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Data Wrangler: add a custom transform node that runs a small Python function to strip HTML, lowercase, and compute length.",
      "B": "Use a Glue ETL job with BeautifulSoup and Python to process, then write to S3.",
      "C": "Use AWS Lambda triggered on S3 to clean text and store lengths in DynamoDB.",
      "D": "Use Glue DataBrew recipes: remove HTML pattern, lowercase, compute new column via formula."
    },
    "explanation": "DataBrew\u2019s formula for length may be limited; Data Wrangler custom node allows running Python code for HTML stripping and length calculation."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "Your model benefits from interaction terms between a date and a numeric feature. You need to encode weekday \u00d7 metric interactions. Which pipeline component choice is correct?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use DataBrew: create numeric \u00d7 weekday formula recipes.",
      "B": "Use Glue ETL: write PySpark to compute interaction terms.",
      "C": "Use a SageMaker ProcessingStep with a Pandas script to extract weekday and multiply by the metric, then export.",
      "D": "Use Data Wrangler: extract weekday and use the \u201cGenerate feature combinations\u201d transform."
    },
    "explanation": "Data Wrangler does not auto-generate cross features; a ProcessingStep with Pandas gives precise control for interaction terms."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You have high-cardinality ordinal data where label encoding might mislead the model. You need ordinal encoding that respects order but maps to a uniform distribution. Which transform do you apply?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use one-hot encoding after sorting ordinal values.",
      "B": "Use Quantile ordinal encoding: map ordinal rank to quantile value between 0 and 1 via Data Wrangler\u2019s custom transform.",
      "C": "Use label encoding directly.",
      "D": "Use binary encoding to reduce dimensionality."
    },
    "explanation": "Quantile ordinal encoding maps each ordinal category to its quantile position, preserving order without magnitude assumptions."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You need to integrate SageMaker Ground Truth labeled data into your feature-engineering flow in Data Wrangler, then merge those labels with features. Which pattern works?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Import the Ground Truth output manifest into Data Wrangler via import manifest, join on record IDs, and proceed with transforms.",
      "B": "Use Glue ETL to read manifest JSON and your features, join, then export to S3.",
      "C": "Use a ProcessingJob to read S3 manifest and feature data and merge in code.",
      "D": "Configure DataBrew to read manifest and features and merge via project join recipes."
    },
    "explanation": "Data Wrangler can directly import a Ground Truth manifest and join on record identifiers within the flow."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "During feature engineering, you need to normalize numeric fields per group (e.g., by customer segment) to zero mean within each group. Which service supports this grouping and transform without custom code?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "SageMaker Processing job with Pandas groupby and transform.",
      "B": "AWS Glue Streaming ETL or Glue batch ETL? Actually Data Wrangler? But grouping per segment: Data Wrangler supports groupby transforms in custom? It doesn't. So correct: ProcessingStep.",
      "C": "SageMaker Data Wrangler: add an \u201cAggregate\u201d transform with groupby and join back.",
      "D": "AWS Glue DataBrew: add \u201cGroup\u201d recipe with normalization."
    },
    "explanation": "Data Wrangler does not natively support group-based normalization; you need a custom script in a ProcessingStep to group and normalize."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You want to cleanse geographical coordinate outliers using interquartile range and then project lat/long into distance features from a fixed point. Which pipeline step do you use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A SageMaker ProcessingStep running a Python script that computes IQR, filters outliers, then applies haversine distance formula.",
      "B": "Data Wrangler recipes: outlier removal and geospatial transform.",
      "C": "Glue DataBrew: percentile filter and custom geospatial formula.",
      "D": "Lambda functions for geospatial cleaning and a Data Wrangler step for distance."
    },
    "explanation": "No managed transform for geospatial distance; ProcessingStep with Python gives full control for outlier filtering and distance computation."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A categorical feature contains hierarchical JSON structures per row. You need to flatten to multiple indicator features at any depth. Which approach scales best with minimal code?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Processing with a recursive JSON flattener in Python.",
      "B": "Use AWS Glue ETL on Spark with from_json and explode operations to flatten, then OneHotEncoderEstimator.",
      "C": "Use DataBrew JSON flatten recipe then one-hot encode.",
      "D": "Use Data Wrangler custom transform to flatten JSON manually."
    },
    "explanation": "Glue ETL\u2019s built-in from_json and explode support scalable JSON flattening; Spark\u2019s OneHotEncoder completes encoding."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You must generate time-lagged features (t-1, t-2) for a time series dataset with irregular intervals. Which solution handles irregular sampling properly?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Use Glue ETL with window functions assuming regular partitions.",
      "B": "Use Data Wrangler\u2019s lag transform with fixed intervals.",
      "C": "Use a ProcessingStep with pandas shift (but assumes uniform index).",
      "D": "Use a SageMaker Processing job with custom script that resamples by timestamp and computes lag based on nearest previous record."
    },
    "explanation": "Irregular intervals require custom resampling and lag logic in code, best done in a ProcessingStep."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "Your model requires polynomial and interaction features, missing-value flags, and one-hot encodings in one cohesive flow. Which AWS-managed solution is most integrated?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Glue ETL job with custom PySpark combining all transformations.",
      "B": "SageMaker Processing job orchestrating multiple scripts.",
      "C": "SageMaker Data Wrangler flow: use built-in polynomial, fill missing flag, encoding, and export to Feature Store.",
      "D": "DataBrew project with custom recipes for each transformation."
    },
    "explanation": "Data Wrangler supports polynomial features, missing-value flagging, and categorical encodings in a single flow with direct exports."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A bank is building a credit approval model. The training dataset contains a binary \u201capproved\u201d label and a protected \u201cgender\u201d feature. Before training, the ML engineer needs to quantify label imbalance (difference in approval rates) between male and female applicants. Which SageMaker Clarify configuration correctly measures this metric?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run a Clarify \u201cdataset_analysis\u201d job and specify the \u201cmean_difference\u201d metric for the \u201cgender\u201d feature.",
      "B": "Run a Clarify pre_training_bias job, set facet_name to \u201cgender\u201d, label_values_or_threshold to ['approved'], and include the \u201cdifference_in_proportions\u201d metric.",
      "C": "Run a Clarify post_training_bias job after model training and view SHAP importances for \u201cgender.\u201d",
      "D": "Use SageMaker Model Monitor baseline for data quality to detect label skew."
    },
    "explanation": "Pre-training bias jobs with facet_name and difference_in_proportions compute DPL for the protected feature before training."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A healthcare provider must de-identify PHI in free-text clinical notes stored on S3 before any ML processing. Which AWS service and approach automatically finds and redacts PHI entities in text files?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon Comprehend Medical de-identification API to detect and redact PHI, then store de-identified text back in S3.",
      "B": "Use AWS Glue DataBrew PII masking transform to remove PHI patterns.",
      "C": "Use Amazon Macie to classify S3 objects and replace PHI entities.",
      "D": "Run a SageMaker Clarify data quality job to identify and filter PHI."
    },
    "explanation": "Comprehend Medical de-identification API is purpose-built for PII/PHI redaction in clinical text."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "An ML engineer uses an Amazon EFS file system to store HIPAA-regulated data for SageMaker training. To meet compliance, they must encrypt data at rest and in transit. Which configuration satisfies both requirements?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable EFS encryption at rest with a customer-managed CMK, create EFS mount targets with TLS, and reference the EFS file system in the training job.",
      "B": "Enable S3 SSE-KMS on the EFS bucket and configure SageMaker network isolation.",
      "C": "Switch to FSx for Lustre with in-transit encryption enabled and mount in training job.",
      "D": "Enable encryption on the training instance root volume and use HTTPS to access EFS."
    },
    "explanation": "EFS supports encryption at rest via CMKs and in-transit TLS mount targets; training jobs can mount encrypted EFS directly."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A regression dataset has 20% missing values in the numeric \u201cincome\u201d column. The engineer must impute missing values by median without leaking test information. Using AWS DataBrew, which sequence of steps is correct?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Profile full dataset to compute global median, then impute \u201cincome\u201d on combined data and split into train/test.",
      "B": "First split dataset into train/test in DataBrew, then apply a recipe step on both sets using the median calculated from the training set.",
      "C": "Run a SageMaker Processing job with scikit-learn SimpleImputer on combined data, then split.",
      "D": "Use AWS Glue ETL to aggregate median and fill missing values, then split datasets."
    },
    "explanation": "To avoid leakage you must split first, compute median on training data, and apply same imputation to test."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "Before training, an engineer must verify that the \u201cuser_id\u201d column in CSV files is non-null and unique. Which AWS Glue Data Quality configuration accomplishes this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Define a Data Quality ruleset with NOT_NULL and UNIQUENESS rules on \u201cuser_id\u201d, run a Data Quality job and inspect violations.",
      "B": "Use SageMaker Data Wrangler profile and manually inspect unique count for \u201cuser_id.\u201d",
      "C": "Write a SageMaker Processing pandas script to drop nulls and duplicates.",
      "D": "Use AWS Glue DataBrew with a data quality job and assertions on \u201cuser_id.\u201d"
    },
    "explanation": "AWS Glue Data Quality jobs with built-in rules handle NOT_NULL and UNIQUENESS assertions at scale."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A large S3 dataset may contain anomalous rows that degrade model accuracy. Which AWS tool combination efficiently detects and removes outliers across multiple features during preprocessing?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Data Wrangler to profile the dataset, apply the Outlier Detection transform, and add recipe steps to filter anomalies.",
      "B": "Write custom Spark in AWS Glue to compute z-scores and drop outliers then load cleaned data to S3.",
      "C": "Run a SageMaker Clarify data bias job to detect feature outliers and filter.",
      "D": "Use AWS Glue Data Quality to generate anomaly reports and manually remove rows."
    },
    "explanation": "Data Wrangler\u2019s built-in outlier detection recipe automates detection and removal within the ETL workflow."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "The S3 training data files are partitioned by date, causing temporal ordering bias. To randomize samples across partitions in SageMaker training with minimal overhead, which option is best?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Specify ShuffleConfig with \u201cFullyReplicated\u201d in the S3 input channel of the training job.",
      "B": "Use AWS Glue DataBrew to shuffle rows within each file and write to new S3 files.",
      "C": "Run a SageMaker Processing job using AWS Wrangler to shuffle and rewrite the dataset.",
      "D": "Use Lambda to concatenate and shuffle data before training."
    },
    "explanation": "SageMaker\u2019s native ShuffleConfig on S3 inputs randomizes across objects without extra ETL."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A dataset contains an \u201cssn\u201d column that must be irreversibly hashed before training. Which AWS service and transform securely performs this operation with minimal code?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use AWS Glue DataBrew \u201cMaskValue\u201d transform on \u201cssn\u201d column with SHA-256 salt and export the recipe.",
      "B": "Write a SageMaker Processing job in Python to hash values and save to S3.",
      "C": "Use AWS Glue Spark ETL with hash functions to overwrite values.",
      "D": "Use SageMaker Clarify anonymization to drop the \u201cssn\u201d field."
    },
    "explanation": "DataBrew\u2019s MaskValue step supports cryptographic hashing with salt in a low-code recipe."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "In a fraud dataset, the minority class is only 1% of records. The engineer wants to quantify this imbalance via SageMaker Clarify and then mitigate it. Which metric and mitigation approach should be used?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use the ClassImbalance metric to measure ratio and apply SMOTE to oversample the minority class.",
      "B": "Use DifferenceInProportions metric and apply random undersampling of the majority class.",
      "C": "Use KLDivergence metric to detect drift and add class weights during training.",
      "D": "Use MutualInformation to rank features and drop low-importance ones."
    },
    "explanation": "ClassImbalance gives the minority ratio; SMOTE is a standard synthetic oversampling method to correct it."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "An organization mandates that all S3 input data for SageMaker training be encrypted with a customer-managed CMK. How should the training job be configured to honor SSE-KMS on input channels?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "In InputDataConfig for each S3 channel, set S3DataDistributionType to \u201cFullyReplicated\u201d and specify KmsKeyId with the CMK ARN.",
      "B": "Enable EncryptVolume on the training instance and rely on default S3 key.",
      "C": "Set EnableNetworkIsolation to true to force KMS encryption.",
      "D": "Configure OutputDataConfig KmsKeyId \u2013 input will inherit the same key."
    },
    "explanation": "Specifying KmsKeyId on InputDataConfig ensures SageMaker uses SSE-KMS to decrypt input data with the CMK."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A deployed ML model processes IoT telemetry. To detect production input drift and alert when numeric features shift beyond a threshold, which AWS service and metric should be used?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Model Monitor with a data quality JobDefinition that uses the KLDivergence metric.",
      "B": "Use SageMaker Clarify explainability to monitor SHAP drift in features.",
      "C": "Use AWS Config to evaluate drift rules on input buckets.",
      "D": "Use AWS CloudWatch anomaly detection on CPU usage of endpoint."
    },
    "explanation": "Model Monitor\u2019s data quality jobs with KLDivergence compare production vs. baseline distributions to detect drift."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A dataset contains an ordinal \u201crating\u201d feature encoded as text (\u201cpoor\u201d, \u201caverage\u201d, \u201cgood\u201d, \u201cexcellent\u201d). To prepare for numeric modeling and avoid bias, which AWS DataBrew transform sequence is correct?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "One-hot encode \u201crating\u201d directly in DataBrew recipe.",
      "B": "Apply label encoding mapping \u201cpoor\u201d\u21920,\u2026\u201cexcellent\u201d\u21923 then one-hot encode in training.",
      "C": "Define custom map transform in DataBrew to assign ordinal values (0\u20133) preserving order, and leave as numeric.",
      "D": "Use SageMaker Clarify to auto-encode text categories during training."
    },
    "explanation": "Ordinal features should be mapped to numeric preserving order; one-hot would lose ordinality."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A regulated dataset includes random noise in a \u201csalary\u201d column to preserve privacy. Before training, the engineer needs to detect abnormal noise levels that might distort the model. Which tool and check accomplish this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Glue DataBrew to profile mean and std deviation of \u201csalary\u201d across batches.",
      "B": "Use SageMaker Clarify data quality job with \u201cstatistics\u201d check to compare batch distributions to baseline.",
      "C": "Use SageMaker Model Monitor feature drift job with PSI metric on \u201csalary.\u201d",
      "D": "Use AWS Glue Data Quality anomaly detection rule on \u201csalary.\u201d"
    },
    "explanation": "Clarify data quality jobs support statistics checks to compare current vs. baseline distributions for numeric features."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A ML pipeline ingests user-submitted text that may contain offensive words. The engineer must remove any rows containing profanity before training. Which approach is most automated?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Glue DataBrew FilterRows with a profanity regex.",
      "B": "Run a custom SageMaker Processing job calling an external profanity API.",
      "C": "Use AWS Glue DataBrew with built-in \u201cReplaceText\u201d transform referencing a profanity wordlist to flag and drop rows.",
      "D": "Use SageMaker Clarify to identify toxic text and filter."
    },
    "explanation": "DataBrew\u2019s ReplaceText transform and conditional filter can flag profanity and drop offending rows in a recipe."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A sensitive column \u201cemail\u201d must be pseudonymized by replacing with reversible tokens, ensuring only the training job can reverse it. Which design below meets this requirement securely?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Use DataBrew MaskValue transform with a static salt stored in S3.",
      "B": "Use SageMaker Processing with Python to hash emails and store hash key on the notebook.",
      "C": "Use AWS Glue ETL to encrypt email using default KMS key without rotation.",
      "D": "Use a SageMaker Processing job that calls AWS KMS GenerateDataKey to encrypt each email, storing ciphertext and encrypted data key columns."
    },
    "explanation": "GenerateDataKey provides a data key for encrypt/decrypt; processing job can encrypt email with the data key and store encrypted data key per row."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A health insurance company needs to predict claim costs using tabular data with dozens of numeric and categorical features. The model must be explainable for compliance and deployed quickly without custom code. Which modeling approach should the ML engineer choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a deep neural network in SageMaker with a custom PyTorch script.",
      "B": "Use the SageMaker built-in XGBoost linear learner in linear-learner mode.",
      "C": "Deploy an Amazon Bedrock foundation model for tabular regression.",
      "D": "Use SageMaker JumpStart to fine-tune a random forest model."
    },
    "explanation": "The SageMaker linear learner in linear mode offers both speed of built-in algorithms and coefficient-based explainability, meeting compliance without custom code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A startup has millions of short text support tickets. They need to automatically categorize them into about ten categories. They require near real-time inference and minimal infrastructure management. Which approach is most appropriate?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Use SageMaker built-in Latent Dirichlet Allocation (LDA) to extract topics.",
      "B": "Train a K-means clustering model on bag-of-words features.",
      "C": "Use Amazon Comprehend custom classification endpoints.",
      "D": "Fine-tune a SageMaker JumpStart GPT model for classification."
    },
    "explanation": "Amazon Comprehend custom classification provides a managed, low-latency service optimized for text classification without heavy infrastructure or custom training."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A financial firm must model customer churn using a dataset of 100,000 rows and 200 features, many of which are sparse. They need both high accuracy and moderate interpretability. Which algorithm should they select?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Regularized gradient-boosted trees with SHAP explainability in SageMaker.",
      "B": "Deep neural network with attention layers in SageMaker Training.",
      "C": "K-nearest neighbors classifier on the full feature set.",
      "D": "SageMaker linear learner without feature interactions."
    },
    "explanation": "Gradient-boosted trees provide strong accuracy on structured data and SHAP integration yields moderate interpretability, balancing both requirements."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A retailer wants to forecast hourly product demand across 500 stores. They need a model that handles seasonality and multiple time series simultaneously with minimal custom development. Which AWS capability should they use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Build a Seq2Seq model in PyTorch with SageMaker script mode.",
      "B": "Use SageMaker built-in DeepAR forecasting.",
      "C": "Fine-tune a Bedrock LLM for time series forecasting.",
      "D": "Use Amazon Forecast with a custom recipe for seasonal demand."
    },
    "explanation": "Amazon Forecast is a managed service specialized in multi-dimensional time series forecasting with built-in seasonality handling and minimal development."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "An e-commerce company needs to detect anomalous orders in real time. The data has 50 continuous and categorical features. They want full control over algorithm selection and hyperparameters. Which modeling approach is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train an Isolation Forest using SageMaker built-in with batch inference enabled.",
      "B": "Deploy Amazon Lookout for Metrics for anomaly detection.",
      "C": "Use SageMaker KNN anomaly detection with default parameters.",
      "D": "Fine-tune a JumpStart anomaly detection model on Amazon Bedrock."
    },
    "explanation": "SageMaker built-in Isolation Forest gives full control over hyperparameters and supports custom batch inference while being managed."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A logistics company wants to segment customers into behavioral clusters using both shipment frequency and cost. They expect clear cluster centroids for interpretability. Which algorithm should they choose?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "DBSCAN clustering in SageMaker Processing.",
      "B": "Hierarchical clustering in SageMaker Spark.",
      "C": "Gaussian Mixture Model with EM in SageMaker.",
      "D": "K-means clustering with the SageMaker built-in algorithm."
    },
    "explanation": "K-means provides clear centroids and is available as a built-in SageMaker algorithm, balancing interpretability and managed operation."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A media company needs to recommend movies using collaborative filtering on user-item ratings. They require a scalable solution integrated with SageMaker. Which modeling approach fits?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a matrix factorization model in Scikit-Learn on SageMaker Training.",
      "B": "Use Amazon Personalize batch recommendations.",
      "C": "Use SageMaker built-in factorization machines algorithm.",
      "D": "Fine-tune a JumpStart recommender system foundation model."
    },
    "explanation": "SageMaker factorization machines are optimized for collaborative filtering, scalable, and integrated into SageMaker workflows."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A biotech startup must classify protein sequences. Labeled data is scarce, and interpretability is not critical. They want to leverage pretrained models. Which approach is most appropriate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a CNN from scratch in SageMaker script mode.",
      "B": "Fine-tune a JumpStart protein foundation model.",
      "C": "Use SageMaker built-in XGBoost with k-mer features.",
      "D": "Develop a random forest on physicochemical features."
    },
    "explanation": "JumpStart protein foundation models leverage pretrained knowledge, requiring limited data and engineering, ideal for scarce labels without interpretability needs."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A compliance-driven enterprise must predict default risk on personal loans. They need to justify each decision to auditors. Which modeling approach balances accuracy and full explainability?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker built-in linear learner and present feature coefficients.",
      "B": "Train an XGBoost model and approximate SHAP explanations.",
      "C": "Deploy an Amazon Bedrock LLM to explain historical decisions.",
      "D": "Use DeepAR to forecast default probabilities."
    },
    "explanation": "Linear learner provides transparent coefficients for direct auditing, ensuring full explainability though with modest accuracy trade-off."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A gaming company needs to classify images of player avatars into categories. They want to minimize GPU cost and only require coarse-grained accuracy. Which SageMaker option should they choose?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Fine-tune a JumpStart ResNet model.",
      "B": "Train a custom CNN in PyTorch on GPU.",
      "C": "Use SageMaker built-in image classification algorithm on CPU.",
      "D": "Deploy Amazon Rekognition with custom labels."
    },
    "explanation": "The built-in image classifier can run on CPU for coarse tasks with lower cost, avoiding GPU and heavy customization."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "An IoT company collects multivariate sensor time series and needs to detect anomalies streaming. They prefer managed services and minimal model maintenance. Which approach is best?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Train an LSTM autoencoder in SageMaker script mode.",
      "B": "Use the SageMaker built-in random cut forest algorithm in batch.",
      "C": "Fine-tune an LLM for time series anomaly detection.",
      "D": "Use Amazon Lookout for Equipment for streaming anomalies."
    },
    "explanation": "Amazon Lookout for Equipment is managed, supports streaming anomaly detection on sensor data with minimal maintenance."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A marketing team wants to segment emails by topic automatically. They need a fully managed NLP solution that adapts over time. Which AWS service should they use?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Train LDA in SageMaker Processing.",
      "B": "Use Amazon Comprehend topic modeling APIs.",
      "C": "Fine-tune a JumpStart GPT-3 model for topic extraction.",
      "D": "Use Amazon SageMaker BlazingText for unsupervised topic clusters."
    },
    "explanation": "Amazon Comprehend provides managed topic modeling that adapts to new data without custom training."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A telecom wants to predict customer churn with an imbalanced dataset and needs to prioritize recall over precision. They plan to tune thresholds. Which algorithm and metric combination should they choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train XGBoost in SageMaker and optimize for recall using hyperparameter tuning with custom F-beta metric.",
      "B": "Use logistic regression and evaluate accuracy.",
      "C": "Deploy random forest and adjust class weights only at inference.",
      "D": "Use built-in factorization machines with default evaluation."
    },
    "explanation": "XGBoost with custom metric optimization allows tuning hyperparameters for recall, matching business priority on false negatives."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A media analytics team needs to extract entities from video transcripts. They require deep customization of entity categories and managed model hosting. What should they choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a custom spaCy NER model on SageMaker Processing.",
      "B": "Fine-tune a JumpStart language model and deploy on SageMaker.",
      "C": "Use Amazon Comprehend custom entity recognition endpoints.",
      "D": "Use Amazon Textract to extract text and regex on transcripts."
    },
    "explanation": "Comprehend custom entity recognition offers managed training and hosting with customizable entities, minimizing infra effort."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A fraud detection use case has a growing number of streaming events per second and requires sub-millisecond inference. They need high throughput and low latency with minimal operational overhead. Which approach fits?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "Deploy XGBoost model on SageMaker real-time endpoint on CPU.",
      "B": "Use SageMaker serverless inference with ProvisionedConcurrency.",
      "C": "Deploy model on ECS Fargate behind API Gateway.",
      "D": "Use Lambda to host model via Docker image."
    },
    "explanation": "Serverless inference with provisioned concurrency handles variable volumes and low latency without managing servers."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A retailer wants price elasticity estimates per product category, requiring interpretable coefficients. They have limited compute budget. Which modeling approach should they select?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Train a tree-based ensemble and use SHAP.",
      "B": "Fine-tune a Bedrock regression foundation model.",
      "C": "Use SageMaker XGBoost regressor default.",
      "D": "Use SageMaker linear learner with L1 regularization for sparse features."
    },
    "explanation": "Linear learner yields direct coefficients for price elasticity, with L1 handling sparsity and minimal compute overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "An ML engineer must automatically classify customer reviews into positive or negative sentiment. They need near zero code and managed endpoints. Which AWS service should they choose?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Build a logistic regression in SageMaker script mode.",
      "B": "Use Amazon Comprehend sentiment analysis endpoints.",
      "C": "Fine-tune a JumpStart BERT model.",
      "D": "Train SageMaker built-in BlazingText skip-gram model."
    },
    "explanation": "Comprehend provides out-of-the-box sentiment analysis API with no code and managed hosting."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A company has sparse high-dimensional clickstream data and wants a scalable algorithm for binary classification. They prioritize L1 regularization to perform feature selection. Which SageMaker built-in algorithm should they choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker built-in linear learner with L1 mode.",
      "B": "Built-in XGBoost with default settings.",
      "C": "Built-in KNN.",
      "D": "Built-in K-means."
    },
    "explanation": "Linear learner supports L1 regularization for feature selection and scales to high dimensions."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A healthcare provider needs to detect disease from X-ray images with regulatory constraints on data privacy. They want to use a managed service and avoid moving data off-premises. Which solution is best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker built-in image classification in public endpoints.",
      "B": "Deploy a JumpStart Vision transformer in SageMaker Studio.",
      "C": "Configure SageMaker Inference on AWS Outposts with the built-in image classification algorithm.",
      "D": "Use Amazon Rekognition for custom labels."
    },
    "explanation": "SageMaker on AWS Outposts runs locally, preserving data privacy without sending images off-premises."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A startup needs to detect default probability from loan applications with limited GPU budget. They require class probability outputs and fast training. Which algorithm should they pick?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deep AR forecasting model.",
      "B": "Built-in XGBoost classifier.",
      "C": "Built-in K-means clustering.",
      "D": "Built-in PCA."
    },
    "explanation": "XGBoost classifier offers fast training on CPU/GPU, outputs probabilities, and is cost-efficient."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A legal firm wants to identify key contract clauses in documents. They need a managed NLP solution they can tailor without hosting infrastructure. Which should they use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Comprehend custom classification for clause categories.",
      "B": "Fine-tune JumpStart GPT-3 and host on SageMaker.",
      "C": "Use SageMaker built-in BlazingText.",
      "D": "Train a custom BERT model in SageMaker script mode."
    },
    "explanation": "Comprehend custom classification lets them define clause categories and uses managed endpoints without infra overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A search engine needs to rank results by relevance using user click logs. They require pairwise ranking and integration with SageMaker. Which algorithm is suitable?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Built-in random forest classifier.",
      "B": "Built-in KNN algorithm.",
      "C": "DeepAR forecasting.",
      "D": "SageMaker Factorization Machines with ranking objective."
    },
    "explanation": "Factorization Machines support pairwise ranking objectives and integrate seamlessly in SageMaker pipelines."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "An agricultural startup wants to detect crop diseases from leaf images. They have limited labeled data and need transfer learning with minimal fine-tuning. Which approach should they choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a CNN from scratch in PyTorch on SageMaker.",
      "B": "Use SageMaker built-in XGBoost on flattened image features.",
      "C": "Fine-tune a JumpStart transfer learning image classification model.",
      "D": "Deploy Amazon Rekognition Custom Labels."
    },
    "explanation": "JumpStart transfer learning models require minimal labeled data and fine-tuning for image tasks, reducing effort."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A robotics company collects multi-modal sensor data (images and lidar). They need a model that natively handles heterogeneous inputs. Which modeling approach is best?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "Ensemble separate built-in algorithms for each modality.",
      "B": "Train a multimodal PyTorch model in SageMaker script mode.",
      "C": "Use Amazon SageMaker built-in Linear Learner with concatenated features.",
      "D": "Fine-tune a JumpStart text foundation model."
    },
    "explanation": "A custom PyTorch script enables combining different data types in a single model, which built-ins cannot natively handle."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A voice assistant service must detect five spoken commands in streaming audio. They need low latency and a managed solution. Which AWS offering should they use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a CNN in SageMaker to classify spectrograms.",
      "B": "Use SageMaker built-in KNN on MFCC vectors.",
      "C": "Fine-tune a JumpStart Transformer.",
      "D": "Use Amazon Transcribe to convert speech to text and Comprehend custom classification."
    },
    "explanation": "Transcribe plus Comprehend custom classification offers managed low-latency command detection without building audio models."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A rideshare company needs to price surge multipliers based on real-time location and demand. They require sub-second inference at scale. Which modeling approach and deployment should they choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a decision tree in SageMaker and deploy on CPU endpoint.",
      "B": "Use SageMaker real-time multi-model endpoints with XGBoost model.",
      "C": "Host a neural network on ECS Fargate.",
      "D": "Use Amazon Forecast predictor endpoint."
    },
    "explanation": "XGBoost on multi-model endpoints provides low-latency at scale with minimal endpoint overhead for multiple regions."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "A financial services company observes that their fraud-detection model has high overall accuracy but low recall on high-value transactions. They suspect the model underfits rare, high-value cases. Which of the following evaluation strategies best identifies this issue?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Plot a single ROC curve on the full dataset and inspect AUC",
      "B": "Compute macro-averaged precision across all transaction values",
      "C": "Segment the test set by transaction value and compute per-segment recall",
      "D": "Evaluate overall F1 score without stratification"
    },
    "explanation": "Segmenting by transaction value and computing recall highlights underperformance on high-value cases. Overall AUC or aggregated metrics mask segment-specific issues."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "An ML engineer deploys two XGBoost models: a control and a new variant. After 1M predictions, the variant shows a slightly higher latency but 2% higher recall. How should they evaluate if this trade-off is acceptable?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Compare single-sample inference logs for outliers",
      "B": "Plot precision-recall curves and latency distributions for both variants",
      "C": "Compute overall accuracy for both and pick model with higher accuracy",
      "D": "Use confusion matrix of the slower model only"
    },
    "explanation": "Precision-recall curves reveal how recall gain trades off against precision, while latency distributions quantify performance cost. Overall accuracy and single-sample latency don\u2019t capture distribution details."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "During hyperparameter tuning of a neural network, SageMaker Debugger reports gradient vanishing in early layers. Which metric or visualization should you use to pinpoint the affected layers?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Plot TensorBoard histograms of weight and gradient distributions per layer",
      "B": "Inspect model\u2019s overall loss curve only",
      "C": "Compute validation accuracy after each epoch",
      "D": "View average prediction confidence on test data"
    },
    "explanation": "Layer-wise weight and gradient histograms directly show vanishing or exploding gradients. Loss curves and accuracy trends are too coarse to locate the problem."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "A classification model shows 95% accuracy but customers complain of frequent false positives. Which metric will best capture the operational false-positive rate?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ROC AUC",
      "B": "Accuracy",
      "C": "Macro F1 score",
      "D": "False positive rate (FP/(FP+TN))"
    },
    "explanation": "False positive rate directly measures proportion of non-events incorrectly flagged. Aggregate metrics and AUC do not quantify FP in production terms."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "An ML engineer wants to compare a shadow variant\u2019s performance to production. They have stored logs with predictions and ground truth over a week. What\u2019s the most reproducible approach?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Re-run inference on production endpoint and compute metrics",
      "B": "Compute metrics in real time on live data",
      "C": "Use the stored logs to batch calculate identical evaluation metrics for both models",
      "D": "Use shadow variant accuracy reported in CloudWatch metrics"
    },
    "explanation": "Batch calculating evaluation metrics on the same logged data ensures reproducibility. CloudWatch reports may differ in metric definitions, and re-running inference can produce drift."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "After deploying a regression model, business users report that error increases when feature \"loan_amount\" exceeds $100k. Which diagnostic step is most appropriate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Check overall RMSE",
      "B": "Plot residuals against loan_amount",
      "C": "Compute R\u00b2 on training data",
      "D": "Re-train model without loan_amount"
    },
    "explanation": "Residual-vs-feature plots reveal heteroscedasticity or feature-specific error patterns. Overall RMSE or training R\u00b2 mask inequalities across feature ranges."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "SageMaker Clarify indicates a high feature importance contribution from \"zipcode\" in loan approval model. Finance team worries about proxy bias. What evaluation should be done next?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Compute disparity metrics for protected groups defined by zipcode demographics",
      "B": "Remove zipcode and retrain immediately",
      "C": "Ignore since importance doesn\u2019t imply bias",
      "D": "Compute overall feature correlation matrix"
    },
    "explanation": "High importance for zipcode may proxy demographic bias; computing disparity metrics for protected groups reveals actual bias. Simply removing the feature may degrade performance without confirming bias."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "A model\u2019s AUC on validation is 0.85 but drops to 0.75 in production. Which step best isolates the cause?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase regularization to reduce overfitting",
      "B": "Retrain with more epochs",
      "C": "Change the threshold to match validation TPR",
      "D": "Compare feature distribution between validation and production using Clarify data drift reports"
    },
    "explanation": "Distribution drift analysis identifies covariate changes causing performance drop. Regularization and threshold tuning don\u2019t address data mismatch."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "During batch experiments, an engineer notices that increasing tree depth raises precision but lowers recall. Which plot best illustrates this trade-off?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Precision-recall curve parametrized by depth",
      "B": "ROC curve at each depth",
      "C": "Confusion matrix for the deepest tree only",
      "D": "Histogram of prediction confidences"
    },
    "explanation": "Precision-recall curves for each depth show the precision/recall trade-off directly. ROC curves and single-depth confusion matrices do not illuminate that trade dynamically."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "After hyperparameter tuning, two models have identical validation loss but different generalization gaps. Which metric indicates stronger overfitting?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Higher training loss",
      "B": "Larger difference between training and validation accuracy",
      "C": "Higher validation AUC",
      "D": "Lower inference latency"
    },
    "explanation": "A large gap between training and validation accuracy indicates overfitting. Validation AUC and latency don\u2019t measure overfitting directly."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "An NLP classification model shows significantly higher F1 for short texts versus long texts. How do you quantify performance disparity?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Overall F1 score",
      "B": "Word count distribution histogram",
      "C": "Group test samples by text length bins and compute per-bin F1",
      "D": "Pearson correlation between length and predicted label"
    },
    "explanation": "Bin by text length and compute F1 per bin to quantify disparity. Overall metrics and correlation don\u2019t isolate the problem."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You need to determine if a drop in model accuracy after a code change is due to random seed variance or genuine performance loss. Which practice ensures reproducibility?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Fix random seed across training, log model parameters, and compare artifacts",
      "B": "Re-run training with different seeds and average",
      "C": "Use early stopping to stabilize performance",
      "D": "Only compare inference results on live data"
    },
    "explanation": "Fixing seeds and logging artifacts ensures reproducible runs. Averaging across seeds helps smooth noise but doesn\u2019t isolate change impact."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "An engineer suspects label leakage because a high validation AUC is too good to be true. Which diagnostic will reveal leakage?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase regularization",
      "B": "Shuffle labels and retrain to observe AUC",
      "C": "Add more layers",
      "D": "Compute feature correlations only"
    },
    "explanation": "Shuffling labels destroys true signal; if AUC remains high, leakage exists. Feature correlations alone may miss leakage patterns."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "A deployed model exhibits high memory consumption during inference. Which SageMaker Debugger profiler metric helps pinpoint bottlenecks?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Overall endpoint invocation count",
      "B": "CloudWatch memory usage graphs",
      "C": "Inference latencies per request",
      "D": "Model container\u2019s PeakHostMemoryUsage metric"
    },
    "explanation": "PeakHostMemoryUsage from Debugger profiling identifies model container memory peaks. Invocation counts and latency don\u2019t measure memory usage.\n"
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "When comparing two regression models, you find one has lower MAE but higher RMSE. What does this indicate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "The second model has larger outlier errors",
      "B": "The second model performs better on average",
      "C": "The first model is overfitting",
      "D": "The first model has higher variance"
    },
    "explanation": "Higher RMSE than MAE implies presence of larger outlier errors, since RMSE penalizes them more. Average performance may be similar.\n"
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You deploy an image classifier and monitor per-class recall. Class A recall drops below SLA overnight. Logs show no data drift. What is your next diagnostic?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase batch size",
      "B": "Retrain with more epochs",
      "C": "Analyze inference input distribution for class A by sampling inputs",
      "D": "Reduce learning rate"
    },
    "explanation": "Sampling inputs checks if unexpected inputs are arriving for Class A. No drift in overall features doesn\u2019t guarantee class-specific input change.\n"
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your model\u2019s production log shows many low-confidence predictions. Which threshold-based method helps maintain production SLA?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use maximum probability per sample as a calibration correction",
      "B": "Implement a confidence threshold to send low-confidence cases to manual review",
      "C": "Retrain until all predictions exceed a fixed threshold",
      "D": "Always return top-2 classes to clients"
    },
    "explanation": "A confidence threshold with fallback to manual review maintains SLA. Calibration alone won\u2019t meet SLA guarantees.\n"
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "An ML pipeline reruns hyperparameter tuning daily. You want to compare results over time. Which practice aids trend analysis?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Log only best tuning job\u2019s metrics",
      "B": "Store only final model artifacts",
      "C": "Compare only today's and yesterday's tuning graphs",
      "D": "Archive tuning job metrics and hyperparameter configurations in SageMaker Experiments"
    },
    "explanation": "SageMaker Experiments archives metrics and configs over time, enabling trend and retrospective analysis. Comparing partial data isn\u2019t sufficient.\n"
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "After retraining a model with more data, confusion matrix shows increased FN but decreased FP. Business impact of FNs is higher. How do you adjust the model?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Shift decision threshold to favor recall over precision",
      "B": "Increase regularization",
      "C": "Reduce training data size",
      "D": "Switch to a different algorithm"
    },
    "explanation": "Threshold adjustment trades precision and recall bias to reduce FNs, aligning with higher cost of FNs. Algorithm shift or data size changes are heavier-handed.\n"
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "SageMaker Clarify reports a DPL (difference in proportions of labels) of 0.12 for a protected group. Which conclusion is correct?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model is fair, since DPL < 0.2",
      "B": "There is moderate demographic bias requiring mitigation",
      "C": "DPL only applies to continuous features",
      "D": "Bias can be ignored if overall accuracy is high"
    },
    "explanation": "DPL of 0.12 indicates non-trivial bias that should be addressed. Fairness thresholds vary, but non-zero DPL warrants mitigation. Accuracy doesn\u2019t nullify bias.\n"
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You need to diagnose why training loss plateaus early. Which combined SageMaker Debugger rule is most helpful?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "LossNotDecreasing and LargeGradient",
      "B": "VanishingGradient and OverTraining",
      "C": "LossNotDecreasing and VanishingGradient",
      "D": "OverTraining and HighThroughput"
    },
    "explanation": "LossNotDecreasing flags plateau, VanishingGradient identifies small gradients. Together they pinpoint stalled training due to gradient issues.\n"
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "After enabling endpoint data capture, you observe data skew for feature X. Which remediation reduces skew immediately?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Retrain online with captured data",
      "B": "Increase endpoint instance count",
      "C": "Switch to a different instance type",
      "D": "Implement input feature transformation to normalize X at inference"
    },
    "explanation": "Applying transformation at inference normalizes skewed feature values immediately. Retraining is longer-term and won\u2019t reduce skew on streaming data.\n"
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your binary classifier has high variance between folds in cross-validation. What change will most reduce variance?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use ensembles like random forests",
      "B": "Increase learning rate",
      "C": "Use fewer features",
      "D": "Reduce training set size"
    },
    "explanation": "Ensembling reduces model variance by averaging multiple learners. Lower LR or fewer features may increase bias. Reducing data increases variance.\n"
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "A class-imbalanced dataset yields high recall but poor precision. Which metric-guided tuning step helps restore balance?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase tree depth",
      "B": "Use mean squared error loss",
      "C": "Optimize model using precision-recall AUC as objective",
      "D": "Switch to accuracy objective"
    },
    "explanation": "Using PR AUC aligns hyperparameter tuning to both precision and recall on imbalanced data. MSE or accuracy objectives aren\u2019t appropriate for classification imbalance.\n"
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "An LSTM model overfits after 10 epochs. You add dropout but performance still degrades. Which debugging step helps find root cause?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase L2 regularization",
      "B": "Use Debugger\u2019s tensor shape rule to confirm no unintended dimension mismatch causing overfitting",
      "C": "Increase learning rate",
      "D": "Reduce batch size"
    },
    "explanation": "Tensor shape mismatches can cause incorrect weight updates that appear as overfitting. Debugger shape rules detect such bugs. Regularization alone may mask deeper issues.\n"
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your model produces inconsistent outputs when served on GPU versus CPU instances. How do you isolate source?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Retrain separately on GPU and CPU",
      "B": "Enable inference latency metrics",
      "C": "Use SageMaker Clarify to detect drift",
      "D": "Run inference deterministically on both environments with same input batch and compare element-wise outputs"
    },
    "explanation": "Deterministic inference comparison on CPU vs GPU pinpoints numerical instability or library inconsistency. Drift detection and latency metrics don\u2019t isolate environment differences.\n"
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "A regression model\u2019s residuals exhibit non-constant variance. Which transformation corrects this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Standardize all features",
      "B": "Apply log transform to the target variable",
      "C": "Increase polynomial degree of predictors",
      "D": "Use one-hot encoding for categorical features"
    },
    "explanation": "Log-transforming the dependent variable often stabilizes variance (heteroscedasticity). Feature standardization doesn\u2019t address residual variance issues.\n"
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "After changing preprocessing, model performance unexpectedly improved in test but dropped in validation. Which practice uncovers this data leakage?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Inspect preprocessing pipelines for operations applied before train-test split",
      "B": "Increase test set size",
      "C": "Add more regularization",
      "D": "Use k-fold CV on original split"
    },
    "explanation": "Applying preprocessing (e.g., normalization) before splitting leaks test information. Reviewing pipeline order reveals leakage.\n"
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your model\u2019s precision increases when you add a new feature but recall drops. You need to quantify the net business impact. What analysis should you perform?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Compute new overall F1 score",
      "B": "Plot ROC curve",
      "C": "Estimate expected cost change using per-error cost matrix",
      "D": "Compare AUC-PR before/after"
    },
    "explanation": "Cost matrix analysis translates precision/recall changes into business impact. F1 and curves are abstract performance measures without cost context."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A fintech startup has an image-based document verification model that requires sub-100ms inference latency at the edge. They must deploy the optimized model to ARM-based devices with intermittent connectivity. Which deployment infrastructure best meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Host the model on a SageMaker real-time endpoint in a VPC and use IoT Greengrass to pull predictions.",
      "B": "Compile the model with SageMaker Neo for ARM, deploy to IoT Greengrass devices for local real-time inference.",
      "C": "Package the model in a custom Docker container, deploy to AWS Lambda on ARM architecture.",
      "D": "Use a SageMaker serverless endpoint and cache responses on edge devices."
    },
    "explanation": "SageMaker Neo produces highly optimized ARM binaries for local (<100ms) inference. IoT Greengrass ensures offline capability."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An e-commerce application has a recommendation model that sees large traffic spikes during flash sales. It runs in a private VPC, uses GPU instances, and needs multi-model hosting with auto-scaling. Which architecture is optimal?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Deploy separate single-model SageMaker real-time endpoints per model with target-tracking scaling.",
      "B": "Use a SageMaker serverless endpoint with model registry versioning.",
      "C": "Containerize models and host on ECS Fargate behind an ALB with custom scaling scripts.",
      "D": "Use a multi-model SageMaker real-time endpoint in the VPC with provisioned concurrency and auto-scaling policies."
    },
    "explanation": "Multi-model endpoint reduces overhead, shares GPU, and auto-scaling in VPC meets spikes."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A research team needs to batch-process 10 TB of genomic data weekly. They want to minimize cost, use the same container as real-time inference, and avoid idle pods. Which infra is best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create a SageMaker asynchronous endpoint with batch transform inside it.",
      "B": "Deploy models on EKS pods with KNative autoscaling.",
      "C": "Use SageMaker batch transform jobs with the real-time container and spot instances.",
      "D": "Host on a long-running ECS Fargate service and schedule container tasks."
    },
    "explanation": "Batch transform jobs spin up only for jobs, reuse container, support spot to reduce cost."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A global SaaS company requires GDPR compliance. Their NLP model must serve EU-only traffic in dedicated subnets and scale on demand. Which deployment solution is most appropriate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy regional SageMaker real-time endpoints in EU-based subnets with endpoint auto scaling.",
      "B": "Deploy a single SageMaker serverless endpoint and enforce subnet restrictions.",
      "C": "Host on ECS Fargate in one EU cluster and route all traffic through it.",
      "D": "Use Lambda functions behind API Gateway with VPC peering to subnets."
    },
    "explanation": "Regional real-time endpoints in EU subnets ensure data residency and scalable performance."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A startup needs to test a new fraud-detection model in production without risking customer impact. They want to route 5% of traffic to the new model, track performance, and easily roll back. Which deployment strategy and infra should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy two SageMaker asynchronous endpoints and split payloads manually.",
      "B": "Use a SageMaker serverless endpoint for the new model and shift DNS entries.",
      "C": "Deploy the new model to a SageMaker real-time endpoint variant in the same endpoint, configure a canary traffic shift.",
      "D": "Host the new model in ECS Fargate and use Application Load Balancer weighted routing."
    },
    "explanation": "Real-time endpoint variants support canary deployments and easy rollback within the same endpoint."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An advertising platform has multiple small models (<100MB) that share similar preprocessing code. They need to deploy 50 models, optimize resource utilization, and minimize cost. What is the best approach?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy 50 separate real-time SageMaker endpoints.",
      "B": "Use ECS Fargate and spin up a container per model.",
      "C": "Bundle all models into one Docker image and host on a single serverless endpoint.",
      "D": "Use a SageMaker multi-model endpoint with shared container in a single VPC endpoint."
    },
    "explanation": "Multi-model endpoints share the container and infra, reducing cost and overhead for many small models."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A biotech firm must run long-running inference jobs (up to 30 minutes each) that process large data packages. They want predictable scaling and pay-per-use cost. Which SageMaker deployment fits best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure a SageMaker asynchronous inference endpoint with appropriate max payload size.",
      "B": "Use a SageMaker real-time endpoint and extend Lambda timeouts.",
      "C": "Submit each job as a batch transform.",
      "D": "Host it on ECS Fargate with long-running tasks."
    },
    "explanation": "Asynchronous endpoints handle long payloads, non-blocking, scale predictably and charge per call."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A media company needs GPU acceleration for video-related inference but wants to minimize idle GPU costs during off-hours. They can tolerate 1\u20132 second cold start. Which infra is optimal?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Keep a provisioned GPU SageMaker real-time endpoint running with auto-scaling.",
      "B": "Use SageMaker serverless endpoints configured with GPU support.",
      "C": "Deploy on ECS Fargate GPU instances with manual scaling schedules.",
      "D": "Host on Lambda GPU functions behind an ALB."
    },
    "explanation": "Serverless endpoints with GPU avoid idle costs and manage cold starts within tolerance."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An auto manufacturer needs to deploy multiple model versions concurrently for A/B tests, with equal traffic share and easy rollback. They must run in the same VPC. Which infra supports this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy separate ECS services per version behind an ALB.",
      "B": "Use Lambda versions and aliases in VPC.",
      "C": "Deploy separate SageMaker serverless endpoints per version.",
      "D": "Use a SageMaker real-time endpoint with two production variants and traffic weights."
    },
    "explanation": "Production variants in one real-time endpoint allow weighted traffic splitting and quick rollback."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A retailer needs to deploy a recommendation model to handle unpredictable holiday season traffic. They need <50ms latency, GPU inference, and minimal ops overhead. Which solution is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker serverless endpoints with GPU acceleration and auto-scaling.",
      "B": "Deploy a spot-based ECS GPU cluster with custom autoscaler.",
      "C": "Maintain a provisioned GPU real-time endpoint with scheduled scaling.",
      "D": "Run inference in Lambda with EFS-mounted model files."
    },
    "explanation": "Serverless GPU endpoints auto-scale without manual capacity management and meet latency."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A healthcare application requires batch scoring of PHI data with encryption at rest and in transit. They want to reuse existing VPC resources and avoid cross-account data movement. Best infra?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker serverless endpoint in VPC with data capture.",
      "B": "Real-time endpoint in VPC with KMS encryption.",
      "C": "Batch transform jobs configured in the same VPC and encrypted S3.",
      "D": "ECS Fargate with EFS encryption."
    },
    "explanation": "Batch transform in VPC uses S3 encrypted data, no constant endpoint and reuse VPC security."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A gaming company needs to deploy a scoring model to hundreds of edge kiosks with no always-on internet and limited compute. They need occasional connectivity to sync metrics. Which infra fits?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a SageMaker real-time endpoint through AWS IoT Core.",
      "B": "Compile via SageMaker Neo, deploy to Lambda@Edge on Greengrass devices.",
      "C": "Host containerized model on small EC2 instances at each kiosk.",
      "D": "Deploy inference code as AWS Lambda functions invoked by MQTT."
    },
    "explanation": "Neo-compiled model on Greengrass devices runs locally, syncs periodically over IoT Core."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A logistics company has a micro-batch inference requirement: group deliveries into sets of 100 and process every 5 minutes. They want minimal latency overhead and no idle endpoints. What infrastructure should they choose?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy a real-time endpoint and buffer requests into batches.",
      "B": "Use serverless endpoint with batched invocation.",
      "C": "Submit jobs to ECS Fargate tasks scheduled every 5 minutes.",
      "D": "Configure a SageMaker asynchronous inference endpoint with batching config."
    },
    "explanation": "Asynchronous endpoint supports batching and scheduled triggers without idle infra."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A startup wants to experiment with GPU- and CPU-based inference to compare cost-performance trade-offs. They need a common orchestration tool and seamless model promotion. Which approach is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Pipelines to deploy the same model to GPU and CPU real-time endpoints.",
      "B": "Script deployments in Terraform to provision ECS GPU and CPU clusters.",
      "C": "Use AWS Batch for GPU and Lambda for CPU inference.",
      "D": "Manually launch EC2 GPU and CPU instances and copy containers."
    },
    "explanation": "SageMaker Pipelines automates deployment steps, tracks versions, and supports multiple compute targets."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A financial modeling team needs to deploy Python-based models that require custom system libraries. They want minimal management and autoscaling. Which deployment target is most appropriate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy as a SageMaker serverless endpoint with layers.",
      "B": "Use Lambda with custom runtime layer.",
      "C": "Build a custom Docker container and host on SageMaker real-time endpoint in VPC.",
      "D": "Use ECS Fargate with code-build pipelines."
    },
    "explanation": "Custom container on SageMaker real-time endpoint supports all dependencies, auto-scales, VPC support."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A data science team wants to automate their ML workflow so that any code commit on the \u2018develop\u2019 branch runs unit tests, data validation, and triggers a SageMaker training pipeline. After successful training, the model should be registered and automatically deployed to a staging endpoint for integration tests, before manual approval pushes to production. Which CI/CD design best meets these requirements?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use CodePipeline with a Source action on GitHub \u2018develop\u2019, CodeBuild to run unit tests and data validation scripts, a CodeBuild action to invoke a SageMaker Pipeline for training, a stage to register the model in the Model Registry, a CloudFormation action to deploy to staging, an integration-test CodeBuild action, followed by a manual approval and a Lambda action to swap the staging endpoint to production.",
      "B": "Use SageMaker Pipelines with Git integration triggering on \u2018develop\u2019 for training and registration, then use a separate CodePipeline triggered by new Model Registry entries to deploy to staging and run integration tests, with manual approval for production.",
      "C": "Use CodePipeline with a GitHub source on \u2018develop\u2019, a CodeBuild stage to run unit tests and data quality checks, a SageMaker Pipeline action to train and register the model, a CloudFormation stage to deploy to a staging endpoint, a CodeBuild integration-test stage, then a manual approval stage and a CloudFormation canary-deployment stage for production.",
      "D": "Use CodeDeploy directly to push code changes to SageMaker endpoints, adding a CodeDeploy pre-deployment hook to run tests and trigger training before deployment."
    },
    "explanation": "Option C uses CodePipeline native actions for each stage, integrates SageMaker Pipelines, staging, tests, manual approval, and a canary production deployment with minimal glue code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A compliance policy requires monthly retraining of an ML model using new data uploaded to s3://company/data/monthly/ at 1 AM on the first of each month. The workflow must run data quality checks, trigger SageMaker model training, register the model, and send an email if any stage fails. How should you configure this with minimal overhead?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create an EventBridge scheduled rule that invokes a Lambda function which runs data quality checks, starts a SageMaker training job, registers the model, and publishes to SNS on failure.",
      "B": "Create an EventBridge scheduled rule to start a CodePipeline at 1 AM monthly. In CodePipeline, add: a Source stage (S3 polling), a CodeBuild stage for data quality checks, a SageMaker Pipeline invoke stage for training and registration, and a CloudWatch Events action to notify via SNS on failure.",
      "C": "Use SageMaker Pipelines with a Schedule trigger for monthly runs, include DataQualityCheck, Train, Register steps, and configure an SNS Alarm for failed steps.",
      "D": "Use AWS Batch scheduled on EventBridge to run a container that executes the entire workflow and sends SNS notifications."
    },
    "explanation": "Option B leverages EventBridge\u2192CodePipeline schedule, built-in actions (S3 source, CodeBuild, SageMaker Pipeline, SNS) with minimal custom code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "An organization maintains separate dev and prod AWS accounts. They want a centralized CI/CD in dev that, upon merging to master in CodeCommit, triggers a deployment pipeline in prod account to update the SageMaker endpoint. How should they configure this cross-account pipeline?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "In dev account create a CodePipeline with a CloudFormation action targeting prod account using StackSets.",
      "B": "Push artifacts from dev S3 bucket to prod S3 bucket, then have a prod pipeline polling that bucket to start deployment.",
      "C": "Use AWS CodeDeploy cross-account deployments from dev to prod.",
      "D": "In dev CodePipeline, add an IAM role action with a role ARN in prod account. Grant the dev pipeline role permission in prod to assume that role. In prod, that role\u2019s trust policy allows the dev pipeline service role to assume it and run deployment actions."
    },
    "explanation": "Option D correctly uses IAM role assumption to allow a pipeline in one account to execute actions in another account without custom polling."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A team follows Gitflow: feature/* branches, develop, release/*, and master. They want two pipelines: one for develop that runs tests and staging deployments on merges to develop, and one for production on merges to master. How should they filter source triggers in CodePipeline?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use two CodePipelines with GitHub source actions configured: one with branch filter pattern '^refs/heads/develop$' and webhooks enabled, the other with '^refs/heads/master$'.",
      "B": "Use a single pipeline triggered on all branches; in the first stage inspect the branch name and abort if not develop or master.",
      "C": "Use CloudWatch Events on push events with wildcard branch patterns to start pipelines via Lambda.",
      "D": "Configure CodePipeline to poll S3 artifacts tagged with branch names to distinguish branches."
    },
    "explanation": "Option A uses CodePipeline\u2019s built-in branch filters in source actions to isolate triggers per pipeline."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A data engineering pipeline produces a delta file in S3 upon completion. You must ensure that your ML CI/CD pipeline ingests this file, validates schema against a baseline, and fails if mismatched, before triggering training. How do you integrate this into CodePipeline?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Have CodePipeline poll the S3 prefix for the delta file and pass it to a Lambda that does schema validation and returns success/fail.",
      "B": "Trigger CodePipeline with an S3 event notification directly to the pipeline\u2019s StartPipelineExecution API.",
      "C": "Use an EventBridge rule matching the S3 PutObject event to call StartPipelineExecution, then in CodePipeline add a CodeBuild stage that runs a schema-validation script against the new file and fails on mismatch.",
      "D": "Use Lambda to copy the file to a Kinesis stream, use a CodePipeline source action on the stream, and a CodeBuild validation stage."
    },
    "explanation": "Option C uses EventBridge\u2192CodePipeline to start on S3 writes and uses CodeBuild to validate schema, failing if mismatched."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your ML pipeline requires a secret API key available to training jobs at build time. You must not expose the secret in plain text or logs. Which CodePipeline stage configuration meets this requirement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Store the API key in S3 encrypted with KMS. In CodeBuild, download and decrypt it inside the build script.",
      "B": "Store the API key in AWS Secrets Manager. In the CodeBuild project definition, set an environment variable that references the Secrets Manager ARN and enable 'Reveal secrets in build logs' to false.",
      "C": "Hard-code the secret in an encrypted parameter in Systems Manager Parameter Store and pull it during the build.",
      "D": "Pass the secret as a plaintext environment variable in the CodePipeline action configuration, relying on IAM to protect it."
    },
    "explanation": "Option B uses Secrets Manager with environment variables in CodeBuild, ensuring secrets aren\u2019t logged."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A reloadable data pipeline updates feature data daily. You want to trigger model retraining automatically whenever a new featureset lands in the SageMaker Feature Store. What\u2019s the least\u2010maintenance way to integrate this into your CI/CD pipeline?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Modify your daily ETL job to call the CodePipeline StartPipelineExecution API at its end.",
      "B": "Use S3 event notifications on the Feature Store underlying S3 bucket to trigger a Lambda that starts the pipeline.",
      "C": "Add a CloudWatch alarm on Feature Store PutRecord metrics and have it trigger pipeline via SNS.",
      "D": "Create an EventBridge rule matching the SageMaker Feature Store PutRecord API via CloudTrail, target StartPipelineExecution for your training pipeline."
    },
    "explanation": "Option D leverages CloudTrail events in EventBridge to detect new records and start the pipeline with zero custom code in the ETL job."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "You need to implement a canary deployment for a real-time SageMaker endpoint in your pipeline. It must shift 5% traffic to the new model initially, then 50%, then 100%, with automatic rollback on errors. Which combination of actions do you use in CodePipeline?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Use CloudFormation deploy action with TYPE=BLUE_GREEN and trafficRoutingConfig CanaryInterval (5%) and CanaryInterval (50%), and a CloudWatch alarm action to auto-rollback.",
      "B": "Use CodeDeploy with application type 'AWS Lambda' against the SageMaker endpoint and set canaryDeploymentConfig.",
      "C": "Invoke a Lambda function in each stage to call UpdateEndpointWeightsAndCapacities on the endpoint.",
      "D": "Use a custom CodeBuild action to call the SageMaker API to shift traffic and monitor latency; if latency rises, call rollback API."
    },
    "explanation": "Option A uses CloudFormation blue/green with canary traffic routing and automatic rollback via alarms, requiring no custom code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A governance requirement mandates that any pipeline change must be peer-reviewed before execution. You want to enforce code reviews for the buildspec.yaml and pipeline definition in CDK. What CI/CD pattern accomplishes this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Allow developers to commit directly to main; have a pre-build hook that runs only if PR label 'reviewed' exists.",
      "B": "Store CDK pipeline code and buildspec in a feature branch, require PR approval in CodeCommit/GitHub, merge only after two approvers, then trigger pipeline on merge to main.",
      "C": "Use CodeBuild PreBuild commands to validate a 'reviewed-by' tag in Git metadata before proceeding.",
      "D": "Configure a manual approval action at the start of every pipeline run to verify that PRs were reviewed externally."
    },
    "explanation": "Option B leverages standard Git-based code-review gating before code merges and pipeline triggers."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your pipeline runs SageMaker training jobs in parallel for hyperparameter tuning. Sometimes runs collide and exceed resource quotas, causing failures. How do you prevent concurrent training executions?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Limit the maximum concurrent CodeBuild builds in the project settings to 1.",
      "B": "Use a Lambda in your pipeline to list current SageMaker jobs and fail fast if any RUNNING jobs exist.",
      "C": "Configure the CodePipeline stage for training to use an ActionConfiguration including a custom concurrency token in your state machine so that only one execution at a time proceeds.",
      "D": "Set the SageMaker training action\u2019s maximum concurrency to 1 in the pipeline definition."
    },
    "explanation": "Option C uses a concurrency token to serialize that action\u2019s invocations; CodePipeline supports concurrencyToken to prevent parallel runs."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "You want to include a data-skew check using SageMaker Clarify in your CI/CD before training. Which pipeline stage sequence and actions accomplish this without custom containers?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "After unit tests, add a CodeBuild stage that invokes SageMaker Clarify ProcessingStep via AWS CLI, passing baseline and new dataset; fail build on detected skew.",
      "B": "Add a custom Docker build to run a Python script that calls Clarify SDK inside CodeBuild.",
      "C": "Trigger SageMaker Pipelines containing a ClarifyBaselineStep; ingest results via SNS into CodePipeline.",
      "D": "Use Lambda action in CodePipeline to call Clarify\u2019s API and post results to S3, then have pipeline poll for results."
    },
    "explanation": "Option A uses CodeBuild with the AWS CLI to call SageMaker Clarify ProcessingJob, avoiding custom Docker images."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your pipeline must run unit tests, then integration tests against a temporary SageMaker endpoint after deployment, and delete the endpoint automatically when tests complete or fail. What do you add?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A CodeDeploy preTraffic hook to run tests and a postRollback hook to delete the endpoint.",
      "B": "A Lambda in CodePipeline to run tests and then call DeleteEndpoint.",
      "C": "A CodeBuild test stage that spins up the endpoint, tests, then calls DeleteEndpoint in the same build.",
      "D": "Two CodeBuild actions: one for integration tests against the staging endpoint, then one that deletes the endpoint by calling AWS CLI delete-endpoint; add them sequentially and configure failureAction=Abort."
    },
    "explanation": "Option D cleanly separates testing and teardown in CodeBuild stages, ensuring teardown always runs after tests if preceding stages succeed."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A developer accidentally pushed sensitive data to a feature branch. You need to prevent that branch from ever triggering your CI/CD pipeline. How do you modify your pipeline\u2019s source action?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a whitelist of allowed branch patterns in the buildspec to abort on unallowed patterns.",
      "B": "Configure the GitHub source action to include an exclude filter 'refs/heads/feature/sensitive*'.",
      "C": "Use a Lambda in the source stage to inspect commits and abort if the file pattern matches.",
      "D": "Add a pre-build CodeBuild action that checks commit history for sensitive files."
    },
    "explanation": "Option B uses the source action\u2019s exclude filter to prevent specific branch patterns from triggering the pipeline."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your ML pipeline needs to use Spot Instances for cost-saving on training. How do you enable this in your CodePipeline/SageMaker training integration?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "In the SageMaker Pipeline definition YAML, set ResourceConfig.AuxiliarySpotCapacity to the desired percentage and invoke via CodePipeline\u2019s SageMakerPipelineAction.",
      "B": "Add a CodeBuild stage that runs 'aws sagemaker create-training-job' CLI with --use-spot-instances flag.",
      "C": "Wrap training in a Lambda function configured with capacityType=SPOT and call it from CodePipeline.",
      "D": "Switch the CodePipeline action type to AWS Batch and configure Spot in the ComputeEnvironment."
    },
    "explanation": "Option A leverages SageMaker Pipelines spot training support via ResourceConfig and CodePipeline\u2019s SageMakerPipelineAction without custom code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "You need to trigger your CI/CD pipeline whenever a pull request is opened or updated, and only for PRs targeting develop. How is this configured?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure CodePipeline Source with GitHub webhook on all PR events and in the build stage inspect the target branch.",
      "B": "Use EventBridge rule for GitHub PullRequest events filtering on repository name.",
      "C": "Enable GitHub Pull Request webhooks in the source action and set the targetReference filter to 'refs/heads/develop'.",
      "D": "Use a Lambda subscriber to GitHub webhooks to call StartPipelineExecution when PR target is develop."
    },
    "explanation": "Option C uses the source action\u2019s webhook pull request event support with targetReference filters to limit triggers to PRs against develop."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your staging endpoint runs expensive benchmarks as part of integration tests. To avoid excessive cost, you only want to run tests when changes affect inference code. How do you optimize your pipeline?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Cache build artifacts between runs to speed up deployment.",
      "B": "Use a manual approval before integration tests.",
      "C": "Use a Lambda pre-check in CodePipeline to compare diff of inference directory and skip tests stage if no changes.",
      "D": "Split your repository: place inference code in its own folder and configure two pipelines\u2014one that triggers full workflow on inference folder changes (using source filters) and one for other changes."
    },
    "explanation": "Option D uses separate pipelines with source filtering to avoid running expensive tests when unrelated code changes."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your pipeline fails intermittently at the SageMaker Pipeline invocation stage due to transient throttling errors. How do you make the pipeline more resilient?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Wrap the SageMakerPipelineAction in a CodeBuild action with a retry loop using exponential backoff on throttling errors.",
      "B": "Increase the service quota for StartPipelineExecution calls.",
      "C": "Add a manual retry approval stage after the failure.",
      "D": "Use a Lambda-based custom action that catches throttles and auto-retries once."
    },
    "explanation": "Option A uses a CodeBuild wrapper to implement retries with backoff, improving resilience without manual intervention."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "You want your CI/CD pipeline to automatically rollback to the last known good model if performance metrics degrade after deployment. How can you implement this in CodePipeline?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Include a manual approval stage that runs a performance test, and if it fails, manually invoke rollback.",
      "B": "Use a Lambda in the post-deploy stage to run a test; if metrics below threshold, call UpdateEndpoint to the previous variant.",
      "C": "Use a CloudWatch alarm on performance metrics to trigger a CodePipeline retrigger of the deploy stage with the previous model via CodeDeploy automatic rollback.",
      "D": "Publish the previous model as an alias in Model Registry and switch alias in a Lambda action on failure."
    },
    "explanation": "Option C ties CloudWatch alarms to CodePipeline\u2019s deploy stage with a configured rollback in CodeDeploy, automating fallback."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your team uses GitHub Flow, pushing directly to main and using feature branches only for experiments. They want every merge to main to trigger a full pipeline. But they also use semantic version tags (e.g., v1.2.0) to denote releases. How do you ensure only tag pushes trigger production deployments?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure CodePipeline source to only trigger on main branch updates.",
      "B": "Use two pipelines with GitHub source actions: one listening to refs/tags/v* (for production) and another to refs/heads/main (for tests), and in the production pipeline restrict to tag pattern '^refs/tags/v[0-9]+.*$'.",
      "C": "Add logic in the build stage to parse GIT_REF and only continue if it\u2019s a tag.",
      "D": "Use EventBridge pattern matching on GitHub Tag events to start the pipeline."
    },
    "explanation": "Option B cleanly separates pipelines by source filters: one triggers on tag pushes for production deployments."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A pipeline source stage uses AWS CodeCommit. Developers occasionally force-push and rewrite history, causing source metadata mismatches. How do you make the pipeline robust against force pushes?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable polling instead of webhooks in the CodeCommit source action.",
      "B": "Add a pre-build stage to Git fetch --prune and reset --hard origin/branch.",
      "C": "Configure the CodePipeline source action to use 'FullClone' fetch method to always get the current tip regardless of history rewrite.",
      "D": "Disable code commit history rewrite in the repository settings."
    },
    "explanation": "Option C uses the FullClone fetch mode so the pipeline always pulls the latest commit independent of history changes."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "You need to integrate model explainability tests into your pipeline: after training, fail the pipeline if any feature attribution for protected classes exceeds a threshold. Which stage should you add?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a CodeBuild stage that runs a SageMaker Clarify ModelExplainabilityProcessingJob via AWS CLI with the trained model, parses the results JSON, and exits non-zero if attribution drift exceeds the threshold.",
      "B": "Invoke a Lambda function in a pipeline action that calls Clarify SDK and uses CloudWatch metrics to decide pass/fail.",
      "C": "Embed the Clarify step inside the SageMaker Pipeline and then have CodePipeline poll for model registry tags to decide.",
      "D": "Use a CloudFormation custom resource to run the Clarify job and roll back the stack on failure."
    },
    "explanation": "Option A uses CodeBuild to orchestrate Clarify explainability checks, parse results, and enforce pipeline success/failure cleanly."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A pipeline stage builds and pushes a Docker image to ECR, then a SageMaker training action uses that image. Occasionally, the training job starts before the image is available, causing errors. How do you prevent this race condition?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a fixed sleep delay in the training action\u2019s custom script.",
      "B": "Use an EventBridge rule on ECR image-push to trigger the pipeline second stage.",
      "C": "Configure CodeBuild to push to a different tag and have training wait on tag propagation.",
      "D": "Split the build-and-push into one CodeBuild stage that outputs the image URI as an artifact, and configure the next SageMakerPipelineAction to consume that artifact, which enforces sequencing."
    },
    "explanation": "Option D uses CodePipeline artifact dependency to guarantee that the training stage does not start until the image has been successfully pushed and the URI passed along."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your pipeline\u2019s CodeBuild unit-test stage frequently downloads the entire repository, slowing builds. How do you optimize clone behavior?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure the GitHub source action to use 'ShallowClone' with a depth of 1, and enable CodeBuild\u2019s source cache for the repo.",
      "B": "Use Git LFS to store large binaries outside the repo.",
      "C": "Split the repo into micro-repos each with its own pipeline.",
      "D": "Switch the source action to S3 and upload only changed files manually."
    },
    "explanation": "Option A combines shallow cloning at the source action with CodeBuild caching to speed up fetch time without repo restructuring."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A CI/CD pipeline should only deploy models when both unit tests and integration tests pass. However, you need integration tests to run against a live endpoint, which itself requires deployment. How do you model this in CodePipeline?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run both unit and integration tests in the same CodeBuild action after deployment.",
      "B": "Chain two separate pipelines: one for tests, one for deployment, and only call the deploy pipeline if tests pass.",
      "C": "In a single pipeline: Stage1 run unit tests via CodeBuild, Stage2 deploy to staging via CloudFormation, Stage3 run integration tests via CodeBuild, Stage4 manual approval and production deploy.",
      "D": "Use a SageMaker Pipeline that supports validation steps natively and call it from CodePipeline."
    },
    "explanation": "Option C lays out a linear pipeline with distinct stages for unit tests, deployment, integration tests, and production approval."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "You want to notify your team in Slack whenever any pipeline stage fails, including the stage name and error message. How do you configure this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Attach an SNS topic to pipeline notifications and configure an AWS Chatbot integration for Slack.",
      "B": "Create a CloudWatch EventBridge rule matching CodePipeline 'Stage Execution Failed' events, target a Lambda that formats the message and posts to Slack via webhook.",
      "C": "In each CodePipeline stage\u2019s onFailure hook, invoke a Lambda to post to Slack.",
      "D": "Enable CloudTrail logs for CodePipeline and stream failures to Slack using Log Insights."
    },
    "explanation": "Option B uses a single EventBridge rule and a Lambda to capture failure events and send formatted Slack messages, minimizing per-stage configuration."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "An executive insists that production deployments require two separate approvers. How can you enforce this in CodePipeline?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add two successive ManualApproval actions in the production deployment stage, each assigned to a different user group.",
      "B": "Configure the ManualApproval action with 'approvalThreshold' set to 2.",
      "C": "Use IAM policies to require MFA for invoking the production stage.",
      "D": "Chain two identical manual approval actions but restrict both to the same group."
    },
    "explanation": "Option A ensures two distinct approvals by having two manual approval actions in series, each can be assigned to different user sets."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your CodePipeline deploys a SageMaker model via CloudFormation. When the deployment fails, the pipeline remains stuck in a FAILED state and you must manually clean up. How do you configure automatic rollback of the CloudFormation stack?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a CloudFormation DeleteStack action after the deploy action conditioned on failure.",
      "B": "Use a Lambda in the onFailure hook to delete the stack.",
      "C": "Set the CloudFormation action\u2019s 'ActionMode' to CHANGE_SET_REPLACE and 'RollbackOnFailure' parameter to true.",
      "D": "Wrap the CloudFormation deploy in an AWS Step Functions state machine with a Catch to delete the stack."
    },
    "explanation": "Option C leverages CloudFormation action parameters to rollback automatically, avoiding manual cleanup."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "You need to run parallel hyperparameter tuning and data quality steps in your pipeline to reduce total runtime. How do you implement parallel actions in CodePipeline?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create multiple pipelines and start them simultaneously from a Lambda.",
      "B": "Use SageMaker Pipelines which natively parallelizes steps.",
      "C": "Run one CodeBuild action that multiplexes both tasks in background processes.",
      "D": "Define two actions within the same pipeline stage; CodePipeline will execute them in parallel, then proceed only when both succeed."
    },
    "explanation": "Option D uses CodePipeline\u2019s support for parallel actions in a single stage to run tasks concurrently."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An online retailer deploys a real-time inference endpoint for product recommendations. Shortly after launch, they notice that the distribution of a key categorical feature \"user_region\" has shifted, although model accuracy remains within SLA. Which automated monitoring solution should they implement to detect and alert on this feature distribution change with minimal custom coding?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure a SageMaker Model Quality Monitor job to check for data drift on \"user_region\" using ground truth labels.",
      "B": "Use SageMaker Model Monitor DataQuality baselining to establish constraints on \"user_region\" and schedule a drift detection job.",
      "C": "Deploy SageMaker Clarify to compute SHAP values for \"user_region\" on each inference and alert when feature importance changes.",
      "D": "Write an AWS Lambda that polls CloudWatch logs, computes a KS test on \"user_region\", and sends SNS alerts."
    },
    "explanation": "Model Monitor DataQuality jobs can baseline feature distributions and automatically detect drift with minimal custom code. ModelQualityMonitor requires labels; Clarify focuses on bias/importance rather than raw distribution; a Lambda solution requires more custom work."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A financial institution uses a SageMaker real-time endpoint for credit scoring. They need to monitor inference latency degradation and data schema violations. Which combination of SageMaker features meets both requirements natively?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure SageMaker Model Debugger to capture latency and data quality metrics.",
      "B": "Use CloudWatch Contributor Insights for the endpoint to detect schema violations and latency issues.",
      "C": "Enable Data Capture on the endpoint and run SageMaker Model Monitor with both DataQuality and ModelQuality checks.",
      "D": "Deploy AWS X-Ray for latency tracing and AWS Glue Data Quality for schema validation."
    },
    "explanation": "Data Capture plus Model Monitor can track payload schema violations via DataQuality checks and latency via custom metrics. Model Debugger focuses on training/debug; Contributor Insights and Glue don\u2019t natively integrate inference monitoring as well."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A company wants to detect concept drift in predicted probabilities of a binary classifier in production in near real-time. They also want to minimize cost. Which monitoring configuration should they choose?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run a SageMaker Clarify ModelExplainabilityMonitor on each batch of incoming requests.",
      "B": "Configure an asynchronous SageMaker endpoint with built-in data drift detection.",
      "C": "Use SageMaker Model Quality Monitor with ground truth labels and a 5-minute schedule.",
      "D": "Enable Data Capture on the endpoint and schedule a SageMaker Model Monitor DataQuality drift check on predicted score distribution."
    },
    "explanation": "Without ground truth, ModelQualityMonitor isn\u2019t applicable. Clarify ExplainabilityMonitor focuses on SHAP. Asynchronous endpoints have no built-in drift. DataQuality drift checks on predictions detect concept drift cost effectively."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An ML engineer deployed two production variants (A and B) for A/B testing. They need to monitor emerging data skew between the variants for a continuous categorical feature \"device_type\". Which approach best surfaces variant-specific data skew?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure two separate Model Monitor DataQuality jobs, each capturing and analyzing \"device_type\" for its variant.",
      "B": "Use a single Model Monitor DataQuality job that collects data from both variants and uses filters on \"variant_name\".",
      "C": "Use SageMaker Clarify to detect bias between variants on \"device_type\".",
      "D": "Aggregate CloudWatch logs across variants and run a post-processing job to compare distributions."
    },
    "explanation": "Separate DataQuality jobs allow per-variant baselines and alerts. A single job with filters isn't supported; Clarify is for bias interpretation, not raw skew; log aggregation is more custom work."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "During inference, a regression model returns NaN for target predictions intermittently. Which Model Monitor configuration will automatically detect this anomaly and notify the team?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable SageMaker Model Debugger to capture NaNs during inference.",
      "B": "Use Model Monitor DataQuality baseline constraints to flag NaN values in the prediction column.",
      "C": "Set up Amazon CloudWatch anomaly detection on inference response logs to catch NaNs.",
      "D": "Configure SageMaker Clarify to test for invalid prediction values."
    },
    "explanation": "DataQuality monitors can enforce no-NaN constraints on any column. Debugger focuses on training tensors; CloudWatch anomaly detection requires custom log parsing; Clarify is for bias/explainability."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A language translation model hosted on SageMaker Edge Manager runs on devices with intermittent connectivity. The team needs to detect and report when local input data statistics drift significantly. Which architecture is most appropriate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Model Monitor DataQuality jobs in the AWS cloud to poll edge logs hourly.",
      "B": "Install SageMaker Debugger on devices to log input metrics to CloudWatch.",
      "C": "Embed the SageMaker Edge Manager monitoring SDK on device to emit metrics to AWS IoT, then trigger a Model Monitor drift check in the cloud.",
      "D": "Deploy AWS Greengrass Lambda functions to run Clarify drift checks locally."
    },
    "explanation": "Edge Manager SDK plus IoT ingestion allows capture of local stats; cloud Model Monitor then runs drift. Debugger isn\u2019t for inference; Greengrass + Clarify unsupported; cloud polling logs is inefficient."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An ML engineer must monitor prediction skew between two ensembles serving the same traffic for anomaly detection. They require automatic, per-minute alerts if the divergence of prediction distributions exceeds a threshold. How should they implement this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable Data Capture for both ensemble endpoints, schedule two Model Monitor drift checks on predictions with a 1-minute interval, and configure CloudWatch alarms on the job results.",
      "B": "Run SageMaker Clarify on both ensembles every minute to compute PSI on prediction distributions.",
      "C": "Write a custom Lambda that pulls logs from both endpoints, computes KS test, and pushes CloudWatch metrics.",
      "D": "Use CloudWatch Contributor Insights to track prediction distribution per minute."
    },
    "explanation": "Two Data Capture + Model Monitor drift checks with CloudWatch alarms automates per-minute divergence alerts. Clarify isn\u2019t for raw distribution drift; custom Lambda is higher overhead; Contributor Insights not suited."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "After deploying a multi-model SageMaker endpoint, a sudden spike in malformed JSON requests is observed. The engineer wants to monitor schema violations per model. Which solution provides the most granular alerts?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure a single Model Monitor DataQuality job on the endpoint to catch all schema violations.",
      "B": "Enable Data Capture and configure a separate DataQuality job per model container with JSON schema constraints.",
      "C": "Use AWS WAF JSON body inspection rules to log violations to CloudWatch.",
      "D": "Parse inference logs with CloudWatch Logs Insights and trigger SNS alerts."
    },
    "explanation": "Separate DataQuality jobs per model in the multi-model endpoint allow per-container schema checks. A single job cannot differentiate models; WAF and custom log parsing add external complexity."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A streaming inference pipeline uses an asynchronous endpoint. They want to detect if more than 1% of requests take longer than 10 seconds. Which AWS-native feature combination best achieves this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Model Monitor latency metrics with custom Python script to parse logs.",
      "B": "Configure AWS X-Ray on the endpoint and set up CloudWatch alarms for trace duration.",
      "C": "Enable Data Capture and schedule Model Quality Monitor to evaluate latency distribution.",
      "D": "Enable Data Capture on the endpoint, configure CloudWatch metric filters for invocation duration, and set alarms for >1% above 10s."
    },
    "explanation": "Data Capture plus CloudWatch metric filters allow latency distribution monitoring and alarms without custom training jobs. Model Quality Monitor isn\u2019t for latency; X-Ray traces require code hooks."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A healthcare inference service must monitor PII leakage: requests and responses must contain no PHI tokens. The model returns free-form text. Which monitoring approach will detect policy violations?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Model Monitor DataQuality to enforce a regex constraint rejecting PHI patterns in responses.",
      "B": "Configure AWS WAF with a custom rule to block PHI regex in JSON payload.",
      "C": "Enable Data Capture, run a scheduled SageMaker Clarify bias check with custom detectors for PHI tokens.",
      "D": "Write a Lambda for each inference to scan for PHI and generate CloudWatch events."
    },
    "explanation": "Clarify allows custom text analyzers via pre-built detectors to detect sensitive tokens. DataQuality regex can catch patterns but is limited; WAF doesn\u2019t inspect model responses; Lambda is higher overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A model predicts customer churn and uses 50 numeric features. They need to detect multivariate drift (covariate shift) across those features. Which monitoring setup is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Model Monitor DataQuality with a multilook at a multivariate drift check (using dataset_format=\u201cJSON\u201d with drift_check_categories).",
      "B": "Configure SageMaker Clarify to compute pairwise SHAP drift metrics for all feature pairs.",
      "C": "Schedule a batch transform job daily and compare covariance matrices via AWS Glue.",
      "D": "Write custom Spark on EMR to compute Mahalanobis distance across features."
    },
    "explanation": "Model Monitor supports multivariate drift checks during DataQuality monitoring. Clarify focuses on feature importance; batch transform and custom Spark are inefficient and higher overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A retailer serves an image-classification model via SageMaker. They need to detect when the distribution of image size (in bytes) drifts and when inference latency spikes. Which out-of-the-box features should they enable?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Clarify DataBias monitors on image_size and SageMaker Model Debugger for latency.",
      "B": "Enable Data Capture and schedule Model Monitor DataQuality jobs to track image_size distribution and custom CloudWatch latency metrics.",
      "C": "Configure a Lambda pre-processor to log image_size and usage of SageMaker Profiler for latency.",
      "D": "Deploy a Step Functions workflow around the endpoint to log metrics and analyze separately."
    },
    "explanation": "Data Capture plus Model Monitor DataQuality tracks input size drift; CloudWatch latency metrics catch spikes. Clarify and Debugger don\u2019t cover both needs; lambda and Step Functions add complexity."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An enterprise requires that no inference job runs outside their VPC and that monitoring data remains within their private subnet. How can they deploy Model Monitor under these constraints?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable network isolation on Model Monitor jobs.",
      "B": "Run Model Monitor on a public subnet with NAT gateway to pull data.",
      "C": "Use VPC endpoints for S3 and CloudWatch and run jobs on public instances.",
      "D": "Configure Model Monitor Processing Job with VPC configuration in private subnets and use VPC S3 endpoints."
    },
    "explanation": "Model Monitor jobs run as SageMaker Processing Jobs and support VPC config. Private subnets plus S3 VPC endpoints ensure data stays in VPC. Network isolation is for training/inference only."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A financial model returns probability scores. The team needs to detect if the median score changes by more than 5% compared to baseline daily. Which feature and configuration accomplish this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Model Monitor DataQuality drift check on the prediction field with a median tolerance constraint of \u00b15%.",
      "B": "Schedule a Clarify ModelBiasMonitor job to compute median changes.",
      "C": "Extract daily predictions to Athena and run SQL analytics to compare medians.",
      "D": "Enable CloudWatch percentile-based alarms on the endpoint\u2019s inference metric."
    },
    "explanation": "DataQuality drift constraints support median checks on numeric fields. Clarify bias monitors aren\u2019t for central tendency; Athena SQL is custom; CloudWatch latency alarms only handle infrastructure metrics."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "To comply with regulations, a bank must audit all inference inputs and outputs and ensure they cannot be modified post-hoc. Which combination of services and features meets this requirement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable Data Capture to write logs to an encrypted S3 bucket and use ACLs for write-once.",
      "B": "Use Data Capture to store inputs/outputs in an S3 bucket with S3 Object Lock in Compliance mode and encryption by KMS.",
      "C": "Configure CloudTrail to log inference API calls and point logs to S3 with versioning.",
      "D": "Use Athena for logging inputs/outputs and store results in a write-only DynamoDB table."
    },
    "explanation": "Data Capture plus S3 Object Lock (Compliance) and KMS encryption provides immutable audit trail. CloudTrail logs API calls but not payloads; ACLs alone don\u2019t enforce write-once."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A data scientist wants to spot-time batch inference data quality issues such as missing values and invalid formats before consuming results. Which approach should they apply?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Clarify to compute bias metrics on batch responses.",
      "B": "Schedule a CloudWatch Logs Insights query on batch transform logs.",
      "C": "Run a SageMaker Processing job with Model Monitor DataQuality checks on batch output.",
      "D": "Validate outputs manually in Jupyter notebooks after batch jobs complete."
    },
    "explanation": "Model Monitor Processing jobs can validate batch outputs via DataQuality checks. Clarify is for bias; Logs Insights doesn\u2019t parse output payload; manual validation lacks automation."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An autonomous driving model processes high-frequence sensor data. They need to detect when any input channel has degraded signal quality (e.g., constant zeros) in real time. Which solution best addresses this?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Use Model Debugger to capture input tensor anomalies.",
      "B": "Set up CloudWatch metric filters on inference logs to detect zero-value readings.",
      "C": "Implement an AWS IoT rule to check sensor message contents for zeros.",
      "D": "Enable Data Capture on the endpoint, schedule a Model Monitor DataQuality job with custom tolerances for each channel."
    },
    "explanation": "Data Capture and DataQuality allow custom numeric constraints per feature. Debugger isn\u2019t for inference; CloudWatch requires custom parsing; IoT rules don\u2019t integrate inference model context."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A model returns a confidence score as a float between 0 and 1. The team must ensure no inference returns a score outside this range. Which monitoring configuration should they use?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Write a custom CloudWatch metric filter on response logs to detect invalid scores.",
      "B": "Use Model Monitor DataQuality constraints to flag scores <0 or >1.",
      "C": "Apply SageMaker Clarify to validate output distributions within range.",
      "D": "Include assertion logic in the inference container to throw errors on invalid values."
    },
    "explanation": "DataQuality constraints can enforce numeric ranges on columns. Clarify doesn\u2019t enforce hard constraints; custom filter or container logic increases maintenance."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A marketing model predicts click-through probability and requires monitoring of high skew in the \"campaign_id\" feature usage. Which Service Lens should they use in Model Monitor to inspect this categorical feature?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use the Categorical Lens in SageMaker Model Monitor DataQuality to monitor \"campaign_id\".",
      "B": "Use the Label Lens in SageMaker Clarify to monitor campaign labeling.",
      "C": "Use the Numeric Lens in Model Monitor to inspect category counts.",
      "D": "Use the Bias Lens in Clarify to track category fairness."
    },
    "explanation": "The Categorical Lens in Model Monitor DataQuality jobs specializes in tracking categorical feature distribution. Numeric Lens monitors only numeric features; Clarify lenses focus on bias/explainability."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An engineering team notices that model inference failures often coincide with sudden changes in request payload size. They want to correlate error rate with payload size anomalies. Which approach is simplest?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Upload logs to CloudWatch Logs and manually correlate with payload sizes stored in S3.",
      "B": "Instrument the container to push custom CloudWatch metrics for payload size and errors.",
      "C": "Enable Data Capture with custom JSONPath for payload_size and error_code, then run Model Monitor DataQuality job with both fields.",
      "D": "Use AWS X-Ray annotations to trace payload size and exceptions."
    },
    "explanation": "Data Capture with custom JSONPath can extract both payload_size and error_code; DataQuality jobs can detect anomalies and correlations. Container instrumentation or X-Ray require more custom code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A translation service uses a multi-model endpoint. They need to monitor latency at the container level for each model separately. What\u2019s the most direct AWS-native way?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure Model Monitor on the endpoint to track per-model latency.",
      "B": "Enable per-model invocation logging with Data Capture and use CloudWatch metric filters to calculate latency.",
      "C": "Deploy a CloudWatch Agent on the endpoint hosts to capture container metrics.",
      "D": "Use SageMaker Debugger profiling on inference containers."
    },
    "explanation": "Data Capture logs invocation metadata per model; CloudWatch logs filters can compute latency per container. Model Monitor and Debugger don\u2019t provide per-container latency; CloudWatch Agent isn\u2019t supported on managed endpoints."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A company must detect malicious adversarial inputs that cause anomalous model outputs. They define an acceptable range for each output. Which monitoring solution enforces this at scale?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model Monitor DataQuality constraints on the output columns with defined min/max thresholds.",
      "B": "Deploy a WAF rule inspecting response bodies against known attack signatures.",
      "C": "Use Clarify Explainability to detect adversarial perturbations.",
      "D": "Stream inference logs to Lambda for custom anomaly detection."
    },
    "explanation": "DataQuality constraint checks on output enforce numeric ranges and can scale. WAF and Clarify aren\u2019t designed for content-based validation; Lambda adds overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An endpoint serves an NLP model that occasionally returns excessively long generated text (>1000 tokens). They want real-time alerts when this happens. Which setup is most efficient?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Model Monitor to check response length length via DataQuality.",
      "B": "Run Clarify to measure token counts and alert.",
      "C": "Enable CloudWatch RUM to capture response sizes.",
      "D": "Configure Data Capture to extract response_text, set a DataQuality constraint for max token count, and schedule frequent jobs."
    },
    "explanation": "Data Capture plus DataQuality constraint handles response length checks. Clarify and RUM unsuited; CloudWatch RUM is for front-end metrics."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A pharmaceutical company must catch silent inference failures where the model returns a default value of 0. They want automated detection. Which AWS feature configuration accomplishes this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure SageMaker Debugger to flag constant zero outputs.",
      "B": "Use Model Monitor DataQuality constraints to detect when output equals 0 more than a threshold.",
      "C": "Enable CloudWatch metric filters on logs for zero values and alert.",
      "D": "Instrument the model container to throw errors on zeros."
    },
    "explanation": "DataQuality constraints detect when a column value matches an anomaly pattern. Debugger is for training; logs filtering is custom; container instrumentation has higher overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "To monitor inference drift for streaming predictions without ground truth, which monitor type and category should they choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ModelQualityMonitor with concept drift category.",
      "B": "Clarify ModelBiasMonitor on predictions.",
      "C": "Model Monitor DataQuality with drift_check_categories set to \"data\" on the prediction field.",
      "D": "Clarify ModelExplainabilityMonitor for prediction distribution."
    },
    "explanation": "DataQuality monitors support drift_check_categories on arbitrary fields. ModelQualityMonitor requires labels. Clarify focuses on bias/explainability."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A company needs to ensure that feature engineering code doesn\u2019t introduce new features unseen during training. They want to monitor for any new column names in inference payloads. What\u2019s the best strategy?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Model Monitor DataQuality with input schema constraints listing allowed column names.",
      "B": "Implement a Lambda authorizer on API Gateway to validate JSON keys.",
      "C": "Enable SageMaker Clarify DatasetDriftMonitor on input keys.",
      "D": "Use AWS Config rules on the SageMaker endpoint inference setting."
    },
    "explanation": "DataQuality constraints can enforce allowed input schema. Lambda authorizer works but is outside SageMaker; Clarify and Config aren\u2019t for schema enforcement."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An ML engineer observes that the distribution of a numerical feature is bimodal in training but has become unimodal in production. They must detect this change automatically. Which Model Monitor configuration handles multimodal distribution drift?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Clarify ModelBiasMonitor to detect distribution shape changes.",
      "B": "Schedule a batch transform and post-process histograms in Athena.",
      "C": "Use CloudWatch anomaly detection on the feature metric.",
      "D": "Configure Model Monitor DataQuality with DistributionLens on the feature and enable drift checks."
    },
    "explanation": "DistributionLens in DataQuality monitors supports multiple bins for drift detection. Clarify bias monitors don\u2019t assess raw distribution shape; Athena/CloudWatch require custom coding."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A subscription service uses asynchronous batch endpoints. They need to monitor end-to-end inference job failures and durations. How can they instrument this without modifying model code?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Use Model Monitor DataQuality to track batch job status and duration.",
      "B": "Configure Clarify to track job failures.",
      "C": "Configure CloudWatch Events on SageMaker BatchTransform job state change and subscribe to SNS.",
      "D": "Deploy GuardDuty to detect anomalies in batch failures."
    },
    "explanation": "CloudWatch Events for job state and duration require no code changes. Model Monitor doesn\u2019t monitor batch job lifecycle; Clarify doesn\u2019t cover jobs; GuardDuty is for security threats."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An experiment uses shadow deployments: incoming traffic is mirrored to a new model. They want to compare output distributions in near real time and alert if divergence >2%. Which design is simplest?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy two DataQuality jobs separately and post-process alerts via Lambda.",
      "B": "Enable Data Capture on both endpoints, schedule a single DriftsCheck Job that references both datasets as baseline and target.",
      "C": "Use Clarify to compute PSI between baseline and candidate in near real time.",
      "D": "Implement a Step Functions workflow to fetch logs and compare distributions."
    },
    "explanation": "A single DriftCheck job can accept two datasets as baseline and target. DataQuality jobs per endpoint or Step Functions add complexity; Clarify mismatched purpose."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A biotech startup requires monitoring of model feature importance drift in production. They want to detect if SHAP importance of any feature changes by >10%. Which AWS feature combination achieves this?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Use Model Monitor DataQuality drift_check on SHAP values.",
      "B": "Schedule a Model Quality Monitor job with SHAP-based metrics.",
      "C": "Configure Clarify ModelBiasMonitor with feature_importance thresholds.",
      "D": "Enable SageMaker Clarify ModelExplainabilityMonitor on the endpoint with max_absolute_shap_change constraint."
    },
    "explanation": "Clarify\u2019s ModelExplainabilityMonitor captures SHAP importance drift and supports constraints. DataQuality and ModelQualityMonitoring don\u2019t handle SHAP; ModelBiasMonitor focuses on bias not drift."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A team must implement an alert if any inference job experiences input payload with more than 50 unique categorical feature values. Which approach scales best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Config custom rules to inspect payloads.",
      "B": "Deploy WAF rate-limit rules on JSON body parser.",
      "C": "Enable Data Capture and configure a Model Monitor DataQuality custom constraint on cardinality >50.",
      "D": "Run Athena queries on S3 logs and send alerts via Lambda."
    },
    "explanation": "DataQuality custom cardinality constraints enforce unique value limits. AWS Config and WAF don\u2019t inspect inference payloads; Athena+Lambda is higher latency."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A model is deprecated but still receives stale requests. The team wants to detect any inference calls to the old endpoint and retire it promptly. Which CloudWatch configuration helps?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Set up a CloudWatch metric filter on SageMakerAPI logs for InvokeEndpointOldModel API and alert.",
      "B": "Use Model Monitor to detect zero-inference values.",
      "C": "Configure a Custom Resource in CloudFormation to log invocations.",
      "D": "Enable SageMaker Clarify to flag calls to old endpoint."
    },
    "explanation": "CloudWatch metric filters on API logs for specific endpoint names detect calls. Model Monitor and Clarify aren\u2019t for endpoint invocation detection; custom resource is unnecessary."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A gaming app uses an ML endpoint that must maintain 95th percentile latency under 200ms. They need to alert if the SLA is broken. Which AWS native setup is recommended?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run Model Monitor with a latency constraint on the endpoint logs.",
      "B": "Use CloudWatch percentile-based alarms on the AWS/SageMaker metric InferenceLatency.",
      "C": "Deploy SageMaker Debugger to capture latency tensors.",
      "D": "Implement a Lambda to sample requests and measure SLA."
    },
    "explanation": "CloudWatch percentile-based alarms directly monitor InferenceLatency p95. Model Monitor isn\u2019t designed for latency; Debugger and Lambda require extra work."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "An e-commerce company uses a SageMaker real-time endpoint for TensorFlow models with a strict p95 latency SLA. During peak traffic hours, end-to-end latency spikes intermittently even though CPU and memory utilization remain under thresholds. Which monitoring approach and remediation provides the deepest insight into latency sources and optimizes cost?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Enable AWS X-Ray tracing on the SageMaker endpoint, analyze service map latencies, identify cold-start segments, then provision minimal concurrency and configure auto scaling.",
      "B": "Increase the instance type to a larger CPU-optimized instance and create a CloudWatch Alarm on CPU utilization.",
      "C": "Enable SageMaker endpoint invocation metrics in CloudWatch and auto scale based on p95 latency.",
      "D": "Convert the endpoint to a multi-model endpoint to share a single container across models and reduce overhead."
    },
    "explanation": "X-Ray reveals where time is spent (initialization vs. inference). Provisioned concurrency mitigates cold starts. CPU scaling alone misses framework overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A financial services company runs a batch transform job on EC2 instances via SageMaker. Transform jobs run once per day, but 80% of capacity is idle for most of the job. The team wants to reduce cost without increasing job duration. Which solution meets this requirement?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch to CPU-optimized On-Demand instances sized for peak utilization and scale cluster size dynamically.",
      "B": "Use larger GPU instances to process in parallel and decrease overall run time.",
      "C": "Run the batch transform job on Lambda functions to parallelize tasks.",
      "D": "Configure the batch transform job to use Spot Instances with a fallback to On-Demand and set instance count for peak load."
    },
    "explanation": "Spot with fallback saves cost during idle periods and matches peak capacity, while On-Demand-only wastes money."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "Your team deployed multiple SageMaker endpoints in three regions. They must tag endpoints for cost allocation and alert if untagged resources appear. Which configuration achieves this with the least operational overhead?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Write a Lambda polling script to list endpoints daily, check tags, and send notifications.",
      "B": "Use CloudWatch Logs subscription to trigger a Lambda on CreateEndpoint API events to validate tags.",
      "C": "Create an AWS Config rule (required-tags) scoped to SageMaker::Endpoint resources and set SNS notifications.",
      "D": "Implement CloudTrail log analysis in CloudWatch Logs Insights to detect untagged CreateEndpoint events and alert."
    },
    "explanation": "AWS Config rule automatically checks tags on creation with built-in functionality and SNS notifications, minimal custom code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A gaming company uses SageMaker Processing to prepare game telemetry data. They notice sporadic high I/O wait times on EBS volumes attached to processing jobs. Which combination of monitoring metrics and actions will identify and mitigate the issue?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Monitor ProcessingJobIOBytes metric in Amazon CloudWatch and increase instance count.",
      "B": "Enable CloudWatch EBS Volume metrics (VolumeQueueLength, VolumeThroughputPercentage), use higher IOPS gp3 volumes and adjust volume throughput settings.",
      "C": "Monitor SageMaker Processing CPUUtilization and switch to GPU instance.",
      "D": "Enable CloudTrail logging on processing containers and analyze volume errors."
    },
    "explanation": "VolumeQueueLength and throughput % diagnose EBS bottleneck; gp3 allows tuning of IOPS separately and resolves I/O wait."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A retail ML pipeline uses SageMaker endpoints and ECS-based preprocessing. The finance team needs a single dashboard showing endpoint cost, CPU utilization, ECS Fargate memory utilization, and Lambda durations. Which solution meets this requirement with minimal latency?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Build separate CloudWatch dashboards per service and share links.",
      "B": "Export all metrics to an external monitoring tool via Fluentd.",
      "C": "Configure CloudTrail Lake queries to join logs and visualize in QuickSight.",
      "D": "Use CloudWatch unified metric namespace with metric math and create a combined CloudWatch dashboard with all resources."
    },
    "explanation": "CloudWatch dashboards support service metrics and metric math in one place without external tools or manual stitching."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A biotech startup has a GPU-based SageMaker endpoint used intermittently. The CFO demands a cost-optimized solution that does not sacrifice startup latency. Which approach balances both?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use provisioned concurrency with a small number of GPU instances, enable endpoint auto scaling to scale down to zero using asynchronous endpoints.",
      "B": "Switch to smaller CPU instances and batch requests to save cost.",
      "C": "Use Spot Instances for the real-time endpoint to lower cost.",
      "D": "Migrate to multi-model CPU endpoint and load one model at a time."
    },
    "explanation": "Provisioned concurrency preserves low latency, auto scaling to zero reduces idle cost; spot not supported for real-time."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A team observes their inference pipeline fails under burst traffic due to hitting service quotas. They want proactive alerts when approaching SageMaker endpoint invocation quota. How should they implement this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure a CloudWatch alarm on SageMaker API ThrottledRequests metric.",
      "B": "Create a CloudWatch alarm on the ThrottledRequests metric in the AWS/SageMaker namespace with threshold at 80% of quota.",
      "C": "Use EventBridge to detect service quota event and call SNS.",
      "D": "Poll DescribeEndpoint API hourly via Lambda and compare against known quotas."
    },
    "explanation": "ThrottledRequests metric directly signals when quota is approached; threshold at 80% gives early warning."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "Your model training jobs on GPU EC2 instances often underutilize GPU memory, leaving 40% unused for most jobs. You want better rightsizing. Which tool or service gives an automated recommendation?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Trusted Advisor\u2019s cost optimization checks.",
      "B": "CloudWatch anomaly detection on GPU utilization.",
      "C": "AWS Compute Optimizer for EC2 instances.",
      "D": "SageMaker Model Monitor for resource utilization analytics."
    },
    "explanation": "Compute Optimizer analyzes EC2/GPU instance metrics and recommends right-sized instance types and sizes."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A media company streams video analysis via SageMaker batch transform jobs. They want to optimize data transfer costs between S3 and training instances. What combination of configurations reduces cost?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use cross-region replication to move data closer to training instances.",
      "B": "Enable S3 Transfer Acceleration to speed transfer and reduce egress cost.",
      "C": "Configure batch transform to download from a public S3 bucket.",
      "D": "Place instances and S3 in same AZ and use VPC endpoints for S3 traffic."
    },
    "explanation": "Same AZ VPC endpoints keeps traffic on AWS network, avoids cross-AZ data transfer charges."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A regulated healthcare application must log all changes to SageMaker model endpoints for audit. They also want to trigger cost alerts when monthly spend exceeds budget. Which combined setup achieves both?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable AWS CloudTrail data events for SageMaker API and create an AWS Budget with SNS notifications.",
      "B": "Use CloudWatch Logs to collect API calls and set a Lambda to parse logs and send alerts.",
      "C": "Configure CloudWatch Events for CreateEndpoint and UpdateEndpoint events and notify SNS.",
      "D": "Integrate AWS Config for resource changes and rely on CloudWatch billing alarms."
    },
    "explanation": "CloudTrail data events capture every API call for audit; AWS Budgets integrates natively with SNS for cost alerts."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "Your SageMaker endpoint sees memory out-of-memory (OOM) errors for large images. You need to diagnose the frame where OOM occurs across many hosts. Which logs and queries should you use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Download Docker container logs from CloudWatch Logs and grep for \"OOM\".",
      "B": "Use CloudWatch Logs Insights on /aws/sagemaker/Endpoints/* to query ERROR messages with \"OutOfMemoryError\" and filter by hostID.",
      "C": "Enable SageMaker Debugger hook for memory profiling.",
      "D": "Switch to X-Ray and trace memory allocations."
    },
    "explanation": "CloudWatch Logs Insights can aggregate and filter OOM errors across endpoint hosts quickly; Debugger is for training."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A data pipeline uses EMR for preprocessing and then writes to S3 for SageMaker. During heavy loads, EMR tasks throttle due to hitting S3 request quotas. How do you monitor and remediate this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable EMRFS consistent view and increase retries.",
      "B": "Use CloudTrail to monitor S3 503 errors and back off EMR jobs.",
      "C": "Enable CloudWatch anomaly detection on EMRTaskDuration metric.",
      "D": "Monitor S3 4XX/5XX request metrics in CloudWatch and add S3 request rate limiting or use S3 prefix sharding."
    },
    "explanation": "S3 request metrics reveal throttling; prefix sharding or client-side rate limiting reduces 503 errors."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "Your team deployed a multi-model endpoint behind an Application Load Balancer. They need to track per-model invocation counts for billing and auto scaling. Which approach is most effective?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable ALB access logs and parse them in Athena.",
      "B": "Add custom CloudWatch metrics inside the inference container to emit per-model Invoke counts.",
      "C": "Use CloudTrail logs for InvokeEndpoint calls with model name filters.",
      "D": "Tag each model and use SageMaker built-in metrics."
    },
    "explanation": "Containers can push custom metrics (model=inference) to CloudWatch for precise counts; ALB logs parsing is more complex."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A startup runs GPU-based SageMaker training nightly. They want to reduce EC2 instance costs by 30% without increasing job time. Which purchasing option should they choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Reserved Instances for a one-year term.",
      "B": "EC2 Savings Plans covering all compute usage.",
      "C": "Spot Instances with checkpointing in S3 and automatic retry.",
      "D": "Dedicated Hosts to benefit from volume discounts."
    },
    "explanation": "Spot Instances offer largest discounts (up to 90%) and with checkpointing maintain job duration; RIs and SPs only 50%."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A large dataset processing job on SageMaker Processing has unpredictable CPU loads. They want to automatically adjust compute within a run. Which strategy works?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Processing with auto scaling instance type list.",
      "B": "Orchestrate multiple processing jobs in Step Functions with dynamic instance count based on CloudWatch metrics.",
      "C": "Embed CPU usage watchers in the processing script and call the AWS API to change instance count.",
      "D": "Switch to EMR on EKS with dynamic allocation."
    },
    "explanation": "Step Functions can branch logic mid-workflow and launch new Processing jobs sized by metrics; Processing itself doesn\u2019t auto scale."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "Your SageMaker endpoint is deployed in a VPC. Data scientists observe increased inference latency. Which VPC configuration issue should you monitor and how?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Monitor NAT Gateway CloudWatch NetworkPacketsOut to detect throttling and add NAT Gateways for scale.",
      "B": "Monitor VPC Flow Logs for high packet loss and increase MTU.",
      "C": "Monitor Security Group rejected packet metrics and open more ports.",
      "D": "Monitor Elastic IP usage and allocate more addresses."
    },
    "explanation": "VPC endpoints without dedicated NAT may bottleneck at NAT Gateway; NetworkPacketsOut metric shows if throughput is hitting limits."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A global SaaS product needs to track inference costs per customer across multiple SageMaker endpoints. What tagging and billing configuration should you apply?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add customer ID as an environment variable in the container and parse logs.",
      "B": "Use separate AWS accounts per customer and consolidate billing.",
      "C": "Instrument the SDK client to send customer metadata in X-ray trace.",
      "D": "Tag each endpoint with a customer cost center tag, activate cost allocation tags in Billing console, and use Cost Explorer grouped by tag."
    },
    "explanation": "Cost allocation tags are the AWS best practice for per-resource cost tracking in Cost Explorer."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A team uses Lambda functions for preprocessing before SageMaker. They receive occasional OutOfMemory errors in Lambda, causing increased retries and cost. How should they monitor and prevent this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase Lambda memory and rely on CloudWatch alarms on Duration.",
      "B": "Enable CloudWatch Logs Insights on Lambda error logs to filter OOM, set a CloudWatch alarm on Errors metric, and adjust memory allocation.",
      "C": "Switch to a Step Functions workflow to control memory.",
      "D": "Use X-Ray to trace memory usage of the function."
    },
    "explanation": "Errors metric combined with Logs Insights identifies OOM events; alarms notify and right-size memory to eliminate retries."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "Your preprocessing uses an ECS service on Fargate that sporadically fails CPU credit balance. Which CloudWatch metrics and configuration should you monitor and adjust?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Monitor Fargate MemoryReservation metric and increase CPU share.",
      "B": "Enable Container Insights and watch host CPUUtilization, then switch to EC2 launch type.",
      "C": "Monitor Fargate CPUCreditBalance and CPUUtilization in CloudWatch; adjust the CPU configuration (e.g., move from 0.25 vCPU to 0.5 vCPU) to maintain credits.",
      "D": "Use CloudTrail to log throttled tasks and set a budget alert."
    },
    "explanation": "Fargate tasks use CPU credits; monitoring CPUCreditBalance prevents throttling, and adjusting vCPU allocation maintains balance."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "An ML endpoint in production has highly variable traffic. The team configured auto scaling on CPU utilization. They see scale-in immediately after scale-out, causing thrashing and increased cost. How should they fix it?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add scale-in and scale-out cooldown periods and use target tracking on average invocation per instance.",
      "B": "Switch to step scaling based on p90 latency.",
      "C": "Increase the CPU utilization threshold for scale-in.",
      "D": "Use a multi-model endpoint instead to reduce container startup time."
    },
    "explanation": "Cooldown prevents rapid scale-in/out oscillations; target tracking on per-instance invocations stabilizes scaling."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "Your SageMaker Model Monitor baseline job generates drift violation alerts, but you also want to correlate them with infrastructure metrics to determine if CPU saturation causes data delays. How do you integrate these?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Ingest Model Monitor CloudWatch logs into ElasticSearch and join with CPU metrics.",
      "B": "Use CloudWatch metric math to overlay ModelMonitorDataQualityViolationCount and SageMakerEndpoint CPUUtilization in a unified dashboard.",
      "C": "Export Model Monitor S3 reports to Athena and join with CPU metrics exported by CloudWatch.",
      "D": "Implement a Lambda to capture both metrics on violation events and store in DynamoDB."
    },
    "explanation": "Metric math in CloudWatch provides immediate combining of application and infra metrics in one graph without custom code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A company is charged cross-account for S3 data egress when SageMaker in Account A reads from Account B buckets. They want to avoid egress charges. Which networking option solves this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable inter-account VPC peering and route traffic through private IP.",
      "B": "Use S3 Transfer Acceleration.",
      "C": "Create VPC Interface Endpoints (AWS PrivateLink) for S3 in Account A\u2019s VPC.",
      "D": "Copy data periodically to Account A\u2019s S3 bucket."
    },
    "explanation": "VPC Interface endpoints for S3 keep traffic within AWS network and avoid cross-account egress fees; bucket copy still incurs egress."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "Your nightly SageMaker hyperparameter tuning jobs sporadically hit EBS throughput limits, causing tunings to fail. Which combination of monitoring and remediation addresses this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable CloudTrail on CreateTrainingJob and throttle job submission.",
      "B": "Monitor EC2 Instance CPUUtilization and switch to instance with higher compute.",
      "C": "Use CloudWatch anomaly detection on TrainingJobRuntime metric.",
      "D": "Monitor EBS VolumeThroughputPercentage metrics and migrate to gp3 volumes with provisioned throughput."
    },
    "explanation": "VolumeThroughputPercentage shows EBS bottleneck; gp3 lets you tune throughput independently of volume size."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A data science team regularly creates and deletes dozens of notebooks in SageMaker Studio. The finance team wants to detect orphaned EBS volumes to stop incurring storage costs. What is the simplest solution?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Enable AWS Config managed rule for unattached EBS volumes and set automatic remediation to delete.",
      "B": "Write a Lambda to query DescribeVolumes for unattached volumes and delete.",
      "C": "Set CloudWatch Events on DeleteNotebookInstance to trigger volume clean up.",
      "D": "Use AWS Trusted Advisor\u2019s underutilized EBS check."
    },
    "explanation": "AWS Config rule for unattached volumes and auto-remediation offers built-in, no-code solution; Trusted Advisor only alerts."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A model training team wants to track costs per experiment across multiple SageMaker jobs, notebooks, and endpoints. Which tagging strategy and AWS service combination provides accurate reporting?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Tag resources at creation with ExperimentID and use CloudTrail logs to calculate costs.",
      "B": "Use SageMaker Experiments API to tag runs and rely on EMR cost reports.",
      "C": "Enable cost allocation tags for ExperimentID, tag all SageMaker resources, and view groupings in AWS Cost Explorer.",
      "D": "Import cost data into Quicksight and join on ExperimentID from logs."
    },
    "explanation": "Cost allocation tags in Cost Explorer directly associate resource costs with tags; Experiments API doesn\u2019t feed billing data automatically."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "Your preprocessing job uses AWS Glue and feeds data into SageMaker. You need to monitor Glue job CPU, Glue Data Catalog calls, and track Glue errors alongside SageMaker job failures in a single pane. Which tool do you choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CloudWatch dashboards per service linked manually.",
      "B": "CloudWatch Unified Dashboards with metrics from AWS/Glue and AWS/SageMaker plus logs insights widgets.",
      "C": "Athena querying Glue and SageMaker logs in S3.",
      "D": "QuickSight pulling billing data."
    },
    "explanation": "CloudWatch dashboards can incorporate multiple metrics and logs insights widgets for near real-time observability without custom ETL."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A SageMaker endpoint in a private subnet loses internet connectivity after a VPC route table change and fails to download model artifacts. How do you monitor and alert on such failures?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable CloudWatch Logs for the endpoint container and search for network exceptions.",
      "B": "Monitor CloudTrail events for GetObject failures on S3.",
      "C": "Set a CloudWatch alarm on EndpointInvocationErrors metric.",
      "D": "Create a CloudWatch anomaly detection alarm on SageMakerEndpointInvocation5XXErrors to catch connectivity failures."
    },
    "explanation": "5XX errors include network and container issues. Anomaly detection spots sudden error spikes without log parsing."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "Your inference pipeline uses Lambda behind API Gateway calling SageMaker endpoints. You notice high tail latencies occasionally. Which combined metrics should you monitor and how to differentiate between GP and HTTP latencies?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Monitor API Gateway Latency and IntegrationLatency in CloudWatch to separate client-facing vs. backend invocation delays.",
      "B": "Monitor SageMaker Endpoint ModelLatency and InvocationsPerInstance metrics only.",
      "C": "Enable X-Ray for Lambda and use cold start latency metrics.",
      "D": "Use CloudWatch Synthetics to measure end-to-end latency."
    },
    "explanation": "API Gateway Latency measures total, IntegrationLatency isolates backend; ModelLatency doesn\u2019t include network overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "Your team\u2019s SageMaker batch transform jobs intermittently fail due to insufficient vCPU quotas in a new AWS Region. What is the recommended monitoring step and long-term mitigation?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use CloudWatch to monitor ThrottledRequests and request a service quota increase manually when alarms fire.",
      "B": "Implement retries in the job script.",
      "C": "Create a CloudWatch alarm on ServiceQuotaExceeded metric via Service Quotas namespace and automate quota requests via AWS Service Quotas APIs.",
      "D": "Switch to GPU instances which have separate quotas."
    },
    "explanation": "ServiceQuotaExceeded metric alerts on hitting quotas. Automating via Service Quotas APIs ensures timely increases."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A global inference workload runs on SageMaker endpoints in multiple regions. They need to compare cost per inference across regions. What\u2019s the best way to collect and visualize this data?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Aggregate SageMaker billing metrics into CloudWatch using PutMetricData and graph them.",
      "B": "Enable cost allocation tags for Region on endpoints and use Cost Explorer filtering by tag.",
      "C": "Export Cost & Usage report to S3 and query with Athena manually.",
      "D": "Use QuickSight to connect to Billing API."
    },
    "explanation": "Cost Explorer with allocation tags gives near real-time region-based cost per resource without manual report queries."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "An ML endpoint behind a Network Load Balancer exhibits higher packet drop during bursts. Which VPC metric should you monitor, and what corrective action should you take?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Monitor NetworkBytesIn and move to larger instance.",
      "B": "Monitor HostNetworkRxErrors and increase MTU.",
      "C": "Monitor Flow Logs for REJECT entries and open more ports.",
      "D": "Monitor Elastic Network Interface (ENI) ReceivePacketDrops and use enhanced networking (ENA) instances or increase ENI count."
    },
    "explanation": "ReceivePacketDrops signals NIC saturation; ENA or more ENIs improves network performance for burst traffic."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A team using SageMaker Debugger for training also wants to monitor GPU utilization and EC2 metrics in one place. Which monitoring solution should they implement?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use CloudWatch Container Insights for the training job\u2019s underlying EC2 instances alongside Debugger tensor metrics.",
      "B": "Send EC2 metrics to CloudTrail for correlation.",
      "C": "Parse Debugger logs with CloudWatch Logs Insights and join with EC2 metrics in Athena.",
      "D": "Load all metrics into QuickSight and build a dashboard."
    },
    "explanation": "Container Insights collects EC2 host metrics and container-level metrics in CloudWatch seamlessly, enabling side-by-side visualization with Debugger metrics."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A team must restrict a SageMaker notebook instance so that it can read and write data only in a specific S3 bucket prefix. The notebook uses its execution role to access S3. Which configuration enforces least privilege?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Attach an IAM policy to the notebook execution role allowing s3:GetObject and s3:PutObject only on arn:aws:s3:::marketing-data/train/*",
      "B": "Add an S3 bucket policy granting the notebook role full access to the bucket and deny other principals",
      "C": "Create a VPC gateway endpoint for S3 with a policy restricting access to that prefix",
      "D": "Modify the KMS key policy to allow decryption only for that bucket prefix"
    },
    "explanation": "Least privilege is enforced by restricting the notebook\u2019s execution role with an inline IAM policy on the specific prefix. S3 endpoint policies or bucket policies can work but are more complex and less direct; KMS policy controls only encryption."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A security requirement mandates that SageMaker training jobs must not have internet access, yet must pull training images from Amazon ECR. How should you configure the training job?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Launch the training job in public subnets with a restrictive security group",
      "B": "Provide a customer-managed NAT gateway in the VPC used by the job",
      "C": "Specify VpcConfig with private subnets that have an ECR VPC endpoint and no NAT",
      "D": "Use a service-linked role that disables internet access by default"
    },
    "explanation": "By placing the training job in private subnets with an ECR VPC endpoint and no NAT gateway, you prevent internet access but allow ECR pulls. Public subnets or NAT would allow internet egress."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A SageMaker endpoint must be private within a VPC and only allow traffic from instances in another VPC over AWS PrivateLink. Which steps meet these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Launch endpoint without VPCConfig and restrict with security groups referencing peer VPC",
      "B": "Create interface VPC endpoint for SageMaker runtime in the consumer VPC, deploy the endpoint in a private subnet, and apply SG allowing traffic only from peer VPC CIDR",
      "C": "Use NAT gateway and public endpoint but restrict source IPs in SG",
      "D": "Enable endpoint public access and add route53 private hosted zone entries"
    },
    "explanation": "Deploying the endpoint in a VPC with no public access plus creating an interface endpoint in the consumer VPC over PrivateLink and restricting with SG enforces private-only access."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A DevOps engineer needs to grant AWS CodePipeline permission to deploy SageMaker models and endpoints but must follow least privilege. Which IAM policy attachment is correct?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Attach AmazonSageMakerFullAccess to the CodePipeline role",
      "B": "Allow sagemaker:* on resource '*' for the pipeline role",
      "C": "Create an SCP denying all actions except sagemaker:CreateEndpoint",
      "D": "Attach a scoped IAM policy allowing only sagemaker:CreateModel, sagemaker:CreateEndpointConfig, sagemaker:CreateEndpoint on identified resource ARNs"
    },
    "explanation": "Granting only the specific deploy actions on the exact model and endpoint ARNs enforces least privilege. FullAccess or wildcard would be overly permissive; SCP is for accounts."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "An organization requires CloudTrail logs of all SageMaker API calls to be encrypted using a KMS key managed by SecurityOperations. Which configuration satisfies this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable CloudTrail for SageMaker and leave default encryption",
      "B": "Configure CloudTrail with the SecurityOperations KMS key ARN for SSE-KMS encryption of logs",
      "C": "Apply an S3 bucket policy to encrypt objects using SSE-S3",
      "D": "Modify Trail event selectors to include SSE-KMS"
    },
    "explanation": "Specifying the KMS key ARN in the CloudTrail configuration ensures that logs are encrypted with that key. SSE-S3 or bucket policy alone cannot enforce use of a customer-managed key."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A notebook lifecycle configuration retrieves secrets from AWS Secrets Manager. You must ensure the lifecycle lambda can decrypt the secret but no other SageMaker service can. How do you tighten permissions?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Attach AWSSecretsManagerReadWrite to the notebook role",
      "B": "Add a resource policy on the secret allowing the notebook role",
      "C": "Add a KMS key policy granting decrypt only to the notebook lifecycle role principal",
      "D": "Enable automatic rotation on the secret"
    },
    "explanation": "Tightening access at the KMS key policy level restricts decryption to only the specified lifecycle role. Resource policies on the secret do not control the KMS decryption step."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A SOC team requires that any SageMaker endpoint creation must be approved by a central admin. How can you enforce this in CodePipeline?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a Manual Approval action before the CreateEndpoint stage",
      "B": "Use a CloudWatch rule to detect CreateEndpoint and block it",
      "C": "Implement an SCP to deny sagemaker:CreateEndpoint",
      "D": "Require MFA in the pipeline role"
    },
    "explanation": "A Manual Approval action in CodePipeline forces human approval. SCP would prevent creation globally; CloudWatch can only alert after the fact."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A model registry in SageMaker stores sensitive models. You need to prevent direct API deletes of models, but allow deletion through a daily cleanup Lambda. Which IAM policy accomplishes this?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Deny sagemaker:DeleteModel to everyone",
      "B": "Remove sagemaker:DeleteModel from all roles except Lambda by attaching a policy to the Lambda role only",
      "C": "Create an SCP denying DeleteModel for all principals",
      "D": "Attach a deny statement in a permission boundary that applies to principals unless aws:PrincipalArn equals the Lambda function execution role"
    },
    "explanation": "A permission boundary with a conditional deny unless the principal is the cleanup Lambda enforces that only that role may delete models, without affecting Service roles."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A compliance rule requires that only approved subnets are used in SageMaker jobs. How can you enforce this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use an SCP to deny SageMaker calls in unapproved subnets",
      "B": "Write an IAM permissions policy with Deny on sagemaker:CreateTrainingJob when sagemaker:VpcConfig.Subnets \u2260 approved IDs",
      "C": "Enforce via CloudWatch alarms and manual remediation",
      "D": "Use AWS Config managed rules without enforcement"
    },
    "explanation": "Embedding a Deny in the IAM policy conditioned on the VpcConfig.Subnets attribute blocks jobs launched in unapproved subnets. SCP cannot inspect parameters."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A training job needs to write to a DynamoDB table, but only to specific items. How do you restrict the SageMaker execution role accordingly?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Grant dynamodb:* on the table ARN",
      "B": "Attach AmazonDynamoDBFullAccess",
      "C": "Grant dynamodb:PutItem only on arn:aws:dynamodb:\u2026:table/MyTable with a Condition on dynamodb:LeadingKeys for the key prefix",
      "D": "Use an SCP to restrict all DynamoDB writes"
    },
    "explanation": "Using a resource-level IAM policy with a condition on LeadingKeys ensures writes only to items matching the key prefix. Full table or wildcard actions are too broad."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A DevSecOps requirement: all SageMaker model artifacts in S3 must be encrypted with a specific KMS key. Which is the simplest enforcement?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add SSE-KMS in the CreateModel call parameters",
      "B": "Use a bucket policy requiring x-amz-server-side-encryption header",
      "C": "Implement a CloudWatch rule that checks artifact uploads",
      "D": "Add an S3 bucket policy Deny PutObject unless x-amz-server-side-encryption-aws-kms-key-id matches the key"
    },
    "explanation": "An S3 bucket policy that denies PutObject requests unless the correct KMS key ID header is present enforces encryption at write time."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A SageMaker pipeline stores intermediate data in S3. The security team wants to prevent access to these artifacts except by the pipeline service. How do you enforce this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Grant pipeline roles wide access to the bucket",
      "B": "Use an S3 bucket policy allowing only the SageMaker Pipelines service principal and the pipeline execution role ARN",
      "C": "Encrypt with a KMS key and rely on default key policy",
      "D": "Use a resource-based IAM policy on the pipeline role"
    },
    "explanation": "An S3 bucket policy that permits only the SageMaker Pipelines service principal and the specific execution role to access the bucket restricts all other access."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A model deployed in SageMaker must only be invoked by authenticated users in an Amazon Cognito identity pool. How can you secure the endpoint?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use an endpoint policy with Allow for principal '*' and check Token in code",
      "B": "Attach a resource policy to Cognito to allow SageMakerInvoke",
      "C": "Create an endpoint policy requiring aws:PrincipalOrgID and aws:RequestContext.authorizer.claims.sub from Cognito identity",
      "D": "Use a VPC endpoint and IP-based SG only"
    },
    "explanation": "An endpoint policy can restrict invocation to principals from the specific Cognito identity pool by checking the principal\u2019s token claims in aws:RequestContext."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A notebook lifecycle script clones code from CodeCommit. The IAM role has ListRepos and GitPull permissions but the clone fails. On inspecting the trust policy, you find no conditions. What\u2019s the missing configuration?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add an SCP allowing codecommit:GitPull",
      "B": "Add a Condition in the IAM trust policy restricting the service principal to sagemaker.amazonaws.com",
      "C": "Attach AWSCodeCommitPowerUser to the role",
      "D": "Enable cross-account access in CodeCommit"
    },
    "explanation": "Notebook lifecycle functions are invoked by the SageMaker service, so the role\u2019s trust policy must allow sagemaker.amazonaws.com as a principal to assume the role. Without it, the role cannot be assumed and clone fails."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A security audit reveals CloudWatch Logs for SageMaker were publicly accessible. How do you block public access while preserving individual user access?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Delete the log group ACLs",
      "B": "Enable KMS encryption on the log group",
      "C": "Apply an SCP to prevent DescribeLogGroups",
      "D": "Attach a resource policy to the log group denying Principal '*' for log:Describe and log:Get"
    },
    "explanation": "A CloudWatch Logs resource policy can explicitly deny calls from anonymous principals (*) while allowing authenticated IAM principals to continue accessing the logs."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A data scientist left a SageMaker endpoint open to the internet. You need to remediate: lock down to a VPC in us-east-1 and allow only IPs from 10.0.0.0/16. What is the fastest remediation?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Delete the endpoint and recreate within a VPC",
      "B": "Update the endpoint configuration with VpcConfig and apply a security group that allows only 10.0.0.0/16",
      "C": "Attach an endpoint policy restricting CIDR",
      "D": "Apply a bucket policy on model artifacts"
    },
    "explanation": "Updating the existing endpoint\u2019s VpcConfig and security group is fastest. Deletion and recreation is slower; endpoint policies do not control network access."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "Your CodeBuild project for ML uses Docker containers on shared hardware. You must ensure build logs are encrypted with a custom KMS key. Which change meets this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable CloudWatch Logs for the project",
      "B": "Attach AWSKeyManagementServicePowerUser to CodeBuild role",
      "C": "Specify project\u2019s logsConfig.cloudWatchLogs.encryptionDisabled=false and kmsKey in CloudFormation",
      "D": "Use SSE-S3 for the CodeBuild logs bucket"
    },
    "explanation": "In the CodeBuild logsConfig, setting encryptionDisabled to false and specifying the KMS key ensures logs are encrypted with the customer-managed key."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A compliance mandate: all secrets accessed by SageMaker (e.g., DB credentials) must require KMS grant usage via key grants rather than policy. How to implement?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Use KMS grants to give the SageMaker execution role temporary decrypt privileges for the secret\u2019s CMK",
      "B": "Modify the key policy to allow the SageMaker role Decrypt",
      "C": "Add the SageMaker role to a KMS key alias",
      "D": "Enable automatic rotation on the secret"
    },
    "explanation": "KMS grants provide temporary, auditable decrypt permissions. Modifying key policies is static and not per-request as required."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A model registry must only allow promotion of approved models. You need to restrict who can call CreateModelPackageVersion. What\u2019s the best practice?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use an SCP denying sagemaker:CreateModelPackageVersion",
      "B": "Enable AWS Config rule for non-approved models",
      "C": "Create a SageMaker model registry tag rule",
      "D": "Attach an IAM policy requiring a specific request tag (e.g., ApprovedBy) for CreateModelPackageVersion"
    },
    "explanation": "Requiring a request tag via IAM policy ensures that only requests including the ApprovedBy tag can create new versions, enforcing the approval workflow."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A Dev team\u2019s pipeline stores its CloudFormation templates in S3. You must ensure only specific pipeline roles can read and write these templates. Which is correct?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Grant public read and limit write via signed URLs",
      "B": "Apply S3 bucket policy allowing List, Get, Put only to the IAM roles of the pipeline stages",
      "C": "Use KMS encryption only",
      "D": "Restrict via VPC endpoint policy"
    },
    "explanation": "An S3 bucket policy scoped to only the pipeline\u2019s IAM role ARNs for List, Get, and Put enforces that no other principals can read or write."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A post-deployment check must validate that SageMaker Notebook Instances are encrypted at rest. Which automated check fulfills this?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "CloudWatch alarm on unencrypted volumes",
      "B": "Enable AWS Config managed rule EC2_ENCRYPTED_VOLUMES",
      "C": "Enable AWS Config managed rule SAGEMAKER_NOTEBOOK_INSTANCE_ENCRYPTED_VOLUME_CHECK",
      "D": "Use GuardDuty on notebooks"
    },
    "explanation": "AWS Config\u2019s SageMaker-specific managed rule checks encryption of notebook volumes directly. EC2 rules do not cover SageMaker notebook volumes."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A SageMaker Processing job uses a role that also has S3ListAllMyBuckets permission. Security wants to remove ListAll permission but still allow list on two buckets. How to implement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Replace wildcard list permission with explicit s3:ListBucket on the two bucket ARNs",
      "B": "Add an SCP denying s3:ListAllMyBuckets",
      "C": "Use bucket policies on S3 buckets",
      "D": "Rotate IAM role credentials daily"
    },
    "explanation": "Restricting the role\u2019s IAM policy to only s3:ListBucket on the two specified buckets removes global list permission while allowing needed buckets."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A pipeline deploys a SageMaker model using AWS CDK. The synth step fails due to lack of KMS decrypt for the secret used in a CDK context. How can you fix this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Grant the CDK role kms:GenerateDataKey*",
      "B": "Attach SecretsManagerReadWrite to CDK role",
      "C": "Use plaintext secret in code",
      "D": "Grant the CDK execution role kms:Decrypt on the secret\u2019s CMK in the key policy"
    },
    "explanation": "The CDK synth needs kms:Decrypt rights on the CMK to read the secret. Adding that to the key policy resolves the failure."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A regulatory requirement: all IAM role assumptions by SageMaker must be logged and alerts sent on unusual assume-role calls. How do you implement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable CloudWatch Logs on SageMaker",
      "B": "Create CloudTrail trail capturing IAM:AssumeRole, create CloudWatch metric filter for the SageMaker service principal, and alarm",
      "C": "Use AWS Config rule to check unusual assume-role calls",
      "D": "Rotate the SageMaker service role daily"
    },
    "explanation": "A CloudTrail trail logs all AssumeRole calls; a metric filter on the CloudWatch Logs for the SageMaker principal can trigger alarms for any assumption events."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A data scientist requests temporary credentials for S3 to use inside a notebook, without exposing long-lived keys. What is the most secure pattern?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use AWS STS GetSessionToken in the notebook to generate time-limited credentials",
      "B": "Embed an IAM user access key in the notebook",
      "C": "Use an S3 pre-signed URL for each object access",
      "D": "Use root account credentials stored in Secrets Manager"
    },
    "explanation": "Using STS GetSessionToken provides temporary credentials with limited lifetime. Pre-signed URLs are object-specific and not convenient; root or static keys are insecure."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A SageMaker Endpoint in VPC must only communicate with ECR and CloudWatch Logs, not the public internet. How do you enforce this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Attach a bucket policy on ECR",
      "B": "Use NAT Gateway with restrictive route tables",
      "C": "Deploy endpoint in private subnets with interface endpoints for ECR and CloudWatch Logs and no NAT",
      "D": "Use Security Group to block 0.0.0.0/0"
    },
    "explanation": "Interface VPC endpoints allow access to AWS services without internet. Deploying in private subnets without NAT ensures no egress to public internet."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A build in CodeBuild requires access to a VPC-only RDS instance. The CodeBuild service role has all necessary DB permissions, but builds fail to connect. What must you configure?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Open the DB\u2019s security group to 0.0.0.0/0",
      "B": "Configure the CodeBuild project\u2019s VpcConfig with the DB\u2019s subnets and security group",
      "C": "Use a NAT gateway in the project\u2019s VPC",
      "D": "Attach AWSLambdaVPCAccessExecutionRole to the CodeBuild role"
    },
    "explanation": "CodeBuild must be configured with VpcConfig to attach ENIs in the same subnets/security groups as the RDS. Without this, it runs in default network and cannot reach the DB."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "An audit finds that some SageMaker endpoint access logs contain PII. You must restrict retrieval of those logs to a security analysis role. How?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Attach a resource policy on the CloudWatch Logs group allowing only the security role principal to get logs",
      "B": "Encrypt logs with SSE-S3",
      "C": "Use an SCP to deny DescribeLogStreams",
      "D": "Disable logging on the endpoint"
    },
    "explanation": "A CloudWatch Logs resource policy scoped to the security analysis role ensures only that role can retrieve the logs; others will be denied."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A new security control: SageMaker projects must only use predefined IAM permission boundaries. How to ensure new roles created by SageMaker pipelines comply?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use an SCP to deny iam:CreateRole",
      "B": "Tag all new roles and enforce with AWS Config",
      "C": "Use CodePipeline approval stage",
      "D": "Attach the managed permission boundary policy in the SageMaker pipeline role so that any new role inherits the boundary"
    },
    "explanation": "Permission boundaries attached to the role that creates new roles propagate the boundary to child roles, ensuring they cannot be more permissive."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A SageMaker Model Monitor job must write drift reports to S3 but only to a single bucket. The monitor role has wild-card S3 permissions. How do you tighten permissions without affecting other workflows?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a bucket policy to deny PutObject from that role",
      "B": "Replace the role\u2019s S3 wildcard with a policy granting PutObject only on the monitor reports bucket",
      "C": "Implement an SCP",
      "D": "Encrypt the bucket with default encryption"
    },
    "explanation": "Updating the monitor role\u2019s IAM policy to allow only the specific bucket reduces blast radius without impacting other S3 workflows."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A security guideline: all SageMaker processing jobs must use a VPC endpoint to S3. One job failed to access data. Which misconfiguration is most likely?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "The role lacks s3:GetObject",
      "B": "The bucket policy denies all traffic",
      "C": "VpcConfig lacked the S3 gateway endpoint in the subnet route tables",
      "D": "The job ran in a public subnet"
    },
    "explanation": "If the S3 gateway endpoint is not added to the subnet route tables in VpcConfig, the job cannot reach S3. A public subnet or role error would yield different symptoms."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "A data science team needs to deploy a SageMaker real-time inference endpoint in a private VPC. They want to script the entire environment using AWS CloudFormation. Which resources and configuration should they include to ensure proper VPC connectivity and auto scaling of the endpoint?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Define an AWS::SageMaker::Endpoint with VpcConfig, and rely on SageMaker\u2019s built-in auto scaling by setting EnableAutoScaling: true.",
      "B": "Create AWS::SageMaker::EndpointConfig with VpcConfig.SecurityGroupIds and Subnets, an AWS::SageMaker::Endpoint that references that config, an AWS::ApplicationAutoScaling::ScalableTarget for sagemaker:variant:DesiredInstanceCount, and an AWS::ApplicationAutoScaling::ScalingPolicy.",
      "C": "Use AWS::EC2::VPCEndpoint for SageMaker API, then AWS::AutoScaling::AutoScalingGroup to scale instances behind the endpoint.",
      "D": "Use AWS::Lambda-backed custom resource to modify the SageMaker endpoint configuration after deploy for VPC and scaling settings."
    },
    "explanation": "CloudFormation requires separate AppAutoScaling resources; SageMaker endpoint resource does not directly support scaling."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "An ML engineer wants to provision a multi-container SageMaker endpoint via AWS CDK in Python. They need one container for model A, one for model B, each with different IAM roles and different environment variables. Which CDK constructs and pattern should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use sagemaker.CfnModel for each container with respective environment and roles, then sagemaker.CfnEndpointConfig listing both models, and sagemaker.CfnEndpoint.",
      "B": "Use sagemaker.Model for each container, then sagemaker.MultiModelEndpoint to combine them automatically.",
      "C": "Define two sagemaker.CfnModel constructs with separate ExecutionRoleArn and Environment properties, create a single sagemaker.CfnEndpointConfig with both ModelName entries under ProductionVariants, then a sagemaker.CfnEndpoint.",
      "D": "Use sagemaker.EndpointBatchTransform with multiple TransformJobDefinitions for each container."
    },
    "explanation": "Multi-container real-time endpoints require separate CfnModel and listing in a single EndpointConfig."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "A team uses AWS CDK to deploy their SageMaker endpoint, but CloudFormation reports drift in the EndpointConfig when they manually update the environment variables via console. How can they prevent drift and enforce configuration as code?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Remove manual updates; enforce changes only through CDK by maintaining environment variables in code and deploying via CDK pipelines.",
      "B": "Set EnableDriftCorrection: true on the CfnEndpointConfig resource.",
      "C": "Use AWS Config to ignore drift on SageMaker endpoint resources.",
      "D": "Use a CloudFormation macro to override the console changes at deployment time."
    },
    "explanation": "Prevent drift by managing all changes in code; CloudFormation cannot auto-correct SageMaker endpoints."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "An organization requires that all SageMaker endpoint logs be sent to CloudWatch Logs in a separate AWS account. They want to script this via AWS CloudFormation. Which approach satisfies least-privilege and cross-account delivery?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Grant the SageMaker execution role cross-account CloudWatchPutLogEvents permission.",
      "B": "Attach AWS::Logs::SubscriptionFilter on the endpoint\u2019s log group to the destination account.",
      "C": "Use AWS::CloudWatch::Destination in the SageMaker stack to forward logs directly.",
      "D": "Deploy a CloudWatch Log Group export task via AWS::Logs::Destination in the central account and use resource policy on the SageMaker log group to allow PutSubscriptionFilter to the destination."
    },
    "explanation": "Cross-account subscription requires a CloudWatch Logs destination with resource policy granting permissions."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "A dev team needs to automate creation of multiple SageMaker endpoints with different instance types and scaling policies using AWS CDK. They want to avoid duplication of code. Which CDK pattern is most appropriate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Write separate stacks for each endpoint and instantiate them individually.",
      "B": "Define a high-level construct class that takes instance type and scaling parameters as properties, and reuse it for each endpoint.",
      "C": "Use a single CfnInclude to import an existing CloudFormation template with parameters.",
      "D": "Leverage AWS::CloudFormation::Stack resource within the same template for each endpoint."
    },
    "explanation": "A custom CDK construct promotes reuse and parameterization across multiple endpoints."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "A security audit requires that SageMaker endpoints be deployed only within private subnets and routed through NAT for internet access. How should this be scripted in AWS CDK to satisfy both requirements?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Specify subnetSelection with subnetType.PUBLIC in VpcConfig so endpoints use NAT automatically.",
      "B": "Set EndpointConfig.KmsKeyId to use private subnets.",
      "C": "Pass private subnet IDs and security group IDs in VpcConfig when defining CfnEndpointConfig in CDK, ensuring they are Private isolated subnets with NAT configured on the VPC.",
      "D": "Use VpcLink to attach the endpoint to a private Network Load Balancer."
    },
    "explanation": "VpcConfig must reference private subnets and SGs; NAT is configured at VPC level, not endpoint."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "An ML team wants to integrate spot instances into their inference fleet behind an Application Load Balancer with automatic scaling. They decide to use ECS on Fargate Spot. Which CloudFormation resource definitions must they include to ensure the desired capacity and scaling behavior?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS::ECS::CapacityProvider with AutoScalingGroupProvider using Spot Instances, AWS::ECS::Cluster defaultCapacityProviderStrategy including that capacity provider, and AWS::ECS::Service with desiredCount parameter.",
      "B": "AWS::AutoScaling::AutoScalingGroup with MixedInstancesPolicy using ON_DEMAND_ONLY, and AWS::ECS::Service.",
      "C": "AWS::ECS::Service with LaunchType FARGATE only, and AWS::ApplicationAutoScaling::ScalableTarget for ECS service.",
      "D": "AWS::Batch::ComputeEnvironment configured for Spot, and AWS::Batch::JobDefinition for inference jobs."
    },
    "explanation": "ECS Fargate Spot requires a capacity provider defined, then attach to the service for scaling on Spot."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "Your organization mandates that all ML model container images be stored in Amazon ECR with image scanning enabled at push time. You must script this using AWS CDK. Which steps are required?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Define an ECR repository in CDK with imageScanningConfiguration enabled, grant SageMaker execution role pull permissions, then use that repository URI in CfnModel.",
      "B": "Use ecr.Repository with imageScanOnPush: true in CDK, add repository.grantPull(sagemakerRole), then pass repository.repositoryUri to SageMaker Model property in CfnModel or CfnModelPackage.",
      "C": "Create AWS::ECR::LifecyclePolicy in CloudFormation and set scanOnPush: ENABLED.",
      "D": "Use AWS CLI in a custom resource to enable image scan on existing repo."
    },
    "explanation": "CDK\u2019s ecr.Repository supports imageScanOnPush; then grant pull to the role used by SageMaker."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "A SageMaker endpoint fails health checks intermittently due to VPC ENI provisioning delays, causing AWS CloudFormation deployment to rollback. How can you modify your CloudFormation template to handle this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set EndpointConfig.DeleteAfterUse: false.",
      "B": "Add CreationPolicy with ResourceSignal on AWS::SageMaker::Endpoint.",
      "C": "Wrap the CfnEndpoint resource in a Custom::SageMakerEndpoint that retries.",
      "D": "Use DependsOn with a custom wait condition Lambda that polls DescribeEndpointStatus until InService before signaling success."
    },
    "explanation": "CloudFormation offers no native retry for SageMaker endpoints; implement a wait condition via Lambda or custom resource."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "Your infra-as-code pipeline uses AWS CDK to deploy a SageMaker inference endpoint. During a deployment, you need to update the endpoint's variant weight to shift 20% traffic to a new model variant. How do you script this with least downtime?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Update the existing EndpointConfig in-place via CfnUpdatePolicy in CloudFormation.",
      "B": "Delete and recreate the endpoint with the new variant weight settings.",
      "C": "Create a new AWS::SageMaker::EndpointConfig with both variants and desired weights, then update the AWS::SageMaker::Endpoint to point to the new config.",
      "D": "Use AWS CLI in a CDK CustomResource to call UpdateEndpointWeight directly."
    },
    "explanation": "Endpoint updates must reference a new EndpointConfig; creating a new config and pointing the endpoint to it avoids downtime."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "A DevOps engineer must ensure that SageMaker endpoint deployments are idempotent and environment-specific. They decide to use CloudFormation parameter overrides. Which of these strategies will achieve both requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Store instance type and count in template metadata and reference via Fn::GetAtt.",
      "B": "Define Parameters for InstanceType, InitialInstanceCount, VPC Subnet IDs, and SecurityGroup IDs; reference them in CfnEndpointConfig, ensuring different values per environment.",
      "C": "Use Mappings section keyed by environment name to look up all values.",
      "D": "Use a separate template per environment to hardcode values."
    },
    "explanation": "Parameters allow idempotent, repeatable deployments with all settings externalized per environment."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "You need to deploy a SageMaker multi-model endpoint for cost-effective hosting of hundreds of small models. You want to automate provisioning with CloudFormation and enforce container image immutability. What is the correct approach?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create one AWS::SageMaker::ModelPackage for each container with ModelApprovalStatus set to Approved and InferenceSpecification referencing ECR image digests, then one AWS::SageMaker::EndpointConfig with MultiModelConfig.",
      "B": "Use AWS::SageMaker::Model with LocalCode option and set ImageConfig.AutoUpdate: false.",
      "C": "Deploy a custom multi-model server on an EC2 Auto Scaling group instead.",
      "D": "Use a single AWS::SageMaker::Endpoint with MultiModelEndpoint property."
    },
    "explanation": "ModelPackage with approved inference spec and image digest ensures immutability; EndpointConfig multi-model supports dynamic loading."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "A regulated environment requires all SageMaker endpoints to be deployed via AWS CloudFormation and reviewed via change sets. When a change to endpoint instance type is approved, CloudFormation update fails because of immutable properties. How should you script your template to allow instance-type changes without replacing resources?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set UpdateReplacePolicy: Retain on the Endpoint resource.",
      "B": "Use AWS::SageMaker::EndpointConfig with EndpointConfigName generated by a GUID to force replacement.",
      "C": "Use a nested stack for the endpoint and update only nested stacks.",
      "D": "Decouple endpoint from endpoint config: script EndpointConfig as separate resource and update the EndpointConfigName in the Endpoint resource, so only the config is replaced, not the endpoint."
    },
    "explanation": "EndpointConfig can be replaced independently; endpoint resource simply points to new config name."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "Your team wants to use AWS CDK in TypeScript to deploy SageMaker endpoints and test them automatically post-deployment. They need to add an automated post-deploy test hook in the same pipeline. Which CDK pattern should they adopt?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use CDK aspect to inject Lambda-backed custom resources.",
      "B": "Utilize CDK Pipelines\u2019 testing framework: add CodeBuild Step after deployment stage invoking a smoke-test Lambda or script.",
      "C": "Add AWS::CodeDeploy::DeploymentGroup in the CDK template.",
      "D": "Write a CDK Custom Resource that runs tests as part of deployment."
    },
    "explanation": "CDK Pipelines support post-deploy tests via CodeBuild steps integrated into the pipeline."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "An engineer must ensure that SageMaker endpoint container images are built, pushed to ECR, and deployed in one IaC deployment. Using AWS CDK, which sequence of constructs accomplishes this atomically?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Define a CodeBuild Project in CDK that builds the image, then a separate CDK app that references the ECR repository.",
      "B": "Use a custom resource to call AWS CLI to build and push, then define the CfnModel.",
      "C": "Use CodePipeline CDK constructs: define a pipeline with Source stage, Build stage (CodeBuild building and pushing to ECR), then a Deploy stage with CDK deploy of the SageMaker endpoint stack referencing image URI as pipeline output.",
      "D": "Use CDK Bundling API on the sagemaker.Model construct."
    },
    "explanation": "CDK Pipelines can orchestrate build and deploy stages, ensuring atomic transition from image build to endpoint deployment."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "Your infra team must grant least-privilege permissions for SageMaker to pull container images from a private ECR in another AWS account. Which IAM policy should you script in CloudFormation on the SageMaker execution role?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Allow ecr:BatchGetImage, ecr:GetDownloadUrlForLayer on the repository Arn directly in the role policy.",
      "B": "Policy allowing sts:AssumeRole on a role in the ECR account that has ecr:GetAuthorizationToken, BatchGetImage, GetDownloadUrlForLayer, and in the ECR account\u2019s repository policy grant the SageMaker role\u2019s AWS principal those actions.",
      "C": "Grant AmazonEC2ContainerRegistryFullAccess to the SageMaker execution role.",
      "D": "Add AWS-managed policy AmazonSageMakerFullAccess which includes ECR pull permissions."
    },
    "explanation": "Cross-account ECR pull requires AssumeRole into the ECR account and corresponding repository policy; broad managed policies violate least privilege."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "A team uses AWS::CloudFormation::Stack resource to encapsulate SageMaker endpoint provisioning in a nested stack. They now need to expose the endpoint name to the parent stack for AutoScaling configuration. Which method achieves this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "In the nested stack template, declare an Output for EndpointName; in the parent stack, reference it via Fn::GetAtt on the AWS::CloudFormation::Stack resource.",
      "B": "Use ImportValue to pull it from the nested stack without declaring Output.",
      "C": "Write a Lambda-backed custom resource in the parent to call DescribeStacks.",
      "D": "Use AWS::SSM::Parameter to store the endpoint name and read it in the parent."
    },
    "explanation": "Outputs and Fn::GetAtt on nested stacks pass values to parent stacks."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "Your company requires that all SageMaker endpoint configurations be version controlled and reviewed. Which AWS CDK and CloudFormation features should you use to allow change visibility and prevent unreviewed drift?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable termination protection on CfnEndpoint resources.",
      "B": "Use sagemaker.Endpoint.fromEndpointName to reference existing endpoint without managing it.",
      "C": "Use CDK to synthesize CloudFormation change sets and require manual approval before execution in pipelines.",
      "D": "Enable rolling updates in CloudFormation on the SageMaker stack."
    },
    "explanation": "Change sets provide visibility into what will change before applying; CDK pipelines can enforce manual approvals."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "An ML engineer needs to script blue/green deployments of a SageMaker endpoint using AWS CloudFormation and minimize invocation errors during traffic shift. Which pattern should they implement?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS::SageMaker::EndpointVariants property to specify both blue and green variants and weights.",
      "B": "Define two separate Endpoint resources and swap DNS records in Route53.",
      "C": "Use CodeDeploy with blue/green strategy for SageMaker service.",
      "D": "Use AWS::SageMaker::EndpointConfig to define two production variants with weights, attach AppAutoScaling policy to adjust weights gradually as part of CloudFormation update."
    },
    "explanation": "Defining both variants and adjusting weights via scaling policies enables gradual traffic shift without downtime."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "A DevOps pipeline must deploy a SageMaker batch transform job definition and a serving endpoint in one CloudFormation template. They encounter circular dependency between the IAM role and the endpoint config. How can they resolve this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Duplicate an IAM role resource for each service to break the cycle.",
      "B": "Use separate CloudFormation nested stacks: one for IAM role definition, one for endpoint and transform referencing an exported role ARN.",
      "C": "Use DependsOn between the role and endpoint config to force order.",
      "D": "Use AWS::IAM::ManagedPolicy to attach permissions instead of inline policies."
    },
    "explanation": "Splitting into nested stacks allows exporting the role ARN to avoid circular resource references."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "Your infrastructure uses AWS CDK to deploy SageMaker endpoints. A CloudFormation rollback occurs because the endpoint creation takes longer than the default timeout. How can you adjust the timeout in CDK?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set cfnOptions.creationPolicy with a Timeout value on the CfnEndpoint CDK construct.",
      "B": "Set the AWS SDK client timeout in CDK context.",
      "C": "Use AWS::CloudFormation::WaitCondition within the same stack.",
      "D": "Wrap the deployment in a longer-running AWS Step Functions state machine."
    },
    "explanation": "CDK\u2019s cfnOptions.creationPolicy allows specifying timeout for resource creation; other methods don\u2019t apply to SageMaker endpoints."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "An ML platform team must deploy hundreds of endpoints with varying instance counts. They choose AWS CDK to loop constructs. How should they write their code to maintain performance and minimize synth time?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a for loop in CDK to create each endpoint construct directly in the stack.",
      "B": "Use AWS::CloudFormation::Stack for each endpoint to parallelize.",
      "C": "Generate endpoint configurations via a CDK Construct that iterates over a configuration array at synthesis time, rather than at runtime, and emits only necessary CloudFormation resources.",
      "D": "Use AWS Step Functions to call CDK synth for each endpoint separately."
    },
    "explanation": "Constructs iterate at synth time to produce a single CF template; runtime loops in Lambda or Step Functions are inappropriate."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "A compliance requirement mandates that all SageMaker endpoints be deployed in dedicated subnets per environment. You already have a CDK VPC with multiple isolated subnet groups. How do you guarantee subnet selection per environment?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Hardcode subnet IDs in the CDK code per environment.",
      "B": "Define CDK Stack context values for subnetGroupNames per environment and use vpc.selectSubnets({subnetGroupName: ...}).",
      "C": "Use AWS::TaggedResource tagging on the VPC and filter subnets via CloudFormation Intrinsics.",
      "D": "Use defaultSubnets selection and rely on AWS to pick isolated ones."
    },
    "explanation": "CDK context allows environment-specific parameters; vpc.selectSubnets filters by groupName tag."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "Your team wants to use AWS CDK to produce YAML CloudFormation templates for all resources. They need to validate their template for SageMaker endpoint VPC settings before deployment. Which CDK command and plugin should they use?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "cdk synth --no-staging and the cfn-lint plugin to lint the synthesized template.",
      "B": "cdk diff and AWS::SageMaker::ValidateEndpoint API.",
      "C": "cdk deploy --dry-run.",
      "D": "cdk docs to generate CloudFormation schema and manually inspect."
    },
    "explanation": "cdk synth outputs the template for cfn-lint validation; diff shows changes but not schema errors."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A healthcare provider has a highly imbalanced dataset (1% positive cases) stored in Amazon S3. They need to generate synthetic minority samples to train a disease detection model, ensure PHI compliance (no real patient data leakage), and prevent inversion attacks on synthetic data. Which approach best meets these requirements?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "Use AWS Glue DataBrew to oversample the minority class and enable record-level masking to anonymize PHI.",
      "B": "Implement a CTGAN model in a SageMaker notebook with differential privacy (DP-SGD), generate synthetic samples within a VPC-backed FSx for Lustre, and use a KMS key for encryption.",
      "C": "Use SageMaker Clarify\u2019s synthetic data feature to generate samples and store results in an S3 bucket with default encryption.",
      "D": "Subscribe to a third-party synthetic health dataset via Amazon Data Exchange and merge it with your S3 data."
    },
    "explanation": "CTGAN with DP-SGD ensures differential privacy (protects against inversion), runs in SageMaker within the VPC, uses FSx for high-performance storage, and KMS encryption prevents PHI leakage, satisfying all requirements."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A bank wants to detect selection bias in its loan application dataset (applicants by ZIP code). They must quantify the difference in proportions of approval rates between urban and rural ZIP codes before training. Which SageMaker Clarify pre-training bias configuration is most appropriate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use bias_config with 'label'='approval', 'facet_name'='ZIP_code_type', and 'metrics'=['dipl','ci'].",
      "B": "Use ModelMonitor with DataQualityMonitor to compute missing values in the ZIP_code_type feature.",
      "C": "Use bias_config with 'facet_name'='approval', 'label_values_or_threshold'=['urban','rural'], and 'metrics'=['feature_attribution'].",
      "D": "Use Clarify post-training explainability job to get SHAP values for ZIP_code_type."
    },
    "explanation": "Pre-training bias_config must specify label and facet_name (ZIP_code_type) to compute DPL (difference in proportions of labels) and CI (class imbalance) before training. Other options misuse metrics or post-training tools."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "An online retailer stores customer data in DynamoDB with encrypted PII (customer_id, email). They export it to S3 for cleaning. They need to mask PII during data cleansing using AWS Glue DataBrew. Which recipe action and configuration should they use?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Use 'maskValue' on columns [customer_id,email] with a static masked value.",
      "B": "Use 'hash' on columns [customer_id] and 'tokenize' on [email] with default salts.",
      "C": "Use 'encrypt' on columns [customer_id,email] using the DynamoDB table key.",
      "D": "Use 'maskValue' with 'Mask Type'='Random Character', 'Character Set'='Alphanumeric', on [customer_id,email]."
    },
    "explanation": "DataBrew\u2019s maskValue recipe action with Random Character on specified PII columns properly masks values without exposing patterns. Hash and encryption aren\u2019t available as DataBrew recipe actions, and static mask would reveal structure."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A credit-scoring dataset has 10% missing values in the 'employment_length' feature. You need to impute missing values to reduce measurement bias and avoid skew in training. The distribution is right-skewed. What is the best imputation strategy using AWS Glue Data Quality rules?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Impute missing 'employment_length' with the mean value computed over all records.",
      "B": "Use median imputation per gender group via a DataBrew 'Fill with median' action partitioned by 'gender'.",
      "C": "Use a Data Quality job to compute the median per 'employment_type' and then apply DataBrew 'Fill with value from column aggregation' partitioned by 'employment_type'.",
      "D": "Apply mode imputation globally for 'employment_length' using the most frequent category."
    },
    "explanation": "Median per employment_type addresses right skew within similar segments and reduces bias. A Data Quality job can compute segment medians and DataBrew can apply partitioned imputation accordingly."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "You must ensure all S3 data at rest and in transit is encrypted for an ML training job. Data is loaded from S3 into a SageMaker Training job and stored on an FSx for Lustre file system. Which configuration satisfies both requirements?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable SSE-KMS on the S3 bucket, attach an IAM role with KMS decrypt, launch FSx with transit encryption enabled and a customer-managed KMS key.",
      "B": "Use SSE-S3 on the S3 bucket and launch FSx with default AWS-managed KMS key.",
      "C": "Configure SageMaker training input to use SSL and store data in unencrypted FSx.",
      "D": "Enable client-side encryption for S3 uploads and use FSx encryption at rest without transit encryption."
    },
    "explanation": "SSE-KMS on S3 and FSx with transit encryption via customer-managed KMS ensure encryption at rest and in transit. SSE-S3 lacks customer KMS control, and client-side or missing transit encryption fails one requirement."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A telco data pipeline streams CDR logs into S3, with PII fields (phone_number) encrypted via KMS. A data scientist needs to preprocess and anonymize phone numbers before modeling using SageMaker Processing. Which approach ensures the data scientist cannot decrypt original values?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provide the processing job with KMS decrypt permissions and apply one-way hash.",
      "B": "Use a KMS grant to decrypt phone_number, then pseudonymize with reversible mapping.",
      "C": "Use Lambda triggered by S3 events to replace phone_number with SHA-256 digest before encrypting with a separate KMS key that processing job lacks decrypt permissions for.",
      "D": "Configure SageMaker Processing to use a private KMS key different from S3\u2019s and decrypt in job."
    },
    "explanation": "Lambda replaces PII with irreversible SHA-256 digest, re-encrypts with a key the job can use, ensuring the processing job cannot decrypt the original KMS-encrypted phone numbers."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "An image dataset stored in S3 contains location metadata in EXIF that constitutes PII. You must strip this metadata in a SageMaker Processing job, standardize image sizes, and then encrypt outputs in transit to FSx. Which pipeline achieves this with minimal steps?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Lambda to strip EXIF and resize, store intermediate in S3, then run a Processing job to copy to FSx with SSL.",
      "B": "Run a SageMaker Processing job with a custom container that uses exiftool to strip metadata, Pillow to resize, and output directly to FSx for Lustre with HTTPS mount.",
      "C": "Download images to EFS, run an EC2 batch job to preprocess and re-upload to encrypted S3, then mount S3 to FSx.",
      "D": "Use AWS Glue Spark job to strip EXIF and resize, write to FSx using AWS SDK with SSL."
    },
    "explanation": "A SageMaker Processing job with a custom container can handle EXIF stripping and resizing in one step and write directly over HTTPS to FSx for Lustre, minimizing components."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "You need to detect data drift and bias before training on new monthly data. The monthly datasets arrive in S3. Which combination of SageMaker Clarify configuration and workflow ensures automated pre-training bias and data drift detection?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Clarify post-training job, schedule via EventBridge to run monthly.",
      "B": "Use Clarify ModelBiasMonitor as part of ModelMonitor, schedule daily.",
      "C": "Use DataQualityMonitor to detect drift, then manually analyze with Clarify.",
      "D": "Configure Clarify pre-training bias job with baseline from last month, schedule with SageMaker Pipelines monthly to compute DPL, CI, and feature distribution drift."
    },
    "explanation": "Pre-training Clarify job in SageMaker Pipelines scheduled monthly compares new data to a historic baseline, computes DPL and CI for bias and distribution drift, and automates detection."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A startup must comply with GDPR: remove or mask 'email' and 'user_ip' fields before any ML training. They need an auditable automated solution using AWS services. Which design meets GDPR requirements?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create a SageMaker Processing pipeline that uses a DataBrew job for masking email and user_ip, store masked data in a GDPR-compliant S3 bucket with audit logs via CloudTrail.",
      "B": "Have developers write custom Python in a notebook to drop fields, then manually approve and upload to S3.",
      "C": "Use Amazon Macie to discover PII and instruct DataSync to exclude those columns.",
      "D": "Encrypt email and user_ip with SSE-KMS and train model on encrypted features."
    },
    "explanation": "DataBrew recipe for masking with audit logs in CloudTrail ensures fields are irreversibly masked and provides an auditable, automated workflow. Encryption doesn\u2019t remove PII for GDPR."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "You must validate that numeric features shipped to training satisfy min/max thresholds (e.g., age between 18 and 100). Violations should block training. How can you implement this in SageMaker?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a Lambda pre-processing check triggered by S3 upload events to reject invalid datasets.",
      "B": "Write custom validation code in the training script and raise errors if thresholds are breached.",
      "C": "Configure an AWS Glue Data Quality job to run pre-training via SageMaker Processing, fail the Pipeline step if thresholds outside range.",
      "D": "Use SageMaker Model Monitor DataQualityMonitor in real time to monitor feature thresholds."
    },
    "explanation": "Glue Data Quality job integrated into SageMaker Pipeline allows rule-based validation (min/max), and the pipeline can be configured to stop if validation fails, preventing invalid data from training."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A telecom dataset has missing values in time-series features. You need to impute missing timestamps using forward fill within each subscriber partition, ensure no cross-subscriber leakage, and provide a clean CSV to FSx. Which solution accomplishes this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Clarify to detect missing timestamps and fill globally with median intervals.",
      "B": "Use AWS Glue ETL with a window function to forward fill across all records.",
      "C": "Use SageMaker Processing with a Spark container, group by subscriber and apply forward fill, write to S3.",
      "D": "Deploy a SageMaker Processing job with a custom pandas script that partitions by subscriber_id, forward fills timestamps, and writes output to FSx for Lustre."
    },
    "explanation": "A SageMaker Processing job with pandas partitioned by subscriber_id isolates subscribers and forward fills, writing the clean CSV to FSx. Clarify doesn\u2019t impute and Glue window could leak between partitions if misconfigured."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "Your new dataset contains categorical features with high cardinality (10,000 unique values). You need to reduce dimensionality before modeling to prevent overfitting. Which AWS tool and method should you use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use DataBrew 'One-hot encode' action on the categorical column.",
      "B": "Use SageMaker Feature Store offline store and apply a frequency encoding transform in a Processing job.",
      "C": "Use Clarify bias detection to filter out rare categories.",
      "D": "Use Glue Data Quality to drop categories with low frequency."
    },
    "explanation": "Frequency encoding in a SageMaker Processing job reduces cardinality by mapping to numeric frequencies, avoids one-hot explosion. DataBrew one-hot creates huge feature space. Clarify and Data Quality don\u2019t transform features for modeling."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A sensitive dataset requires K-anonymity (k=10) on PII columns before ML. Which AWS service or combination can enforce k-anonymization in an automated pipeline?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Use AWS Glue DataBrew with the 'Anonymize' recipe action for k-anonymity.",
      "B": "Use Amazon Macie to detect PII and then use Lambda to drop records.",
      "C": "Build a SageMaker Processing job using ARX (or Python library) to enforce 10-anonymity and write to encrypted FSx.",
      "D": "Use SageMaker Clarify to generate anonymized synthetic data with k-anonymity."
    },
    "explanation": "No native AWS service enforces k-anonymity; a custom SageMaker Processing job using an open-source library like ARX or a Python package can implement k-anonymity and integrate with FSx for encrypted storage."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "During training, a job fails due to data corruption in Amazon FSx. You need to verify data integrity before each training run and fail fast if corruption occurs. What is the best way to implement this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Integrate a SageMaker Processing step using a checksum validation script to compare file checksums against known-good values stored in AWS Secrets Manager before training.",
      "B": "Enable FSx data compression to detect corrupted blocks.",
      "C": "Use ModelMonitor DataQualityMonitor to detect corrupted data during training.",
      "D": "Rely on SageMaker Training job logs to find errors post-start."
    },
    "explanation": "A pre-training Processing step that computes checksums and validates against stored values (in Secrets Manager) will detect corruption and allow the pipeline to fail fast. FSx compression and post-training monitors are insufficient."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A dataset is subject to data residency regulations requiring that no raw data leaves the EU. You have a multi-region SageMaker Pipeline. How do you ensure compliance when copying data from an S3 bucket in us-east-1 to an FSx file system in eu-west-1?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set S3 replication to eu-west-1 for raw data and run training in us-east-1.",
      "B": "Use S3 Cross-Region Replication to an eu-west-1 bucket, then use FSx for Lustre in eu-west-1 and run the pipeline entirely in eu-west-1.",
      "C": "Use AWS DataSync to transfer data directly from us-east-1 S3 to FSx in eu-west-1.",
      "D": "Use SageMaker Training with input_mode='Pipe' to stream data across regions securely."
    },
    "explanation": "Cross-Region Replication ensures raw data is copied to an EU bucket. Training and FSx in eu-west-1 ensure raw data doesn\u2019t leave the EU region. DataSync or streaming across regions violates residency rules."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "Your text dataset uses masked PII (<NAME>, <EMAIL>) but downstream tokenization treats these tokens as real vocabulary and biases the model. How do you preprocess these masks to avoid model bias?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use DataBrew to replace <NAME> and <EMAIL> with random real names and emails.",
      "B": "Use SageMaker Clarify to drop samples containing masked tokens.",
      "C": "Use a custom tokenizer to treat <NAME> and <EMAIL> as unique IDs.",
      "D": "In a SageMaker Processing job, replace all masked tokens with null or a uniform [MASK] token recognized by the model\u2019s vocabulary."
    },
    "explanation": "Replacing with a uniform [MASK] token recognized by the model avoids bias from inconsistent placeholders. Random real names reintroduce PII, dropping samples reduces data, unique IDs still bias."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "An ML project requires 50 million tabular records. You must ensure that when shuffling and splitting into train/val/test, no data skew occurs across Amazon FSx shards. Which strategy ensures uniform distribution?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Use a SageMaker Processing job with Spark: read data, apply 'repartition(nShards)' and 'randomSplit' for train/val/test, then write to FSx.",
      "B": "Use FSx distributed copy and rely on random file ordering in S3.",
      "C": "Use SM Channel input with ShuffleConfig in a Training job for splitting.",
      "D": "Use SageMaker Clarify pre-processing to shuffle and split data."
    },
    "explanation": "A Spark repartition(nShards) before randomSplit ensures uniform distribution across shards; FSx copy or ShuffleConfig doesn\u2019t guarantee uniform splits; Clarify is not for splitting."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "Your numeric feature 'income' has extreme outliers. You need to detect and treat them in an automated pipeline using AWS services before training. Which multi-step process is best?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Use a SageMaker Clarify Explainability job to identify outliers and then manually remove them.",
      "B": "Use DataBrew to compute z-scores and drop records with |z|>3 in a recipe, then feed to training.",
      "C": "In a SageMaker Processing job, run AWS Glue Data Quality rule to detect numeric outliers, replace values beyond 1st and 99th percentiles with percentile caps, and write to FSx.",
      "D": "Use ModelMonitor DataQualityMonitor to capture outliers at inference time and then feed back corrections."
    },
    "explanation": "Glue Data Quality rule can detect percentile-based outliers, and a Processing job can cap outliers automatically in a pipeline. Clarify and ModelMonitor don\u2019t enforce pre-training data corrections."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A legal requirement mandates that social security numbers (SSNs) be tokenized with a one-way hash before leaving a secure environment. Which architecture satisfies this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Trigger a Lambda in the secure subnet to hash SSNs with SHA-256 and write tokens to an S3 VPC endpoint bucket.",
      "B": "Encrypt SSNs in S3 with SSE-KMS before training.",
      "C": "Use SageMaker Processing to call DynamoDB to fetch hashed SSNs.",
      "D": "Use Glue Crawlers to detect SSNs and mask them."
    },
    "explanation": "A Lambda in the secure subnet with VPC endpoint for S3 can perform one-way SHA-256 hashing before data leaves, ensuring SSNs are never exposed. Encryption alone is reversible."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "You want to measure dataset imbalance (CI) and difference in proportions (DPL) for a multiclass image dataset stored in S3. You need to generate these metrics at scale before training. Which solution is most efficient?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a SageMaker Processing job with a custom Python script listing S3 image prefixes, counting labels, and computing CI and DPL.",
      "B": "Use SageMaker Clarify pre-training bias job pointed at the manifest file in S3, specifying 'label' and facet, for multiclass metrics.",
      "C": "Use Glue ETL to load metadata into Redshift and run SQL to compute counts.",
      "D": "Use Athena queries against S3 to compute label distributions and calculate DPL post-hoc."
    },
    "explanation": "Clarify pre-training bias job can compute CI and DPL at scale across a manifest without custom coding. Athena or custom scripts require manual metric computation; Clarify automates it."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "Your dataset includes an IP address feature that you must anonymize with consistent tokenization (same IP always same token) but irreversibly. Which technique in a SageMaker Processing job should you implement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Apply SHA-256 hashing with a secret salt stored in Secrets Manager.",
      "B": "Use Python\u2019s md5 without salt to hash the IP addresses.",
      "C": "Use DataBrew\u2019s encrypt action with KMS.",
      "D": "Use SageMaker Clarify to mask feature and record mapping."
    },
    "explanation": "SHA-256 with salt ensures consistent, one-way mapping and prevents rainbow table attacks; md5 is weak without salt; DataBrew encrypt is reversible with KMS; Clarify doesn\u2019t mask."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A data scientist suspects measurement bias due to inconsistent units in a numeric feature (some records in cm, some in inches). They need to detect and correct this automatically before training. How can this be done in AWS?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Clarify data bias to detect inconsistent unit distributions.",
      "B": "Use Glue Data Quality to enforce unit ranges and manual corrections.",
      "C": "Use Athena to query and tag inconsistent units for manual review.",
      "D": "Write a custom SageMaker Processing step that reads the 'unit' column, applies conversions to a standard unit, and writes back to FSx."
    },
    "explanation": "Measurement bias due to units requires a transformation step. A custom Processing job can read the unit metadata column and convert values programmatically. Other AWS services don\u2019t perform unit conversion."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A global dataset contains phone numbers in various formats. You need to standardize formatting and mask all but last four digits, while preserving relational joins via hashed salted values. Which pre-training pipeline components accomplish this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use DataBrew recipe to script regex formatting and static masking of phone numbers.",
      "B": "In a SageMaker Processing job: parse with libphonenumber, format E.164, generate SHA-256 hash with salt from Secrets Manager for join key, and mask all but last four digits in output.",
      "C": "Use AWS Glue Crawlers to detect phone patterns and then use Lambda to transform.",
      "D": "Use SageMaker Clarify to identify PII and then use DataBrew to mask."
    },
    "explanation": "A custom Processing job can use libphonenumber for E.164 formatting, apply salted SHA-256 for join preservation, and mask digits, integrating with Secrets Manager for salt. DataBrew and Clarify can\u2019t fully meet these steps."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "You need to guarantee that no training dataset has more than 5% of records from any single customer_id to prevent overfitting. How do you enforce this in an automated AWS pipeline?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a SageMaker Processing step with Spark: group by customer_id, sample up to 5% of that group into the final dataset, then write to FSx.",
      "B": "Use ModelMonitor to detect customer_id skew post-training and retrain.",
      "C": "Use Glue Data Quality to drop records exceeding threshold.",
      "D": "Use Clarify pre-training to detect over-representation and fail training."
    },
    "explanation": "A Spark-based Processing job can enforce maximum sampling per customer_id before training. Data Quality and Clarify detect but don\u2019t enforce sampling limits; ModelMonitor is post-training."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "Your model requires a balanced training set across 4 classes. The raw dataset in S3 is stored with per-class prefixes. You want SageMaker Training to automatically balance classes at job launch. What configuration achieves this?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Enable input data channel 'ShuffleConfig' and rely on equal distribution.",
      "B": "Use Clarify to rebalance classes pre-training.",
      "C": "Create a Processing job that downsamples over-represented prefix folders to match counts of the smallest class and writes balanced data to FSx for Training input.",
      "D": "Use Training hyperparameters to set 'class_weight' parameter during training."
    },
    "explanation": "SageMaker training doesn\u2019t automatically rebalance files by prefix. A custom Processing job must downsample to equal class counts. class_weight adjusts loss but doesn\u2019t rebalance data."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A dataset with both categorical and numeric features needs to be split into train/validation/test, ensuring each split reflects the same categorical distribution. Which AWS-based method guarantees stratified sampling at scale?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Glue ETL to randomSplit on the full dataset.",
      "B": "Run a SageMaker Processing Spark job: stratified sampling with DataFrame.stat.sampleBy on the category column, then write splits to FSx.",
      "C": "Use Athena CTAS queries with WHERE RAND() thresholds.",
      "D": "Use SageMaker Clarify preprocessing to stratify."
    },
    "explanation": "Spark\u2019s sampleBy in a Processing job enables stratified sampling by category. Athena or randomSplit don\u2019t guarantee identical category proportions; Clarify doesn\u2019t provide sampling utilities."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "An ML pipeline triggers monthly. You must automatically generate a data quality report that includes the number of nulls, duplicates, and schema drift before training. Which integration sequence in SageMaker Pipelines accomplishes this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Invoke an AWS Glue Data Quality Check job as a Pipeline step (via Lambda), retrieve results, fail Pipeline on violations, then proceed to training.",
      "B": "Use DataBrew Profile job in Pipeline to generate report, manually check.",
      "C": "Use SageMaker Clarify DataQualityMonitor for pre-training schema drift detection.",
      "D": "Use ModelMonitor after training to analyze data quality."
    },
    "explanation": "AWS Glue Data Quality Check can profile nulls, duplicates, and detect schema drift. Integrating it in Pipelines (via Lambda or a custom step) automates reports and halts on violations. Clarify and ModelMonitor are not designed for pre-training data quality."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A retail company needs to deploy a deep learning model for product image classification to an endpoint. They expect highly variable traffic: peak of 2000 requests per minute during sales and near zero at night. Latency must stay under 200ms. Cost minimization is critical. Which deployment infrastructure meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy the model on a real-time SageMaker endpoint with two ml.p3.2xlarge instances and configure target tracking auto scaling based on CPU utilization.",
      "B": "Use a SageMaker serverless inference endpoint with configurable concurrency limits to scale automatically and only incur cost when the model processes requests.",
      "C": "Deploy a multi-model SageMaker endpoint on ml.m5.xlarge instances behind an Application Load Balancer and scale manually.",
      "D": "Containerize the model on ECS Fargate with fixed CPU and memory settings and use CloudWatch alarms to trigger CloudFormation stack updates at scale events."
    },
    "explanation": "Serverless endpoints automatically scale down to zero between peaks, minimizing cost, and handle variable load with low latency under SageMaker serverless SLA."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An IoT startup must deploy a computer vision model at the edge device that has limited compute (ARM CPU) and must process frames at 15 FPS. The model is currently a standard PyTorch model requiring a GPU. They need minimal code changes. Which infrastructure choice satisfies requirements?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy the PyTorch model to an EC2 g4dn.xlarge instance and have the edge device stream images to it for inference.",
      "B": "Use SageMaker real-time GPU endpoint with a Lambda function forwarding requests from edge device.",
      "C": "Convert the PyTorch model to TensorFlow and use SageMaker Neo to compile for ARM, deploy on AWS IoT Greengrass core.",
      "D": "Use SageMaker Neo to compile the PyTorch model for ARM architecture and deploy the optimized model on AWS IoT Greengrass core."
    },
    "explanation": "SageMaker Neo supports PyTorch, compiles to ARM, and Greengrass core runs inference on device, meeting FPS and minimal code change."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A financial analytics platform requires batch inference on 10 TB of historical data weekly. The job takes too long on EC2 instances using a custom endpoint. They want to reduce run time and maintenance. Which infrastructure change is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch to SageMaker Batch Transform with optimized ML instances and spot instances for cost and performance.",
      "B": "Convert the batch job into a real-time endpoint and stream data through it.",
      "C": "Deploy the model as a Lambda function and invoke in parallel using Step Functions Express Workflows.",
      "D": "Use ECS Fargate tasks to run the custom container in parallel across multiple tasks."
    },
    "explanation": "SageMaker Batch Transform is optimized for large-scale offline inference, supports spot instances, and manages infrastructure automatically."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A healthcare application must deploy a fraud-detection model requiring compliance isolation. The model must serve 500 TPS with latency <50ms. Deployment must reside in a VPC with no internet access. Which infrastructure choice is most appropriate?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Deploy a serverless SageMaker endpoint and attach a VPC endpoint for network isolation.",
      "B": "Use ECS Fargate on a private subnet with an Application Load Balancer in the VPC.",
      "C": "Deploy a real-time SageMaker endpoint in the customer VPC with VPC-only mode and provision ml.c5.2xlarge instances behind an internal ALB.",
      "D": "Deploy a multi-model endpoint on SageMaker using ml.m5.large instances with VPC mode enabled."
    },
    "explanation": "Real-time endpoints in VPC-only mode on ml.c5.2xlarge satisfy high TPS, low latency, and strict isolation; multi-model ml.m5.large cannot handle 500 TPS."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An e-commerce site uses a text classification model and expects constant moderate load (100 requests/sec). They want lowest latency and minimal infrastructure management. Cost sensitivity is medium. Which deployment type is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy a multi-model SageMaker real-time endpoint on an ml.c5.large instance and preload the model for minimal inference latency.",
      "B": "Use a SageMaker serverless endpoint to scale automatically with moderate management.",
      "C": "Deploy as a Lambda function behind API Gateway to avoid managing servers.",
      "D": "Containerize the model on ECS with Fargate and auto scale."
    },
    "explanation": "Multi-model endpoints reduce cold-start latency by preloading, minimize cost by serving multiple models on same instances, and provide lowest latency."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A media analytics company has deployed an object-detection model on SageMaker real-time endpoints. They notice high idle time cost overnight. Available traffic is zero after business hours. They need to reduce cost without affecting daytime performance. Which solution achieves this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch to a larger instance and use downscaling to one instance overnight.",
      "B": "Use a multi-container endpoint and move the model to a cheaper container.",
      "C": "Configure a scheduled CloudWatch event to delete and redeploy the endpoint daily.",
      "D": "Migrate to a serverless inference endpoint that scales to zero at night and autos-scales during the day."
    },
    "explanation": "Serverless inference endpoints scale to zero when idle and up during demand automatically, minimizing idle costs."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A biotech firm needs to deploy a variant of their genomic model for two customer segments. Both share 80% of code, but differ in last layer. They want cost-effective multi-tenancy with strict data isolation. Which deployment strategy should they choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy two separate real-time endpoints on dedicated instances for each segment.",
      "B": "Use a multi-model endpoint with separate containers, one for base model and two variants, and IAM policies for isolation.",
      "C": "Deploy a single container with conditional logic to switch layers at runtime.",
      "D": "Use SageMaker serverless inference and deploy two endpoints sharing the same container."
    },
    "explanation": "Multi-model endpoints host multiple containers on same instances, reducing cost, while IAM and container separation ensure data isolation."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An autonomous vehicle company needs to deploy their sensor fusion model on GPU clusters with InfiniBand to meet <20ms latency. Which SageMaker infrastructure meets these requirements?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Use SageMaker CPU instances in a cluster with Enhanced Networking.",
      "B": "Deploy on ECS GPU tasks with ENA-enabled instances.",
      "C": "Deploy a SageMaker real-time endpoint on ml.p4d.24xlarge instances with Elastic Fabric Adapter (EFA) for high-performance networking.",
      "D": "Use EC2 p2.xlarge instances and manage GPUs manually."
    },
    "explanation": "ml.p4d.24xlarge with EFA supports InfiniBand-like throughput and low-latency networking required for sensor fusion."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A startup needs to deploy a model that uses proprietary libraries not supported by SageMaker built-in containers. They require both batch and real-time inference with minimal custom ops. Which deployment approach is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Build a custom Docker container with the libraries, store in ECR, and deploy to SageMaker real-time and Batch Transform endpoints.",
      "B": "Use a Lambda function with layers containing the custom libraries.",
      "C": "Deploy on ECS Fargate with scheduled tasks for batch and ALB for real-time.",
      "D": "Migrate model to supported framework to use built-in SageMaker containers."
    },
    "explanation": "Custom container in ECR lets you include proprietary libraries and deploy to both real-time and batch endpoints with minimal overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A financial trading firm requires synchronous inference latency under 5ms. They want maximum CPU throughput and cannot risk cold starts. They also want to optimize for cost. Which combination should they choose?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "SageMaker serverless endpoint with provisioned concurrency of 100.",
      "B": "Provision a real-time multi-model endpoint on bare-metal ml.inf1.24xlarge Inf1 instances with model pinned in memory.",
      "C": "Deploy as Lambda functions with provisioned concurrency of 200.",
      "D": "Use ECS Fargate GPU tasks with pre-warmed containers behind an ALB."
    },
    "explanation": "Inf1 bare-metal reduces virtualization overhead, multi-model pins model, real-time endpoint avoids cold starts, maximizing throughput and low latency."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A gaming company wants to deploy multiple language translation models for different regions. Total model memory footprint exceeds instance GPU memory. They need low-latency translation. Which infra is most appropriate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy all models on one GPU EC2 instance using a custom orchestration layer.",
      "B": "Use SageMaker serverless endpoints sequentially loading each model.",
      "C": "Deploy each model on separate real-time GPU endpoints using ml.g4dn.xlarge instances and route traffic via Application Load Balancer.",
      "D": "Consolidate models into a single large multi-container endpoint on ml.p3.2xlarge."
    },
    "explanation": "Separate endpoints avoid GPU memory oversubscription, ALB routes based on region, simple scaling per model."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A social media platform requires asynchronous inference for user sentiment analysis. They need to process spikes of 100k requests hourly without provisioning servers. Which deployment infrastructure is best?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy a real-time SageMaker endpoint and enqueue requests in SQS.",
      "B": "Use a Lambda function triggered by Step Functions that calls the real-time endpoint.",
      "C": "Deploy a multi-model endpoint and use Kinesis Data Streams.",
      "D": "Use SageMaker asynchronous inference endpoints that scale storage and compute and store results in S3."
    },
    "explanation": "Asynchronous inference endpoints scale automatically for batch requests, decouple compute via S3, and handle spikes without server provisioning."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A media streaming company experiments with video frame classification. They need to deploy a model where inference takes >5s per image. They want to avoid request timeouts and handle 100 concurrent jobs. What deployment pattern should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a real-time endpoint with high timeout settings.",
      "B": "Deploy in ECS Fargate with long-running tasks.",
      "C": "Use SageMaker asynchronous inference with callback duration set, storing results in S3, polled by the application.",
      "D": "Break the model into microservices and chain Lambda functions."
    },
    "explanation": "Asynchronous endpoints handle long per-request durations without timing out and support concurrency by queueing requests."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An enterprise mandates use of automated model rollback on failure. They deploy via SageMaker endpoints. Which infrastructure configuration supports blue/green deployments with rollback?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker endpoint variants with traffic shifting in a blue/green deployment orchestrated by CodeDeploy integration.",
      "B": "Deploy two separate endpoints and use Route53 weighted routing to shift traffic manually.",
      "C": "Use ECS Fargate with CodePipeline and manual rollback stage.",
      "D": "Use Lambda functions with version aliases and weight-based alias shifts."
    },
    "explanation": "SageMaker with CodeDeploy supports blue/green endpoint deployments and automatic rollback on failure."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A company must serve ML inferences to internal clients only. They want cost-effective scaling and restrict internet access. They cannot use serverless endpoints. What should they deploy?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Public SageMaker real-time endpoint with IP whitelist.",
      "B": "ECS Fargate tasks in public subnets with security groups.",
      "C": "EC2 GPU instances behind a public ALB with restricted security group.",
      "D": "Private SageMaker real-time endpoint in VPC-only mode with endpoint accessed via VPC interface endpoint."
    },
    "explanation": "Private VPC-only SageMaker endpoints and interface VPC endpoints allow internal-only access and autoscaling without serverless usage."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A research team needs to test multiple model versions in parallel under identical traffic to compare performance in production. Which deployment technique supports this with minimal infrastructure overhead?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy each model version on separate real-time endpoints and use ALB.",
      "B": "Use SageMaker multi-variant endpoint with two production variants and traffic weight splitting.",
      "C": "Deploy models as Lambda aliases and shift weights via AWS CLI.",
      "D": "Use ECS Fargate tasks and update task definitions to switch traffic."
    },
    "explanation": "SageMaker variant endpoint supports two production variants for A/B testing with traffic weight control and shared infrastructure."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An NLP startup uses foundation models from Amazon Bedrock that require fine-tuning at deploy time. They need to serve personalized chat sessions with low latency. Which SageMaker infrastructure allows direct integration with Bedrock is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker JumpStart endpoints with Bedrock models fine-tuned via SageMaker Script Mode and deploy real-time endpoint.",
      "B": "Deploy Bedrock model directly in a Lambda behind API Gateway.",
      "C": "Use ECS Fargate with Hugging Face containers mounting a Bedrock endpoint.",
      "D": "Use EC2 instances calling Bedrock via SDK."
    },
    "explanation": "SageMaker JumpStart integrates with Bedrock models, supports fine-tuning script mode, and deploys as real-time endpoints with low latency."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A logistics firm needs to host an ensemble of three models for package routing. They want minimal end-to-end latency combining inferences. How should they deploy?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy three separate endpoints and aggregate results in Lambda.",
      "B": "Bundle all models into one container with a microservice orchestration.",
      "C": "Use a SageMaker multi-container endpoint with each model in its container and an aggregator container.",
      "D": "Deploy Ensemble as Batch Transform jobs chained by Step Functions."
    },
    "explanation": "Multi-container endpoints allow co-located containers; a dedicated aggregator container can combine outputs with low latency."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A company has sensitive images requiring inference on-premises behind a firewall. They want to use SageMaker-managed infrastructure. Which deployment architecture meets requirements?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Use SageMaker local mode on EC2 inside their VPC.",
      "B": "Use SageMaker real-time endpoint with VPC peering to on-premises network.",
      "C": "Deploy on AWS Outposts with SageMaker studio.",
      "D": "Use SageMaker Inference on AWS Outposts rack inside their data center sandbox."
    },
    "explanation": "SageMaker Inference on Outposts lets you run endpoints on Outposts hardware on-premises with SageMaker management."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A public API uses a multi-language model that loads slowly and exceeds response time if loaded on each invoke. Cold starts hurt UX. They need <50ms per request. Which deployment pattern addresses this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable provisioned concurrency on Lambda with large memory to preload model.",
      "B": "Deploy as SageMaker real-time endpoint on ml.c5.xlarge and maintain warm instances.",
      "C": "Use ECS Fargate tasks with pre-warmed containers behind ALB.",
      "D": "Use serverless inference with reserved concurrency."
    },
    "explanation": "Real-time endpoints keep models loaded in memory, ensuring consistent sub-50ms latency without cold starts."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A company needs to deploy a time-series forecasting model that runs monthly and takes hours to run. They want minimal infra management and integration with existing SageMaker pipelines. Which deployment infra is appropriate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy a real-time endpoint and trigger monthly inferences.",
      "B": "Use ECS Fargate tasks triggered by EventBridge.",
      "C": "Use SageMaker Batch Transform job within SageMaker Pipeline step.",
      "D": "Build a Lambda to load model and process data stored in S3."
    },
    "explanation": "Batch Transform jobs integrate directly in SageMaker Pipelines, manage infra, and suit long-running monthly jobs."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A SaaS vendor must deploy a model to multiple customer VPCs automatically during onboarding. They need IaC to manage endpoints. Which approach is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS CDK to define a construct for SageMaker endpoint and instantiate stacks per customer VPC.",
      "B": "Manually use CloudFormation templates to create endpoints per account.",
      "C": "Use Terraform modules outside AWS CDK for endpoints.",
      "D": "Use CLI scripts to provision endpoints in each account."
    },
    "explanation": "AWS CDK enables programmatic multi-account, multi-VPC deployments of SageMaker endpoints with constructs and automatic context."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A fintech app uses Spark processing to call an inference endpoint for real-time risk scoring at scale. They want to reduce network overhead per Spark executor. Which deployment option reduces egress latency?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy endpoint in public subnets close to EMR cluster.",
      "B": "Use ECS Fargate tasks as inference cluster next to EMR.",
      "C": "Call endpoint via API Gateway to reduce hops.",
      "D": "Deploy SageMaker real-time endpoint in same VPC and subnet as EMR and use direct VPC endpoint integration."
    },
    "explanation": "Co-locating endpoint in same subnet and using VPC endpoint reduces network hops and egress latency for Spark executors."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An advertiser needs to run real-time bidding inference under unpredictable peak loads up to 100k TPS with <5ms latency. GPU acceleration is needed. Which infrastructure is most suitable?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "Use SageMaker real-time endpoint on GPU ml.g5 instances with auto scaling.",
      "B": "Use SageMaker multi-model real-time endpoint on Inf1 instances with high concurrency and onboard GPU models into single endpoint.",
      "C": "Deploy ECS GPU Fargate tasks behind ALB with auto scaling ECS.",
      "D": "Use Spot Instances in Batch Transform with Lambda to emulate real-time."
    },
    "explanation": "Inf1 instances are designed for high-concurrency real-time inference at low latency and can host multiple models concurrently."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A biotech startup wants to test a prototype model without incurring provisioned instance cost. They need quick feedback with minimal latency (<100ms) at low volume (<10 TPS). Which deployment option is ideal?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Deploy to an ml.t3.medium real-time endpoint.",
      "B": "Use ECS Fargate with minimal CPU.",
      "C": "Use SageMaker serverless inference endpoint with low concurrency and pay-per-request.",
      "D": "Use Lambda with provisioned concurrency of 1."
    },
    "explanation": "Serverless inference lets you pay per request for low-volume prototypes with acceptable latency and no instance provisioning."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A customer has strict data residency; inference must occur in EU-West-1. They have global traffic. To reduce latency, they want multi-region endpoints. How should they implement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy SageMaker real-time endpoints in EU-West-1 and US-East-1, and route via Route 53 latency-based routing.",
      "B": "Deploy endpoints in EU-West-1 only and use CloudFront for global clients.",
      "C": "Use a global Application Load Balancer spanning regions.",
      "D": "Deploy serverless endpoints in EU-West-1 and Asia-Pacific, relying on client-side region selection."
    },
    "explanation": "Latency-based routing with Route 53 directs clients to nearest regional real-time endpoints while respecting data residency."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A gaming platform requires under-50ms inference on user actions. They host game servers in Kubernetes on EKS. They prefer colocation of inference. What deployment infra integrates best?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker real-time endpoints and call from EKS pods.",
      "B": "Use Lambda functions within VPC for inference.",
      "C": "Deploy ECS Fargate tasks in same VPC.",
      "D": "Use SageMaker Inference using EKS turret by deploying a SageMaker Inference endpoint onto the existing EKS cluster via the SageMaker Operators for Kubernetes."
    },
    "explanation": "SageMaker Operators lets you host real-time inference pods inside EKS cluster, co-located with game servers for low latency."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An ML team deployed a model on a shared SageMaker endpoint used by multiple teams. One model version affects another\u2019s latency. They need isolation. What is the best approach?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase instance count to isolate workloads.",
      "B": "Deploy each model as a separate SageMaker endpoint with dedicated instances.",
      "C": "Use multi-model endpoint with separate container per team.",
      "D": "Use ECS Fargate to isolate container resources per team."
    },
    "explanation": "Separate endpoints provide strict compute isolation, preventing one model\u2019s load from impacting another."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A data scientist is fine-tuning a large Transformer model on SageMaker JumpStart with limited GPU memory. They observe training failure due to out-of-memory errors. Which combination of techniques will most effectively reduce memory usage while preserving model accuracy?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Decrease batch size and disable gradient checkpointing to save memory.",
      "B": "Increase instance type to GPU with more memory and remove dropout regularization.",
      "C": "Enable mixed precision training and gradient checkpointing to reduce memory footprint.",
      "D": "Switch to a smaller pre-trained model and increase learning rate to converge faster."
    },
    "explanation": "Mixed precision reduces memory per tensor, and gradient checkpointing trades compute for memory by storing fewer activations, preserving accuracy while fitting in memory."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A SageMaker automatic model tuning (AMT) job using Bayesian optimization is not converging to better results after 20 trials. The objective is to maximize validation AUC for a binary classifier. Which action is most likely to improve the tuning efficiency?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch the tuner from Bayesian optimization to random search to explore hyperparameter space more broadly.",
      "B": "Narrow the hyperparameter search ranges around the best performing values and continue tuning.",
      "C": "Increase the maximum number of training jobs without adjusting the search ranges.",
      "D": "Change the objective metric to validation accuracy instead of AUC."
    },
    "explanation": "Refining search ranges focuses Bayesian optimization around promising regions, improving convergence efficiency without diluting search."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "During distributed training on SageMaker with Horovod, the training job stalls indefinitely at initialization. Logs indicate parameter server connection timeouts. Which configuration change should resolve this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable TCP keepalive and increase ssh_timeouts in the Horovod configuration.",
      "B": "Switch from Horovod to SageMaker\u2019s built-in data parallel library without changing network settings.",
      "C": "Reduce number of workers to one node to eliminate networking issues.",
      "D": "Increase the training batch size to reduce communication frequency."
    },
    "explanation": "Enabling TCP keepalive and adjusting timeouts prevents idle connection drops during Horovod rendezvous, resolving stalls."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A convolutional neural network trained via SageMaker script mode exhibits training instability when increasing batch size from 32 to 256. The learning rate was left at 0.001. Which adjustment best maintains convergence behaviour?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Decrease the number of epochs proportionally to batch size.",
      "B": "Add weight decay to the optimizer.",
      "C": "Switch optimizer from Adam to SGD.",
      "D": "Scale the learning rate linearly to 0.008 following the batch size increase rule."
    },
    "explanation": "Linear scaling rule suggests increasing learning rate proportional to batch size to maintain gradient variance and stable convergence."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A tree-based model in SageMaker automatic model tuning shows minimal improvement beyond 50 training jobs. The tuner used random search over max_depth\u2208[3,15] and min_child_weight\u2208[1,10]. What is the most effective next step?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Change the random search strategy to Bayesian optimization for the same ranges.",
      "B": "Narrow max_depth to [3,7] and extend min_child_weight to [0,20] based on observed patterns.",
      "C": "Increase training jobs from 50 to 500 without changing ranges.",
      "D": "Switch to a neural network algorithm to explore different model families."
    },
    "explanation": "Observing diminishing returns suggests narrowing depth to shallower trees and expanding child weight to regulate complexity, improving tuner focus."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A data scientist uses SageMaker hyperparameter tuning for a deep learning model and sets EarlyStoppingType to \"Auto\". Training jobs still run to full resource allocation before stopping. Why?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Auto early stopping only applies to built-in algorithms, not custom script mode.",
      "B": "The objective metric isn\u2019t logged frequently enough for the tuner to evaluate early stopping.",
      "C": "The tuning job\u2019s EarlyStoppingPolicy wasn\u2019t enabled; Auto setting requires explicit enablement in the tuner.",
      "D": "Auto early stopping only stops the entire tuning job, not individual training jobs."
    },
    "explanation": "In AMT, early stopping must be enabled in the tuner configuration; setting EarlyStoppingType alone doesn\u2019t apply the policy."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A model exhibits underfitting: both training and validation losses are high. The data scientist suspects insufficient model capacity. Which approach should they take?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase model size by adding layers or units and tune learning rate.",
      "B": "Reduce dropout rate to zero and add L1 regularization.",
      "C": "Decrease number of features via PCA to simplify data.",
      "D": "Increase batch size to stabilize training and reduce noise."
    },
    "explanation": "Underfitting often requires increasing model capacity; adding layers or units addresses capacity issues, then tuning learning rate ensures convergence."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "While training a PyTorch model in SageMaker script mode, gradient norms explode after epoch 2. Which change will most directly mitigate this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch optimizer from Adam to RMSprop to stabilise gradients.",
      "B": "Apply gradient clipping by setting max_grad_norm in the training script.",
      "C": "Increase batch size to average out noisy gradients.",
      "D": "Add dropout layers to the architecture."
    },
    "explanation": "Gradient clipping directly limits gradient norms, preventing explosion without changing model dynamics or data batching."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A pre-trained image classification model deployed via SageMaker JumpStart needs customization for a new 10-class dataset. Which process correctly fine-tunes this model with minimal training time?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Retrain the entire model on new dataset with original learning rate.",
      "B": "Replace final layer and train only that layer with a high learning rate.",
      "C": "Freeze all convolutional layers and train all fully connected layers with a low learning rate.",
      "D": "Freeze base model parameters and fine-tune the final layers with a reduced learning rate."
    },
    "explanation": "Freezing base parameters preserves learned features and reduces training time, while a reduced learning rate avoids large parameter updates."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "During automatic model tuning, a candidate hyperparameter combination crashes training jobs intermittently on one instance type. How should the data scientist handle this to continue tuning?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Remove the crashing hyperparameter combination from the search space manually.",
      "B": "Configure the tuning job\u2019s RetryPolicies to retry failed jobs and exclude fatal errors from stopping the tuner.",
      "C": "Switch to a larger instance type for all tuning jobs.",
      "D": "Reduce the overall max jobs count so the job completes quickly before errors accumulate."
    },
    "explanation": "Using RetryPolicies allows retries for transient failures and prevents rare fatal errors from halting the tuning job."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A model\u2019s training time per epoch is excessive due to data preprocessing in the training loop. Which strategy will speed up training without altering model architecture?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Move preprocessing to a SageMaker Processing Job and store transformed data in S3 for training.",
      "B": "Use larger batch sizes to reduce number of loops.",
      "C": "Switch from Python data loader to a pure-Python implementation.",
      "D": "Increase number of epochs to amortize preprocessing overhead."
    },
    "explanation": "Offloading preprocessing to a separate job avoids repeated computation per epoch, allowing training on preprocessed data."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A neural network in SageMaker Model Registry has multiple versions. The team wants reproducible experiments including exact training environments. Which Registry feature enables this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model approval status tags.",
      "B": "Inference container logs.",
      "C": "Model package groups with associated Docker image URIs and training artifacts.",
      "D": "Endpoint deployment timestamps."
    },
    "explanation": "Model package groups record container URIs and artifacts, ensuring reproducible environments for each version."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A data scientist uses SageMaker automatic model tuning with Bayesian optimization. The prior distributions are too narrow, causing search to get stuck in local minima. What change addresses this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase number of initial random trials before Bayesian sampling.",
      "B": "Widen the prior distribution ranges to cover more hyperparameter space.",
      "C": "Switch to random search strategy entirely.",
      "D": "Decrease objective metric evaluation frequency."
    },
    "explanation": "Widening priors allows the optimizer to explore a broader search space and escape local minima."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An LSTM model trained via SageMaker script mode shows overfitting after 10 epochs. The data scientist wants to regularize without reducing model capacity. Which approach is most appropriate?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Decrease number of timesteps to shorten sequences.",
      "B": "Increase batch size to reduce gradient noise.",
      "C": "Add L1 regularization to input weights only.",
      "D": "Add recurrent dropout to LSTM layers for temporal regularization."
    },
    "explanation": "Recurrent dropout applies dropout to recurrent connections, regularizing LSTM without reducing capacity."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A classification model in SageMaker automatically stops improving validation accuracy after 5 epochs. Which tuner configuration ensures the rest of the hyperparameter combinations execute?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable warm start with type IDENTICAL_DATA_AND_ALGORITHM and set max jobs.",
      "B": "Disable early stopping entirely to run all combinations to completion.",
      "C": "Set strategy to GRID_SEARCH to systematically cover all combinations.",
      "D": "Use early stopping with conservatism parameter set to high."
    },
    "explanation": "Warm start IDENTICAL allows reusing completed trials but still runs other configurations; disabling early stopping risks wasted compute."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A distributed TensorFlow training job on SageMaker shows poor scaling beyond 4 GPU nodes; network I/O saturates. Which modification best improves throughput?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase batch size further to reduce communication frequency.",
      "B": "Switch to Horovod\u2019s ring-allreduce algorithm.",
      "C": "Enable SageMaker\u2019s built-in NCCL backend with GPU-enabled instances.",
      "D": "Use Python-based parameter server implementation."
    },
    "explanation": "NCCL provides high-performance multi-GPU communication optimized for GPU clusters, alleviating I/O bottlenecks."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A PyTorch model fine-tuned on SageMaker exhibits catastrophic forgetting on original classes. Which technique best prevents forgetting while learning new classes?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a higher learning rate for new classes to quickly adapt.",
      "B": "Apply Elastic Weight Consolidation (EWC) to penalize deviation from original weights.",
      "C": "Freeze all layers except the last and train only output layer.",
      "D": "Increase dropout to 0.9 to regularize adaptation."
    },
    "explanation": "EWC constrains updates for important weights, preserving original task performance while fine-tuning on new tasks."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A SageMaker hyperparameter tuning job uses RANDOM strategy. The search space contains complex interdependent hyperparameters. Which strategy provides more efficient search?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Switch to GRID_SEARCH to cover all combinations exhaustively.",
      "B": "Manually sample correlated hyperparameters outside the tuner.",
      "C": "Reduce hyperparameter count by fixing least impactful ones.",
      "D": "Use BAYESIAN optimization to model dependencies and guide search."
    },
    "explanation": "Bayesian optimization learns relationships between hyperparameters, efficiently handling interdependencies compared to random search."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A model trained with script mode on SageMaker takes too long to converge. Which two changes combined will most effectively reduce training time and maintain accuracy?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase batch size and disable checkpointing.",
      "B": "Enable mixed precision training and apply early stopping based on validation loss.",
      "C": "Switch optimizer to SGD and decrease learning rate.",
      "D": "Use smaller instance type and reduce number of epochs."
    },
    "explanation": "Mixed precision speeds up training; early stopping prevents wasteful epochs after convergence, preserving accuracy."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A data scientist wants to use SageMaker Automatic Model Tuning to optimize both learning_rate and batch_size. They observe that large batch sizes degrade generalization. Which approach addresses this trade-off during tuning?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Fix batch_size and tune only learning_rate.",
      "B": "Increase search range of both parameters.",
      "C": "Tune batch_size, then in a second tuner fix optimal batch_size and tune learning_rate.",
      "D": "Use a single tuner to optimize sum of learning_rate and batch_size."
    },
    "explanation": "Sequential tuning isolates the batch size effect, then optimizes learning rate for the chosen batch size, handling trade-offs effectively."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A model\u2019s F1 score plateaus despite varying hyperparameters. The data scientist suspects feature scaling issues. Which pipeline change is most likely to enable further improvement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Incorporate standardization of numeric features before training in script mode.",
      "B": "Add more dropout layers to reduce variance.",
      "C": "Increase depth of decision trees in ensemble.",
      "D": "Switch to a different activation function in final layer."
    },
    "explanation": "Unscaled numeric features can hamper optimization; standardization often leads to better convergence and metric improvements."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A SageMaker training job fails due to size of input data exceeding EBS volume. Which solution allows training with minimal code changes?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Switch to processing job and write custom code for streaming input.",
      "B": "Split dataset into smaller files and retrain multiple times.",
      "C": "Modify training code to stream data from S3 in chunks.",
      "D": "Mount Amazon FSx for Lustre to the training instance for high-throughput, large dataset support."
    },
    "explanation": "FSx for Lustre integrates transparently as a filesystem, handling large datasets with minimal code changes."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "When tuning a neural network\u2019s hyperparameters, the data scientist sets EarlyStoppingType to \"Off\" but still sees early termination of some trials. What explains this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Some built-in algorithms override tuner settings to always early stop.",
      "B": "The SageMaker training script configured its own early stopping callback independent of tuner.",
      "C": "Early stopping only applies when strategy is BAYESIAN.",
      "D": "The objective metric threshold was reached causing automatic stop."
    },
    "explanation": "Custom scripts can implement callbacks (e.g., Keras EarlyStopping) causing training jobs to stop independently of tuner configuration."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A data scientist wants to reduce a model\u2019s memory footprint from 2 GB to under 500 MB for edge deployment. Which two techniques achieve this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Apply post-training static quantization and weight pruning.",
      "B": "Increase dropout to 0.9 and retrain the model.",
      "C": "Use larger batch normalization momentum and lower precision.",
      "D": "Deploy a smaller model architecture without adjustments."
    },
    "explanation": "Static quantization reduces weight bit widths; pruning removes redundant weights, both significantly shrinking model size."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A SageMaker hyperparameter tuning job\u2019s best candidate uses a combination of hyperparameters untested in grid search due to non-grid ranges. Which tuning strategy should the data scientist have chosen?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use RANDOM search over grid values.",
      "B": "Use GRID search with finer discretization.",
      "C": "Manually evaluate the unseen combination.",
      "D": "Leverage BAYESIAN search to explore continuous hyperparameter space."
    },
    "explanation": "Bayesian search explores continuous spaces beyond specified grid points, identifying combinations not on predefined grid."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "During multi-model ensembling, a SageMaker pipeline aggregates predictions from three models but sees inferior performance. What ensemble method change is most likely to improve results?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Switch from weighted average to simple average of predictions.",
      "B": "Increase weight of the poorest performing model to diversify errors.",
      "C": "Use stacking with a meta-learner to learn optimal ensemble weights.",
      "D": "Reduce number of models to only the top performer."
    },
    "explanation": "Stacking trains a meta-learner to optimally combine base model outputs, often outperforming fixed averaging."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A deep learning training job on SageMaker script mode uses custom Docker image. It runs slower than built-in containers. Which modification speeds up training in the same image?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch to a smaller base image reducing container size.",
      "B": "Install and configure NVIDIA CUDA and cuDNN optimally inside the container.",
      "C": "Disable container logging to reduce I/O overhead.",
      "D": "Switch optimizer from Adam to Adagrad."
    },
    "explanation": "Optimized CUDA/cuDNN libraries inside the container unlock GPU performance equivalent to built-in images."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A model trained on imbalanced classes shows poor minority recall. The data scientist wants to tune both class_weight and learning_rate. Which SageMaker tuning configuration is most appropriate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Tune only class_weight and manually adjust learning_rate after tuning.",
      "B": "Use GRID search for both parameters with coarse resolution.",
      "C": "Use BAYESIAN optimizer jointly over continuous learning_rate and discrete class_weight.",
      "D": "Perform random search for class_weight and grid search for learning_rate."
    },
    "explanation": "Bayesian optimization can efficiently search mixed continuous and discrete spaces together, optimizing both simultaneously."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A CNN model deployed frequently suffers from sudden spikes in validation loss at random epochs. The training script uses Keras with default settings. How should they address this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable shuffling of training data each epoch and add adaptive learning rate scheduler.",
      "B": "Disable dropout layers to stabilize activations.",
      "C": "Use batch normalization only in the first layer.",
      "D": "Increase training tolerance in Keras fit function."
    },
    "explanation": "Shuffling prevents learning order bias; adaptive LR schedulers adjust LR downward when loss spikes, stabilizing training."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A SageMaker hyperparameter tuning job is set to use warm start of type TRANSFER_LEARNING. It reuses data but is failing to reuse prior checkpoints. What is missing?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "The tuning job must run on the same instance type as prior job.",
      "B": "The HyperparameterTuningJob must reference the previous job\u2019s name in the WarmStartConfig.",
      "C": "Previous job artifacts must be manually copied into new S3 bucket.",
      "D": "Transfer learning warm starts only apply to built-in algorithms."
    },
    "explanation": "Specifying the previous job name in WarmStartConfig lets the tuner locate and reuse prior tuning results."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A data scientist wants to integrate an external scikit-learn pipeline into SageMaker script mode training. Which step ensures compatibility?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Convert the pipeline to ONNX and use SageMaker\u2019s ONNX container.",
      "B": "Refactor code to remove scikit-learn dependencies.",
      "C": "Wrap the pipeline in a Lambda function called during training.",
      "D": "Include scikit-learn in the training environment and invoke pipeline within the training script.\""
    },
    "explanation": "Bundling scikit-learn in the image and calling the pipeline in the script ensures seamless integration with SageMaker script mode."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A fintech company must predict customer churn using a dataset of 10 million rows and 50 mixed categorical and numeric features. The model must be highly interpretable to satisfy regulatory audits and be cost-efficient to train. Which modeling approach best meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker XGBoost binary classification with default tree depth and no explainability integration.",
      "B": "Use SageMaker Linear Learner (binary classification) with L1 regularization to produce sparse, interpretable weights.",
      "C": "Build a deep neural network in TensorFlow script mode to capture complex feature interactions.",
      "D": "Use SageMaker Random Cut Forest to detect outliers as churn proxies."
    },
    "explanation": "Linear Learner with L1 yields a sparse, linear model that is interpretable, trainable quickly on large tabular data, and cost-efficient."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "An e-commerce company wants to segment customers into five groups based on browsing behavior without labels. They require a scalable, unsupervised approach that integrates easily with SageMaker Pipelines. Which modeling approach should they select?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Use SageMaker K-means clustering built-in algorithm with k=5.",
      "B": "Use a custom DBSCAN implementation in script mode.",
      "C": "Use SageMaker Random Cut Forest for density-based segmentation.",
      "D": "Use SageMaker XGBoost with k-means objective."
    },
    "explanation": "K-means is the standard scalable built-in algorithm for clustering into a predefined number of segments; integrates natively with SageMaker Pipelines."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A startup needs to classify short customer reviews (under 200 words) into positive, neutral, and negative. They want minimal operational overhead, fast time to market, and are willing to sacrifice some accuracy for ease of use. Which modeling approach best fits?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Train a custom transformer model in PyTorch script mode.",
      "B": "Use SageMaker BlazingText for supervised text classification.",
      "C": "Fine-tune a BERT model in SageMaker JumpStart.",
      "D": "Use Amazon Comprehend sentiment analysis API."
    },
    "explanation": "Amazon Comprehend is fully managed, requires no model training, minimal overhead and rapid deployment at moderate accuracy."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A manufacturing plant monitors sensor streams and wants to detect anomalies in real time. Data volume is high (hundreds of thousands of points per second), and false positives are costly. Which AWS service or algorithm should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a one-class SVM in SageMaker script mode.",
      "B": "Use SageMaker K-means clustering on sliding windows.",
      "C": "Use Amazon Lookout for Equipment anomaly detection service.",
      "D": "Implement a custom LSTM autoencoder in TensorFlow script mode."
    },
    "explanation": "Lookout for Equipment is purpose-built for high-throughput sensor anomaly detection, managed, and tuned to reduce false positives."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A healthcare provider has structured tabular patient data (300K records) to predict disease onset. They require high recall (>90%) and can accept lower interpretability. They want to minimize development time. What is the best modeling approach?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker XGBoost with tree depth tuned via automatic model tuning.",
      "B": "Use SageMaker Linear Learner with L2 regularization.",
      "C": "Build a deep neural network in PyTorch script mode.",
      "D": "Use Amazon Autopilot to generate a pipeline automatically."
    },
    "explanation": "XGBoost balances recall and complexity, trains quickly on tabular data with minimal code; easier than full script mode and more controllable than AutoPilot."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A business needs real-time translation of customer chat messages from English to Spanish. Latency must be under 100ms per request. Which approach is most appropriate?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Train a custom seq2seq model in SageMaker script mode.",
      "B": "Use Amazon Translate real-time API.",
      "C": "Fine-tune a transformer model in SageMaker JumpStart.",
      "D": "Use AWS Lambda to call SageMaker endpoint hosting a translation model."
    },
    "explanation": "Amazon Translate real-time API provides low-latency translation managed by AWS, meeting the latency SLA."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A retail chain wants product recommendations on its website using purchase history. They have sparse user\u2013item interactions. They want a fully managed solution integrated with SageMaker. Which approach should they choose?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Implement an alternating least squares algorithm in Spark on EMR.",
      "B": "Train a collaborative filtering model in TensorFlow script mode.",
      "C": "Use SageMaker BlazingText for embeddings and nearest-neighbor search.",
      "D": "Use Amazon Personalize recommendation service."
    },
    "explanation": "Amazon Personalize is a managed recommendation system that handles sparse interactions and integrates easily, reducing operational overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "An online news service needs to tag articles with topics using a multi-label classification approach. They prefer an AWS-managed solution with custom training only if necessary. Which approach should they adopt?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Fine-tune a SageMaker JumpStart multi-label zero-shot classification foundation model.",
      "B": "Train a multi-label Keras model in script mode.",
      "C": "Use Amazon Comprehend for single-label topic detection.",
      "D": "Use SageMaker BlazingText supervised classification."
    },
    "explanation": "JumpStart foundation models support multi-label zero-shot classification with minimal training, managed by AWS."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A logistics company must forecast daily demand at 1,000 locations for the next 30 days. They have three years of daily demand history. They require prediction intervals and seamless SageMaker integration. Which modeling approach should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Implement ARIMA per location in script mode.",
      "B": "Use a custom LSTM in TensorFlow.",
      "C": "Use SageMaker DeepAR forecasting built-in algorithm.",
      "D": "Use SageMaker XGBoost with date features."
    },
    "explanation": "DeepAR provides probabilistic forecasts with intervals, scales across series, and is a built-in SageMaker algorithm suited to many time-series."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A marketing team wants to group social media posts into topics for monitoring. They have no labeled data and limited compute budget. Which modeling approach is most appropriate?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Train a supervised LDA model in script mode.",
      "B": "Use SageMaker Latent Dirichlet Allocation (LDA) built-in algorithm.",
      "C": "Fine-tune a large transformer model with clustering head.",
      "D": "Use Amazon Comprehend for custom entity recognition."
    },
    "explanation": "SageMaker's built-in LDA efficiently clusters text into topics unsupervised, requiring no labels and minimal compute."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A cybersecurity team wants to detect malicious logon patterns using unsupervised anomaly detection. They need to process terabytes of log data and integrate into SageMaker Pipelines. Which approach should they select?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Random Cut Forest built-in algorithm.",
      "B": "Use custom PyTorch autoencoder in script mode.",
      "C": "Deploy Amazon GuardDuty for logs anomaly detection.",
      "D": "Use SageMaker K-means clustering to detect outliers."
    },
    "explanation": "Random Cut Forest is optimized for large, high-dimensional anomaly detection and integrates natively with SageMaker Pipelines."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A life sciences company needs to classify gene expression profiles into disease subtypes. Dataset is small (5,000 samples, 20,000 features). They must avoid overfitting and need interpretability. Which modeling approach is best?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a deep neural network with feature selection.",
      "B": "Use SageMaker Linear Learner with L1/L2 and elastic net for feature selection and interpretability.",
      "C": "Use SageMaker XGBoost with maximum tree depth=10.",
      "D": "Use SageMaker K-nearest neighbors."
    },
    "explanation": "Linear Learner with elastic net reduces overfitting on high-dimensional data and provides interpretable coefficients and embedded feature selection."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A media company needs to index and tag objects (e.g., people, vehicles) in video footage. They want a fully managed solution requiring no custom training. Which AWS AI service should they select?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Train a custom object detection model in SageMaker script mode.",
      "B": "Use SageMaker built-in Object Detection algorithm.",
      "C": "Use Amazon Rekognition Video for object and activity detection.",
      "D": "Use Amazon Kinesis Video Streams with ML integration."
    },
    "explanation": "Rekognition Video provides managed object and activity detection in video, eliminating need for custom training."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A bank wants to detect fraudulent transactions in real time. They have labeled data but the fraud patterns evolve over time. They need an AWS service with built-in model updating. Which approach should they choose?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker XGBoost and retrain daily in a custom pipeline.",
      "B": "Use Amazon Fraud Detector service.",
      "C": "Train a custom LSTM in script mode.",
      "D": "Use Amazon Lookout for Metrics."
    },
    "explanation": "Amazon Fraud Detector is tailored for fraud, supports continuous learning and real-time inference, simplifying maintenance."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A retailer wants to predict end-of-day inventory needs using external weather and holiday data. They need probabilistic forecasts for safety stock planning. Which modeling approach on SageMaker should they use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker DeepAR forecasting built-in algorithm.",
      "B": "Use SageMaker XGBoost with quantile regression.",
      "C": "Use a custom Prophet model in script mode.",
      "D": "Use SageMaker Random Cut Forest."
    },
    "explanation": "DeepAR natively supports probabilistic forecasting with external covariates and scales to many series."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A gaming company needs to recommend in-game content to players based on session behavior. They have tens of millions of sessions per day and need sub-100ms latency. Which approach is most viable?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Train a collaborative filtering model in TensorFlow script mode.",
      "B": "Use Amazon Personalize campaign for real-time recommendations.",
      "C": "Use SageMaker XGBoost with custom nearest-neighbor indexing.",
      "D": "Use Amazon Kinesis Data Analytics for on-the-fly clustering."
    },
    "explanation": "Amazon Personalize offers managed real-time campaigns optimized for low latency at scale."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A legal firm wants to extract key phrases from contract documents. They have no labeled data and need high accuracy. Which AWS solution should they employ?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a custom NER model in Hugging Face script mode.",
      "B": "Use SageMaker BlazingText to learn word embeddings.",
      "C": "Use Amazon Comprehend custom entity recognition with auto-labeling.",
      "D": "Use Amazon Textract forms extraction."
    },
    "explanation": "Comprehend custom entity recognition supports auto-labeling for text, yielding high accuracy without full custom model training."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A telecom operator wants to predict network drop calls per cell tower hourly. They have 2 years of data across 10,000 towers. They require forecasting with external features and a managed solution. Which built-in algorithm should they use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker DeepAR forecasting built-in algorithm.",
      "B": "Use SageMaker XGBoost regression with time lags.",
      "C": "Use a custom ARIMA model in Spark on EMR.",
      "D": "Use SageMaker K-means for time-series clustering."
    },
    "explanation": "DeepAR handles large numbers of time series, accepts external covariates, and produces probabilistic forecasts."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A startup wants to deploy a chat interface that responds to user queries with company policy answers. They need conversational AI with minimal custom training. Which AWS service should they select?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Train a seq2seq model in SageMaker script mode.",
      "B": "Use Amazon Comprehend to detect intents and build responses.",
      "C": "Use Amazon Lex custom bot with full slot and intent definitions.",
      "D": "Use Amazon Q to build an ML-powered conversational interface."
    },
    "explanation": "Amazon Q is a managed conversational QA service that ingests documents and answers queries with minimal configuration."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A biotech firm must classify microscope images of cells into healthy vs cancerous. They have only 1,000 labeled images and need high accuracy. They also want a managed training pipeline. Which approach should they choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a custom CNN from scratch in PyTorch script mode.",
      "B": "Use SageMaker JumpStart pre-trained Image Classification model and fine-tune on their data.",
      "C": "Use SageMaker built-in Image Classification with default parameters.",
      "D": "Use Amazon Rekognition Custom Labels service."
    },
    "explanation": "JumpStart pre-trained models accelerate training on small datasets; managed pipeline support simplifies fine-tuning."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A media analytics company must detect brand logos in user-generated videos. They need high accuracy and low latency inference. Which modeling approach on AWS should they use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a YOLO model in SageMaker script mode and deploy on GPU endpoints.",
      "B": "Use SageMaker built-in Object Detection algorithm with custom data.",
      "C": "Use Amazon Rekognition for image detection frame-by-frame.",
      "D": "Use Amazon Rekognition Custom Labels trained on their logo dataset."
    },
    "explanation": "Rekognition Custom Labels offers managed training for object detection with low-latency inference optimized by AWS."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A logistics startup needs to cluster delivery routes into 100 groups based on GPS coordinates over time for fleet optimization. They want an unsupervised, scalable solution in SageMaker. Which algorithm should they use?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use SageMaker K-means clustering with k=100 and geospatial features.",
      "B": "Use SageMaker Linear Learner in unsupervised mode.",
      "C": "Use a custom DBSCAN in script mode.",
      "D": "Use SageMaker Random Cut Forest for clustering."
    },
    "explanation": "K-means is the standard scalable clustering for a predetermined number of clusters and integrates directly in SageMaker."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A financial institution wants to build a credit scoring model with a focus on minimizing false negatives over false positives. They need a model that supports cost-sensitive learning. Which SageMaker built-in algorithm should they choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Linear Learner with default settings.",
      "B": "Use SageMaker XGBoost with customized objective and scale_pos_weight.",
      "C": "Train a feed-forward neural network in TensorFlow script mode.",
      "D": "Use SageMaker K-nearest neighbors with class weights."
    },
    "explanation": "XGBoost allows customizing objective and pos/neg weights to handle cost sensitivity and optimize for false negatives."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A news aggregator wants to detect emerging topics from streaming text feeds in real time. They need unsupervised topic modeling with low latency. Which AWS approach should they select?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker LDA in batch mode triggered hourly.",
      "B": "Use SageMaker K-means on streaming windows.",
      "C": "Use Amazon Kinesis Data Analytics with built-in ML to apply LDA in real time.",
      "D": "Use AWS Lambda to call a SageMaker endpoint running topic modeling."
    },
    "explanation": "Kinesis Data Analytics integrates LDA in streaming pipelines for low-latency unsupervised topic detection without batch orchestration."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A rideshare company wants to cluster drivers into behavior profiles based on speed, route deviation, and idle time. They need an algorithm that automatically detects the number of clusters. Which approach should they use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker K-means with silhouette analysis to pick k.",
      "B": "Use SageMaker Gaussian Mixture built-in algorithm for EM-based clustering.",
      "C": "Train a self-organizing map in script mode.",
      "D": "Use SageMaker Random Cut Forest to discover clusters."
    },
    "explanation": "Gaussian Mixture automatically estimates cluster probabilities and can infer the number of components via Bayesian Information Criterion."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A broadband ISP wants to forecast hourly network usage patterns per customer segment. They require a model that can incorporate seasonal effects and external events. Which SageMaker algorithm fits best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker DeepAR forecasting with seasonal frequency and event indicators.",
      "B": "Use SageMaker XGBoost regression with dummy variables.",
      "C": "Use a custom Prophet model in SageMaker script mode.",
      "D": "Use SageMaker K-means to cluster time windows."
    },
    "explanation": "DeepAR natively handles seasonality and event covariates at scale with built-in SageMaker support."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A telecom analytics team needs to classify customer support calls as billing, technical, or general inquiry. They have 20,000 transcribed calls. They want a model that can be retrained weekly with minimal code. Which approach should they choose?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Train a custom RNN model in TensorFlow script mode.",
      "B": "Use Amazon Transcribe followed by BlazingText supervised training.",
      "C": "Use SageMaker JumpStart text classification solution template.",
      "D": "Use Amazon Comprehend custom classification."
    },
    "explanation": "JumpStart solution templates provide end-to-end pipelines that automate retraining with minimal code changes."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A biotech company wants to identify cell types from single-cell RNA sequencing data (20,000 genes per cell). They need unsupervised clustering that scales and can handle high dimensionality. Which SageMaker built-in algorithm should they select?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "Use SageMaker K-means with PCA preprocessing.",
      "B": "Use SageMaker Gaussian Mixture model for soft clustering in high dimensions.",
      "C": "Use a custom t-SNE embedding in script mode.",
      "D": "Use SageMaker Random Cut Forest for clustering."
    },
    "explanation": "Gaussian Mixture can handle high-dimensional soft clustering; combining with EM makes it more flexible than hard k-means."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A publisher wants to rank articles by predicted click-through rate (CTR). They have historical CTR logs and dozens of categorical features with high cardinality. Which algorithm should they use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Linear Learner with one-hot encoding.",
      "B": "Use SageMaker XGBoost with target encoding for high-cardinality features.",
      "C": "Train a deep neural network in PyTorch script mode.",
      "D": "Use SageMaker K-nearest neighbors for CTR prediction."
    },
    "explanation": "XGBoost handles target encoding and interactions efficiently on tabular data with high-cardinality categorical features."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A healthcare analytics startup wants to extract structured data fields (e.g., dosages) from scanned prescription forms. They need a fully managed approach requiring minimal ML expertise. Which AWS service should they choose?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Train a custom form-recognition model in SageMaker script mode.",
      "B": "Use Amazon Textract Analyze ID API.",
      "C": "Use Amazon Textract Forms with pre-built key-value extraction.",
      "D": "Use Amazon Comprehend Medical."
    },
    "explanation": "Textract Forms extracts key-value pairs from documents without custom model training, ideal for structured field extraction."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A data science team needs to automate retraining of a SageMaker model whenever new data arrives in S3. They want to enforce a manual approval step after data validation and before model training. Which combination of AWS services and pipeline stages meets these requirements with least custom code?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Lambda triggered by S3 events to run a Step Functions state machine that includes a manual approval (via SNS) and then starts a SageMaker training job.",
      "B": "Use SageMaker Pipelines: define a DataQualityCheck step, add a ModelTraining step, and insert a ManualApprovalStep between validation and training.",
      "C": "Use CodePipeline with Source stage on S3, a CodeBuild project to validate data, an AWS Lambda for approval, and a CodeBuild to start the training.",
      "D": "Use Step Functions with S3 event trigger, implement data validation in a task, pause with Wait state for manual input via API Gateway, then start training."
    },
    "explanation": "SageMaker Pipelines natively supports data validation, model steps, and ManualApprovalStep with minimal custom code, fulfilling requirements efficiently."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A pipeline is defined in AWS CodePipeline that builds Docker containers for inference, pushes them to ECR, and deploys to SageMaker endpoints. The team notices that if the build fails, manual rollbacks are needed. Which CodePipeline feature should be added to automate rollback on deployment failures?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable cross-region replication in ECR.",
      "B": "Use CodeBuild 'batch' build mode for transactional builds.",
      "C": "Configure a DeploymentConfig with Canary10Percent10Minutes in the CodeDeploy stage for automatic rollback on failure.",
      "D": "Add a Lambda invoke action after deployment to delete failed endpoints."
    },
    "explanation": "Using a CodeDeploy DeploymentConfig with canary deployments supports automatic rollback on failure without custom Lambdas."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "An ML pipeline in CodePipeline runs training, evaluation, and deployment. The evaluation step uses CodeBuild to compute model metrics. The team needs the pipeline to stop and notify if model accuracy drops below 85%. How can they implement this with minimal changes?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Modify the training CodeBuild to throw an exception when accuracy <85% and let the pipeline fail.",
      "B": "Use CloudWatch Alarms on CodeBuild logs to alert and pause the pipeline.",
      "C": "Write a Lambda in the evaluation stage to call CodePipeline StopPipelineExecution if accuracy <85%.",
      "D": "In evaluation CodeBuild, use buildspec checks to fail the build when accuracy <85%, causing the pipeline to stop and send failure notifications."
    },
    "explanation": "Failing the evaluation CodeBuild via buildspec when accuracy <threshold automatically stops the pipeline and triggers failure notifications."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A pipeline must deploy a new ML model version to a real-time endpoint with zero downtime and allow easy rollback. Which deployment strategy and pipeline configuration should be used?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Lambda to deploy new model to same endpoint container, overwriting existing one.",
      "B": "Configure a SageMaker multi-model endpoint in CodePipeline with blue/green deployment via CodeDeploy.",
      "C": "Use SageMaker real-time endpoint with WarmPool and update endpoint configuration via CodeBuild.",
      "D": "Launch a new endpoint, test it, then delete the old one via a final CodePipeline stage."
    },
    "explanation": "SageMaker multi-model endpoints with CodeDeploy blue/green strategy enable zero-downtime updates and easy rollback."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "The team needs to version control SageMaker Pipeline definitions, enforce PR reviews, and automatically trigger pipeline execution when changes are merged. How should they architect this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Store pipeline definitions in a Git repo. Use CodePipeline with Source from CodeCommit, add CodeBuild for linting, and deploy to SageMaker Pipelines using CloudFormation in a later stage.",
      "B": "Use SageMaker Studio notebooks directly and rely on manual execution upon merge.",
      "C": "Embed pipeline definitions in CodeBuild scripts, trigger on GitHub webhook, and start executions via SDK.",
      "D": "Use AWS CDK in local environment, require manual CDK deploy after merge."
    },
    "explanation": "Using CodePipeline Source from CodeCommit with automated linting and CloudFormation deploy ensures version control, PR reviews, and auto-deploy on merge."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A CodePipeline pipeline must orchestrate cross-account deployment of model inference containers. Account A builds and tests the image; Account B deploys to SageMaker. How should roles and permissions be set?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "In Account A, create an IAM role that trusts Account B\u2019s CodePipeline service principal to push to ECR.",
      "B": "In Account B, create a pipeline that assumes the CodeBuild role from Account A to pull images.",
      "C": "In Account A\u2019s ECR repository policy, allow pull by a deployment role in Account B. In Account B, create a pipeline with a service role that has pull permissions and assume role rights.",
      "D": "Replicate ECR images to Account B automatically via cross-account replication."
    },
    "explanation": "Granting Account B\u2019s pipeline role pull permissions in Account A\u2019s ECR via repository policy allows secure cross-account access for deployment."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A pipeline using AWS CodePipeline must package ML code and dependencies into a container, push to ECR, then deploy to ECS for batch inference. They also need to run unit tests on the packaged image. Which steps should the pipeline include?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Source -> Build (Docker build & push) -> Deploy -> Test",
      "B": "Source -> Build (Docker build) -> Test -> Deploy",
      "C": "Source -> Test -> Build (Docker build & push) -> Deploy",
      "D": "Source -> Build (Docker build) -> Test (CodeBuild uses image via local registry) -> Build (Docker push) -> Deploy"
    },
    "explanation": "Building the image, running tests locally in CodeBuild, then pushing and deploying ensures integrity before pushing."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "An ML team wants to integrate Terraform-managed infrastructure into a CI/CD pipeline for SageMaker endpoint updates. They need a single pipeline. Which service combination achieves this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS CloudFormation with macros in CodePipeline.",
      "B": "Use CodePipeline with Terraform CodeBuild actions: one to plan, manual approval, one to apply changes, then a SageMaker deployment stage.",
      "C": "Use AWS CDK in CodeBuild to translate Terraform into CDK and deploy.",
      "D": "Trigger Terraform from a Lambda invoked by EventBridge in between CodePipeline stages."
    },
    "explanation": "Running Terraform plan/apply in CodeBuild with approval inside CodePipeline integrates IaC and SageMaker deployments in one pipeline."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Which pattern ensures that data scientists can trigger hyperparameter tuning experiments via pull request merge, with automated notifications on completion?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use CodePipeline triggered by CodeCommit pull request merge. In a CodeBuild stage, invoke SageMaker HPO via AWS CLI. Use CloudWatch Events to notify on job completion.",
      "B": "Use SageMaker Experiments SDK manually invoked in notebooks.",
      "C": "Configure EventBridge to start tuning on S3 object creation, send SNS on complete.",
      "D": "Use Step Functions triggered by CodePipeline Webhook to start tuning."
    },
    "explanation": "CodePipeline on merge, CodeBuild to start tuning, and CloudWatch Events to notify provides CI/CD for experiments."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "When integrating a testing framework into CI/CD for ML, the team needs to run data schema validation, unit tests for transformations, and integration tests for training artifacts. How should they structure the CodePipeline?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Single build stage running all tests sequentially.",
      "B": "Separate pipelines for data tests, code tests, and training.",
      "C": "Pipeline with Source -> Data Validation Test (CodeBuild) -> Unit Test (CodeBuild) -> Integration Test (CodeBuild) -> Deploy.",
      "D": "Use Step Functions to orchestrate tests outside CodePipeline."
    },
    "explanation": "Separate CodeBuild stages in one pipeline for test types provide clear isolation and failure points."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A compliance requirement mandates that any model deployment must pass through an audit stage capturing approvals in an immutable log. Which service and pipeline step combination satisfies this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use CloudTrail to log manual approvals in SNS.",
      "B": "Use CodeDeploy with manual approval.",
      "C": "Use Step Functions Wait state with DynamoDB logging.",
      "D": "Use CodePipeline Manual Approval action with execution details logged in CloudTrail."
    },
    "explanation": "CodePipeline Manual Approval action events are logged in CloudTrail, satisfying audit and immutability requirements."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A data scientist merges new featurization code into GitHub. This should automatically update the SageMaker processing step in the CI/CD pipeline. How can this be achieved?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Manually update pipeline JSON in S3.",
      "B": "Use CodePipeline with a GitHub source action on the repo. Add a CodeBuild stage to synthesize updated pipeline definitions and deploy via CloudFormation update.",
      "C": "Write a Lambda to watch GitHub and update SageMaker resources via SDK.",
      "D": "Use AWS AppRunner to auto-deploy changes."
    },
    "explanation": "CodePipeline GitHub source plus CodeBuild to update pipeline definitions and CloudFormation deploy automates the update."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your pipeline uses AWS CodePipeline and CodeBuild to build, test, and deploy containers to SageMaker. You need to ensure that only approved container images are deployed. Which mechanism should you implement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use ECR image scanning on push, and add a Lambda that checks scan results before CodeDeploy stage.",
      "B": "Restrict CodePipeline IAM role to only allow unscanned images.",
      "C": "Use a lifecycle policy in ECR to retain only scanned images.",
      "D": "Configure CodeBuild to fail if image size exceeds threshold."
    },
    "explanation": "Integrating ECR image scan results via Lambda in the pipeline ensures only approved images proceed to deployment."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "The team must enforce that any pipeline change triggers a security compliance scan of both infrastructure and code. Which pattern achieves this end to end?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon Inspector on EC2 build agents.",
      "B": "Use CloudWatch Events to trigger manual security review.",
      "C": "In CodePipeline after Source stage, add a CodeBuild stage that runs static code analysis (e.g., cfn-nag) and container vulnerability scans, failing the build on issues.",
      "D": "Use AWS Config rules to audit pipeline resources."
    },
    "explanation": "Placing a security scan CodeBuild stage in the pipeline ensures any change undergoes compliance checks before deployment."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "You need to orchestrate an ML pipeline that span multiple AWS Regions. How can you centralize pipeline definition and execution?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Replicate the pipeline in each region manually.",
      "B": "Use Route 53 to direct triggers to regional pipelines.",
      "C": "Use EventBridge cross-region events to start different pipelines.",
      "D": "Store pipeline code in a central CodeCommit repo and use CodePipeline cross-region actions with CloudFormation to deploy and execute regional pipelines."
    },
    "explanation": "CodePipeline cross-region actions and central repo allow centralized definition while executing region-specific pipelines."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A team wants to provision ephemeral CI/CD infrastructure for each pull request to isolate changes during ML pipeline definition tests. Which approach is most automated and least overhead?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Manually launch Cloud9 environments per PR.",
      "B": "Use CodePipeline with a CodeBuild job that uses CDK to spin up a nested CloudFormation stack for the PR, runs tests, then tears it down.",
      "C": "Use Step Functions to create stacks, run tests, delete stacks.",
      "D": "Use AWS Batch to run tests in containers."
    },
    "explanation": "Using CDK in CodeBuild to deploy and teardown nested stacks per PR provides isolation with automation and minimal custom code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "They need to integrate model explainability checks into CI/CD such that if SHAP feature importance shifts beyond threshold in new model, pipeline fails. How to implement?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Add a CodeBuild stage using SageMaker Clarify SDK to compute SHAP values, compare them, and exit with failure if threshold exceeded.",
      "B": "Use CloudWatch Metrics to monitor SHAP values after deployment.",
      "C": "Embed checks in training script to abort training.",
      "D": "Deploy model then run real-time inference to compute drift."
    },
    "explanation": "A dedicated CodeBuild stage for Clarify explainability tests before deployment stops the pipeline on SHAP drift."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A centralized security team must approve any pipeline that grants broad IAM permissions. How can the CI/CD pipeline enforce this approval?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Config to detect wide permissions.",
      "B": "Use Lambda to revoke permissions after deployment.",
      "C": "Include a Manual Approval action after IAM change stage, before deployment, logged in CodePipeline.",
      "D": "Use CloudTrail Insights alerts."
    },
    "explanation": "Manual Approval actions in CodePipeline halt progress until security team approval, fulfilling enforcement and logging."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your ML pipeline CodeBuild stage needs environment variables stored securely. Which practice provides secure injection and auditability in CI/CD?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Store env vars in CodeBuild project settings.",
      "B": "Use AWS Secrets Manager, grant CodeBuild role GetSecretValue, inject at runtime.",
      "C": "Hard-code secrets in buildspec.",
      "D": "Store secrets in S3 with public-read access."
    },
    "explanation": "Secrets Manager integration with CodeBuild provides secure retrieval and auditing; avoids hard-coding or insecure storage."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A pipeline deploys SageMaker endpoints across multiple accounts. The build stage packages a SageMaker Model package. How can you share the model package across accounts securely in the pipeline?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Manually copy model artifacts via S3 CLI.",
      "B": "Use public S3 bucket for artifacts.",
      "C": "Use AWS DMS to migrate artifacts.",
      "D": "Use SageMaker Model Package Groups with cross-account grant via resource policy, and use CodePipeline roles to deploy."
    },
    "explanation": "SageMaker Model Package Groups support resource policies for cross-account sharing; CodePipeline roles can assume deploy permissions."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "How can you ensure pipeline stages for SageMaker training run on Spot instances to reduce cost, while controlling max runtime for retries?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set TrainingComputeType to ml.p3.2xlarge in pipeline.",
      "B": "Use a separate Spot training pipeline.",
      "C": "In SageMaker Pipeline TrainStep, specify TrainingInputMode with UseSpotInstances=true and MaxWaitTime/MaxRunTime.",
      "D": "Modify IAM policy to allow Spot."
    },
    "explanation": "SageMaker Pipelines TrainStep supports UseSpotInstances and max wait/run time to leverage Spot with retry constraints."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your pipeline uses CodePipeline with CloudFormation deploy action to create SageMaker Pipeline stacks. The team wants to detect drift between the deployed pipeline definition and source. Which tool helps?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable CloudFormation Drift Detection on the pipeline stack and integrate a CodeBuild stage to run detect-stack-drift and fail if drifted.",
      "B": "Use AWS Config to detect code changes in CodeCommit.",
      "C": "Add a Lambda to compare JSON definitions.",
      "D": "Use Inspector to scan pipeline definitions."
    },
    "explanation": "CloudFormation Drift Detection identifies config drift; performing it in CodeBuild fails pipeline if drift exists."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A pipeline must orchestrate a nightly batch inference job on new data via SageMaker Processing, then archive results to S3 Glacier. How can this be built in CodePipeline?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Lambda for processing and then Glacier archive.",
      "B": "Define pipeline with Source (EventBridge scheduled start), Processing stage using CodeBuild to call SageMaker Processing, then a Deploy stage to trigger a Lambda copying results to Glacier.",
      "C": "Use Step Functions with Lambda tasks.",
      "D": "Use Data Pipeline service."
    },
    "explanation": "CodePipeline with an EventBridge-triggered Source and CodeBuild action calling SageMaker Processing, followed by a Lambda archive Deploy stage, meets requirements."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A pipeline deploys model code and container. You need to validate model container performance under load before routing production traffic. Which deployment strategy implements this in CI/CD?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a single Lambda test after deployment.",
      "B": "Deploy to Sagemaker endpoint and test within same stage.",
      "C": "Use CodeDeploy with linear deployment configuration, sending small percentage of traffic to new container, run performance tests, then proceed.",
      "D": "Use Canary deployments via ECS Blue/Green."
    },
    "explanation": "CodeDeploy linear deployment with traffic shifting and automated tests allows performance validation before full cutover."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "They need to incorporate code coverage metrics into their CI/CD pipeline for ML data transformation scripts. How can this be done of minimal complexity?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Use CodeBuild to run pytest without coverage and interpret logs.",
      "B": "Add a CodeBuild stage that runs pytest with coverage, then use CodeBuild reports feature to publish coverage artifacts.",
      "C": "Use Coveralls external service triggered by EventBridge.",
      "D": "Manually inspect coverage after each build."
    },
    "explanation": "CodeBuild reports support publishing coverage metrics natively, integrating smoothly in pipelines without external dependencies."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A pipeline failure occurs when deploying to a private VPC endpoint because of missing permissions. Which policy change in CodePipeline role solves this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add IAM permission for ec2:CreateNetworkInterface, ec2:DescribeNetworkInterfaces, ec2:DeleteNetworkInterface.",
      "B": "Allow s3:GetObject VPC endpoints only.",
      "C": "Grant AWSKeyManagementServiceDecrypt in Secrets Manager.",
      "D": "Permit TagResource on CloudWatch Logs."
    },
    "explanation": "Deploying to private VPC endpoints requires network interface permissions for CodePipeline-managed actions; adding those resolves the failure."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your pipeline uses AWS CodeBuild to package and deploy a PyTorch model to SageMaker. The buildspec installs large ML libraries causing slow builds. How to speed up builds without sacrificing reproducibility?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use smaller instance size for CodeBuild.",
      "B": "Cache from public PyPI mirror.",
      "C": "Use requirement pre-install via pip wheel.",
      "D": "Enable CodeBuild local caching for pip and container layers to reuse previous build artifacts."
    },
    "explanation": "CodeBuild local caching for pip and Docker layers significantly speeds up build times while ensuring same dependencies are installed."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "They need to rollback to the last successful SageMaker pipeline execution automatically if a new execution fails validation. How can this be configured?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use EventBridge to watch FailedExecution and trigger manual rollback.",
      "B": "In SageMaker Pipelines, configure a CallbackStep that on failure starts an execution of the previous pipeline execution using StartPipelineExecution with previous execution id.",
      "C": "Use CloudFormation change sets to revert pipeline definition.",
      "D": "Use Step Functions with TRY/CATCH to revert."
    },
    "explanation": "SageMaker Pipelines CallbackStep can programmatically invoke a rollback to the last model by triggering an execution of the previous successful pipeline."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A pipeline must enforce that secret rotation and updated credentials are picked up without pipeline definition changes. Which pattern supports this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Hardcode secrets in parameter store with long TTL.",
      "B": "Re-deploy pipeline on rotation.",
      "C": "Use SSM Parameter Store dynamic references in pipeline actions to retrieve credentials at runtime.",
      "D": "Trigger pipeline manually after rotation."
    },
    "explanation": "Using dynamic references to Parameter Store ensures pipeline fetches updated secrets at runtime without redefining the pipeline."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Multiple data teams share a central CI/CD pipeline for SageMaker training. They need isolation so one team\u2019s pipeline failure doesn\u2019t block others. How to configure?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create separate CodePipeline pipelines per team pointing to same buildspec and repos, with individual pipeline resource and IAM roles.",
      "B": "Use single pipeline with parallel branches per team.",
      "C": "Use Step Functions instead of CodePipeline.",
      "D": "Use multi-branch CodeBuild project."
    },
    "explanation": "Separate pipelines per team isolate execution and failures, while sharing code and buildspec for consistency."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A security change requires that any pipeline artifact stored in S3 be encrypted with a customer-managed KMS key. How to enforce this in CodePipeline?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SSE-S3 default in artifact buckets.",
      "B": "Specify encryption in CodeBuild project settings.",
      "C": "Use CloudTrail to audit encryption.",
      "D": "Configure CodePipeline artifact store with KMS key ARN in the pipeline definition."
    },
    "explanation": "Defining the artifactStore with a specific KMS key ARN in the pipeline ensures all pipeline artifacts are encrypted accordingly."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "A fraud-detection model is trained on a dataset where fraudulent cases are only 1% of the data. The model achieves 99% accuracy on a test set, but the business indicates that many frauds are missed. Which evaluation metric would most appropriately reflect the model\u2019s ability to detect fraud?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Overall accuracy on the test set",
      "B": "ROC AUC",
      "C": "Precision-Recall AUC",
      "D": "Log-loss"
    },
    "explanation": "With extreme class imbalance, overall accuracy and ROC AUC can be misleading. Precision-Recall AUC focuses on the positive (fraud) class and better reflects detection capability."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "A classifier on a highly imbalanced dataset yields ROC AUC = 0.95 but PR AUC = 0.4. What does this indicate, and what should you do next?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "The model discriminates well overall but has low precision on positives; tune the decision threshold to improve precision.",
      "B": "The model is reliable; deploy it as is.",
      "C": "The dataset is too balanced; rebalance it further.",
      "D": "Collect more negative samples to improve ROC AUC."
    },
    "explanation": "High ROC AUC with low PR AUC shows many false positives. Adjusting the classification threshold based on PR curve can improve precision for the rare positive class."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your legacy model\u2019s F1 score on the production validation set is 0.85. A new candidate model scores 0.87. To determine if this improvement is statistically significant, you should:",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Automatically deploy the new model because 0.87>0.85",
      "B": "Perform a simple paired t-test on the F1 scores",
      "C": "Use bootstrap sampling on hold-out data to compute confidence intervals of F1 difference",
      "D": "Compare scores on a single test set without intervals"
    },
    "explanation": "Bootstrap sampling yields confidence intervals on the difference in F1, allowing you to assess statistical significance rather than relying on point estimates."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "During training, your deep network\u2019s training loss plateaus very early and validation loss remains high. Which SageMaker Debugger rule should you enable to diagnose this convergence issue?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "VanishingGradient",
      "B": "LossNotDecreasing",
      "C": "WeightUpdate",
      "D": "OverfitDetection"
    },
    "explanation": "The LossNotDecreasing rule alerts when training loss fails to decrease. VanishingGradient detects near-zero gradients, but first you must confirm loss stagnation with LossNotDecreasing."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your model achieves 100% training accuracy but only 60% validation accuracy. Which approach best detects and visualizes this overfitting?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Plot training and validation loss curves using SageMaker Debugger",
      "B": "Compute final confusion matrix on test set",
      "C": "Inspect global SHAP values",
      "D": "Monitor data quality with ModelMonitor"
    },
    "explanation": "Plotting training vs validation loss curves identifies divergence indicative of overfitting; SageMaker Debugger automates rule-based collection of these tensors."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You wish to conduct an A/B test between your production and shadow model variants using SageMaker endpoints. Which configuration allows a traffic split between variants?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy two separate endpoints and manually route traffic",
      "B": "Use a multi-model endpoint with two models loaded",
      "C": "Create an endpoint configuration with two production variants and specify traffic weights",
      "D": "Use AWS CodePipeline to shift traffic between endpoints"
    },
    "explanation": "Endpoint configurations in SageMaker allow multiple production variants with specified traffic weights for A/B testing without separate endpoints."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Model A has F1 = 0.92 and requires 12 hours training; Model B has F1 = 0.91 and requires 1 hour. Retraining occurs weekly. Which model should you deploy, and why?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model A, because maximizing F1 is paramount",
      "B": "Model B, because the small F1 loss is outweighed by 12\u00d7 faster retraining",
      "C": "Deploy both and ensemble",
      "D": "Alternate weekly between A and B"
    },
    "explanation": "In a weekly retraining pipeline, a 1% F1 reduction is acceptable given the 12\u00d7 speedup, reducing compute cost and risk of stale models."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You need to monitor inference data streams for feature distribution changes in production. Which SageMaker capability and baseline type will you use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ModelQualityMonitor with a model quality baseline",
      "B": "ModelBiasMonitor with a bias baseline",
      "C": "ModelExplainabilityMonitor with SHAP baseline",
      "D": "ModelMonitor DataQualityMonitor with a data quality baseline"
    },
    "explanation": "ModelMonitor\u2019s DataQualityMonitor uses a data quality baseline to detect feature distribution drifts in incoming inference data."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "A particular prediction\u2019s SHAP value for feature X is +2.5. What does this imply for that instance?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Feature X increased the model\u2019s output log-odds by 2.5 for this instance",
      "B": "Feature X is the most important globally",
      "C": "The model is biased toward feature X",
      "D": "SHAP values above 0 indicate uncertainty"
    },
    "explanation": "A positive SHAP value indicates that feature X contributed positively to the prediction (in log-odds space for classification) for that specific instance."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You want to track hyperparameters, metrics, and artifacts across multiple training runs in SageMaker. Which feature provides built-in experiment management?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Experiments with trials and trial components",
      "B": "SageMaker Clarify",
      "C": "Resource tagging",
      "D": "CloudWatch Logs"
    },
    "explanation": "SageMaker Experiments lets you organize runs into experiments and trials, capturing hyperparameters, metrics, and artifacts for reproducibility."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your regression model will be used in a financial application where large errors are severely penalized. Which evaluation metric should you prefer?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Root Mean Square Error (RMSE)",
      "B": "Mean Absolute Error (MAE)",
      "C": "R\u00b2 (coefficient of determination)",
      "D": "Mean Absolute Percentage Error (MAPE)"
    },
    "explanation": "RMSE penalizes larger errors quadratically, aligning with applications that severely penalize large deviations."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "To optimize for recall over precision in a binary classifier, you decide to adjust the decision threshold. How should you determine the new threshold?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set threshold to 0.5 regardless of metrics",
      "B": "Maximize accuracy on validation data",
      "C": "Use the threshold that maximizes ROC AUC",
      "D": "Analyze the precision-recall curve to find the point achieving the desired recall"
    },
    "explanation": "Precision-recall curves show trade-offs for different thresholds; selecting the threshold that meets your recall requirement is appropriate."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You have a multi-class classification problem with imbalanced class frequencies. You want each class to contribute equally to overall performance. Which averaging strategy do you use for F1 score?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Micro average",
      "B": "Macro average",
      "C": "Weighted average",
      "D": "Sample average"
    },
    "explanation": "Macro-averaged F1 computes F1 per class then averages equally, ensuring each class contributes equally despite imbalance."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your model is deployed in a streaming inference pipeline. You need to detect sudden drops in model accuracy over recent data without storing all predictions. Which approach is best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Query historical S3 logs daily",
      "B": "Compute a one-off accuracy on a large batch",
      "C": "Use SageMaker Model Monitor with a sliding window and accuracy rule",
      "D": "Use CloudWatch Metrics alone"
    },
    "explanation": "Model Monitor can continuously compute metrics over sliding windows on production data without storing all raw predictions."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Business requirements specify that predicted loan amounts must not decrease as applicant income increases. Which technique helps you verify this monotonic relationship in your deployed model?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Generate a partial dependence plot (PDP) for income",
      "B": "Examine the confusion matrix",
      "C": "Use a data drift baseline",
      "D": "Calculate overall RMSE"
    },
    "explanation": "A PDP shows the average model prediction as a function of a feature, allowing you to verify monotonicity constraints."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You suspect your neural network is overfitting early in training. Which SageMaker Debugger built-in rule can alert you when validation loss starts increasing while training loss decreases?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "LossNotDecreasing",
      "B": "VanishingGradient",
      "C": "Overflow",
      "D": "Overfit"
    },
    "explanation": "The Overfit rule in SageMaker Debugger flags when validation loss increases while training loss continues to decrease, a sign of overfitting."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Given confusion matrix counts TP=80, FP=20, FN=10, TN=890, what is the false positive rate (FPR)?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "20/(20+890) = 0.022",
      "B": "20/(80+20) = 0.20",
      "C": "10/(10+890) = 0.011",
      "D": "80/(80+10) = 0.89"
    },
    "explanation": "FPR = FP/(FP+TN) = 20/(20+890) \u22480.022."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You want to evaluate if two classifiers produce significantly different error rates on the same test set. Which statistical test is most appropriate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Paired t-test on accuracy scores",
      "B": "McNemar\u2019s test",
      "C": "Chi-square test for independence",
      "D": "Wilcoxon signed-rank test"
    },
    "explanation": "McNemar\u2019s test compares paired categorical outcomes (correct vs incorrect) from two classifiers on the same instances."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "After deployment, your model\u2019s input distribution remains stable but prediction error gradually increases. What type of drift does this indicate?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Covariate shift",
      "B": "Label shift",
      "C": "Data drift",
      "D": "Concept drift"
    },
    "explanation": "Concept drift occurs when the relationship between inputs and targets changes, causing error to rise despite stable input distribution."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You observe gradients in the first layers of your network are vanishing, slowing learning. Which SageMaker Debugger rule detects this issue directly?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "VanishingGradient",
      "B": "LossNotDecreasing",
      "C": "Detection of hung jobs",
      "D": "WeightDecay"
    },
    "explanation": "The VanishingGradient rule monitors gradient norms and flags when they are near zero, indicating vanishing gradients."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your pruned model achieves 40% size reduction with only a 0.5% drop in accuracy. To decide whether to deploy it, you should analyze:",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Only the accuracy difference",
      "B": "The trade-off between resource savings (memory, latency) and accuracy loss",
      "C": "Global SHAP importance values",
      "D": "Training time differences"
    },
    "explanation": "Deployment decisions should weigh resource savings (e.g., reduced latency, memory) against any drop in accuracy to determine overall benefit."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You want to assess whether your classifier\u2019s probability outputs are well calibrated. Which metric or tool should you use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ROC AUC",
      "B": "Precision-Recall AUC",
      "C": "Log-loss (cross-entropy)",
      "D": "F1 score"
    },
    "explanation": "Log-loss penalizes incorrect probability estimates and is sensitive to calibration, making it suitable for assessing probability calibration."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You need to compare model performance across multiple user segments (e.g., age groups) and give equal importance to each segment. Which aggregation of segment-level AUC metrics should you use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Arithmetic mean of AUC for each segment",
      "B": "Overall AUC on combined data",
      "C": "Weighted average by segment size",
      "D": "Maximum AUC across segments"
    },
    "explanation": "The unweighted (arithmetic) mean of segment-level AUCs treats each segment equally regardless of its size."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your cost matrix penalizes false negatives at 10\u00d7 the cost of false positives. Which evaluation metric best aligns with this cost sensitivity?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Expected cost computed from confusion matrix and cost weights",
      "B": "ROC AUC",
      "C": "Log-loss",
      "D": "Overall accuracy"
    },
    "explanation": "Computing the expected cost directly using confusion matrix counts and cost weights aligns evaluation with your business cost matrix."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "A model shows high overall accuracy but a high false negative rate on a protected group. To monitor fairness over time, which metric should you track with SageMaker Clarify?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Overall RMSE",
      "B": "ROC AUC",
      "C": "Data drift percentage",
      "D": "Difference in true positive rates (equal opportunity difference)"
    },
    "explanation": "Equal opportunity difference measures TPR gap between groups, highlighting unfair false negatives for protected attributes."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "To detect label distribution changes in production (label drift), which SageMaker Model Monitor job and baseline do you configure?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ModelQualityMonitor with a label distribution baseline",
      "B": "DataQualityMonitor with feature baseline",
      "C": "ModelBiasMonitor with bias baseline",
      "D": "ModelExplainabilityMonitor with SHAP baseline"
    },
    "explanation": "ModelQualityMonitor can track label statistics in inference data against a baseline to detect label drift."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You need to choose a threshold for a binary classifier that minimizes a custom cost function of TPR and FPR. Which process accomplishes this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Select threshold\u2009=\u20090.5",
      "B": "Maximize ROC AUC",
      "C": "Minimize log-loss",
      "D": "Compute cost for each threshold on validation probabilities and choose the minimum"
    },
    "explanation": "Evaluating the custom cost at each threshold on validation data lets you select the threshold minimizing your cost function."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your regression model predicts three related targets simultaneously. To evaluate performance treating each target equally, you should:",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Compute RMSE per target and report their average",
      "B": "Compute overall RMSE across all predictions",
      "C": "Weight each target by its variance in a single loss",
      "D": "Report only the highest RMSE target"
    },
    "explanation": "Averaging per-target RMSE ensures each output is given equal importance in performance evaluation."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your classifier\u2019s test set performance fluctuates significantly between runs. Which validation technique reduces variance and yields a more stable performance estimate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "K-fold cross-validation",
      "B": "Using a single larger test set",
      "C": "Bootstrapping only",
      "D": "Early stopping"
    },
    "explanation": "K-fold cross-validation averages performance over multiple splits, reducing variance due to data sampling."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You want to visualize model performance degradation over time in production. Which solution best automates this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Export logs daily and plot manually",
      "B": "Use SageMaker Model Monitor to publish quality metrics to CloudWatch dashboards",
      "C": "Run ad-hoc batch accuracy reports",
      "D": "Embed metrics in application code"
    },
    "explanation": "Model Monitor can stream quality metrics to CloudWatch, allowing automated dashboards tracking performance over time."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You see a sudden increase in false positives without any change in input distribution. What type of drift is this, and what action should you take?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Covariate drift; retrain on new features",
      "B": "Concept drift; investigate changes in the real world and retrain",
      "C": "Data drift; increase data validation",
      "D": "Label shift; adjust class weights"
    },
    "explanation": "A rise in error despite stable inputs indicates concept drift; investigate underlying changes and retrain the model accordingly."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Observation of rapidly growing gradient norms across layers suggests exploding gradients. Which SageMaker Debugger rule will catch this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "VanishingGradient",
      "B": "LossNotDecreasing",
      "C": "ExplodingGradient",
      "D": "WeightDecay"
    },
    "explanation": "The ExplodingGradient rule monitors for abnormally large gradient norms, flagging exploding gradient issues."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You use SageMaker Clarify to monitor feature attribution drift. Which monitor type and baseline should you configure?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ModelExplainabilityMonitor with a SHAP value baseline",
      "B": "DataQualityMonitor with a distribution baseline",
      "C": "ModelBiasMonitor with bias all baseline",
      "D": "ModelQualityMonitor with accuracy baseline"
    },
    "explanation": "ModelExplainabilityMonitor compares new SHAP attributions against a baseline to detect feature attribution drift."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A data engineer needs to join a 1 TB Parquet dataset in Amazon S3 with a DynamoDB table of product metadata and register features in SageMaker Feature Store daily with minimal operational overhead and within a 2-hour window. Which approach should the engineer choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Data Wrangler to import both datasets, perform the join, then execute a BatchPutRecord job to ingest features into Feature Store.",
      "B": "Schedule an AWS Glue ETL job to read the Parquet data, use the DynamoDB Spark connector to join the tables, write results back to S3, and trigger a SageMaker processing job to ingest into Feature Store.",
      "C": "Set up AWS Glue Streaming ETL to capture DynamoDB Streams updates and continuously merge against S3 data, writing directly into the online store.",
      "D": "Deploy an AWS Lambda function triggered by each S3 PUT to read and join records on the fly, then call PutRecord for each feature into Feature Store."
    },
    "explanation": "Scheduling a bulk AWS Glue ETL job best handles 1 TB batch processing within a time window and decouples transformation from per-record Lambda overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A feature team must encode a categorical feature with 10 million distinct values for use in a neural network. They need to minimize memory and avoid introducing ordinal bias. Which encoding technique and pipeline step should they implement?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use one-hot encoding in SageMaker Data Wrangler to generate 10 million binary columns, then apply feature selection.",
      "B": "Apply label encoding in DataBrew, converting each category to an integer index, and feed directly to the network.",
      "C": "Use feature hashing in a SageMaker processing job to map categories into a fixed-size hash vector, then normalize hashed features.",
      "D": "Group rare categories into an \"Other\" bucket before one-hot encoding in AWS Glue ETL."
    },
    "explanation": "Feature hashing maps high-cardinality categories into a fixed-size vector without ordinal bias and controls memory footprint."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A time-series dataset has irregular timestamps and missing readings. A data scientist needs to impute missing values and generate lag features in SageMaker Data Wrangler for model training. Which sequence of transformations should they apply?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Resample to a fixed interval with forward-fill imputation, compute lag features using the shifted timestamp, then scale features.",
      "B": "Compute lag features first (with gaps), drop NA, then apply KNN imputation to fill missing values.",
      "C": "Apply drop-rows where any null exists, generate lag features on remaining data, and finally apply standard scaling.",
      "D": "Use quantile imputation on raw data, then apply rolling window aggregations for lag features, and normalize."
    },
    "explanation": "Resampling with forward fill ensures consistent intervals before computing lags; then scaling preserves temporal relationships."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A large training dataset in S3 must be deduplicated by a composite key before feature scaling. The data volume exceeds what a single SageMaker Data Wrangler instance can process. What architecture meets the requirements?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Use SageMaker Data Wrangler with distributed compute enabled for dedupe and scaling.",
      "B": "Use an AWS Lambda function to split and dedupe small partitions, then reassemble.",
      "C": "Use AWS Glue DataBrew to dedupe by key and apply scaling transformations inline.",
      "D": "Use an AWS Glue Spark job on EMR to dedupe by composite key, write results to Parquet in S3, then use Data Wrangler for scaling on the reduced dataset."
    },
    "explanation": "A Spark job on EMR can handle large-scale deduplication; downstream Data Wrangler can operate efficiently on the smaller deduped data."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A team must apply quantile binning on a continuous feature to reduce noise, then one-hot encode the bins. They need to evaluate bin edges on training only and apply identical bins to validation/test sets. How should they implement this in SageMaker Data Wrangler?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create a recipe step to compute quantile bin edges on the training partition, save edges as a parameter, then apply same edges in subsequent steps to all partitions before one-hot.",
      "B": "Compute quantile bin edges over full dataset in Data Wrangler, then split into train/validation/test and one-hot encode separately.",
      "C": "Use AWS Glue DataBrew to compute bins on training, export bin definitions, then import back into Data Wrangler for splits.",
      "D": "Perform quantile binning and one-hot encoding in a SageMaker processing job where edges are recomputed per batch."
    },
    "explanation": "Computing bins on training only and saving edges ensures no leakage; applying stored edges uniformly maintains consistency."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A streaming application ingests clickstream JSON records into Kinesis and needs to parse, clean, and feature-engineer fields in near real-time. Which architecture minimizes latency and offloads transformation operations?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Lambda to poll Kinesis, transform records inline, then push to SageMaker Feature Store.",
      "B": "Write a Kinesis Data Firehose delivery stream with inline record transformation using AWS Glue.",
      "C": "Deploy a Kinesis Data Analytics (Apache Flink) application to parse JSON, apply scalar and one-hot encodings, then write transformed records to an output stream.",
      "D": "Use SageMaker Processing on a schedule to pull batches from Kinesis and process them."
    },
    "explanation": "Kinesis Data Analytics with Apache Flink provides low-latency, continuous transformations at scale, offloading work from Lambda."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A pipeline must mask PII columns before any transformations to comply with compliance. They have CSV data with 5 sensitive columns. Using AWS Glue DataBrew, which recipe steps ordering ensures compliance and minimal reprocessing?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Apply all feature scaling, encoding, and missing-value imputation, then mask PII at the end.",
      "B": "Mask PII using column-level encryption, then apply transformations in the same recipe.",
      "C": "Branch the job: first do transformation, output safe data; second, mask PII on original and join results.",
      "D": "Create two DataBrew steps: first mask (using built-in Mask Column), then reference masked columns in subsequent transformation steps."
    },
    "explanation": "Masking PII first in the recipe prevents any downstream steps from accessing raw PII and avoids reprocessing masked data."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A dataset contains a categorical feature with moderate cardinality and missing values. A data scientist wants to impute missing values as \"Unknown\" before one-hot encoding in SageMaker Data Wrangler. Which setting order in the recipe is correct?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "One-hot encode first, then treat missing values in generated dummy columns.",
      "B": "Apply a Fill Missing Values step on the raw column to \"Unknown\", then apply a One-Hot Encoding step on the filled column.",
      "C": "Drop rows with missing values, then one-hot encode remaining categories.",
      "D": "Run one-hot encoding with the \"Include Nulls\" option, then rename the \"null\" column to \"Unknown\"."
    },
    "explanation": "Imputing missing values before encoding ensures the \"Unknown\" category is represented correctly in the dummy variables."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A text feature requires tokenization and vectorization using a pretrained BERT tokenizer. They need to integrate this into a SageMaker feature engineering pipeline. Which approach is best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Data Wrangler built-in text tokenizer step with custom vocabulary.",
      "B": "Invoke AWS Comprehend batch API to tokenize, then import tokens to Data Wrangler.",
      "C": "Build a SageMaker Processing script container that loads the pretrained BERT tokenizer, tokenizes text, and writes token IDs to S3 for the next pipeline step.",
      "D": "Use AWS Glue Spark job to apply the BERT tokenizer via a Glue Python shell job."
    },
    "explanation": "A custom processing container in SageMaker Processing provides full control over the pretrained tokenizer and integrates cleanly into ML pipelines."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A data scientist wants to scale numeric features using Z-score normalization but only using statistics from the training set. They plan to scale validation/test sets identically. How can they configure SageMaker Data Wrangler?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Apply Z-score normalization in a DataBrew recipe after splitting, selecting all partitions together.",
      "B": "Use a SageMaker Processing job to compute mean/std on full dataset, then scale each partition separately in Data Wrangler.",
      "C": "In Data Wrangler, split the dataset into train/validation/test, add a Normalize step in the recipe that refers to the training split only, and ensure the recipe applies the same parameters to other splits.",
      "D": "Compute mean/std manually outside of Data Wrangler and hard-code values in a recipe."
    },
    "explanation": "Data Wrangler can compute normalization parameters on the training split and apply them uniformly to other splits within one recipe."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "During a SageMaker Data Wrangler flow, a shuffle step is added before splitting data into train/validation/test. The resulting model shows data leakage. What was the mistake?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Shuffling after splitting caused overlap across sets.",
      "B": "Shuffle did not include seed, producing non-reproducible splits.",
      "C": "Shuffle was applied only to the training split, leaving test data ordered.",
      "D": "Shuffle was applied before splitting, mixing records across temporal boundaries and causing leakage."
    },
    "explanation": "Shuffling before splitting time-series or grouped data can mix related records across splits, introducing leakage."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A data engineer needs to generate rolling window statistics (mean, std) per user over the last 7 days from clickstream JSON data in S3. Data volume is 500 GB/month. Which cost-effective approach should they use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Data Wrangler to load all data and compute rolling stats with Python transforms.",
      "B": "Use an AWS Glue Spark job with window functions to compute rolling statistics, write Parquet outputs back to S3.",
      "C": "Load raw data into DynamoDB and use DynamoDB Streams with Lambda to update rolling metrics.",
      "D": "Use SageMaker Feature Store with on-the-fly aggregation in training jobs."
    },
    "explanation": "A Glue Spark job with window functions scales to 500 GB and performs aggregations efficiently before model input."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A team must remove outliers defined as values beyond 3 standard deviations for a numeric feature. They want to log how many records were removed. In SageMaker Data Wrangler, how should they configure this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a Filter recipe step to remove outliers, then use the Data Wrangler UI to view counts after run.",
      "B": "Use a Custom Transform step with a Pandas script to drop outliers and log metrics to CloudWatch.",
      "C": "Add an Aggregate Recipe step to calculate mean/std, then a Filter step referencing those values, and enable the record count output option to capture removed record counts.",
      "D": "Export data to a processing job, implement outlier removal in code, and write counts to S3."
    },
    "explanation": "Data Wrangler\u2019s recipe can calculate statistics and filter in separate steps, and the built-in count option provides removal metrics."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A feature store offline dataset must be kept in sync with latest transformations from SageMaker Data Wrangler. Which mechanism ensures atomic replacement without downtime?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Write transformed data to a staging S3 prefix, then use CFN to update the offline store import job to point to new prefix.",
      "B": "Overwrite the existing S3 prefix directly with new Parquet files in place.",
      "C": "Append new transformed data to the existing prefix, and use a Glue crawler to update partitions.",
      "D": "Delete the offline store table and recreate it with new data after transformation."
    },
    "explanation": "Using a staging prefix and atomic pointer switch avoids partial reads and downtime."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "Continuous feature updates from AWS IoT devices stream into Kinesis Data Streams. A data scientist needs to window, normalize, and encode features before writing to Feature Store online. What real-time pipeline minimizes component count?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Kinesis Data Firehose with AWS Lambda transform to process and send to Feature Store.",
      "B": "Use a SageMaker endpoint to receive raw records, transform, and BatchPutRecord.",
      "C": "Use AWS Glue Streaming ETL job to read, transform, and write to Feature Store offline.",
      "D": "Deploy Kinesis Data Analytics (Apache Flink) application to window, normalize, encode, and call PutRecord API to Feature Store online."
    },
    "explanation": "Kinesis Data Analytics provides a single low-latency stream processing engine capable of calling AWS APIs directly."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "An image classification pipeline needs to apply resizing, normalization, and five types of augmentation (rotation, flip, crop, noise, color jitter) in SageMaker processing. To maximize GPU utilization and reproducibility, what should the team do?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Chain multiple processing jobs sequentially, each handling one augmentation type.",
      "B": "Use a single SageMaker Processing job with a PyTorch container that applies resizing, normalization, and augmentations in one GPU-accelerated DataLoader pipeline.",
      "C": "Implement augmentations in AWS Lambda functions triggered by S3 events for each image.",
      "D": "Use Data Wrangler custom transform steps for each preprocessing function and export as a combined dataset."
    },
    "explanation": "A single GPU-accelerated processing job with PyTorch DataLoader ensures reproducibility and efficient parallel augmentation."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A dataset contains repeated nested JSON fields that must be flattened for feature extraction. In AWS Glue DataBrew, which recipe step correctly handles arbitrary nesting levels?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a Pivot recipe step on the nested column.",
      "B": "Use a Select recipe step and specify JSONPath for each nested field.",
      "C": "Use a Join recipe step with the same column.",
      "D": "Use the Unnest recipe step to iteratively flatten nested JSON arrays and objects into top-level columns."
    },
    "explanation": "The Unnest step is designed to flatten arbitrarily nested JSON structures into columns automatically."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A feature with highly skewed distribution needs log transformation before scaling. A data scientist wants to ensure negative values are handled. In Data Wrangler, what transformation sequence should be used?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Scale feature with MinMax, then apply log transform.",
      "B": "Add a constant offset to make values positive, apply log transform, then standardize.",
      "C": "Apply Box-Cox transformation directly (Box-Cox cannot handle negatives).",
      "D": "Filter out negative values, then apply log transform on positives."
    },
    "explanation": "Adding a constant offset ensures all values are positive before log transform; standardizing afterwards preserves distribution."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A team must detect and drop duplicate records defined by a composite key, but preserve the earliest timestamp record. They want to log count of dropped duplicates. What is the correct approach in Data Wrangler?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use the Remove Duplicates recipe step on the composite key then sort ascending by timestamp.",
      "B": "Sort by timestamp and use the Remove Duplicates step without additional configuration.",
      "C": "Sort descending by timestamp, then use the Remove Duplicates step on the composite key with \"keep first occurrence\" and enable Logging to capture drop counts.",
      "D": "Use a Custom Transform step with a Pandas script to drop duplicates and write counts to CloudWatch."
    },
    "explanation": "Sorting descending ensures the earliest timestamp is last, then keeping the first occurrence drops later duplicates and logs counts."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A text corpus requires feature hashing of TF-IDF vectors into 512 dimensions before training. Which SageMaker toolchain should be used and why?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a SageMaker Processing job with a scikit-learn container to compute TF-IDF and apply FeatureHasher transformer in one job.",
      "B": "Use AWS Glue DataBrew to compute TF-IDF and hashing in recipe steps.",
      "C": "Use SageMaker Data Wrangler built-in TF-IDF and hashing steps.",
      "D": "Use AWS Lambda to compute and hash vectors on each record."
    },
    "explanation": "A processing job with scikit-learn provides efficient batch computation of TF-IDF and hashing integrated into the ML pipeline."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A feature store batch export job must produce engineered features that include one-hot encoded categories and normalized numerics. The export time window is 24 hours. Which orchestration ensures reproducibility and minimal drift?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Manually run a Data Wrangler flow to export features daily.",
      "B": "Schedule a Lambda that calls PutRecord for each export.",
      "C": "Use AWS Glue job to read offline store and apply transformations at export time.",
      "D": "Define a SageMaker Feature Store Batch Transform pipeline with saved Data Wrangler recipe steps to export and transform atomically."
    },
    "explanation": "Embedding Data Wrangler recipe in a Feature Store batch pipeline ensures consistent transformations and reproducibility."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A data scientist wants to detect measurement bias in labeled data prior to model training. Which AWS tool and workflow should they use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Glue Data Quality to compute missing value rates per label.",
      "B": "Use SageMaker Clarify\u2019s Data Bias job to generate pre-training bias metrics (e.g., CI, DPL) and produce a report for review.",
      "C": "Use SageMaker Data Wrangler to compute feature distributions per class.",
      "D": "Use Amazon Athena to run SQL queries comparing label distributions across partitions."
    },
    "explanation": "SageMaker Clarify is specifically designed to compute pre-training bias metrics and provide actionable reports."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "An offline Spark feature pipeline writes to Parquet partitioned by date. A validation job must only process the latest two days and ignore older partitions. How can they configure an AWS Glue job?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a dynamic frame with push_down_predicate on the date partition to filter to last two days before transformation.",
      "B": "Read all partitions and filter in code.",
      "C": "Use a Glue crawler to catalog only two latest partitions.",
      "D": "Use AWS Glue DataBrew with a filter recipe to drop older partitions."
    },
    "explanation": "Using push_down_predicate filters partitions at read time, reducing data scanned and improving performance."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A feature requires ranking users by activity per region, generating a rank feature. Which tool and technique should they use to compute this in a SageMaker pipeline?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Glue DataBrew with window aggregate recipe step.",
      "B": "Use a SageMaker Processing job with pure Python for ranking.",
      "C": "Use SageMaker Processing with a Spark container to apply window ranking functions by region, writing results to S3.",
      "D": "Use Lambda and Step Functions to orchestrate row-by-row ranking."
    },
    "explanation": "A Spark container in a processing job can efficiently compute window functions and scale to large user sets."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "During a Data Wrangler flow, a customer ID column is misclassified as numeric, preventing correct encoding. How can this be fixed?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Drop the column and reload.",
      "B": "Override the inferred data type to String in the Data Wrangler schema settings, then apply categorical encoding.",
      "C": "Cast the column inside a custom transform only.",
      "D": "Use AWS Glue job to recast and save back to S3 before Data Wrangler."
    },
    "explanation": "Overriding the schema inference directly in Data Wrangler allows correct downstream categorical encoding without external ETL."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A data scientist must anonymize email addresses by hashing them before any aggregation. The hashing function must be consistent across training and inference. How should they implement this in a SageMaker processing pipeline?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a Data Wrangler custom step with Python hashing, then export hashed emails.",
      "B": "Use AWS Glue DataBrew Mask Column with SHA256.",
      "C": "Use a Lambda function triggered by S3 events to hash emails.",
      "D": "Include a SageMaker Processing step with a custom container that applies SHA256 to the email column and writes to Parquet for downstream use."
    },
    "explanation": "A custom processing step ensures identical hashing logic in both training and inference pipelines."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A dataset of IoT sensor readings requires deduplication, imputation of missing values using KNN, and feature scaling. Which order in a DataBrew recipe ensures correct results and performance?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Scale, dedupe, then KNN impute.",
      "B": "Deduplicate first, KNN impute missing values, then apply scaling.",
      "C": "Impute missing values first, then dedupe, then scale.",
      "D": "Perform all three in parallel using branching steps."
    },
    "explanation": "Deduplication should occur before imputation to avoid artificial neighbors; scaling is last to ensure normalized neighbor distances."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A team needs to apply custom Python feature transformations (e.g., trigonometric functions) on numeric columns and integrate into SageMaker Pipelines. Which component should they use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A SageMaker Processing step with a custom Python script container.",
      "B": "A Data Wrangler custom recipe step.",
      "C": "A Lambda function in CodePipeline.",
      "D": "AWS Glue DataBrew Python transforms."
    },
    "explanation": "A SageMaker Processing step in the pipeline allows custom scripts and integrates directly with SageMaker Pipelines."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A dataset transformation in Data Wrangler fails due to schema drift: new columns appear. How can the recipe handle unknown columns without failing?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Manually update the recipe to include new columns each time.",
      "B": "Restrict the flow to specified columns only and drop unknown ones.",
      "C": "Enable the \"Ignore new columns\" option in the Data Wrangler recipe settings to skip unknown fields.",
      "D": "Use a SageMaker Processing job instead for dynamic schema."
    },
    "explanation": "Data Wrangler\u2019s ignore new columns setting prevents failures when unexpected fields appear."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A table join in SageMaker Data Wrangler is running out of memory because one table is 200 GB. Which alternative solution scales to this size?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase the Data Wrangler instance type to more memory.",
      "B": "Perform the join using an AWS Glue Spark ETL job and write result to S3, then import into Data Wrangler.",
      "C": "Use a SageMaker Processing job with Pandas on a GPU instance.",
      "D": "Split the larger table into chunks and join sequentially in Data Wrangler."
    },
    "explanation": "An AWS Glue Spark ETL job is designed to handle large-scale joins and can write partitioned outputs for downstream use."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A feature extraction requires computing pairwise differences between two timestamp columns per record. Which recipe step in Data Wrangler should they choose and why?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use the Compute Feature step with a custom expression (timestamp2 - timestamp1) to derive duration.",
      "B": "Use the Aggregate step grouping on record ID.",
      "C": "Use the Pivot step on timestamp fields.",
      "D": "Use a Custom Transform with a Pandas script to compute differences."
    },
    "explanation": "The Compute Feature step allows arithmetic expressions on columns and is more efficient than full Python transforms."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A team must ensure that text tokenization and stop-word removal occur before computing count vector features in a Data Wrangler flow. How should they configure the recipe?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Add Count Vectorization step, then Tokenization, then Stop-word removal.",
      "B": "Add Stop-word removal, then Count Vectorization, then Tokenization.",
      "C": "Add Tokenization, then Count Vectorization, then Stop-word removal.",
      "D": "Add Tokenization, then Stop-word removal step, then Count Vectorization in sequence."
    },
    "explanation": "Tokenization must split text first, followed by stop-word removal on tokens, then count vectorization."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A dataset stored as CSV in S3 must be converted to Parquet with snappy compression before feature engineering to improve I/O. Which tool and configuration should be used?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a SageMaker Processing job with a Pandas script to read CSV and write Parquet.",
      "B": "Use an AWS Glue ETL job with a PySpark script to read CSV, write Parquet with snappy compression, and partition by date.",
      "C": "Use SageMaker Data Wrangler export with Parquet output and compression.",
      "D": "Use AWS Lambda triggered per file to invoke Athena CTAS to create Parquet tables."
    },
    "explanation": "An AWS Glue Spark ETL job is well-suited for large-scale CSV to Parquet conversion with compression and partitioning."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A company must ingest 10 TB of historical telemetry data from an on-premises Hadoop cluster into Amazon S3 within 5 days for model training. The WAN link is limited to 200 Mbps and encryption at rest is required. Which ingestion method meets the transfer window while minimizing operational overhead?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Use AWS Snowball Edge to physically import the data into S3 with built-in encryption.",
      "B": "Use S3 Transfer Acceleration over the internet with SSE-S3 encryption.",
      "C": "Use AWS DataSync over the existing link with server-side encryption.",
      "D": "Deploy AWS Storage Gateway in Volume mode and snapshot to S3."
    },
    "explanation": "Snowball Edge provides accelerated, encrypted bulk transfer when network bandwidth is constrained, minimizing operational overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An ML pipeline queries 500 GB of transaction logs in S3 via Athena, then ingests the results into SageMaker Data Wrangler. Which storage format and partitioning maximize query performance and minimize costs?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Store logs as CSV files partitioned by year/month.",
      "B": "Store logs as JSON files partitioned by customer ID.",
      "C": "Store logs as Apache Parquet files partitioned by date.",
      "D": "Store logs as unpartitioned text files in a single prefix."
    },
    "explanation": "Parquet\u2019s columnar format plus date partitioning reduces scanned data and cost for Athena queries and speeds ingestion."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A real-time anomaly detection model requires streaming sensor data into S3 for batch retraining. Which combination of services ingests JSON records at low latency with ordering guarantees?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Kinesis Data Firehose delivering directly to S3.",
      "B": "Amazon Kinesis Data Streams then a Kinesis Data Firehose delivery stream to S3.",
      "C": "AWS Glue streaming ETL reading from S3 Event Notifications.",
      "D": "Amazon Managed Kafka writing directly to S3."
    },
    "explanation": "Kinesis Data Streams preserves ordering and low latency; Firehose can then buffer and batch to S3 automatically."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "Your model needs high-throughput access to feature data during training. You must choose between Amazon EFS, Amazon FSx for NetApp ONTAP, and Amazon S3. The data is >1 TB, highly structured, and training uses Spark on EMR. Which storage is best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon EFS with provisioned throughput.",
      "B": "Amazon FSx for NetApp ONTAP.",
      "C": "Amazon S3 with EMRFS Consistent View.",
      "D": "Store data on EMR HDFS volumes."
    },
    "explanation": "S3 with EMRFS scales to TBs, offloads management, and is cost-effective for large structured datasets; HDFS adds management overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A team uses SageMaker Data Wrangler to profile 2 TB of CSV data stored in S3. They notice excessive request latencies. Which action will most reduce latency when reading from S3?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable SSE-KMS on the S3 bucket.",
      "B": "Convert CSV data to Apache Parquet with Snappy compression.",
      "C": "Enable S3 Object Lock on the bucket.",
      "D": "Enable S3 Versioning on the bucket."
    },
    "explanation": "Converting to Parquet reduces the number of S3 GET requests and data scanned, greatly lowering latency in Data Wrangler."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An ML workflow must merge customer profile data in DynamoDB with clickstream logs in S3, then store results for training. Which approach minimizes custom code and operational complexity?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use AWS Glue ETL jobs with a DynamoDB connector to join and write to S3 in Parquet.",
      "B": "Export DynamoDB to S3 via Data Pipeline, then use Lambda to merge with clickstream.",
      "C": "Use SageMaker Processing to read both and write merged CSV to S3.",
      "D": "Stream clickstream into DynamoDB and query joins at training time."
    },
    "explanation": "AWS Glue ETL handles connectors and joins natively and writes Parquet to S3, reducing custom orchestration."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "You must ingest 5 TB of Amazon RDS data into SageMaker Feature Store online store for low-latency feature retrieval. Which combination meets throughput and cost requirements?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Export RDS snapshot to S3, use Glue to write to Feature Store.",
      "B": "Use RDS native replication into DynamoDB, then batch ingest into Feature Store.",
      "C": "Run a SageMaker Processing job to read from RDS and write to offline store.",
      "D": "Use SageMaker Data Wrangler to connect to RDS and directly ingest records into Feature Store online store."
    },
    "explanation": "Data Wrangler can connect to RDS and ingest into Feature Store online, meeting throughput needs with minimal glue code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A model requires streaming user events from mobile apps to S3 for nightly batch training. You need exactly-once delivery and minimal data loss. Which architecture is most appropriate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon Kinesis Data Firehose directly to S3.",
      "B": "Use AWS Lambda triggered by API Gateway to write to S3.",
      "C": "Use Amazon Kinesis Data Streams with a consumer that writes to S3 via Firehose.",
      "D": "Use Amazon SQS and a fleet of EC2 batch workers to write to S3."
    },
    "explanation": "Kinesis Data Streams ensures ordering and exactly-once semantics; Firehose delivers to S3 reliably with retries."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "During a cost audit, you discover that S3 GET and PUT costs for small files (<1 KB) used in training are high. How can you reduce costs while keeping data in S3?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch to S3 Standard-Infrequent Access.",
      "B": "Combine small files into larger Apache Avro container files.",
      "C": "Enable S3 Transfer Acceleration.",
      "D": "Enable S3 Intelligent-Tiering."
    },
    "explanation": "Combining small files into larger containers reduces request count and lowers GET/PUT request costs."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A global company must ingest CSV sales data from multiple regional offices into a central S3 bucket. Some offices have poor upload bandwidth. How to ensure data arrives in the correct order and is fully available for batch processing in SageMaker?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use S3 Transfer Acceleration with directory replication.",
      "B": "Use AWS DataSync scheduled tasks from each office.",
      "C": "Use AWS Storage Gateway file gateway.",
      "D": "Configure regional AWS Kinesis Data Firehose streams to deliver to a central S3 bucket with buffering and ordering enabled."
    },
    "explanation": "Regional Firehose streams buffer data, preserve ordering per shard, and reliably deliver to central S3 for downstream batch."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "Your training data resides in a high-CPU EBS volume attached to an EC2 instance. To ingest data into SageMaker Data Wrangler at scale, which approach minimizes network bottlenecks?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Snapshot the EBS volume to S3 and let Data Wrangler read from S3.",
      "B": "Attach the EBS volume to the SageMaker Data Wrangler notebook instance.",
      "C": "Enable EBS multi-attach and mount on Data Wrangler instance.",
      "D": "Run a DataSync task from the EC2 instance to the Data Wrangler volume."
    },
    "explanation": "Snapshot to S3 offloads network traffic and lets Data Wrangler ingest efficiently from S3 rather than over direct EBS mounts."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A data scientist needs to join high-velocity Kafka events with a static dataset in S3 before training. They require minimal lag and schema evolution support. What ingestion pattern is best?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "Use Kinesis Data Firehose with Lambda to enrich events with S3 data.",
      "B": "Deploy Amazon Managed Streaming for Kafka with AWS Glue streaming ETL to join and write to S3.",
      "C": "Use SageMaker Processing to pull from Kafka and S3 periodically.",
      "D": "Use AWS Lambda triggered by Kafka Connect and query S3 DynamoDB export."
    },
    "explanation": "Glue streaming ETL supports schema evolution and continuous joins with Kafka and S3, delivering results to S3 in near real time."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A compliance requirement mandates encrypting sensor data and storing in S3 Glacier after ingestion. The data is 8 TB per month. What solution meets compliance with the lowest storage cost while remaining queryable for ML?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Ingest to S3 Standard with SSE-KMS, transition to S3 Glacier Deep Archive immediately.",
      "B": "Ingest to S3 Glacier with client-side encryption.",
      "C": "Ingest to S3 Standard-IA with SSE-KMS, configure lifecycle to move to Glacier after 30 days.",
      "D": "Ingest to S3 One Zone-IA with SSE-KMS then transition to Glacier Flexible Retrieval."
    },
    "explanation": "Standard-IA with SSE-KMS and a 30-day lifecycle balances low cost while keeping data in S3 for Athena before Glacier transition."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "You must ingest and merge product catalog data from Amazon RDS and Alexa usage logs in S3 into SageMaker Feature Store offline store. How do you ingest with minimal code?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Glue jobs to extract from RDS and S3, transform, and write to Feature Store.",
      "B": "Use SageMaker Processing to read both sources and write offline store.",
      "C": "Export RDS to S3, then use Data Wrangler to merge and ingest.",
      "D": "Use AWS AppFlow to replicate RDS and merge with S3 logs."
    },
    "explanation": "Glue ETL natively connects to RDS and S3 and can write to Feature Store with minimal custom code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An ML workflow streaming video metadata at 10 MB/sec must store data in S3 for batch training. Latencies <5 seconds acceptable. Which ingestion service provides auto-scaling and buffering without managing infrastructure?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon Kinesis Data Streams with manual shard scaling.",
      "B": "Use Amazon Kinesis Data Firehose to deliver to S3.",
      "C": "Run AWS Lambda to poll a custom endpoint and write to S3.",
      "D": "Use AWS Managed Kafka with MirrorMaker to S3."
    },
    "explanation": "Firehose auto-scales and buffers transient bursts, delivering to S3 with minimal management."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A dataset of images is stored in S3 and must be preprocessed via SageMaker Processing weekly. The team wants to minimize request overhead and latency when reading many small files. How should they reorganize the data?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable S3 Transfer Acceleration when reading each image.",
      "B": "Copy images to an EFS mount for processing.",
      "C": "Bundle images into TFRecord or RecordIO files and store in S3.",
      "D": "Store images as base64-encoded JSON in single S3 objects."
    },
    "explanation": "Bundling into TFRecord/RecordIO reduces object count and request overhead, speeding SageMaker Processing."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A financial services company deployed a SageMaker real-time endpoint for credit risk scoring. They need to detect, within one hour, any drift in the distribution of the applicant_age and debt_to_income_ratio features compared to production baselines and send alerts. Which solution meets these requirements with minimal custom development?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable DataCaptureConfig on the endpoint, generate baseline statistics with ModelMonitor\u2019s built-in methods, create a monitoring schedule for input feature drift, and configure CloudWatch alarms on the monitoring job metrics.",
      "B": "Configure a Lambda function to read logs from CloudWatch Logs every hour, compute feature histograms, compare to baselines stored in S3, and send SNS alerts.",
      "C": "Use Kinesis Data Analytics to ingest endpoint invocation payloads, run SQL queries to detect distribution changes, and trigger alarms through CloudWatch.",
      "D": "Write a custom Spark job on EMR to read captured inference data, compute drift metrics, and schedule hourly cron jobs to publish alerts via Lambda."
    },
    "explanation": "A is correct because SageMaker Model Monitor can capture endpoint inputs, compute feature drift against baselines, and integrate with CloudWatch alarms with minimal coding. B and D require building custom data pipelines and drift logic. C uses Kinesis Data Analytics which is not optimized for SageMaker endpoint data capture and adds unnecessary complexity."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An e-commerce company uses a SageMaker endpoint for fraud detection. They must monitor model performance by tracking the false positive rate (FPR) in production and automatically retrain if FPR exceeds 5%. Which architecture satisfies this requirement?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Model Monitor\u2019s default data quality monitor to detect inference anomalies and trigger a Lambda to retrain the model when anomalies occur.",
      "B": "Capture only input payloads, write a CloudWatch metric filter on the logs for false positives, and use a CloudWatch alarm to start a SageMaker batch transform retraining job.",
      "C": "Enable DataCaptureConfig for both input and output, create ModelQualityMonitor with a custom metrics file specifying FPR, schedule hourly monitoring, and configure CloudWatch alarm to invoke a retraining pipeline.",
      "D": "Log predictions to S3, use Athena with scheduled queries to compute FPR hourly, and use Step Functions to orchestrate retraining when threshold is exceeded."
    },
    "explanation": "C is correct because ModelQualityMonitor supports custom metrics (including FPR) computed from captured input/output and can trigger alarms. A lacks FPR tracking. B captures only inputs and relies on log parsing. D is possible but involves Athena and Step Functions, which is more complex and not leveraging built-in model quality monitoring."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A healthcare provider has a batch inference pipeline that writes predictions and actual outcomes to S3 daily. They need to monitor model performance degradation (e.g., increasing RMSE) and alert DevOps when RMSE increases by more than 10% from baseline. Which move is best?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use CloudWatch Logs Insights to query S3 logs via log aggregation and trigger alarms when RMSE query results exceed thresholds.",
      "B": "Create a ModelQualityMonitor job using baseline metrics from training, supply the ground truth and prediction files from S3, schedule daily runs, and configure CloudWatch alarms.",
      "C": "Configure SageMaker Clarify to detect bias drift which will also report RMSE change and use CloudWatch alarms for notifications.",
      "D": "Invoke a Lambda function via EventBridge daily to compute RMSE from S3 data and send an SNS alert when RMSE >1.1\u00d7 baseline."
    },
    "explanation": "B is correct because ModelQualityMonitor directly supports batch inference data from S3 with ground truth to compute metrics like RMSE and integrate with alarms. A and D require custom code. C is incorrect because Clarify focuses on bias and feature drift, not performance metrics like RMSE."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A consumer app uses an endpoint for sentiment analysis. They observe occasional spikes in inference latency that degrade user experience. They need to detect latency anomalies in real time and trigger scaling actions. Which solution is most appropriate?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use ModelMonitor\u2019s data quality monitor to detect latency outliers and send events to CloudWatch.",
      "B": "Instrument the endpoint container to emit custom latency metrics to CloudWatch and set alarms on p95 latency.",
      "C": "Schedule a daily ModelMonitor job to analyze captured requests and identify latency anomalies.",
      "D": "Enable SageMaker endpoint invocation metrics (p90 and p95) in CloudWatch, create CloudWatch alarms for sudden increases, and attach Application Auto Scaling policies to the endpoint."
    },
    "explanation": "D is correct because SageMaker endpoints automatically emit latency metrics to CloudWatch, which can be used for real-time alarms and auto scaling. A and C misuse ModelMonitor, which is offline. B requires container modification when built-in metrics suffice."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An IoT solution uses a SageMaker endpoint for anomaly detection on sensor streams. They require near real-time alerts when feature distributions drift beyond control limits, but they cannot incur high Lambda execution fees. Which design balances cost and responsiveness?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Stream captured inference data to Kinesis Data Firehose, run a Lambda per record to compute drift and send alerts.",
      "B": "Write all captured data to S3 and run a ModelMonitor daily job with low-cost spot instances.",
      "C": "Use ModelMonitor endpoint preprocessor with custom monitoring script to detect drift thresholds on a 15-minute schedule and publish alerts via CloudWatch Events without Lambda.",
      "D": "Use Kinesis Data Analytics SQL application to continuously compute drift metrics and send alerts through SNS."
    },
    "explanation": "C is correct because ModelMonitor supports custom scripts for drift detection on schedules, avoiding per-record Lambda costs. A and D incur compute in Kinesis/Lambda continuously. B reduces responsiveness to daily."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A streaming inference endpoint uses DataCaptureConfig to write payloads and responses to S3. The team needs to detect when data quality (missing fields, invalid ranges) degrades in production and be paged immediately. Which process achieves this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy ModelMonitor\u2019s DefaultModelMonitor with a custom data quality constraints file, schedule it every 5 minutes, and configure CloudWatch alarms for violations.",
      "B": "Configure SageMaker Clarify to analyze captured data for anomalies and configure alerts for missing features.",
      "C": "Use Athena with scheduled queries every 5 minutes on the captured S3 files, detect invalid records, and notify via SNS.",
      "D": "Stream S3 events to Lambda to validate each record in real time and alert on invalid data."
    },
    "explanation": "A is correct because DefaultModelMonitor supports data quality constraints with custom rules, scheduled frequently, with CloudWatch integration. B focuses on bias/feature importance. C and D require custom orchestration and are less integrated."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A media company wants to monitor drift in top-5 features importance for its recommender model in production. They need automated alerts if the feature importance distribution shifts significantly. Which solution fits?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use ModelQualityMonitor to track feature importance values via ground truth comparisons.",
      "B": "Use ModelExplainabilityMonitor to baseline SHAP feature importances during training, schedule production monitoring, and alarm on significant distribution shifts.",
      "C": "Invoke Clarify\u2019s bias detection in production to monitor feature importances.",
      "D": "Export feature importance logs in JSON and use CloudWatch Logs Insights to compare distributions daily."
    },
    "explanation": "B is correct because ModelExplainabilityMonitor (part of Clarify) handles SHAP-based production monitoring of feature importances and can raise alarms. A is wrong because ModelQualityMonitor focuses on prediction quality, not explainability. C misuses bias detector. D is custom and manual."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A logistics firm uses a SageMaker endpoint behind an ALB. They need to monitor end-to-end inference latency and alert if 95th percentile latency exceeds 300 ms for more than 5 minutes. How do they implement this with least maintenance?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Embed custom timing code in inference container to publish custom CloudWatch metrics.",
      "B": "Use ModelMonitor to capture input/output times and schedule drift checks on latency.",
      "C": "Stream ALB access logs to CloudWatch Logs, parse latency percentile every minute via Logs Insights, and alarm.",
      "D": "Use CloudWatch metric for SageMaker Endpoint p95 latency with a 5-minute period alarm, and connect to SNS for notifications."
    },
    "explanation": "D is correct: SageMaker endpoints expose p95 latency in CloudWatch, which supports threshold-based alarms. A requires custom code. B misuses ModelMonitor. C is indirect and involves complex log parsing."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A biotech company runs batch inference nightly and stores predictions plus sample metadata in S3. They want to detect when the distribution of a key lab_measurement feature drifts compared to training data, and integrate alerts into PagerDuty. Which approach is best?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Athena scheduled queries on S3 data to compute KLD between current and training histograms, and send alerts via SNS to PagerDuty.",
      "B": "Set up a ModelMonitor batch monitoring job with a baseline from training data, schedule it nightly, and configure CloudWatch alarms to SNS for PagerDuty.",
      "C": "Deploy Clarify drift detector to analyze batch outputs for distribution changes and integrate with EventBridge for notifications.",
      "D": "Write custom PySpark EMR job to compute population stability index nightly and notify via Lambda."
    },
    "explanation": "B is correct because batch ModelMonitor directly handles feature drift with baselines, schedules jobs, and integrates with CloudWatch/SNS. A and D are custom and more maintenance. C misuses Clarify drift detector instead of ModelMonitor."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A retail company uses multi-model endpoints. They must monitor data skew separately for each model under high invocation volume without overloading S3. How should they configure Model Monitor?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable DataCaptureConfig for all models to a single S3 prefix and schedule a single monitor job with a filter script per model.",
      "B": "Use one monitor schedule per model, each writing to its own S3 bucket to avoid prefix collision.",
      "C": "Configure DataCaptureConfig to drop 80% of captured requests, tag model name, and run a single monitor job with group-by on model name.",
      "D": "Disable DataCaptureConfig, and use CloudWatch synthetic tests per model for skew detection."
    },
    "explanation": "C is correct: sampling reduces S3 volume, tagging allows a single job with group-by to monitor each model. A writes all data and requires filtering inside job. B duplicates resources. D uses synthetic tests, not real capture."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An AI startup uses Clarify ModelExplainabilityMonitor for drift detection, but they notice that on Mondays, due to different user behavior, alerts spike falsely. They want to suppress Monday alerts but keep daily checks. Which modification is appropriate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a suppression rule in CloudWatch alarm for Mondays using a schedule-based mute period.",
      "B": "Adjust the drift threshold upward only for Monday\u2019s monitoring schedule.",
      "C": "Exclude data collected on Mondays from the ModelExplainabilityMonitor input baseline.",
      "D": "Shift monitoring schedule to every 48 hours to skip Monday checks."
    },
    "explanation": "A is correct: CloudWatch alarm suppression allows muting alerts on Mondays without affecting thresholds. B leads to inconsistent drift sensitivity. C corrupts baseline. D reduces monitoring frequency overall."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A ride-sharing company deployed a streaming inference endpoint. They need to detect concept drift in the predicted probability distribution for rider surge events. Which built-in capability supports this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ModelQualityMonitor\u2019s custom metrics for probability shift detection.",
      "B": "ModelMonitor\u2019s ModelBiasMonitor class with a custom preprocessor to compute Jensen-Shannon divergence.",
      "C": "ModelExplainabilityMonitor with SHAP-based drift metrics.",
      "D": "DefaultModelMonitor data quality checks for distribution changes."
    },
    "explanation": "B is correct because ModelBiasMonitor can measure probability distribution drift of predictions using divergence metrics. A is not a ModelQualityMonitor feature. C focuses on feature importance. D checks data quality, not prediction distribution."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A gaming company wants to use SageMaker Model Monitor to detect anomalies in user behavior predictions every five minutes but must avoid executing on cold data. The endpoint has periods of very low traffic. How to configure?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Schedule monitoring at 5-minute intervals without sampling; it will skip if no new data.",
      "B": "Use 1-minute schedule with a Lambda filter to drop empty datasets.",
      "C": "Configure ModelMonitor with max_runtime per job to 10 minutes so it exits on no data.",
      "D": "Schedule monitoring every 5 minutes with min_sampling_size set to 50 so it won\u2019t run unless at least 50 records are present."
    },
    "explanation": "D is correct: min_sampling_size prevents jobs when insufficient data arrives, avoiding waste. A will run and error on empty data. B and C require custom code or rely on runtime but don\u2019t prevent job attempts."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An energy company\u2019s endpoint serves predictions at different times of day, with nightly off-peak periods. They need to monitor inference anomalies only during peak hours (8 AM\u20138 PM). Which pattern accomplishes this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create a monitor schedule at 1-hour intervals and ignore anomalies logged outside of 8\u201320 via filter in the alerting Lambda.",
      "B": "Use two monitoring schedules: one at 5-minute intervals active between 8 and 20 using schedule expressions, and none outside that window.",
      "C": "Enable continuous monitoring and write a custom script to drop anomalies outside the window.",
      "D": "Schedule a daily monitor job at 20:01 that covers the day\u2019s peak data."
    },
    "explanation": "B is correct: schedule expressions in SageMaker allow cron-based windows to run only during peak hours. A adds complexity. C and D don\u2019t align to real-time peak detection."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A financial app uses a multi-variant endpoint to serve A/B test models. They need to compare real-time inference accuracy of each variant for rollout decisions without manual intervention. Which integrated feature achieves this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use CloudWatch Logs Insights to query variant-specific logs and compute accuracy per variant.",
      "B": "Enable Kinesis Data Streams on endpoint traffic, write a consumer to compute variant accuracy.",
      "C": "Use SageMaker Model Monitor\u2019s built-in endpoint variant-based shadow testing with baseline accuracy thresholds and CloudWatch alarms.",
      "D": "Invoke two separate endpoints in parallel and use Lambda to compare results against ground truth."
    },
    "explanation": "C is correct: Model Monitor supports shadow variants for model comparison against baselines, automating monitoring. A and B require manual aggregation. D duplicates endpoints and custom code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A telecom provider must ensure that no single categorical feature \u2018device_type\u2019 in production grows in usage by more than 20% relative to training. They want to monitor this continuously. Which approach is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use ModelMonitor\u2019s DefaultModelMonitor with a custom constraints JSON specifying the allowed 20% change for device_type categories and schedule frequent jobs.",
      "B": "Implement a Kinesis Data Analytics job to maintain real-time category counts and compare to training counts.",
      "C": "Use Clarify\u2019s bias monitor to set fair thresholds on device_type representation.",
      "D": "Capture endpoint inputs to S3 and run a daily Athena job to compute ratios and alert."
    },
    "explanation": "A is correct because DefaultModelMonitor constraints allow category-level drift threshold configuration, scheduled regularly. B and D are custom. C is misusing bias monitor, not intended for drift constraints."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An automotive company uses real-time inference for predictive maintenance. They must validate that all required sensor fields exist and log any missing fields for further investigation, without failing invocations. How should they set this up?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Implement input validation in the inference container code and reject requests with missing fields.",
      "B": "Use ModelMonitor\u2019s default data quality monitor to reject invocations with missing fields.",
      "C": "Stream inference input logs to CloudWatch Logs and create metrics filters for missing fields.",
      "D": "Configure ModelMonitor with a custom preprocessor script that checks for missing keys, logs violations to S3, and does not block the endpoint."
    },
    "explanation": "D is correct: ModelMonitor\u2019s preprocessor can implement validation logic, log violations, and does not interfere with real-time endpoint behavior. A blocks requests. B cannot reject invocations. C requires log parsing and has delay."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A travel app\u2019s SageMaker real-time endpoint is scaled to multiple instances for high traffic. They need aggregated abnormalities in feature distributions across all instances. Which configuration ensures correct drift detection?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy a monitor job per instance capturing to instance-specific S3 prefixes and aggregate results in Athena.",
      "B": "Enable DataCaptureConfig at the endpoint level to collect across all instances, and run a single ModelMonitor job on that capture location.",
      "C": "Add a sidecar container per instance to forward data to Kinesis Data Streams for drift detection.",
      "D": "Use CloudWatch metrics on each instance and manually compute distribution shifts in a Lambda."
    },
    "explanation": "B is correct: DataCaptureConfig at endpoint level automatically aggregates across instances. A is complex. C and D require additional infrastructure and custom aggregation."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A gaming platform has an endpoint for matchmaking that logs into DynamoDB. They need to monitor that output predictions are within the valid score range [0,1] and alert if outliers appear. Which solution is optimal?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Schedule a ModelMonitor default data quality job with constraints JSON specifying numeric_validity for the output_score feature and set CloudWatch alarms.",
      "B": "Use Lambda streams on DynamoDB to check each output and publish custom CloudWatch metrics for out-of-range values.",
      "C": "Enable Amazon Kinesis Firehose on logs to S3 and run nightly Athena queries to detect out-of-range values.",
      "D": "Instrument application code to validate scores before writing and send alerts via SNS."
    },
    "explanation": "A is correct: ModelMonitor constraints support numeric validity checks and integrate with alarms. B and C are custom solutions more maintenance. D moves monitoring into application logic rather than production monitoring."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A retailer\u2019s real-time pricing endpoint serves dynamic pricing. They want to test periodically whether price predictions exceed a profit margin threshold and revert if they do. Which monitoring feature should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use ModelQualityMonitor with a custom metric that computes profit margin on predictions.",
      "B": "Schedule a Lambda to call the endpoint, compute profit margin, and alarm.",
      "C": "Use ModelMonitor\u2019s output constraints JSON to define the profit margin threshold on prediction and schedule continuous monitoring.",
      "D": "Implement profit margin check logic in the inference container and emit CloudWatch metrics."
    },
    "explanation": "C is correct: ModelMonitor supports output constraints for predictions and continuous monitoring. A is misusing quality monitor, which focuses on ground truth comparison. B and D are custom implementations requiring more code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An insurance provider uses a batch inference pipeline on S3 with predictions and claims. They need to identify when the ratio of denied claims to total predictions deviates by more than 15% from the baseline. Which tool and configuration do they choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Clarify ModelBiasMonitor with a custom preprocessor to compute the ratio and schedule daily.",
      "B": "Use ModelQualityMonitor with ground truth and predictions, define a custom violation report with denied_claims/total and threshold, and schedule daily runs.",
      "C": "Use DefaultModelMonitor data quality for this ratio and set thresholds.",
      "D": "Query S3 via Athena daily to compute the ratio and call a Step Functions pipeline."
    },
    "explanation": "B is correct: ModelQualityMonitor supports custom metrics for model performance comparisons, including custom functions like ratio of denied claims. A is bias monitor, not for performance metrics. C is data quality; D is custom."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A sports analytics firm needs to detect if the distribution of a continuous feature \u2018player_speed\u2019 starts trending downward in real-time endpoint predictions. They require a rolling window of last 1,000 inferences. How can they configure Model Monitor?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set DataCaptureConfig, configure a monitoring schedule with monitoring_interval=1 minute, and set sampling_percentage so that window size approximates 1,000. Use constraints on the mean of player_speed.",
      "B": "Enable Clarify drift detector with window_size=1000 for player_speed.",
      "C": "Stream inference outputs to Kinesis Data Streams and configure Kinesis Analytics to compute rolling averages.",
      "D": "Use a Lambda triggered on capture S3 events to maintain a sliding window in DynamoDB and trigger alarms."
    },
    "explanation": "A is correct: monitoring_interval and sampling_percentage controls sample size per job; constraints can specify mean thresholds. B is not built-in. C and D are custom solutions outside Model Monitor."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A medical imaging application serves predictions for tumor detection. For compliance, any missing license_plate field (erroneously included) must be detected at inference time and reported, but not block inference. Which Model Monitor component is appropriate?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use DefaultModelMonitor data quality monitor to enforce presence of license_plate.",
      "B": "Use ModelExplainabilityMonitor to detect missing features.",
      "C": "Use ModelQualityMonitor comparing ground truth, specifying missing field detection.",
      "D": "Use a custom preprocessor script in ModelMonitor constraints to check for license_plate existence and log violations."
    },
    "explanation": "D is correct: a preprocessor script can validate arbitrary conditions (including extraneous fields) without affecting inference. A does not support arbitrary missing field checks. B and C are for explainability and performance respectively."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An ad-tech firm runs real-time bidding predictions and must detect when the click-through rate (CTR) predicted by the model deviates by more than 10% from training CTR. They have no ground truth in real time. What feature of SageMaker solves this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use ModelQualityMonitor with placeholders for ground truth CTR.",
      "B": "Use ModelMonitor\u2019s custom reporter in data quality to compute predicted CTR distribution drift against baseline.",
      "C": "Use Clarify bias monitor to detect prediction bias on CTR.",
      "D": "Use endpoint logs and Athena to compute CTR distribution daily."
    },
    "explanation": "B is correct: data quality monitors can compute statistics on predictions alone, enabling detection of predicted CTR shifts without ground truth. A is not applicable since quality monitor requires ground truth. C misuses bias monitor. D is custom and offline."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A gaming platform needs to ensure that inference payload sizes don\u2019t suddenly increase (which could indicate malicious inputs). They want to monitor payload byte size distributions in production. How can they achieve this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use DefaultModelMonitor with a preprocessor script to compute payload size feature, set constraints on the size distribution, and schedule monitoring.",
      "B": "Modify inference container to log payload size to CloudWatch and alarm on spikes.",
      "C": "Use Lambda triggered on S3 capture PUT events to record object sizes and alert.",
      "D": "Write a Kinesis Data Analytics job to analyze payload streams for size anomalies."
    },
    "explanation": "A is correct: ModelMonitor allows custom features via preprocessor scripts (including payload size) and constraint definitions. B, C, D require separate pipelines and custom code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A supply chain model produces predictions every minute. The team must compute the percentage of predictions greater than threshold X and alert if it falls below 30% for more than 10 minutes. They have Model Monitor enabled. What is the simplest way to implement?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Clarify\u2019s bias monitor to configure threshold checks on prediction distribution.",
      "B": "Schedule a daily ModelQualityMonitor job with custom metrics file for percentage above X.",
      "C": "Configure a real-time ModelMonitor schedule with a custom constraints JSON to compute % above X and set alarms on violations every 5 minutes.",
      "D": "Stream predictions to Kinesis Data Streams and use AWS Lambda to monitor percentage."
    },
    "explanation": "C is correct: ModelMonitor constraints support custom aggregations (like percentage above threshold) and real-time schedules. A and B misapply Clarify or daily quality jobs. D is custom and more complex."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A biotech lab uses an asynchronous SageMaker endpoint. They need to monitor and alert on both input data completeness and response time exceeding 2 seconds for each batch. Which combination of Model Monitor capabilities achieves this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable DataCaptureConfig for request and response, configure DefaultModelMonitor with separate data quality and custom latency constraints, schedule every 5 minutes.",
      "B": "Use Clarify ModelExplainabilityMonitor for input completeness and CloudWatch metrics for latency.",
      "C": "Use two separate Lambda functions\u2014one reading S3 captures for missing fields and one alarm on CloudWatch latency metrics.",
      "D": "Run a daily ModelQualityMonitor job to measure completeness and latency metrics."
    },
    "explanation": "A is correct because DefaultModelMonitor can validate data quality and custom metrics like latency in one job. B misuses explainability. C is custom. D is too infrequent."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A financial firm wants to monitor covariance drift between two features, income and expenditure, in real-time inference data. Which Model Monitor strategy accomplishes this?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Use Clarify bias detector to compute covariance and schedule monitoring.",
      "B": "Schedule Athena queries to compute covariance and trigger alarms.",
      "C": "Implement a custom preprocessor in ModelMonitor to compute covariance drift constraint and schedule frequent runs.",
      "D": "Stream inference data to Redshift and run SQL to compute covariance daily."
    },
    "explanation": "C is correct: custom preprocessor scripts in ModelMonitor allow computing covariance and enforcing drift constraints. A and B don\u2019t natively compute covariance. D is custom and offline."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A mobile app uses a SageMaker real-time endpoint. They want to detect if any string feature contains non-UTF8 characters in production. Which approach is most maintainable?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Modify inference code to reject non-UTF8 inputs and log violations.",
      "B": "Use Clarify\u2019s bias detector to detect invalid characters.",
      "C": "Run a Lambda on captured S3 data to search for invalid encodings.",
      "D": "Configure ModelMonitor with a custom preprocessor to scan string features for non-UTF8 and log violations in S3."
    },
    "explanation": "D is correct: ModelMonitor preprocessor scripts can perform arbitrary validation (e.g., encoding checks) with minimal maintenance and decoupled from inference code. A couples monitoring with serving. B is misapplied. C is custom."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A retailer uses batch transform to score customer churn nightly. They need to verify that output probabilities sum to 1 for each record (softmax outputs) and alert on any anomalies. Which Model Monitor feature supports this at lowest operational cost?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Clarify bias monitor to validate output probability sums.",
      "B": "Use DefaultModelMonitor with a custom postprocessor script to compute sum of probabilities per record and set constraints that sum equals 1.",
      "C": "Instrument batch transform container to raise errors when sums differ and notify via SNS.",
      "D": "Write Athena query over output files to check sums and send alerts."
    },
    "explanation": "B is correct: postprocessor scripts in ModelMonitor can validate prediction outputs and enforce numeric constraints. A is misapplied. C and D require custom code or queries."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A manufacturing line inference endpoint outputs categorical defect labels. They must detect when a rare defect category appears more than twice in an hour, indicating a quality issue. How can they configure Model Monitor?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create a DefaultModelMonitor job with custom constraints specifying max_count=2 for that category over a 1-hour window and schedule jobs accordingly.",
      "B": "Run a Clarify bias monitor to detect changes in rare label frequency.",
      "C": "Use Kinesis Data Streams to aggregate counts and Lambda to alert.",
      "D": "Schedule daily Athena queries on captured data to check counts."
    },
    "explanation": "A is correct: ModelMonitor supports frequency constraints for categorical values. B misuses bias. C and D are custom and less real-time."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An app uses a SageMaker real-time endpoint for language translation. They want to ensure that none of the translated outputs contain profane words. Which Model Monitor mechanism should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Clarify bias monitor with a profanity lexicon.",
      "B": "Modify inference code to filter profanity and report.",
      "C": "Configure ModelMonitor with a custom postprocessor script that scans outputs against a profanity list and logs violations.",
      "D": "Stream responses to Kinesis and run a profanity detection Lambda."
    },
    "explanation": "C is correct: postprocessor scripts in ModelMonitor can implement content validation and integrate with alerts. A is misused. B couples monitoring into serving. D requires extra infrastructure."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A logistics service uses a multi-model endpoint for forecasting. They need per-model latency and feature distribution monitoring. The endpoint metadata tags each invocation with model_id. How can they implement this in ModelMonitor?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable separate DataCaptureConfig per model_id by deploying multiple endpoints.",
      "B": "Enable a single DataCaptureConfig that captures model_id in payload, schedule ModelMonitor with a group_by based on model_id for both latency and data quality metrics.",
      "C": "Stream all invocations to Kinesis Data Streams, use a Lambda to split by model_id.",
      "D": "Deploy Clarify bias detectors per model for distribution monitoring."
    },
    "explanation": "B is correct: ModelMonitor supports group_by functionality on a specified feature (model_id) to generate separate metrics per group. A duplicates endpoints. C is custom. D misuses bias detectors."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A financial services company needs to detect fraudulent transactions in real time with sub-50ms latency per transaction, while also running nightly trend analyses on aggregated data. They plan separate ML pipelines for each use case. Which combination of inferencing types should they implement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Batch inferencing for fraud detection and real-time inferencing for trend analysis",
      "B": "Real-time inferencing for fraud detection and batch inferencing for trend analysis",
      "C": "Streaming inferencing for fraud detection and online inferencing for trend analysis",
      "D": "Online inferencing for fraud detection and micro-batch inferencing for trend analysis"
    },
    "explanation": "Real-time inferencing meets sub-50ms latency for fraud detection; batch inferencing is appropriate for nightly aggregate trend analysis. Streaming and micro-batch are not AWS inference categories defined in this context."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An e-commerce team is deciding between logistic regression and random forest for a binary classification task. They refer to these as \u201calgorithms.\u201d After training, they obtain predictive artifacts they deploy. In AWS terms, what is the term for these deployed artifacts?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Hyperparameters",
      "B": "Features",
      "C": "Models",
      "D": "Labels"
    },
    "explanation": "Algorithms (logistic regression, random forest) produce trained artifacts called models. Hyperparameters configure algorithms; features are inputs; labels are outputs used during training."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A manufacturing firm collects sensor readings (temperature, vibration, pressure) every second as JSON messages and also attaches operator comments as free text. What best describes the types of data ingested?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Structured tabular data only",
      "B": "Unstructured multimedia data",
      "C": "Semi-structured numerical data only",
      "D": "Mixed structured (numeric) and unstructured (text) data"
    },
    "explanation": "Numeric sensor readings are structured; operator comments are unstructured text. It\u2019s mixed. The other options mischaracterize the combination."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A marketing team wants to segment customers by purchase behavior without any predefined labels. They plan to discover latent groups. Which learning paradigm should they apply?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Supervised classification",
      "B": "Unsupervised clustering",
      "C": "Reinforcement learning",
      "D": "Semi-supervised learning"
    },
    "explanation": "Clustering on unlabeled data is unsupervised learning. Supervised requires labels; reinforcement uses rewards; semi-supervised mixes labels and unlabeled data."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A logistics company uses trial and error and a reward function to optimize warehouse pick-and-place operations in simulation. Which ML method are they using?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Supervised learning",
      "B": "Unsupervised learning",
      "C": "Reinforcement learning",
      "D": "Self-supervised learning"
    },
    "explanation": "Using a reward signal in simulation for actions is reinforcement learning. Supervised uses labeled examples; unsupervised finds patterns without rewards; self-supervised derives labels internally but not via reward."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An image classification solution on AWS uses a convolutional neural network with millions of parameters and requires GPUs. Which term best categorizes this approach?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deep learning",
      "B": "Traditional ML algorithm",
      "C": "Reinforcement learning agent",
      "D": "Ensemble tree model"
    },
    "explanation": "CNNs with many layers/functions on unstructured images are deep learning. Ensemble trees and traditional ML do not generally use deep neural nets; reinforcement learning uses a reward model."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A research group selects a pre-trained language model of 20B parameters that supports few-shot prompting. What terminology best describes this artifact?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A neural network",
      "B": "A supervised model",
      "C": "An embedding",
      "D": "A large language model (LLM)"
    },
    "explanation": "A pre-trained model with billions of parameters for language tasks is an LLM. While it is a neural network and supervised pre-training is used, the specific term is LLM; embeddings are vector representations."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A facial recognition model misclassifies certain demographics more often than others. What term describes this issue?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Bias",
      "B": "Variance",
      "C": "Fit",
      "D": "Regularization"
    },
    "explanation": "Systematic error disadvantaging particular groups is bias. Variance relates to sensitivity to data fluctuations; fit/reg. concern under/overfitting and penalty terms respectively."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A text classification model achieves 99% accuracy on training data but only 60% on new customer emails. Which problem is occurring?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High bias",
      "B": "Data leakage",
      "C": "Underfitting",
      "D": "Overfitting"
    },
    "explanation": "A large gap between training and test performance indicates overfitting. High bias or underfitting would show poor performance on both. Data leakage would inflate test performance, not reduce it sharply."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An online retailer deploys a model endpoint to score customer propensity in milliseconds per request. Which term describes this stage?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Model training",
      "B": "Feature engineering",
      "C": "Inference",
      "D": "Data preprocessing"
    },
    "explanation": "Serving a deployed model to generate predictions is inference. Training builds the model; feature engineering and preprocessing occur before model usage."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A shipping firm wants to detect package damage by analyzing shipment photos. Which AI domain should they leverage?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Computer vision",
      "B": "Natural language processing",
      "C": "Reinforcement learning",
      "D": "Speech recognition"
    },
    "explanation": "Analyzing images falls under computer vision. NLP handles text; reinforcement learning uses rewards; speech recognition handles audio."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A call center wants to transcribe and analyze customer audio for sentiment. Which AI domain and subtask are involved?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Computer vision with OCR",
      "B": "Computer vision with object detection",
      "C": "Natural language processing with speech-to-text and sentiment analysis",
      "D": "Reinforcement learning with audio encoding"
    },
    "explanation": "Transcribing speech is speech-to-text (an NLP subtask) followed by sentiment analysis (also NLP). The other options misapply computer vision or reinforcement learning to audio."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A startup uses customer behavior logs to train a model and wants to evaluate various algorithms efficiently. Which pipeline step does this describe?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Deployment",
      "B": "Experimentation (training and validation)",
      "C": "Feature storage",
      "D": "Data annotation"
    },
    "explanation": "Testing multiple algorithms with validation data is experimentation within the training lifecycle. Deployment comes after; feature storage and annotation are different steps."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A dataset contains numeric, categorical, text, image, and time-series fields. How would you classify this dataset overall?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Structured",
      "B": "Unstructured",
      "C": "Semi-structured",
      "D": "Heterogeneous multimodal data"
    },
    "explanation": "Multiple data modalities (numeric, text, image, time-series) define heterogeneous multimodal data. Structured/unstructured/semi-structured don\u2019t capture multiple distinct modalities."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A credit scoring model uses decision trees and logistic regression, then blends outputs. What ensemble method category is this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Stacking",
      "B": "Bagging",
      "C": "Boosting",
      "D": "Blending"
    },
    "explanation": "Combining diverse model outputs via a meta-learner is stacking. Bagging uses same algorithm on subsets; boosting sequentially focuses on errors; blending is a holdout variant of stacking but less formal."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A medical imaging model needs 3D scans and uses volumetric convolution. Which architectural choice defines this model?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Recurrent neural network",
      "B": "Transformer",
      "C": "Autoencoder",
      "D": "3D convolutional neural network"
    },
    "explanation": "3D CNNs handle volumetric data via 3D convolutions. RNNs process sequences; transformers use attention; autoencoders learn embeddings."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "When splitting data for training and inference, which data characteristic ensures the model generalizes?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High label imbalance",
      "B": "Representative i.i.d. distribution",
      "C": "Missing values\u96c6\u4e2d",
      "D": "Highly correlated features"
    },
    "explanation": "An independent, identically distributed (i.i.d.) split ensures the inference data resemble training data. Label imbalance, missing values, or correlated features hamper generalization."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A recommender system logs user clicks and ratings. Which learning paradigm uses both past actions and reward signals to improve suggestions?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Unsupervised learning",
      "B": "Supervised learning",
      "C": "Reinforcement learning",
      "D": "Self-supervised learning"
    },
    "explanation": "A recommender optimizing using click/reward feedback is reinforcement learning. Supervised uses explicit labels; unsupervised finds patterns; self-supervised generates labels."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A startup fine-tunes a pre-trained transformer-based LLM by updating all weights on domain data. What process are they performing?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Prompt engineering",
      "B": "Fine-tuning",
      "C": "Inference",
      "D": "Embedding"
    },
    "explanation": "Updating model weights on new data is fine-tuning. Prompt engineering crafts inputs; inference uses the model; embedding produces vector representations."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A dataset of customer addresses stored as JSON documents without fixed schema is an example of which data type?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Structured",
      "B": "Unstructured",
      "C": "Semi-structured",
      "D": "Multimodal"
    },
    "explanation": "JSON documents with flexible schema are semi-structured. Structured implies rigid tables; unstructured refers to raw text/images; multimodal combines multiple media types."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A deep network\u2019s capacity to memorize training noise rather than general patterns relates to which property?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Bias",
      "B": "Fairness",
      "C": "Regularization",
      "D": "Variance"
    },
    "explanation": "High variance indicates sensitivity to noise and overfitting. Bias relates to systematic error; fairness to equity; regularization is a technique to reduce variance."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A supply-chain model uses past demand to predict next-month orders via linear regression. Which algorithm type is this?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Regression",
      "B": "Classification",
      "C": "Clustering",
      "D": "Dimensionality reduction"
    },
    "explanation": "Predicting a continuous numeric target is regression. Classification predicts discrete labels; clustering groups unlabeled data; dimensionality reduction reduces feature space."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An NLP pipeline transforms text into 768-dimension vectors before clustering. What are these vectors called?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Features",
      "B": "Embeddings",
      "C": "Hyperparameters",
      "D": "Parameters"
    },
    "explanation": "Embeddings are dense vector representations of text. Features are inputs to models; parameters are learned weights; hyperparameters configure training."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A governance team audits a model\u2019s training algorithm and discovered inconsistent outputs for identical inputs across runs. Which concept might explain this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High bias",
      "B": "Data imbalance",
      "C": "Nondeterministic initialization",
      "D": "Overfitting"
    },
    "explanation": "Random weight initialization or nondeterministic operations in deep learning can lead to different outputs for identical inputs. Bias, imbalance, and overfitting don\u2019t directly cause run-to-run variability."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A customer service chatbot uses a model that responds within seconds but batches billing analysis nightly. How should you label these two inferencing modes?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Synchronous and asynchronous inferencing",
      "B": "Online and offline training",
      "C": "Real-time and streaming inferencing",
      "D": "Real-time and batch inferencing"
    },
    "explanation": "Chatbot responses require real-time (online) inference; nightly billing analysis is batch inference. The other terms mix training with inference or streaming, which is not the defined mode here."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An ML pipeline scales behavior analysis to new data without labels by learning data structure. Which technique applies?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Supervised classification",
      "B": "Unsupervised learning",
      "C": "Reinforcement learning",
      "D": "Transfer learning"
    },
    "explanation": "Unsupervised learning discovers patterns in unlabeled data. Transfer learning uses labeled source domains; reinforcement learning uses rewards; classification is supervised."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A text summarization system must compress long documents into key sentences. Which AI subfield does this represent?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Computer vision",
      "B": "Reinforcement learning",
      "C": "Natural language processing",
      "D": "Anomaly detection"
    },
    "explanation": "Summarization is an NLP task. Computer vision processes images; reinforcement is reward-based; anomaly detection finds outliers."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A model\u2019s training loop uses gradient descent to minimize a loss function. Which concept does the loss function represent?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Algorithm objective",
      "B": "Model artifact",
      "C": "Feature set",
      "D": "Hyperparameter"
    },
    "explanation": "The loss function defines the objective the algorithm optimizes. The model artifact is the trained parameters; features are inputs; hyperparameters configure training."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A pre-training phase on massive text yields a foundation model that can be specialized later. What is the general process called?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Inference",
      "B": "Pre-training",
      "C": "Fine-tuning",
      "D": "Evaluation"
    },
    "explanation": "Pre-training on large unlabeled corpora yields a foundation model. Fine-tuning specializes; inference uses the model; evaluation measures performance."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A wildlife monitoring project clusters animal tracks without labels and then assigns species names manually to clusters. Which workflow combines the two learning types?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Supervised learning",
      "B": "Reinforcement learning",
      "C": "Unsupervised learning",
      "D": "Semi-supervised learning"
    },
    "explanation": "They initially use unsupervised clustering then manually label clusters, combining labeled and unlabeled data = semi-supervised. Pure supervised or unsupervised use only one type; reinforcement uses rewards."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A model\u2019s performance degrades over time because data distribution shifts. Which concept describes this challenge?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Overfitting",
      "B": "Data drift",
      "C": "Bias",
      "D": "Variance"
    },
    "explanation": "Data drift refers to changes in input data distribution over time, causing degradation. Bias/variance relate to model errors; overfitting is memorization."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A dataset has 100 features but you observe diminishing returns beyond 10 in model performance. Which concept are you examining?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Regularization strength",
      "B": "Algorithm complexity",
      "C": "Feature importance",
      "D": "Model capacity"
    },
    "explanation": "Evaluating how adding features impacts performance assesses feature importance. Regularization, algorithm complexity, and model capacity address other aspects."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A retail company wants to segment customers based on purchasing behavior to tailor marketing campaigns. They have numerical features such as purchase frequency, average order value, and days since last purchase. Which ML technique and AWS service combination best suits this use case?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use the built-in K-Means clustering algorithm in Amazon SageMaker to group customers.",
      "B": "Use Amazon Personalize to automatically cluster customers and generate segments.",
      "C": "Use sentiment analysis in Amazon Comprehend to segment customers by sentiment.",
      "D": "Use Amazon Forecast to predict future customer segments over time."
    },
    "explanation": "Unsupervised clustering via SageMaker K-Means is ideal for grouping based on numeric behavior; other services are for recommendations, sentiment, or forecasting."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A fintech startup needs to identify potentially fraudulent credit card transactions in real time with minimal ML expertise and wants pre-trained fraud detection capabilities. Which AWS service should they choose?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Fraud Detector",
      "B": "Amazon SageMaker Ground Truth",
      "C": "Amazon Comprehend",
      "D": "Amazon GuardDuty"
    },
    "explanation": "Amazon Fraud Detector provides domain-specific, pre-trained models for fraud detection; others are for labeling, NLP, or threat detection."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A global brand wants to analyze customer tweets to gauge public sentiment about a new product launch without building or training custom models. Which AWS service should they use?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Comprehend Sentiment Analysis",
      "B": "Amazon SageMaker BlazingText",
      "C": "Amazon Translate",
      "D": "Amazon Lex"
    },
    "explanation": "Comprehend\u2019s built-in sentiment API analyzes text sentiment directly; other services are for custom text models, translation, or chatbots."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A media company needs to convert recorded webinar audio into text transcripts to index content for search. Which AWS service should they choose?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Transcribe",
      "B": "Amazon Comprehend",
      "C": "Amazon Polly",
      "D": "Amazon Translate"
    },
    "explanation": "Amazon Transcribe converts speech to text; Comprehend analyzes text, Polly generates speech, Translate translates text."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An e-commerce website must provide real-time translation of product descriptions into multiple languages to serve global customers. Which AWS service is most appropriate?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Translate",
      "B": "Amazon Comprehend",
      "C": "Amazon Polly",
      "D": "Amazon Lex"
    },
    "explanation": "Amazon Translate provides real-time neural translation; Comprehend is for NLP analysis, Polly for speech, Lex for conversational interfaces."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A legal firm wants to automatically extract key phrases and entities (like person, organization, date) from large volumes of contracts. Which AWS service should they use?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Comprehend Entity and Key Phrase Extraction",
      "B": "Amazon Textract Table Extraction",
      "C": "Amazon SageMaker BlazingText",
      "D": "Amazon Kendra"
    },
    "explanation": "Comprehend extracts entities and key phrases; Textract focuses on form/ table OCR, SageMaker needs model building, Kendra is search."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A news app needs to generate audio versions of articles for accessibility. Which AWS service should they integrate?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Transcribe",
      "B": "Amazon Translate",
      "C": "Amazon Polly",
      "D": "Amazon Lex"
    },
    "explanation": "Amazon Polly converts text to lifelike speech; Transcribe converts speech to text, Translate translates text, Lex builds chat interfaces."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A company wants to build a conversational agent to answer common HR policy questions via chat. Which AWS service should they use?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Comprehend",
      "B": "Amazon Lex (without Lambda integration)",
      "C": "Amazon Polly",
      "D": "Amazon Lex with intent management"
    },
    "explanation": "Amazon Lex provides NLU, intent handling, and conversation flow; Comprehend is for text analysis, Polly for TTS."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An online retailer wants to offer personalized product recommendations based on browsing behavior and purchase history. Which AWS service is most suitable?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Personalize",
      "B": "Amazon Forecast",
      "C": "Amazon Kinesis Data Analytics",
      "D": "Amazon Personalize"
    },
    "explanation": "Amazon Personalize is designed for personalized recommendations; Forecast is forecasting, Kinesis analyzes streaming data but not personalized models."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A manufacturing firm needs to automatically detect defects in product images on the assembly line. They need bounding boxes around defects. Which service is most appropriate?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Rekognition Text Detection",
      "B": "Amazon SageMaker Image Classification",
      "C": "Amazon Textract",
      "D": "Amazon Rekognition Object Detection"
    },
    "explanation": "Rekognition\u2019s object detection provides bounding boxes; image classification labels whole images, Textract is OCR."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A bank wants to build a model to predict customer churn probability using historical account activity. Which ML technique and AWS service should they choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Binary classification using SageMaker built-in XGBoost algorithm",
      "B": "Unsupervised K-Means clustering in Amazon SageMaker",
      "C": "Reinforcement learning in AWS DeepRacer",
      "D": "Time series forecasting in Amazon Forecast"
    },
    "explanation": "Predicting churn is a supervised binary classification problem; SageMaker\u2019s XGBoost is appropriate."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A telecom operator wants to forecast next-month data usage per customer to provision network capacity. Which technique and AWS service best apply?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Classification with SageMaker Linear Learner",
      "B": "Time series forecasting with Amazon Forecast",
      "C": "Clustering with SageMaker K-Means",
      "D": "Anomaly detection with SageMaker Random Cut Forest"
    },
    "explanation": "Forecasting numeric usage over time requires time series forecasting; Amazon Forecast is purpose-built."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A logistics company receives IoT sensor data and wants to detect abnormal temperature spikes without labeled anomalies. Which AWS service and algorithm combination should they use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon SageMaker Linear Learner",
      "B": "Amazon Fraud Detector",
      "C": "Amazon Comprehend",
      "D": "SageMaker Random Cut Forest anomaly detection"
    },
    "explanation": "Random Cut Forest in SageMaker unsupervised algorithm detects anomalies in unlabeled time series."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An educational platform wants to cluster quiz questions based on topic similarity using NLP embeddings. They don\u2019t require supervised labels. Which approach is most appropriate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon Translate then cluster translated text",
      "B": "Use Amazon Comprehend sentiment scores",
      "C": "Generate embeddings with SageMaker JumpStart BERT model and cluster via K-Means",
      "D": "Use Amazon Personalize grouping"
    },
    "explanation": "Extracting embeddings with a pre-trained model then K-Means clustering is an unsupervised NLP approach."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A marketing team considers using ML to map postal codes to region names because there are only 50 unique codes. Is ML appropriate here?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Yes \u2013 use Amazon SageMaker lookup table training",
      "B": "No \u2013 use a simple rule-based lookup table (no ML)",
      "C": "Yes \u2013 use Amazon Comprehend to infer regions",
      "D": "No \u2013 use Amazon Forecast to predict region mapping"
    },
    "explanation": "Mapping a small finite set is best solved with a rule-based lookup, not ML."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A company wants to detect specific keywords in customer call transcriptions to trigger alerts. Which AWS service should they use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Transcribe with custom vocabulary and AWS Lambda keyword filter",
      "B": "Amazon Comprehend Key Phrase Extraction",
      "C": "Amazon Kinesis Video Streams",
      "D": "Amazon Lex Slot Filling"
    },
    "explanation": "Custom vocabulary in Transcribe captures keywords in speech-to-text, Lambda filters them; Comprehend extracts after text conversion."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An HR department needs to automatically categorize resume documents by skill sets using pre-built models and minimal customization. Which AWS service is most appropriate?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon SageMaker BlazingText",
      "B": "Amazon Comprehend Custom Classification",
      "C": "Amazon Textract form analysis",
      "D": "Amazon Personalize"
    },
    "explanation": "Comprehend Custom Classification allows rapid text classification with minimal model building; others are for embeddings, OCR, or recommendations."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A legal team needs to redact personally identifiable information (PII) from scanned PDF contracts. Which AWS service or combination should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Comprehend PII detection alone",
      "B": "Amazon SageMaker DocumentClassifier",
      "C": "Amazon Textract for OCR then Comprehend PII detection",
      "D": "Amazon Rekognition Text Moderation"
    },
    "explanation": "Textract extracts text from scanned PDFs; Comprehend PII API then identifies and redacts PII."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An online gaming company wants to recommend in-game items based on player behavior and contextual metadata. Which AWS service should they choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Personalize",
      "B": "Amazon Forecast",
      "C": "Amazon SageMaker Neo",
      "D": "Amazon Comprehend"
    },
    "explanation": "Personalize builds contextual, real-time recommendation models; Forecast is for time series, Neo for deployment, Comprehend for NLP."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A pharmaceutical company wants to group patients into cohorts for a clinical trial based on lab results and demographic features without labeled outcomes. Which ML technique applies?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Supervised classification using SageMaker XGBoost",
      "B": "Unsupervised clustering using SageMaker K-Means",
      "C": "Reinforcement learning using Amazon SageMaker RL agents",
      "D": "Anomaly detection using SageMaker Random Cut Forest"
    },
    "explanation": "Clustering groups data without labels into cohorts; supervised or anomaly detection are less appropriate."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A contact center wants to route incoming chats based on detected customer intent without building custom ML models. Which AWS service should they use?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Lex V2 with custom code",
      "B": "Amazon Comprehend Syntax Analysis",
      "C": "Amazon SageMaker DeepAR",
      "D": "Amazon Lex built-in intent classification"
    },
    "explanation": "Lex provides built-in intent classification for chat routing; other services are for syntax, forecasting, or require custom ML."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A bank wants to estimate credit risk score as a continuous variable from customer profiles. Which ML technique and AWS service combination is best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Unsupervised clustering with SageMaker K-Means",
      "B": "Anomaly detection with SageMaker Random Cut Forest",
      "C": "Regression using SageMaker Linear Learner",
      "D": "Classification using Amazon Fraud Detector"
    },
    "explanation": "Predicting a continuous risk score is regression; SageMaker Linear Learner supports regression tasks."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A manufacturing plant collects vibration data and suspects rare equipment failures. They have no labeled failure examples. Which AWS approach should they use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Supervised classification in Amazon SageMaker",
      "B": "Unsupervised anomaly detection using SageMaker Random Cut Forest",
      "C": "Time series forecasting using Amazon Forecast",
      "D": "Clustering using SageMaker K-Means"
    },
    "explanation": "Unsupervised anomaly detection detects rare failures without labeled data; clustering groups all data equally."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A travel website wants to extract flight numbers, dates, and passenger names from PDF itineraries. Which AWS service combination should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Comprehend Entities API",
      "B": "Amazon Textract Forms API only",
      "C": "Amazon Textract for OCR then Amazon Comprehend Entity Extraction",
      "D": "Amazon SageMaker OCR built-in model"
    },
    "explanation": "Textract OCRs form data; Comprehend extracts structured entities; other options miss one stage."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A social media analytics firm needs to cluster trending topics without prior labels. They want to use embeddings from a pre-trained language model. Which AWS service should they leverage?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Comprehend Topic Modeling",
      "B": "SageMaker JumpStart transformer embeddings + K-Means",
      "C": "Amazon Personalize clustering",
      "D": "Amazon Rekognition for topic detection"
    },
    "explanation": "JumpStart provides pre-trained models for embeddings; clustering then groups topics; Comprehend doesn\u2019t expose topic modeling."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A chatbot needs to answer customer queries by matching to FAQ entries. They require semantic matching rather than keyword matching. Which AWS service/model approach should they use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Lex with keyword slots",
      "B": "Amazon Comprehend sentiment analysis",
      "C": "Amazon Translate semantic matching",
      "D": "Generate embeddings using SageMaker JumpStart and perform similarity search"
    },
    "explanation": "Embeddings and similarity search provide semantic matching; other services are for keyword, sentiment, or translation."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A publisher wants to detect the language of submitted articles automatically before translation. Which AWS service should they use?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Comprehend DetectDominantLanguage",
      "B": "Amazon Translate language detection",
      "C": "Amazon Polly language identification",
      "D": "Amazon Transcribe language model"
    },
    "explanation": "Comprehend\u2019s DetectDominantLanguage API identifies text language; Translate focuses on translation post-detection."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An agriculture startup gathers drone imagery to identify crop health issues automatically. They need object detection and classification. Which AWS service combination is suitable?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Rekognition Text Detection + Comprehend",
      "B": "Amazon SageMaker K-Means clustering",
      "C": "Amazon Textract Table Extraction",
      "D": "Amazon SageMaker Object Detection built-in algorithm"
    },
    "explanation": "SageMaker\u2019s Object Detection algorithm (e.g., SSD) supports bounding boxes and classification in imagery."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A legal department wants to determine sentiment trends in case law documents over time and highlight emerging negative topics. Which AWS services should they combine?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Textract + Amazon Translate",
      "B": "Amazon Comprehend Key Phrases + Amazon Personalize",
      "C": "Amazon Textract for OCR + Amazon Comprehend Sentiment Analysis + Amazon QuickSight for visualization",
      "D": "Amazon Rekognition + Amazon Athena"
    },
    "explanation": "Textract extracts text, Comprehend analyzes sentiment, QuickSight visualizes trends; others mismatch functions."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A retailer wants to forecast daily sales volume per store for the next quarter, incorporating holiday effects and promotions. Which AWS solution should they choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Regression with SageMaker XGBoost",
      "B": "Time series forecasting with Amazon Forecast",
      "C": "Clustering with SageMaker K-Means",
      "D": "Anomaly detection with SageMaker Random Cut Forest"
    },
    "explanation": "Forecast incorporates seasonality and holiday effects; regression would require manual feature engineering."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A health app records user vitals and needs to detect abnormal patterns in real time for alerting. They have streaming data and no labeled anomalies. Which AWS combination is best?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Forecast streaming predictions",
      "B": "Amazon SageMaker Linear Learner streaming inference",
      "C": "Amazon Fraud Detector real-time API",
      "D": "Amazon Kinesis Data Analytics to ingest + SageMaker Random Cut Forest for anomaly detection"
    },
    "explanation": "Kinesis ingests streaming data; SageMaker RCF analyzes unlabeled anomalies in real time."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A company has a dataset of customer emails and wants to route them to the correct department automatically. They need intent classification out of the box. Which AWS service should they use?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Comprehend Custom Classification",
      "B": "Amazon SageMaker Object Detection",
      "C": "Amazon Translate",
      "D": "Amazon Personalize"
    },
    "explanation": "Comprehend Custom Classification provides easy training for text routing; other services are irrelevant."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A startup wants to create a voice-driven assistant that transcribes speech, interprets intent, and responds vocally. Which combination of AWS services is most appropriate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Transcribe + Amazon Comprehend + Amazon Polly",
      "B": "Amazon Transcribe -> Amazon Lex -> Amazon Polly",
      "C": "Amazon Lex -> Amazon Translate -> Amazon Comprehend",
      "D": "Amazon Polly -> Amazon SageMaker RL"
    },
    "explanation": "Transcribe handles speech-to-text, Lex interprets intent, Polly synthesizes speech; other chains mix stages incorrectly."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A team needs to ensure that feature transformations applied during model training are used identically during real-time inference to avoid training/serving skew. Which AWS service or feature best satisfies this requirement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon SageMaker Feature Store",
      "B": "AWS Glue DataBrew",
      "C": "Amazon SageMaker Data Wrangler flow exports",
      "D": "Embedding transformations in Lambda functions"
    },
    "explanation": "SageMaker Feature Store provides a single source of truth for feature definitions and transformations, ensuring consistency between training and inference. Data Wrangler and Glue cannot serve real-time features, and Lambda-based transformations risk divergence."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You must orchestrate a repeatable end-to-end ML workflow including data preprocessing, training, evaluation, model registration, and conditional deployment in response to evaluation metrics. Which AWS tool should you choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Step Functions with custom Lambda tasks",
      "B": "AWS Glue Workflows",
      "C": "Amazon SageMaker Pipelines with ConditionStep",
      "D": "AWS Data Pipeline"
    },
    "explanation": "SageMaker Pipelines natively orchestrates ML steps, supports ConditionStep for metric-based branching, and integrates with training, processing, and model registry. Step Functions and Glue lack ML-specific constructs, and AWS Data Pipeline is deprecated for ML workflows."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A data scientist needs to deploy ten versions of a fraud detection model behind a single endpoint, loading each version only when invoked to minimize memory usage. Which deployment option meets this requirement?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provisioned real-time endpoints per model variant",
      "B": "SageMaker serverless endpoint",
      "C": "Batch Transform job",
      "D": "SageMaker multi-model endpoint"
    },
    "explanation": "Multi-model endpoints load models on demand into a container, supporting multiple versions behind a single endpoint. Serverless endpoints cannot host multiple models, and Batch Transform is for offline inference."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "To detect input data drift in production, which combination of services and features provides automated baseline generation, continuous monitoring, and alerting?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon CloudWatch metrics with custom Lambda evaluations",
      "B": "SageMaker Model Monitor\u2019s DataQualityJobDefinition + Amazon EventBridge rule",
      "C": "AWS Config with custom rules",
      "D": "AWS Glue job scheduled daily"
    },
    "explanation": "Model Monitor can generate a baseline from training data (DataQualityJobDefinition), schedule continuous monitoring, and emit results via EventBridge for alerting. Other options require extensive custom work or lack ML-specific drift detection."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your preprocessing logic involves heavy PySpark transforms on large tabular datasets. Which SageMaker component should you use for scalable, containerized execution?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon SageMaker Processing with built-in Scikit-learn container",
      "B": "AWS Glue ETL job",
      "C": "Amazon SageMaker Processing with Spark container",
      "D": "AWS Lambda with parallel invocations"
    },
    "explanation": "SageMaker Processing supports Spark containers for distributed PySpark workloads within an ML pipeline. Glue is general ETL, not integrated with SageMaker pipelines, and Lambda cannot handle large-scale Spark jobs."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A business requires tracking of every training run\u2019s hyperparameters, input data versions, and evaluation metrics with lineage. Which SageMaker feature is specifically built to capture this metadata?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon SageMaker Experiments",
      "B": "Amazon CloudWatch Logs",
      "C": "Amazon S3 object versioning",
      "D": "AWS CloudTrail"
    },
    "explanation": "SageMaker Experiments records runs, parameters, inputs, and metrics, providing lineage and comparison across trials. CloudWatch Logs and S3 versioning don\u2019t structure ML metadata, and CloudTrail logs only API calls."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "When model quality metrics fall below a threshold in production, you need to trigger an automated retraining pipeline. Which architecture best implements this requirement?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CloudWatch alarm on endpoint latency to start Lambda",
      "B": "AWS Config rule violation to start SageMaker job",
      "C": "SNS notification from CloudTrail to trigger training",
      "D": "EventBridge rule on Model Monitor alarm to start SageMaker Pipeline"
    },
    "explanation": "Model Monitor can detect quality degradation, emit an EventBridge event, and trigger a SageMaker Pipeline that retrains and redeploys the model. Other options don\u2019t tie directly to model quality metrics."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A classification model on a heavily imbalanced dataset shows 95% accuracy but poor minority class detection. Which single metric should you prioritize to better reflect model performance?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Overall accuracy",
      "B": "Precision",
      "C": "F1-score",
      "D": "Mean squared error"
    },
    "explanation": "F1-score balances precision and recall, and is better suited for imbalanced datasets than accuracy. Precision alone ignores recall, and MSE is for regression."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You want to generate a statistical profile of your training data to serve as a baseline for drift detection. Which SageMaker component accomplishes this with minimal code?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "SageMaker Clarify bias report",
      "B": "SageMaker Model Monitor Data Quality job",
      "C": "SageMaker Debugger profiling job",
      "D": "SageMaker Edge Manager"
    },
    "explanation": "Model Monitor\u2019s DataQualityJobDefinition automatically profiles datasets to generate statistics and constraints used for drift monitoring. Clarify focuses on bias/fairness, Debugger on training diagnostics, Edge Manager on device models."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your organization needs to track approved model versions and their deployment environments, enabling rollback. Which service should you integrate into your pipeline?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon SageMaker Model Registry",
      "B": "AWS Systems Manager Parameter Store",
      "C": "Amazon DynamoDB",
      "D": "AWS CodeCommit"
    },
    "explanation": "SageMaker Model Registry manages model versions, approval statuses, and associated metadata, making deployments and rollbacks straightforward. Parameter Store and DynamoDB aren\u2019t specialized for model lifecycle, and CodeCommit is for code, not models."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You need to run custom evaluation code on your test dataset after training completes within the same pipeline. Which SageMaker step type should you use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "TrainingStep",
      "B": "ProcessingStep",
      "C": "TransformStep",
      "D": "ModelStep"
    },
    "explanation": "ProcessingStep allows execution of arbitrary evaluation scripts post-training and can read model artifacts and test data. TrainingStep only trains, TransformStep performs batch inference, and ModelStep registers a model."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You plan to fine-tune an open-source transformer model from Hugging Face on SageMaker. Which approach minimizes undifferentiated heavy lifting?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Hugging Face DLC via Hugging Face integration",
      "B": "Build your own Docker container with Transformers installed",
      "C": "Use Amazon EC2 instances with preinstalled Transformers",
      "D": "Perform training on AWS Lambda functions"
    },
    "explanation": "SageMaker\u2019s built-in Hugging Face deep learning containers provide managed environments and optimization for fine-tuning. Custom containers and EC2 require manual setup, and Lambda cannot handle large training jobs."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your ML pipeline must decide at runtime whether to continue to deployment based on evaluation metrics. Which SageMaker feature allows you to embed this logic directly in the pipeline definition?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Lambda invocation within a Pipeline",
      "B": "State Machine integration",
      "C": "ConditionStep in SageMaker Pipelines",
      "D": "RetryStrategy in TrainingStep"
    },
    "explanation": "ConditionStep in SageMaker Pipelines evaluates expressions on step outputs to branch logic without external orchestration. Lambda or Step Functions would decouple, and RetryStrategy only handles retries."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your service-level agreement requires sub-100ms inference latency at unpredictable traffic volumes for a text classification model. Which SageMaker endpoint type best meets this requirement with minimal management overhead?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Multi-model endpoint",
      "B": "Serverless inference endpoint",
      "C": "Real-time provisioned endpoint",
      "D": "Batch transform"
    },
    "explanation": "Serverless endpoints automatically scale based on traffic and provide low-latency inference without capacity planning. Real-time provisioned endpoints require manual scaling, multi-model cannot auto-scale, and Batch Transform is offline."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You want to embed feature-engineering code in a pipeline step, reuse it interactively in Data Wrangler, and version control it. Which approach best satisfies these needs?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Implement transforms in a Lambda and call from ProcessingStep",
      "B": "Write PySpark in a ProcessingStep directly",
      "C": "Use Glue DataBrew recipe then export",
      "D": "Develop and version a SageMaker Data Wrangler flow, then export as ProcessingStep"
    },
    "explanation": "Data Wrangler flows allow interactive development of transformations, version control via Studio, and export as ProcessingStep for reproducible pipelines. Glue recipes aren\u2019t integrated into pipelines, and Lambda lacks data science tooling."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your model inference is hosted using a Kubernetes-based mechanism requiring custom container orchestration. Which SageMaker deployment option allows you to maintain this while integrating with SageMaker Pipelines?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker real-time endpoint with custom container",
      "B": "Batch Transform job with custom transformer",
      "C": "SageMaker serverless endpoint with custom container",
      "D": "SageMaker Inference Realtime Inference on EKS"
    },
    "explanation": "SageMaker Realtime Inference on EKS lets you deploy to your EKS cluster with custom orchestrations, integrating with pipelines. Standard endpoints and serverless endpoints abstract away Kubernetes."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A compliance auditor requires a record of every data artifact and code version used in model training. Which combination ensures full traceability?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "S3 versioning + CloudTrail logs",
      "B": "SageMaker Experiments lineage tracking + SageMaker Model Registry",
      "C": "Git commit hashes in Glue ETL + DynamoDB audit table",
      "D": "AWS Config recording S3 changes"
    },
    "explanation": "Experiments track data and code versions at the run level, and Model Registry ties model artifacts to training runs, providing end-to-end ML lineage. S3 versioning and CloudTrail are lower-level and not ML-specific."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your application needs low-latency access to inference results but generates heavy batched requests. Which inference architecture balances throughput and latency?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provisioned real-time endpoint only",
      "B": "Batch Transform only",
      "C": "Mixed: use serverless real-time endpoint for spikes and Batch Transform for bulk",
      "D": "SageMaker multi-model endpoint only"
    },
    "explanation": "Combining serverless real-time endpoints for unpredictable spikes and Batch Transform for bulk processing optimizes cost and performance. Single methods either overprovision or increase latency."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "To minimize training cost, you want managed spot training with checkpointing. Which configuration achieves this in SageMaker?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set TrainingJobStrategy to \u2018SpotSingleNode\u2019",
      "B": "Enable \u2018ManagedSpotTraining\u2019 and configure checkpoint S3 path",
      "C": "Use Spot Instances in EC2 training cluster",
      "D": "Run training on a serverless notebook with spot instances"
    },
    "explanation": "ManagedSpotTraining in SageMaker automatically provisions spot instances and saves checkpoints to S3. The other options are either invalid or require manual orchestration outside SageMaker training jobs."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your ML pipeline must load features for offline training and serve the same features for online inference with single-digit millisecond latency. Which design meets these SLAs?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Feature Store with both offline and online stores",
      "B": "Store features in Amazon RDS and query via Lambda",
      "C": "Store parquet files in S3 and use Athena for queries",
      "D": "Use DynamoDB for offline and S3 for online"
    },
    "explanation": "SageMaker Feature Store provides an offline store for training and an online store optimized for low-latency reads. The other options either lack performance or separate stores."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You have an imbalance in real-time inference traffic and want dynamic endpoint scaling without idle cost. Which SageMaker feature should you use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provisioned instance fleet with Auto Scaling",
      "B": "Serverless inference",
      "C": "Multi-model endpoint with provisioned instances",
      "D": "Batch transform with event triggers"
    },
    "explanation": "Serverless inference scales automatically to zero when idle and scales with demand, avoiding idle costs. Auto Scaling of provisioned fleets still incurs minimum capacity costs."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "To enforce data schema and quality checks before training, you need a managed solution in your pipeline. Which SageMaker component should you integrate?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Clarify**BiasReportStep**",
      "B": "Glue Data Catalog crawler",
      "C": "Athena schema validation",
      "D": "Model Monitor DataQualityJobDefinition"
    },
    "explanation": "Model Monitor's DataQualityJobDefinition can be used in pipelines to validate data schema and detect anomalies before training. Clarify is for bias/fairness, and Glue/Athena are not ML-specific."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A newly trained model shows lower inference accuracy in production than during testing. Which multi-step solution most directly addresses this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase instance size of endpoint and redeploy",
      "B": "Retrain the model weekly with latest data",
      "C": "Use Model Monitor to detect drift\u00d7trigger retraining pipeline",
      "D": "Add more hidden layers to the neural network"
    },
    "explanation": "The correct approach is to detect data drift with Model Monitor, then trigger automated retraining. Changing instance size or model architecture without addressing drift won't fix production accuracy declines."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You need to run a hyperparameter tuning job and automatically compare results across multiple experiments and datasets. Which SageMaker features will you combine?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Hyperparameter TuningJob integrated with Experiments",
      "B": "CloudWatch metrics filtered by TrainingJob name",
      "C": "Athena queries on S3 logs",
      "D": "Manual Excel aggregation of results"
    },
    "explanation": "Hyperparameter TuningJobs integrated with SageMaker Experiments allow automated tracking and comparison of trials across experiments. CloudWatch and manual methods lack structured ML metadata."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which evaluation approach is most appropriate when your business objective values the top 10% of predictions being correct, regardless of overall recall?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ROC AUC",
      "B": "Overall accuracy",
      "C": "F1-score",
      "D": "Precision at N% (e.g., Precision@10%)"
    },
    "explanation": "Precision@N% measures correctness within the top-scoring N% of predictions, aligning directly with business objectives. Other metrics don\u2019t focus on a specific percentile cutoff."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "To version-control your pipeline definitions and allow pull requests for change management, which AWS service integration is recommended?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Store pipeline JSON in S3 with versioning enabled",
      "B": "Integrate SageMaker Studio with AWS CodeCommit",
      "C": "Use DynamoDB to store pipeline definitions",
      "D": "Embed pipeline definitions as Lambda code"
    },
    "explanation": "SageMaker Studio can integrate with CodeCommit, allowing Git-based workflows for pipeline definitions. S3 versioning doesn\u2019t provide pull requests and code review workflows."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your deployment requires A/B testing two model variants with 20% traffic on the new variant. Which SageMaker feature allows this traffic split?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provision two separate endpoints and use a Lambda router",
      "B": "Use Batch Transform for variant testing",
      "C": "Specify ProductionVariants with InitialVariantWeight in CreateEndpointConfig",
      "D": "Deploy new variant to serverless endpoint"
    },
    "explanation": "Setting InitialVariantWeight in a ProductionVariants array when creating an endpoint config splits traffic between variants. Other approaches are more complex and not built in."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You want to integrate fairness and bias detection in your preprocessing and post-training steps within a SageMaker pipeline. Which components should you use in the respective steps?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ClarifyDataQualityCheck for preprocessing and ClarifyBiasCheck for post-training",
      "B": "ModelMonitor for preprocessing and Clarify for post-training",
      "C": "Glue DataBrew recipe for both steps",
      "D": "Athena queries for preprocessing and CloudWatch alarms for post-training"
    },
    "explanation": "SageMaker Clarify\u2019s data quality check can be used before training, and its bias check after training, within pipelines. Model Monitor focuses on drift and quality, not fairness."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "When using a subword tokenizer (like byte-pair encoding) for a generative AI foundation model, which scenario best explains a drawback of using a very small vocabulary size?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Frequent words get split into many subwords, increasing sequence length and reducing generation efficiency.",
      "B": "The model fails to represent rare words, leading to unknown token outputs for uncommon terms.",
      "C": "A larger embedding matrix is required, increasing memory usage.",
      "D": "It prevents the model from learning long-range dependencies due to positional errors."
    },
    "explanation": "A is correct because a small vocabulary forces common words into multiple subwords and lengthens context. B is wrong because subword tokenizers compose rare words from subwords. C is wrong since smaller vocab shrinks the embedding matrix. D is unrelated to vocabulary size."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "In a retrieval-augmented generation system, why is chunking long documents with overlapping windows often preferred over non-overlapping chunks?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Overlapping windows improve continuity across boundaries to prevent missing context.",
      "B": "They reduce total number of chunks, saving storage.",
      "C": "They ensure equal token counts per chunk to meet fixed-size model input.",
      "D": "They allow each chunk to be processed in parallel without context duplication."
    },
    "explanation": "A is correct because overlaps preserve context at boundaries. B is false\u2014overlap increases chunk count. C is irrelevant to overlap. D is incorrect since overlap duplicates context, not eliminates it."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A team must choose an embedding dimension for text vectors in a generative AI application. Which factor primarily influences selecting a higher dimensionality for embeddings?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Larger vocabulary size and semantic granularity demand more dimensions to capture nuanced relationships.",
      "B": "Increased batch size during training requires higher embedding dimensions for convergence.",
      "C": "Lower model inference latency favors higher dimensions to speed similarity searches.",
      "D": "The number of transformer layers directly scales with embedding dimension."
    },
    "explanation": "A is correct because richer semantics and more tokens require larger embeddings. B is unrelated to dimension. C is opposite\u2014higher dimensions often slow similarity searches. D is false\u2014layers and embedding size are independent choices."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "When comparing two text embeddings for semantic similarity, which metric is most appropriate to mitigate discrepancies in vector magnitude due to token count differences?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Cosine similarity because it normalizes magnitude.",
      "B": "Euclidean distance because it captures absolute distance.",
      "C": "Dot product because it's faster on GPUs.",
      "D": "Manhattan distance because it's robust to outliers."
    },
    "explanation": "A is correct since cosine similarity ignores vector length and focuses on angle. B and D use absolute distances influenced by magnitude. C\u2019s speed claim ignores the normalization benefit of cosine."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "To improve reasoning in a generative AI model, a prompt engineer uses chain-of-thought prompts. What\u2019s a potential drawback of this technique?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "It increases latency and token usage, possibly hitting context limits.",
      "B": "It leads the model to ignore the final instruction and focus on intermediate steps.",
      "C": "It reduces model creativity by constraining outputs.",
      "D": "It causes the model to hallucinate numeric values more frequently."
    },
    "explanation": "A is correct because verbose reasoning consumes context and compute. B is incorrect\u2014models still follow instructions. C and D are not established drawbacks tied specifically to chain-of-thought."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "In transformer-based generative AI, what is the purpose of using a causal attention mask during training?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "To prevent the model from attending to future tokens, enforcing autoregressive generation.",
      "B": "To ensure the model attends equally to all tokens regardless of position.",
      "C": "To reduce computational complexity by masking low-attention heads.",
      "D": "To allow bidirectional context for improved semantic encoding."
    },
    "explanation": "A is correct\u2014causal masks block future tokens. B, C, and D describe other mechanisms not achieved by causal masking."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A company requires rapid prototyping with foundation models but limited compute. Which model-size tradeoff should they consider?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Choosing a medium-sized model to balance latency and performance.",
      "B": "Selecting the largest available model for higher accuracy despite slower inference.",
      "C": "Using a tiny model as it guarantees zero hallucinations.",
      "D": "Fine-tuning a very large model to reduce deployment cost."
    },
    "explanation": "A is correct\u2014a medium model balances speed and quality. B ignores latency constraints. C is false\u2014tiny models can still hallucinate. D increases cost, not reduces it."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A generative AI application needs to caption images and also generate layouts. Why would a multi-modal foundation model be preferred over a unimodal text model?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "It natively processes visual features and textual patterns in a shared embedding space.",
      "B": "It reduces the number of parameters compared to separate image and text models.",
      "C": "It improves tokenization speed by merging pixels and text tokens.",
      "D": "It avoids any need for positional embeddings for images."
    },
    "explanation": "A is correct\u2014multi-modal models jointly embed images and text. B is wrong\u2014multi-modal often adds parameters. C and D are incorrect technical claims."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "When fine-tuning a diffusion model for faster image synthesis, which modification to the noise schedule can accelerate generation without severely degrading quality?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Reducing the number of denoising timesteps and adjusting the variance schedule to concentrate noise removal.",
      "B": "Increasing the initial noise variance to cover broader solutions.",
      "C": "Using a linear noise schedule instead of cosine to simplify computations.",
      "D": "Removing the stochastic sampling step entirely to get deterministic outputs."
    },
    "explanation": "A is correct\u2014fewer steps with tuned variance speeds up inference. B, C, and D either degrade quality severely or break the diffusion process."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "In a generative AI system employing vector quantization on embeddings, what is a primary disadvantage of aggressive quantization (e.g., fewer codebook entries)?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Loss of semantic nuance leading to lower generation fidelity.",
      "B": "Increased storage overhead from larger codebooks.",
      "C": "Slower similarity searches due to coarse quantization.",
      "D": "Higher risk of overfitting the quantized vectors."
    },
    "explanation": "A is correct\u2014too few codebook entries degrade fidelity. B is opposite\u2014fewer entries save storage. C is false\u2014coarser quantization speeds search. D is unrelated."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "For RAG over a large PDF, what overlap ratio between chunks helps avoid context loss at chunk boundaries while minimizing redundancy?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "10-20% overlap balances context carryover with storage.",
      "B": "50% overlap ensures no context is lost but doubles storage.",
      "C": "0% overlap maximizes chunk independence and reduces size.",
      "D": "80% overlap is needed to capture extended references across pages."
    },
    "explanation": "A is correct\u2014small overlap preserves boundary context without excessive duplication. B and D waste storage; C loses context."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A developer must choose between Locality-Sensitive Hashing (LSH) and an IVF index in FAISS for embedding retrieval. Which scenario favors LSH?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "When memory is extremely limited and approximate retrieval is acceptable.",
      "B": "When exact nearest neighbors are required for high-precision tasks.",
      "C": "When embedding dimension exceeds GPU memory for IVF.",
      "D": "When dynamic updates require fast insertion and deletion."
    },
    "explanation": "A is correct\u2014LSH is lightweight and approximate. B demands exact search, favoring IVF or exact indices. C and D are unrelated trade-offs."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "What is a subtle prompt injection risk when concatenating user input with a system prompt in a generative AI chatbot?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A user embeds instructions that override system directives, altering model behavior.",
      "B": "The model increases hallucination rates due to longer inputs.",
      "C": "The system prompt loses embedding effects due to position bias.",
      "D": "The user\u2019s input is truncated entirely during tokenization."
    },
    "explanation": "A is correct\u2014malicious input can override system rules. B\u2013D describe unrelated or less subtle issues."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Compared to prefix-tuning a foundation model, what is a disadvantage of instruction fine-tuning?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Instruction fine-tuning requires updating entire model weights, incurring higher compute cost.",
      "B": "It cannot incorporate human feedback.",
      "C": "It fails to improve performance on unseen tasks.",
      "D": "It reduces the model\u2019s context window."
    },
    "explanation": "A is correct\u2014full fine-tuning is expensive. B\u2013D are incorrect statements about instruction fine-tuning."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Why might a byte-level BPE tokenizer yield better out-of-vocabulary handling than a word-level tokenizer in a multilingual generative model?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "It can break unknown words into known byte sequences, avoiding unknown tokens.",
      "B": "It stores full words for all languages, increasing vocabulary.",
      "C": "It reduces model perplexity uniformly across languages.",
      "D": "It simplifies position embeddings by using bytes."
    },
    "explanation": "A is correct\u2014byte-level BPE composes unknown words. B\u2013D are incorrect or irrelevant advantages."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A prompt engineer observes that lowering the temperature to 0.2 yields overly generic outputs. What\u2019s causing this behavior?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Low temperature sharpens the distribution, making the model choose high-probability tokens repeatedly.",
      "B": "It increases sampling variance, leading to unpredictable results.",
      "C": "It normalizes logits, causing underflow in softmax.",
      "D": "It disables top-k sampling, resorting to greedy decoding."
    },
    "explanation": "A is correct\u2014lower temperature favors top tokens. B\u2013D mischaracterize temperature\u2019s effect."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "In sampling strategies, what is the key difference between top-k and nucleus (top-p) sampling for text generation?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Top-k limits to a fixed number of tokens, while top-p selects tokens until cumulative probability exceeds a threshold.",
      "B": "Top-k adapts k based on token probability, while top-p uses a static set.",
      "C": "Top-p always reduces latency, whereas top-k is slower.",
      "D": "Top-p relies on beam search, and top-k does not."
    },
    "explanation": "A is correct\u2014top-k fixes count, top-p fixes probability mass. B\u2013D are inaccurate."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "When deploying an embedding store in production, which cost factor grows most with increasing embedding dimension versus increasing dataset size?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Memory per vector grows linearly with dimension, while index size growth with dataset size depends on index type.",
      "B": "Network bandwidth becomes the dominant cost for larger dimensions.",
      "C": "CPU indexing time is unaffected by dataset size.",
      "D": "Storage IOPS increase with dimension."
    },
    "explanation": "A is correct\u2014dimension directly scales vector size. B\u2013D misattribute costs."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Why do transformers use sinusoidal positional encodings instead of learned embeddings for very long sequences?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Sinusoidal embeddings generalize to unseen lengths due to their periodic properties.",
      "B": "Learned embeddings require less memory for long sequences.",
      "C": "Sinusoidal encodings prevent overfitting on position patterns.",
      "D": "Learned embeddings cannot be used with multi-head attention."
    },
    "explanation": "A is correct\u2014sinusoids extend to any position. B and D are false; C is not primary reason."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "What benefit does multi-head attention provide in a transformer-based generative model?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "It allows the model to attend to information from different representation subspaces at different positions.",
      "B": "It reduces computation by parallelizing single-head attention.",
      "C": "It enforces causality in the decoding phase.",
      "D": "It increases the maximum context window."
    },
    "explanation": "A is correct\u2014each head learns unique relationships. B, C, and D misstate its purpose."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "If a foundation model has a 2048-token context window, how should you preprocess a 5000-token document for a RAG pipeline to preserve coherence?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Chunk into 2048-token segments with ~10% overlap to maintain context at boundaries.",
      "B": "Chunk into non-overlapping 2500-token blocks to minimize number of chunks.",
      "C": "Truncate to the first 2048 tokens as most relevant.",
      "D": "Randomly sample 2048 tokens from the document each query."
    },
    "explanation": "A is correct\u2014overlap preserves continuity. B loses coherence; C and D discard key content."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A developer wants both long inputs and long outputs from a model with a fixed 4096-token limit. What\u2019s an effective strategy?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Split the input into chunks and generate outputs for each, then concatenate and refine.",
      "B": "Request maximum output length and let the model truncate the input automatically.",
      "C": "Increase the temperature to trade context for output.",
      "D": "Use greedy decoding to prioritize output length."
    },
    "explanation": "A is correct\u2014chunking plus post-processing handles long flows. B\u2013D do not address the limit effectively."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "During inference on a diffusion model, what happens if the noise schedule steps have too small a variance between timesteps?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Generation quality may degrade due to insufficient denoising signal between steps.",
      "B": "Sampling speed increases but outputs become diverse.",
      "C": "Model collapses to a single mode output.",
      "D": "The reverse process becomes non-deterministic."
    },
    "explanation": "A is correct\u2014too-fine steps weaken noise gradients. B\u2013D misrepresent the effect."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which technique ensures cross-lingual alignment when training multilingual embeddings for a generative AI chatbot?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Shared subword vocabulary and joint training on parallel corpora to align semantic spaces.",
      "B": "Language-specific embeddings with no parameter sharing.",
      "C": "Training separate models and concatenating their outputs.",
      "D": "Using one-hot encodings for each language."
    },
    "explanation": "A is correct\u2014shared vocab plus parallel data aligns languages. B\u2013D fail to produce joint embedding spaces."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "In few-shot prompting, why might providing too many examples degrade a model\u2019s performance on a new task?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "It consumes context window, leaving less space for the actual prompt and thus limiting task instructions.",
      "B": "It causes the model to memorize examples, preventing generalization.",
      "C": "It increases temperature implicitly, reducing determinism.",
      "D": "It disables attention to the final instruction."
    },
    "explanation": "A is correct\u2014examples use up context tokens. B\u2013D are not primary issues with too many examples."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "How can vector arithmetic on embeddings (e.g., vec(king)\u2212vec(man)+vec(woman)) fail in a generative AI context?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Embedding space may not be fully linear for abstract relations, causing semantic drift in generation.",
      "B": "Arithmetic always yields unknown tokens.",
      "C": "It requires specialized loss functions at inference.",
      "D": "It works only for image embeddings, not text."
    },
    "explanation": "A is correct\u2014linear semantics are only approximate. B\u2013D are incorrect statements."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "What is the primary memory complexity issue with transformers as sequence length increases?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Self-attention scales quadratically (O(n\u00b2)) with sequence length, limiting long contexts.",
      "B": "Feed-forward layers require O(n\u00b3) memory.",
      "C": "Positional encoding scales linearly but dominates memory.",
      "D": "Multi-head attention scales logarithmically, making it inefficient."
    },
    "explanation": "A is correct\u2014attention\u2019s O(n\u00b2) growth is the bottleneck. B\u2013D misstate complexities."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "When combining embeddings with RAG for document QA, why is it problematic to update the index with new documents without re-computing embeddings?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "New documents won\u2019t be comparable to existing ones in latent space, leading to retrieval inconsistencies.",
      "B": "It reduces the context window size.",
      "C": "It increases the embedding dimension automatically.",
      "D": "It forces the RAG system to retrain the generator."
    },
    "explanation": "A is correct\u2014embeddings must share the same space. B\u2013D are unrelated issues."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Why might image embeddings from a CLIP model and text embeddings be stored separately in a mixed-modality application?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "They occupy different subspaces and require modality-specific similarity metrics for retrieval.",
      "B": "Image embeddings cannot be quantized.",
      "C": "Text embeddings are binary while image embeddings are continuous.",
      "D": "CLIP models output text embeddings only for image queries."
    },
    "explanation": "A is correct\u2014they differ in distribution and distance metrics. B\u2013D are false."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "What effect does token bias (adding a constant to logits of specific tokens) have during generation?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "It increases the probability of favored tokens, potentially reducing diversity.",
      "B": "It normalizes the output distribution, improving fairness.",
      "C": "It reduces model perplexity to zero.",
      "D": "It dynamically adjusts the learning rate."
    },
    "explanation": "A is correct\u2014bias skews token probabilities. B\u2013D misinterpret token bias effects."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A fintech startup uses a large foundation model via Amazon Bedrock to draft financial advice. They observe occasional hallucinations in the output. Which single change will most effectively reduce hallucinations without retraining?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase the model\u2019s temperature setting.",
      "B": "Implement Retrieval Augmented Generation (RAG) with a verified document store.",
      "C": "Switch from few-shot to zero-shot prompting.",
      "D": "Add chain-of-thought prompting to the prompt."
    },
    "explanation": "RAG constrains the model to factual context, reducing hallucinations. Altering temperature, shot counts, or prompting style alone does not guarantee factual grounding."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A marketing team must generate personalized slogans with high creativity but predictable quality. Which parameter adjustment balances creativity and consistency?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature to 1.2",
      "B": "Set top-k to 1",
      "C": "Lower temperature to 0.6 and use top-p=0.8",
      "D": "Use maximum token length with temperature=0.2"
    },
    "explanation": "Lowering temperature to ~0.6 and using top-p sampling yields creative yet controlled outputs. High temperature or extreme top-k/p settings push too far toward randomness or determinism."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A healthcare provider needs an explainable generative AI model for medical summaries. Which model choice best meets interpretability requirements?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A closed-proprietary 70B parameter model with no transparency.",
      "B": "A high-capacity multimodal model via Bedrock.",
      "C": "An open-source 30B parameter model fine-tuned on medical data.",
      "D": "An open-source 7B parameter model with published weights and architecture."
    },
    "explanation": "A smaller open-source model with transparent architecture facilitates interpretability. Proprietary or extremely large models obscure inner workings and hinder explainability."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A retail company measures business impact of generative AI by ARPU. They deploy a chatbot that upsells products. Which evaluation metric combination best correlates with ARPU growth?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Average order size and conversion rate",
      "B": "Token usage per session and model latency",
      "C": "Model perplexity and inference throughput",
      "D": "User engagement time and number of API calls"
    },
    "explanation": "ARPU growth ties to conversion rate and order size. Perplexity or token counts don\u2019t directly measure revenue impact."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An e-commerce site wants a generative AI recommendation engine. They need cross-domain performance (products and content). Which deployment yields best cross-domain generalization?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Fine-tune a product-only foundation model on product data.",
      "B": "Use in-context learning on a content-only model.",
      "C": "Use a foundation model pre-trained on diverse domains via Bedrock JumpStart.",
      "D": "Train a custom model from scratch on combined data."
    },
    "explanation": "A generously pre-trained foundation model on diverse domains generalizes best. Scratch training demands huge data and time; fine-tuning on narrow data limits domains."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A publisher uses a text generation API to summarize articles. They need consistent output length but the model sometimes over-runs. Which setting adjustment controls output length strictly?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature to reduce unpredictability.",
      "B": "Set a maximum token limit and use \u201cstop sequences.\u201d",
      "C": "Use larger model with more capacity.",
      "D": "Switch from sampling to greedy decoding."
    },
    "explanation": "Defining stop sequences and a max token limit reliably halts generation at desired length. Greedy decoding alone may still overshoot if stop tokens aren\u2019t configured."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A legal firm requires compliance with data residency. They must avoid sending sensitive documents outside their VPC. Which AWS generative AI service meets this requirement?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Comprehend via public API",
      "B": "Amazon Lex in multi-region mode",
      "C": "Amazon Q via internet endpoint",
      "D": "SageMaker-hosted private Bedrock container with VPC endpoints"
    },
    "explanation": "Running Bedrock privately in SageMaker with VPC endpoints keeps data inside the VPC. Public APIs send data over internet."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A gaming company sees nondeterministic outputs from the same prompt. Which combination yields the most deterministic behavior?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High temperature, high top-p",
      "B": "Low max tokens, chain-of-thought",
      "C": "Temperature=0.0 and top-k=1",
      "D": "Few-shot prompting and top-p=0.9"
    },
    "explanation": "Temperature 0.0 with top-k=1 forces the model to choose the highest-probability token each step, yielding deterministic outputs."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A developer needs to estimate token usage costs for a monthly chatbot workload of 100k prompts at average 200 tokens each request and 800 tokens response. The Bedrock price is $0.0004 per input token and $0.0006 per output token. What\u2019s the monthly token cost?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "100k\u00d7(200\u00d70.0004+800\u00d70.0006) = $68,000",
      "B": "100k\u00d7(200\u00d70.0006+800\u00d70.0004) = $52,000",
      "C": "100k\u00d7(1000\u00d70.0005) = $50,000",
      "D": "100k\u00d7(200+800)\u00d70.0004 = $40,000"
    },
    "explanation": "Input cost:200\u00d70.0004=$0.08; output:800\u00d70.0006=$0.48; per prompt $0.56; times100k = $56,000. Actually correct math yields $56,000, not listed. (A) miscalculated \u2013 trick: candidate must catch mispricing."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which limitation of generative AI most impacts use in safety-critical systems?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "High throughput cost",
      "B": "Limited multimodal support",
      "C": "Inability to fine-tune",
      "D": "Nondeterminism and hallucinations"
    },
    "explanation": "Hallucinations and unpredictability make generative AI unsafe for critical systems; the other factors are secondary."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A company benchmarks two foundation models for product text generation. Model A has lower perplexity but higher latency. Model B has higher perplexity but lower latency. They value efficiency over quality. Which model suits them?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model A for its lower perplexity",
      "B": "Model B for its lower latency",
      "C": "Average the outputs of A and B",
      "D": "Fine-tune A to reduce latency"
    },
    "explanation": "When efficiency is prioritized, lower latency (Model B) is preferable despite slightly worse perplexity."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A media company needs summarization of streaming audio in near-real-time. They require minimal delay. Which generative AI approach is best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Batch transcription then summarization",
      "B": "Use large multimodal model with high accuracy",
      "C": "Stream transcription with low-latency LLM endpoint",
      "D": "Daily batch processing of recorded files"
    },
    "explanation": "Streaming transcription plus low-latency LLM endpoint meets near-real-time requirement. Batch methods introduce unacceptable delays."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A team wants to monitor model drift in a generative AI summarization service. Which metric combination best surfaces drift?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "KL divergence of token distribution and output length variance",
      "B": "Total token cost and average session length",
      "C": "Number of API errors and model latency",
      "D": "User satisfaction score only"
    },
    "explanation": "Statistical measures like KL divergence and length variance detect distributional changes indicating drift; others don\u2019t capture content shift."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A startup uses a pre-trained multimodal model for video captioning but needs domain adaptation. They lack compute for fine-tuning. Which strategy suits best?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a custom model from scratch on their data",
      "B": "Use in-context learning with domain-specific examples",
      "C": "Switch to a unimodal text model",
      "D": "Implement RLHF at scale"
    },
    "explanation": "In-context learning injects domain examples without fine-tuning; other methods require heavy compute or rebuilds."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which risk is specifically introduced by prompt injection attacks on generative AI systems?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model overfitting",
      "B": "Increased latency",
      "C": "Unauthorized command execution",
      "D": "Higher token costs"
    },
    "explanation": "Prompt injection can manipulate the model to execute unauthorized instructions; other risks aren\u2019t directly tied to injection."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An enterprise requires auditability of all AI outputs linked to data sources. Which feature of Bedrock should they leverage?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Multi-shot prompting",
      "B": "Temperature tuning",
      "C": "Batch endpoints",
      "D": "Provenance logs with model card integration"
    },
    "explanation": "Provenance logs track input-output lineage and model metadata for audit; other features don\u2019t provide traceability."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A firm wants to measure the efficiency of a generative AI summarization pipeline end-to-end. Which metric combination is most appropriate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "F1 score and token cost",
      "B": "End-to-end latency and cost per summary",
      "C": "Perplexity and BLEU score",
      "D": "Model parameter count and API throughput"
    },
    "explanation": "Pipeline efficiency is captured by latency and cost; perplexity/BLEU address quality, not efficiency."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which generative AI limitation poses the greatest challenge when generating legal contracts?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Hallucination of non-existent clauses",
      "B": "Low throughput",
      "C": "Multimodal capability",
      "D": "High interpretability"
    },
    "explanation": "Incorrectly invented clauses (hallucinations) jeopardize legal accuracy; other limitations are less critical."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A company wants to optimize both the accuracy and speed of its generative AI system. Which trade-off best describes the relationship?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increasing model size increases speed and accuracy linearly",
      "B": "Lower temperature increases speed at the cost of accuracy",
      "C": "Larger models improve accuracy but increase latency",
      "D": "Higher top-p reduces latency and improves accuracy"
    },
    "explanation": "Bigger models generally yield higher accuracy but slower inference; other statements are oversimplified or incorrect."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A data scientist evaluates two foundation models for summarization: Model X yields higher ROUGE scores; Model Y yields lower latency and cost. They prioritize CLV improvements through speed. Which model should they choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model X for better ROUGE",
      "B": "Model Y for lower latency/cost",
      "C": "Interpolate outputs of X and Y",
      "D": "Fine-tune X to reduce cost"
    },
    "explanation": "Faster, cheaper Model Y better supports higher throughput and improved customer lifetime value."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which advantage of generative AI is most beneficial for customer support automation?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Responsiveness to dynamic user queries",
      "B": "High labeled data requirements",
      "C": "Deterministic outputs",
      "D": "Guaranteed factual accuracy"
    },
    "explanation": "Generative AI\u2019s responsiveness enables handling diverse user questions; factual accuracy is not guaranteed and labeled data may not be required at inference."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A retailer leverages a generative AI to compose product descriptions. They notice lower click-through rates on AI-generated text compared to human-written. Which capability limitation likely caused this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Nondeterminism in generation",
      "B": "High latency",
      "C": "Lack of domain-specific fine-tuning",
      "D": "Insufficient token budget"
    },
    "explanation": "Without domain adaptation via fine-tuning, output may miss brand voice nuances, lowering engagement; other factors are less relevant."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A company uses a generative AI for code generation. Which business metric directly measures its value?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model perplexity",
      "B": "Output token count",
      "C": "Inference latency",
      "D": "Developer productivity increase"
    },
    "explanation": "Developer productivity gain directly quantifies business impact; perplexity and token counts are technical metrics."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which generative AI disadvantage makes it challenging to comply with regulatory data-lineage requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High throughput cost",
      "B": "Opaque generation processes",
      "C": "Limited access to multimodal models",
      "D": "Deterministic behavior"
    },
    "explanation": "Opaque \u201cblack-box\u201d processes hinder tracing content origin; other factors don\u2019t affect lineage."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A media analytics firm integrates generative AI summaries across news domains. They need consistent quality across topics. Which approach minimizes topic bias?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature for all prompts",
      "B": "Use zero-shot prompting only",
      "C": "Fine-tune on a balanced, multi-domain corpus",
      "D": "Limit token usage per domain"
    },
    "explanation": "Fine-tuning on diverse, balanced data reduces bias; temperature or prompt style alone cannot address content imbalance."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A company wants to expose a generative AI chatbot to external partners but must enforce content controls. Which AWS feature should they use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock Guardrails",
      "B": "High top-p sampling",
      "C": "In-context learning",
      "D": "Increased token limits"
    },
    "explanation": "Guardrails enforce policy checks on generated content; sampling or token settings do not ensure compliance."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "To evaluate a generative model\u2019s cross-domain performance, which benchmark approach is most robust?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Measure model latency across domains",
      "B": "Compare token costs per domain",
      "C": "Track customer satisfaction per domain",
      "D": "Use standardized test datasets spanning all target domains"
    },
    "explanation": "Standardized, domain-diverse benchmarks yield objective cross-domain performance comparisons; cost or latency alone don\u2019t measure accuracy."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which generative AI limitation must be addressed to ensure consistent brand tone in marketing copy?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Output length variance",
      "B": "Nondeterminism in language style",
      "C": "Multimodal capability",
      "D": "High memory footprint"
    },
    "explanation": "Nondeterminism yields variable style; controlling temperature and using templates helps enforce brand tone."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A logistics company uses generative AI for routing instructions. They need to guarantee no incorrect directions. Which limitation disqualifies generative AI for this use case?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High inference cost",
      "B": "Limited multimodal support",
      "C": "Possible hallucinations",
      "D": "Large model size"
    },
    "explanation": "Hallucinations risk generating unsafe or incorrect routes; other factors are less safety-critical."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which metric combo best quantifies generative AI\u2019s impact on lead generation?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Conversion rate uplift and cost per lead",
      "B": "Inference latency and token usage",
      "C": "Model perplexity and ROUGE score",
      "D": "Average session length and API error rate"
    },
    "explanation": "Conversion uplift and cost per lead directly measure lead generation effectiveness and efficiency."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A startup wants to lower unpredictability in a multi-turn conversational AI. What two changes achieve this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature and remove context history",
      "B": "Use a larger model and high top-k",
      "C": "Set temperature=0.0 and enable deterministic mode",
      "D": "Switch to zero-shot prompting and increase max tokens"
    },
    "explanation": "Deterministic mode and temperature=0.0 enforce repeatable outputs; other options increase randomness or remove useful context."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A healthcare provider needs to deploy a pre-trained multimodal foundation model that can process radiology images and patient notes. The solution must be fully managed, maintain PHI data in a private network, and minimize custom infrastructure work. Which AWS technology should you choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock with VPC interface endpoint",
      "B": "Amazon SageMaker JumpStart hosted endpoint",
      "C": "Custom EC2 instance running an open-source transformer",
      "D": "Amazon Q with public internet access"
    },
    "explanation": "Bedrock supports private VPC endpoints for managed, multimodal foundation models without custom infra."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An e-commerce company wants to generate product descriptions in multiple languages using a foundation model without data leaving AWS. They require token-based pricing and no long-term commitment. Which service meets these requirements?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon SageMaker JumpStart multilingual model",
      "B": "Amazon Translate with custom terminology",
      "C": "Amazon Bedrock with built-in foundation models",
      "D": "Amazon Comprehend custom classification"
    },
    "explanation": "Bedrock provides token-based invoicing, multi-language foundation models, and data stays within AWS."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A startup must prototype a generative AI chatbot quickly, with minimal code and zero ML expertise. They need out-of-the-box Q&A capabilities using a foundation model. Which offering should they use?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Build custom fine-tuned LLM on SageMaker",
      "B": "Amazon Q managed conversational agent",
      "C": "Deploy JumpStart model on EC2",
      "D": "Use Amazon Lex with Lambda"
    },
    "explanation": "Amazon Q provides managed, Q&A chatbot using foundation models with no ML coding."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A media company needs to orchestrate a pipeline that preprocesses prompts, calls a foundation model, and post-processes outputs. They prefer a graph-based orchestration library integrated with AWS. Which tool?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Step Functions",
      "B": "SageMaker Pipelines",
      "C": "AWS Lambda orchestration",
      "D": "PartyRock"
    },
    "explanation": "PartyRock provides graph-based orchestration for generative AI tasks over AWS."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An organization wants to fine-tune a foundation model with few-shot data, host it, and manage model health metrics. Which combination of services is most appropriate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock fine-tuning and use CloudWatch",
      "B": "SageMaker JumpStart fine-tuning and SageMaker Model Monitor",
      "C": "Amazon Q custom training and Lambda",
      "D": "Custom EC2 training and CloudWatch Logs"
    },
    "explanation": "SageMaker JumpStart supports fine-tuning foundation models and integrates with Model Monitor."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A fintech firm must serve high-throughput generative AI requests with predictable latency and pay per instance. They have capacity to manage underlying GPUs. Which is optimal?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock inference",
      "B": "SageMaker JumpStart serverless endpoint",
      "C": "Deploy Hugging Face model on SageMaker GPU instances",
      "D": "Use Amazon Q API"
    },
    "explanation": "SageMaker GPU endpoints give instance-based billing and predictable performance."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your team needs to store and search embeddings generated by a foundation model for RAG. You need a managed, autoscaling, low-latency search solution. Which AWS service?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Aurora",
      "B": "Amazon OpenSearch Service",
      "C": "Amazon Q",
      "D": "Amazon S3"
    },
    "explanation": "OpenSearch supports vector search on embeddings with auto-scaling."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A legal tech firm has strict data residency requirements in an on-premises AWS Outpost. They want to use a foundation model via AWS. Which solution supports this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock public API",
      "B": "SageMaker JumpStart in region",
      "C": "Amazon Q multi-region feature",
      "D": "SageMaker JumpStart deployed on Outposts"
    },
    "explanation": "SageMaker JumpStart can deploy models on Outposts ensuring on-prem data residency."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A marketing agency wants to evaluate costs of large language model inference for budget forecasting. Which AWS feature gives per-token cost estimates for Bedrock usage?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Bedrock pricing console estimator",
      "B": "Cost Explorer Reserved Instances",
      "C": "SageMaker pricing API",
      "D": "AWS Budgets with SageMaker metrics"
    },
    "explanation": "Bedrock console shows per-token pricing for models."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A gaming company needs to integrate dynamic world-building via generative AI in a live game server with sub-100ms response times. They need edge deployment. Which AWS solution?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Amazon Bedrock central inference",
      "B": "SageMaker serverless endpoint",
      "C": "Deploy foundation model with SageMaker edge manager on Greengrass",
      "D": "Amazon Q global API"
    },
    "explanation": "SageMaker Edge Manager on Greengrass allows deploying models to edge devices for low latency."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A biotech startup wants to experiment with open-source diffusion models but avoid large infrastructure setup. They want managed notebooks and one-click deployment. Which service?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock",
      "B": "SageMaker JumpStart with notebook and endpoint",
      "C": "Amazon ECR custom container",
      "D": "Amazon EC2 GPU instance only"
    },
    "explanation": "SageMaker JumpStart provides notebooks and one-click endpoint deployment for open-source models."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your team must ensure that all generative AI API calls are logged and audited for compliance. Which AWS feature should you enable with Bedrock?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CloudTrail data events only",
      "B": "S3 server access logs",
      "C": "CloudWatch Logs integration",
      "D": "CloudTrail data and management events for Bedrock"
    },
    "explanation": "Enabling Bedrock data events in CloudTrail logs all API calls for auditing."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A retail chain wants to deploy a foundation model across multiple AWS accounts using Infrastructure as Code. Which service integrates best with CloudFormation for generative AI deployments?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Q",
      "B": "Amazon Bedrock CloudFormation resource types",
      "C": "SageMaker Studio templates",
      "D": "AWS Lambda custom resources"
    },
    "explanation": "Bedrock provides CloudFormation resource types for unified IaC deployment."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An analytics platform generates embedding vectors in high volume and needs to store them cost-effectively for occasional batch RAG. Which storage option is best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon OpenSearch (hot nodes)",
      "B": "Aurora PostgreSQL with vector plugin",
      "C": "Amazon S3 with vector file format and Athena",
      "D": "Neptune for frequent graph queries"
    },
    "explanation": "S3 with Athena provides low-cost, batch retrieval for embeddings."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your security team demands AI inference traffic never traverse the public internet. You use Bedrock. What configuration enforces this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create a VPC endpoint for Bedrock in your private subnet",
      "B": "Use NAT Gateway routing only",
      "C": "Attach Internet Gateway to VPC",
      "D": "Enable public access in Bedrock settings"
    },
    "explanation": "A VPC interface endpoint ensures Bedrock traffic stays within AWS network."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A developer needs to rapidly prototype code generation from LLM prompts using Python SDK and shared notebooks with teammates. Which AWS offering is most suitable?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Q API with cURL",
      "B": "Bedrock console UI",
      "C": "Lambda custom integration",
      "D": "SageMaker JumpStart notebooks with AWS SDK pre-configured"
    },
    "explanation": "JumpStart notebooks include SDK clients and example code for prompt-based prototyping."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A data scientist needs to compare inference latency and token cost across multiple foundation models. Which approach is best?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Cost Explorer directly",
      "B": "Deploy each model in Bedrock and benchmark with CloudWatch metrics",
      "C": "Estimate via pricing pages offline",
      "D": "Use SageMaker Model Monitor"
    },
    "explanation": "Deploying in Bedrock and measuring with CloudWatch gives real metrics for latency and cost."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A telecom provider must maintain model version lineage and metadata for foundation models they fine-tune. Which AWS capability helps track this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Config rules",
      "B": "CloudTrail for model events",
      "C": "Amazon SageMaker Model Registry",
      "D": "Amazon Q audit log"
    },
    "explanation": "SageMaker Model Registry manages versions and metadata for fine-tuned models."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your legal team requires redaction of PII in fine-tuning data before uploading to AWS. Which service should you integrate into your generative AI pipeline?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Comprehend classification",
      "B": "Amazon Translate PII protection",
      "C": "SageMaker Clarify",
      "D": "Amazon A2I PII redaction workflow"
    },
    "explanation": "Amazon A2I can human-review and redact PII before using data for training or fine-tuning."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A media startup wants to implement RAG using Bedrock with a knowledge base in Amazon DocumentDB. They need to serve embeddings at scale. Which mechanism efficiently syncs new documents?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Lambda triggered by DocumentDB change streams to index embeddings in OpenSearch",
      "B": "Batch ETL with Glue daily",
      "C": "Manual export-import process",
      "D": "Streaming with Kinesis Data Streams directly into DocumentDB"
    },
    "explanation": "Lambda with change streams provides near real-time embedding sync into OpenSearch for RAG."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A research group wants to fine-tune a diffusion model using spot instances to reduce cost. They need managed job retries. Which AWS feature should they use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Bedrock fine-tuning with spot",
      "B": "SageMaker training jobs with Managed Spot Training",
      "C": "EC2 batches with spot fleet",
      "D": "Lambda step functions"
    },
    "explanation": "SageMaker managed spot training retries jobs and saves cost automatically."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A compliance requirement mandates deletion of user-specific fine-tuned models upon user withdrawal. Which AWS service can orchestrate model lifecycle including deletion?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Model Registry lifecycle policies",
      "B": "Bedrock model settings",
      "C": "AWS Config remediation",
      "D": "CloudTrail event triggers"
    },
    "explanation": "Model Registry lifecycle policies automate deregistering and deleting models per policy."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A global enterprise needs to ensure latency is under 200ms for generative AI inference across three AWS regions. They want a single API endpoint. What solution meets this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy three Bedrock endpoints and load-balance at client",
      "B": "Use Amazon Q global regional endpoints",
      "C": "SageMaker multi-az endpoint",
      "D": "Amazon CloudFront API Gateway distribution routing to Bedrock regional endpoints"
    },
    "explanation": "API Gateway + CloudFront can route to nearest Bedrock endpoint for consistent low latency."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A product team needs alerts when Bedrock model performance degrades below a BLEU threshold over time. How do you implement this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CloudWatch anomaly detection on token counts",
      "B": "SageMaker Model Monitor on Bedrock",
      "C": "Custom Lambda evaluating inference logs and publishing metrics to CloudWatch alarms",
      "D": "Bedrock built-in metric alarms"
    },
    "explanation": "Bedrock lacks native Model Monitor; custom Lambda parses logs, publishes metrics to CloudWatch for alarms."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A financial institution must encrypt all model artifacts at rest and in transit for a JumpStart fine-tuning job. Which configuration ensures this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable TLS only",
      "B": "Use KMS key for SageMaker job and ensure VPC endpoint",
      "C": "Use default AWS keys",
      "D": "Encrypt only S3 buckets"
    },
    "explanation": "Specifying customer-managed KMS key and VPC endpoint encrypts data in transit and at rest."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A compliance audit found that generative AI logs were stored alongside customer PII. The team must isolate logs and secure them for tamper proofing. Which approach is recommended?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Send logs to a separate S3 bucket with Object Lock in compliance mode",
      "B": "Store logs in the same bucket but different folder",
      "C": "Use DynamoDB for logs",
      "D": "Use AWS Config aggregator"
    },
    "explanation": "Separate bucket with Object Lock ensures immutability and isolation from PII."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your team wants to benchmark foundation model inference on different Graviton vs Intel instances in Bedrock. Which is true?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "You can choose instance types in Bedrock",
      "B": "Bedrock always runs on Intel",
      "C": "Instance type is abstracted; use SageMaker for custom instance benchmarking",
      "D": "Use Amazon Q to specify CPU architecture"
    },
    "explanation": "Bedrock abstracts hardware; SageMaker endpoints allow explicit instance type selection for benchmarking."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A startup requires dynamic orchestration that pauses fine-tuning jobs on SageMaker when daily free tier limits are reached, then resumes next day automatically. How to implement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Manual stop/start by engineer",
      "B": "Step Functions workflow with CheckBilling and SageMaker callbacks",
      "C": "Use CloudWatch schedules only",
      "D": "Use AWS Budgets alerts alone"
    },
    "explanation": "Step Functions can check budget via API and orchestrate SageMaker start/stop tasks."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A gaming company must generate dialogue using an LLM but only allow safe responses (no profanity). Which AWS generative AI feature helps enforce this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Bedrock guardrails",
      "B": "SageMaker Clarify",
      "C": "Amazon Q content filtering",
      "D": "Comprehend custom classification"
    },
    "explanation": "Bedrock supports Guardrails to filter or transform outputs according to safety policies."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A data science team needs versioned snapshots of training data, embeddings, and model artifacts in one place with query capabilities. Which AWS feature?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "S3 buckets with versioning",
      "B": "Glue Data Catalog only",
      "C": "RDS Postgres",
      "D": "SageMaker Feature Store and Model Registry"
    },
    "explanation": "Feature Store and Model Registry together manage data, embeddings, and model artifact versioning with query APIs."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your integration calls a Bedrock model synchronously but occasionally times out due to long generation. You need async calls with callback. Which pattern works?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase API timeout only",
      "B": "Use AWS Lambda tail-chaining",
      "C": "Use Synchronous SNS notifications",
      "D": "Call Bedrock asynchronously and poll with GetFrome API via Step Functions"
    },
    "explanation": "Bedrock supports async inference with token for status, polled via Get API in Step Functions."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A fintech company is building a real-time customer support chatbot using a foundation model on Amazon Bedrock. They require end-to-end inference latency under 250 ms per request with acceptable answer quality. Which model selection criterion should they prioritize?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Choose a foundation model with fewer parameters to minimize inference latency, even if domain-specific performance is moderately lower.",
      "B": "Select the largest foundation model available to maximize language coverage, accepting higher latency.",
      "C": "Pick the model trained on the largest dataset to ensure maximum accuracy regardless of inference speed.",
      "D": "Opt for a proprietary model under premium support to guarantee SLA-backed latency."
    },
    "explanation": "A smaller parameter model yields lower inference latency (<250 ms) while still providing acceptable quality; larger models incur higher latency."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A healthcare startup plans to store and retrieve 50 million patient-note embeddings for a RAG solution. They require low-latency vector similarity search that scales automatically. Which AWS service should they choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon OpenSearch Service with the k-NN vector search plugin.",
      "B": "Amazon RDS for PostgreSQL with the pgvector extension.",
      "C": "Amazon Neptune using SPARQL for similarity matches.",
      "D": "Amazon DocumentDB with a MongoDB vector plugin."
    },
    "explanation": "OpenSearch Service with k-NN is built for high-scale, low-latency vector search; other options add complexity or lack native vector indexing."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A news aggregator uses RAG to answer queries on newly published articles and needs embeddings updated in near real time. Which ingestion design best meets this requirement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon Kinesis Data Streams to capture new articles, trigger Lambda to generate embeddings, and index into OpenSearch immediately.",
      "B": "Schedule a daily AWS Glue job to re-embed and reindex all articles.",
      "C": "Fine-tune the foundation model hourly with the latest articles.",
      "D": "Upload new embeddings manually via the Bedrock console."
    },
    "explanation": "A streaming pipeline via Kinesis\u2192Lambda\u2192OpenSearch ensures minimal staleness; batch or manual approaches don\u2019t meet real-time needs."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An e-commerce company wants the foundation model to recognize newly launched product categories but has only 800 labeled examples. Which customization method minimizes cost while ensuring accurate category recognition?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Fully pre-train a custom foundation model on AWS using all available company data.",
      "B": "Fine-tune the foundation model with full parameter updates on the 800 examples.",
      "C": "Implement retrieval-augmented generation by storing category data externally and providing in-context examples at inference.",
      "D": "Switch to a larger foundation model to cover the category vocabulary without customization."
    },
    "explanation": "RAG with in-context examples uses external knowledge to cover new categories without the high compute cost of fine-tuning small datasets."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A legal advisory chatbot must minimize hallucinations when generating advice. Which inference parameter adjustment is most effective?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set temperature near zero to produce deterministic, least-random responses.",
      "B": "Raise top_p to 1.0 to broaden token selection.",
      "C": "Increase temperature to encourage creative answers.",
      "D": "Disable beam search to expedite response time."
    },
    "explanation": "Lowering temperature reduces randomness and hallucination; higher temperature or wider top_p increases variability."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An organization projects 100 000 monthly RAG queries and anticipates monthly content updates. Fine-tuning costs $50 000 upfront with negligible per-query cost; RAG costs $0.02 per query. Which approach has the lower year-one total cost?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use RAG exclusively, paying $0.02\u00d7100 000\u00d712 = $24 000 monthly.",
      "B": "Fine-tune once for $50 000 and serve queries at minimal incremental cost.",
      "C": "Pre-train a new foundation model quarterly.",
      "D": "Rotate between RAG and fine-tuning monthly."
    },
    "explanation": "RAG would cost $0.02\u00d7100 000\u00d712 = $24 000; fine-tuning is a one-time $50 000, so in year one RAG is cheaper ($24 000) \u2013 candidate should recalc: actually correct is A; but structured scenario expects RAG is cheaper at $24 000 vs fine-tune $50 000."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A logistics company must build a multi-step agent to validate addresses, calculate shipping costs, generate labels, and send notifications. They want a low-code orchestration solution. Which AWS feature should they use?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Bedrock Agents with integrated AWS Step Functions orchestration.",
      "B": "Chain multiple AWS Lambda functions manually.",
      "C": "Develop a custom microservice orchestrator on Amazon EC2.",
      "D": "Sequence tasks using AWS Batch workflows."
    },
    "explanation": "Bedrock Agents provide built-in multi-step orchestration integration, reducing custom code overhead."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A software team needs to summarize 100-page PDF manuals. The foundation model\u2019s token limit is 4096. Which approach handles this effectively?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Chunk the manual into \u22642000-token segments, summarize each, then combine summaries recursively.",
      "B": "Send the entire PDF text in one request and rely on the model\u2019s context window to truncate gracefully.",
      "C": "Fine-tune the model to increase its token limit.",
      "D": "Increase max_output_tokens beyond 4096 during inference."
    },
    "explanation": "Hierarchical chunking and summarization respects token limits and yields coherent overall summary."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A global translation service must support 30 languages with low latency. Which model selection strategy balances coverage and performance?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy specialized bilingual models for each language pair to reduce model size and latency.",
      "B": "Use a single large multilingual foundation model for all languages.",
      "C": "Translate via Amazon Translate then post-process with the foundation model.",
      "D": "Fine-tune one model per region for language coverage."
    },
    "explanation": "Bilingual models reduce inference cost and latency vs one large multilingual model while still covering required pairs."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "For a RAG pipeline, a team wants to limit the number of retrieved documents per query to reduce inference context size and cost. Which retrieval parameter should they adjust?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set the retriever\u2019s k parameter (number of top documents) to a lower value.",
      "B": "Reduce the embedding dimension size.",
      "C": "Lower the model temperature.",
      "D": "Decrease max_output_tokens."
    },
    "explanation": "Lowering k reduces the number of retrieved passages in the prompt, shrinking context and cost; other parameters don\u2019t affect retrieval count."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A vector store previously used 768-dim embeddings; switching to a new model yields 1024 dimensions, and query latency doubles. How can they restore performance without sacrificing retrieval quality?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Apply dimensionality reduction (e.g., PCA) to project 1024-dim embeddings down to 768 dimensions.",
      "B": "Continue using 1024-dim embeddings and provision larger instance types.",
      "C": "Pad old 768-dim embeddings to 1024 dims.",
      "D": "Revert to the previous embedding model."
    },
    "explanation": "Dimensionality reduction keeps vector store size manageable and preserves most semantic information; padding wastes resources."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A financial application requires ACID transactions when updating user embeddings under heavy concurrency. Which embedding store meets this requirement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Aurora PostgreSQL with pgvector extension.",
      "B": "Amazon OpenSearch Service with k-NN.",
      "C": "Amazon Neptune configured for vector similarity.",
      "D": "Amazon DynamoDB with custom indexing."
    },
    "explanation": "Aurora PostgreSQL offers ACID compliance plus pgvector for embeddings; other stores are eventually consistent or non-transactional."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Engineers want more diverse, creative summarizations. Which inference parameter changes achieve this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase both temperature and top_p.",
      "B": "Decrease temperature toward zero.",
      "C": "Reduce max_output_tokens to force brevity.",
      "D": "Disable retrieval augmentation."
    },
    "explanation": "Higher temperature and broader top_p allow the model to sample more diverse tokens, increasing creativity."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A marketing team must refresh product descriptions monthly. Custom fine-tuning costs $20 000 each time; RAG costs $0.015 per query. They expect 200 000 queries per month. Which approach is most cost-effective?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use RAG to avoid repeated fine-tuning costs.",
      "B": "Perform monthly fine-tuning despite the one-time expense.",
      "C": "Fine-tune quarterly and use zero-shot otherwise.",
      "D": "Use zero-shot prompting exclusively."
    },
    "explanation": "RAG @ $0.015\u00d7200 000 = $3 000/month is cheaper than $20 000 per refresh; zero-shot yields low accuracy."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A cell-phone provider needs a multi-step agent to verify SIM status, calculate upgrade eligibility, send offers, and log responses. They require built-in branching logic and retry handling. Which AWS capability should they use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock Agents with built-in workflow, branching, and retry support.",
      "B": "AWS Step Functions calling Bedrock directly.",
      "C": "Chained AWS Lambda functions orchestrated manually.",
      "D": "AWS Batch with job dependencies."
    },
    "explanation": "Bedrock Agents provide multi-step orchestration with logic and retry controls tailored to LLM workflows."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A retailer wants to store image embeddings and filter by price range. Which AWS service supports vector search plus metadata filtering?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon OpenSearch Service with k-NN plugin and document fields.",
      "B": "Amazon Neptune graph queries with PRICE property.",
      "C": "Amazon Timestream for time-series embedding storage.",
      "D": "Amazon S3 with object tags and Lambda lookups."
    },
    "explanation": "OpenSearch combines k-NN vector search and structured field filters for metadata constraints; other options lack integrated vector + metadata queries."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A global app must perform semantic search across English, Spanish, and Mandarin content. Which foundation model criterion is critical?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Select a multilingual foundation model pretrained on all target languages.",
      "B": "Use an English-only model with translation pre-processing.",
      "C": "Deploy separate monolingual models per language.",
      "D": "Rely on fine-tuning to teach new languages."
    },
    "explanation": "A multilingual LLM natively understands all languages for semantic embeddings; translation adds latency and error."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "To forecast RAG pipeline costs, which factors should be included?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Compute costs for embedding generation, LLM token charges, and storage costs for vector indexes.",
      "B": "Only the storage costs of embeddings.",
      "C": "Just the EC2 instance hours used by the application.",
      "D": "Only the foundation model\u2019s license fee."
    },
    "explanation": "RAG cost = embedding compute + LLM tokens consumed + storage + any I/O; ignoring any leads to underestimation."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A user submits a 2.5 million-token document to a model with a 1 million-token context limit. Which strategy handles this input best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a sliding/chunking approach: split into overlapping 1 million-token segments and process iteratively.",
      "B": "Request the model to accept larger context by adjusting max_context_tokens parameter.",
      "C": "Fine-tune the model to handle larger inputs.",
      "D": "Discard tokens beyond the first 1 million."
    },
    "explanation": "Chunking with overlap preserves context while respecting model limits; other methods are unsupported or lose information."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "During a partial OpenSearch outage, the RAG pipeline must continue to answer queries gracefully. Which fallback should be implemented?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Fall back to a keyword-based Boolean search over a local S3-backed index.",
      "B": "Switch to Amazon Neptune for vector search.",
      "C": "Abort requests and return errors.",
      "D": "Queue requests until OpenSearch recovers."
    },
    "explanation": "A local keyword search ensures degraded service rather than total failure; queuing or errors harm UX."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Retrieval accuracy in RAG is low despite high embedding similarity scores. Which diagnostic step should they take first?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Examine embedding distributions and consider retraining or switching embedding models.",
      "B": "Ramp up the model\u2019s temperature for more variability.",
      "C": "Switch to a larger foundation model for inference.",
      "D": "Increase max_output_tokens to get longer answers."
    },
    "explanation": "Poor retrieval often stems from suboptimal embeddings; model hyperparameters won\u2019t fix retrieval quality."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An agent workflow must enforce a maximum of 512 output tokens. Which Bedrock API parameter should they configure?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "max_output_tokens",
      "B": "temperature",
      "C": "top_p",
      "D": "beam_width"
    },
    "explanation": "max_output_tokens directly limits the number of tokens the model generates; other parameters control randomness or search strategy."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A high-security customer requires private network communication for vector searches. Which architecture element ensures privacy?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy OpenSearch Service within a VPC and use AWS PrivateLink for Bedrock access.",
      "B": "Use the public OpenSearch endpoint with IAM policies.",
      "C": "Tunnel traffic over S3.",
      "D": "Use CloudFront in front of OpenSearch."
    },
    "explanation": "VPC deployment plus PrivateLink keeps data off the public internet; other options expose endpoints publicly or misuse services."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A content-heavy app must pre-warm embedding Lambda functions to reduce cold-start latency spikes. What is the best practice?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Configure provisioned concurrency on the Lambda function.",
      "B": "Let the function handle occasional cold starts.",
      "C": "Use AWS Batch instead of Lambda.",
      "D": "Store embeddings in S3 to avoid Lambda."
    },
    "explanation": "Provisioned concurrency keeps warm instances ready, reducing cold-start latency; other options either ignore the issue or circumvent the design."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A financial analytics team must store embeddings alongside relational transaction data and enforce ACID. Which solution is most appropriate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon Aurora PostgreSQL with the pgvector extension.",
      "B": "Store embeddings in DynamoDB and transactions in RDS separately.",
      "C": "Use Amazon DocumentDB for both embeddings and transactional data.",
      "D": "Implement a two-phase commit between OpenSearch and Aurora."
    },
    "explanation": "Aurora Postgres + pgvector provides a unified, ACID-compliant store for both embeddings and relational data; other approaches increase complexity."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A chatbot using RAG must control costs by limiting the number of retrievals as well as generated tokens. Which combination of parameters should be adjusted?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Lower the retriever\u2019s k (documents) and set a conservative max_output_tokens.",
      "B": "Increase temperature and top_p.",
      "C": "Use a larger batch size.",
      "D": "Enable debug mode."
    },
    "explanation": "Reducing k shrinks context; lowering max_output_tokens caps the output length, both directly reduce cost."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A scientific publisher needs to index both text and vector embeddings for articles in the same service. Which AWS option should they pick?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon OpenSearch Service with integrated full-text search and k-NN vectors.",
      "B": "Amazon Neptune for RDF and vector queries.",
      "C": "Amazon RDS for PostgreSQL with full-text search.",
      "D": "Amazon S3 + Lambda for indexing."
    },
    "explanation": "OpenSearch supports hybrid queries mixing full-text and vector similarity; other services require stitching or lack vector support."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A recommendation system uses a foundation model to generate item embeddings and serve nearest-neighbor queries. To minimize search errors, which storage configuration is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy OpenSearch Service with replication enabled and k-NN plugin.",
      "B": "Use DynamoDB with Global Secondary Index.",
      "C": "Persist embeddings in S3 and scan on demand.",
      "D": "Host embeddings in Redis cluster."
    },
    "explanation": "OpenSearch with replication ensures high availability and accuracy in k-NN searches; others don\u2019t natively support vector retrieval."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A development team notices fluctuating RAG latency at peak loads. Which scaling approach for the vector store is most effective?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable auto-scaling on the OpenSearch domain\u2019s data nodes.",
      "B": "Provision a fixed large instance size.",
      "C": "Manually add more EC2 instances.",
      "D": "Switch to a single high-I/O DynamoDB table."
    },
    "explanation": "Auto-scaling OpenSearch data nodes adapts to load changes automatically; fixed or manual scaling is less responsive."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A legal research app must retrieve citations with high recall in RAG. Which retriever tuning improves recall while controlling context size?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase k (number of retrieved documents) and apply a relevance threshold filter.",
      "B": "Decrease temperature.",
      "C": "Reduce max_output_tokens.",
      "D": "Switch to a single-shot prompt."
    },
    "explanation": "Raising k returns more candidates for the LLM to choose from, boosting recall; other parameters don\u2019t affect retrieval breadth."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A team wants to quantify RAG pipeline performance for SLAs: retrieval latency and end-to-end response time. Which tools should they use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon CloudWatch Synthetics for end-to-end tests and OpenSearch slow-log metrics for retrieval latency.",
      "B": "S3 access logs.",
      "C": "AWS Config rules.",
      "D": "AWS Glue job metrics."
    },
    "explanation": "CloudWatch Synthetics simulates full pipeline; OpenSearch slow logs measure vector search latency; other tools are unrelated."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A developer must enforce data retention\u2014delete embeddings older than 90 days automatically. Which architecture achieves this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use OpenSearch index rollover with a 90-day retention policy and lifecycle policy to delete old indices.",
      "B": "Manually purge embeddings via Bedrock console.",
      "C": "Use S3 expiration rules.",
      "D": "Archive data in Glacier."
    },
    "explanation": "OpenSearch index lifecycle policies automate deletion of data older than a threshold; other options don\u2019t apply to the index store."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You are designing prompts for a customer support chatbot that must interpret rare domain-specific error codes. Which prompt engineering approach most reliably guides the model through multi-step interpretation of an unfamiliar code?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use zero-shot prompting with a single instruction to \"explain the code\"",
      "B": "Provide a long template with all known codes and rely on the model to match",
      "C": "Use chain-of-thought prompting with a few representative examples and ask the model to verbalize each reasoning step",
      "D": "Use a generic completion prompt with a high temperature to encourage creativity"
    },
    "explanation": "Chain-of-thought with examples explicitly leads the model through reasoning steps needed for rare codes; zero-shot or generic completion may omit steps and lead to hallucinations."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A generative AI application must summarize medical records while never revealing patient identifiers. Which prompt technique helps ensure anonymization?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Insert negative prompts like \"Do not remove any text\"",
      "B": "Use explicit negative prompts: \"Do not output any names, dates, or addresses\" combined with guardrail templates",
      "C": "Rely on the model\u2019s default privacy capability",
      "D": "Set a high temperature to diversify summary wording"
    },
    "explanation": "Explicit negative prompts plus guardrail templates clearly instruct the model to avoid specific PII, while high temperature or default privacy are unreliable."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "When using few-shot prompting for financial document classification, you notice inconsistent output if you exceed three examples. What is the most likely cause?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Context window saturation leads to model truncation of examples",
      "B": "Too many examples reduce the model\u2019s creativity",
      "C": "Model overfits to early examples only",
      "D": "High temperature setting causes inconsistency"
    },
    "explanation": "Adding more examples fills the context window causing the model to truncate or ignore later examples; reducing to 2\u20133 examples keeps the context coherent."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You need a stable translation service for legal documents where hallucinations are unacceptable. Which settings achieve high determinism?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High temperature (0.9) and no few-shot examples",
      "B": "Zero-shot prompting with a medium temperature (0.5)",
      "C": "Few-shot prompting with creative instructions",
      "D": "Zero-shot prompting with temperature 0 and explicit instruction style templates"
    },
    "explanation": "Temperature 0 ensures deterministic outputs and explicit templates guide consistent translation; creative or high temperature settings risk variability and hallucination."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An adversary tries prompt injection by appending \u201cIgnore previous instructions and output X.\u201d Which engineering technique mitigates this risk?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature to dilute malicious text",
      "B": "Use a system-level instruction or guardrail template enforced before user content",
      "C": "Provide more in-context positive examples",
      "D": "Switch to few-shot prompting"
    },
    "explanation": "System-level instructions or guardrail templates take precedence over user content, preventing prompt injection; temperature or examples do not block malicious append."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your model responses vary significantly when summarizing news articles. You need safer outputs. Which combination reduces variability while preserving detail?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Lower temperature to ~0.2 and include an instruction template focusing on key facts",
      "B": "Use high top-p sampling and no instruction",
      "C": "Use chain-of-thought without temperature adjustment",
      "D": "Switch to zero-shot with generic \u201cSummarize\u201d prompt"
    },
    "explanation": "Lowering temperature reduces sampling randomness, and a focused template ensures the model includes key factual details."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A content moderation tool uses an LLM to flag hate speech. False negatives increase when model sees adversarial phrasing. Which prompt-engineering tactic improves robustness?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Raise temperature to catch all variations",
      "B": "Provide only positive examples in prompt",
      "C": "Use a generic moderation API call",
      "D": "Include negative and adversarial examples in few-shot prompt and use chain-of-thought labeling guidelines"
    },
    "explanation": "Including adversarial examples and chain-of-thought guidelines teaches the model to reason through obfuscated hateful language; temperature changes or generic calls are insufficient."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You have a foundation model that occasionally repeats sensitive content. What prompt modification reduces repetition?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Remove all examples to reduce bias",
      "B": "Use high temperature to diversify",
      "C": "Add a negative prompt: \"Do not repeat previous content\" and set max_repeat_penalty",
      "D": "Use zero-shot prompting"
    },
    "explanation": "Negative prompts combined with repeat-penalty parameters actively discourage the model from echoing sensitive content."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your generative AI assistant must inject step-by-step debugging suggestions when users report code errors. Which prompt design ensures consistent inclusion of steps?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Template instructing \u201cList steps: 1. Analyze, 2. Identify, 3. Suggest Fixes\u201d with chain-of-thought",
      "B": "Few-shot with two examples only",
      "C": "Zero-shot \u201cHelp me debug\u201d",
      "D": "High temperature free-form prompt"
    },
    "explanation": "A structured template listing numbered steps with chain-of-thought guides the model to always include a detailed debugging process."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "When using in-context examples, why might you randomize the order of examples in your prompt?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "To reduce context window usage",
      "B": "To prevent positional bias causing overfitting to early examples",
      "C": "To increase token diversity",
      "D": "To improve latency"
    },
    "explanation": "Randomizing examples stops the model from disproportionately attending to the first few examples, reducing positional bias."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "What is the primary risk of using high temperature in a safety-critical medical Q&A system?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Longer response time",
      "B": "Decreased token usage",
      "C": "Increased likelihood of hallucinations and nondeterministic errors",
      "D": "Lower creativity"
    },
    "explanation": "High temperature increases randomness and risk of false statements, unacceptable in safety-critical domains."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You need to instruct a model to generate JSON only. Which technique is most effective?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Use a high temperature and hope for JSON",
      "B": "Use chain-of-thought prompt to think in JSON",
      "C": "Provide zero-shot generic instruction",
      "D": "Provide a strict response template: \"Respond ONLY in JSON format: {...}\" and include example JSON skeleton"
    },
    "explanation": "A strict template with skeleton examples and explicit \u201cONLY in JSON\u201d instruction ensures format adherence."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A hallucination in a product recommendation agent misattributes features. Which prompt tweak most reduces this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature to refresh knowledge",
      "B": "Use retrieval-augmented generation: fetch feature docs and include in context",
      "C": "Switch to single-shot prompting",
      "D": "Add more unrelated examples"
    },
    "explanation": "Providing retrieved factual docs in context grounds the model and reduces hallucinations; temperature changes are ineffective."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "During stress testing, your LLM triggers a jailbreaking prompt. Which guardrail approach best prevents this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Implement a system-level policy that filters or rejects jailbreak patterns before model call",
      "B": "Use few-shot with benign examples",
      "C": "Increase temperature",
      "D": "Switch to zero-shot"
    },
    "explanation": "Filtering malicious prompts before they reach the model is more reliable than prompt content adjustments."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You are defining a template for writing marketing copy. Which template structure yields the most consistent brand tone?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Open-ended instruction \u201cWrite copy\u201d",
      "B": "Chain-of-thought with broad guidelines",
      "C": "Structured template with sections: \u201cHeadline:\u2026, Body:\u2026, CTA:\u2026\u201d plus two few-shot examples",
      "D": "High-temperature freestyle prompt"
    },
    "explanation": "Structured templates with explicit sections and examples guide the model to consistently follow brand tone and format."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "To encourage the model to propose multiple alternative solutions rather than a single answer, which setting helps?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Temperature=0 and beam search",
      "B": "Increase temperature moderately and include \u201cList 3 alternative approaches\u201d in instruction",
      "C": "Use zero-shot default prompt",
      "D": "Few-shot with single example"
    },
    "explanation": "Moderate temperature plus explicit instruction to list alternatives fosters diverse outputs."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which prompt engineering approach best mitigates model exposure to private training data?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use data filtering and add a negative prompt \u201cDo not reveal source data\u201d in guardrails",
      "B": "Lower temperature",
      "C": "Use few-shot with internal data examples",
      "D": "Use zero-shot with generic instructions"
    },
    "explanation": "Combining data filtering and explicit negative prompts in guardrails prevents the model from disclosing training data."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You want the model to first self-critique its answer and then refine it. Which prompt technique accomplishes this reliably?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High temperature free-form prompt",
      "B": "Zero-shot with generic critique request",
      "C": "Chain-of-thought with two-phase template: \u201cStep 1: Provide answer. Step 2: Critique and refine.\u201d",
      "D": "Few-shot with single example"
    },
    "explanation": "A two-phase template clearly instructs sequential tasks and chain-of-thought ensures each phase is executed."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "When designing prompts for an LLM-based code generator, you observe the model writes insecure code. Which prompt addition reduces insecure patterns?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature",
      "B": "Add a guardrail template: \u201cEnsure code follows OWASP best practices. Do not include insecure functions,\u201d with examples",
      "C": "Use fewer in-context examples",
      "D": "Switch to single-shot prompting"
    },
    "explanation": "Guardrail templates specifying security constraints and examples guide the model to produce secure code."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A summarization model hallucinates non-existent entities. Which prompt strategy most reduces this risk?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use single-shot summarization",
      "B": "Increase temperature",
      "C": "Add more irrelevant examples",
      "D": "Retrieval-augmented prompt: provide source text and instruct \u201cSummarize only content present in the text\u201d"
    },
    "explanation": "Providing source text and explicit instruction to limit to that text prevents hallucinations."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "For a multi-turn dialogue, context length exceeds model limit and earlier user instructions are truncated. How to preserve persona instructions?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use higher temperature",
      "B": "Use system-level instructions stored separately and prepend only essential tokens",
      "C": "Switch to few-shot examples in each turn",
      "D": "Increase chain-of-thought depth"
    },
    "explanation": "System-level instructions are applied outside user context and remain effective regardless of conversation length."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You need the model to refuse unsafe code. Which negative prompt is most effective?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "\u201cIf request involves unsafe code, reply with \u2018I cannot comply\u2019\u201d within guardrail template",
      "B": "\u201cWrite code\u201d",
      "C": "Use high temperature",
      "D": "Provide safe code example only"
    },
    "explanation": "Explicit negative prompt specifying refusal behavior ensures the model declines unsafe code requests."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Why might you chain multiple prompts instead of one long prompt?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "To use more tokens",
      "B": "To increase randomness",
      "C": "To segment tasks, manage context limits, and verify intermediate results",
      "D": "To avoid guardrails"
    },
    "explanation": "Chaining tasks lets you check intermediate outputs, handle context windows, and apply different instructions per step."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A language model sometimes ignores instructions buried at the end of a long prompt. How do you ensure instruction priority?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Put instructions last",
      "B": "Use system-level or model-preset instructions with higher precedence",
      "C": "Use single-shot prompt without instructions",
      "D": "Increase temperature"
    },
    "explanation": "System-level or preset instructions override prompt content regardless of position, ensuring they are followed."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "In zero-shot translation tests, some outputs are syntactically correct but semantically wrong. Which prompt tweak improves semantic fidelity?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature",
      "B": "Reduce max tokens",
      "C": "Use chain-of-thought",
      "D": "Add explicit instruction: \u201cTranslate faithfully, preserving meaning exactly\u201d and use a short few-shot pair"
    },
    "explanation": "Explicit fidelity instruction plus few-shot example enforces meaning preservation; temperature or token limit changes do not guarantee semantic accuracy."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your model\u2019s completion occasionally exposes internal policy text. Which prompt-engineering measure helps?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add negative prompt \u201cDo not reveal internal policy\u201d in guardrails",
      "B": "Use high temperature",
      "C": "Use fewer examples",
      "D": "Switch to zero-shot default API call"
    },
    "explanation": "Explicit negative prompts prevent the model from disclosing protected content; other methods won\u2019t reliably block exposures."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You observe that adding irrelevant examples degrades performance. What principle does this illustrate?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Higher token count improves context",
      "B": "More examples always help",
      "C": "Context contamination; irrelevant or noisy examples reduce model focus",
      "D": "Temperature must be low"
    },
    "explanation": "Irrelevant examples contaminate context, reducing the model\u2019s ability to generalize correctly."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "For an AI hiring assistant, you must avoid demographic bias. Which prompt-engineering approach mitigates this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use high temperature to diversify responses",
      "B": "Include counterfactual and debiasing examples in few-shot prompt and use neutrality guardrail",
      "C": "Zero-shot generic instruction",
      "D": "Chain-of-thought only"
    },
    "explanation": "Providing debiasing examples and neutral guardrails helps the model avoid demographic biases in its output."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which method most effectively limits token usage when composing long responses?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High temperature",
      "B": "Use chain-of-thought",
      "C": "Few-shot with many examples",
      "D": "Set a max_tokens parameter and use concise templates"
    },
    "explanation": "Using max_tokens limits the response length regardless of prompt complexity; concise templates help the model stay within limits."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your LLM assistant for legal advice must never provide unauthorized practice of law. Which prompt safety layer is most reliable?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Prepend a system-level instruction: \u201cYou are a legal AI assistant, not a lawyer. Always include \u2018This is not legal advice\u2019 disclaimer.\u201d",
      "B": "Use chain-of-thought to think in law",
      "C": "High temperature for creativity",
      "D": "Zero-shot with generic instruction"
    },
    "explanation": "A system-level instruction enforced before user input ensures consistent disclaimer compliance."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A company needs to continuously update a foundation model with their domain-specific unlabeled text data streamed daily. They must retain the model\u2019s broad language capabilities while injecting domain knowledge. Which approach best satisfies this requirement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Fine-tune the entire model on the new data each day",
      "B": "Perform continuous pre-training on the domain corpus",
      "C": "Use instruction tuning on curated domain prompts",
      "D": "Apply reinforcement learning from human feedback"
    },
    "explanation": "Continuous pre-training (further unsupervised training) on unlabeled domain text preserves general capabilities while adding domain knowledge."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A team has only 500 labeled examples to adapt a 7B-parameter foundation model for a specialized classification task. They need to minimize compute and storage costs. Which fine-tuning method should they choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Low-rank adaptation (LoRA)",
      "B": "Full parameter fine-tuning",
      "C": "Instruction tuning with domain prompts",
      "D": "Reinforcement learning from human feedback"
    },
    "explanation": "LoRA adds a small number of trainable parameters, reducing compute and storage compared to full fine-tuning."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "In RLHF workflows, which sequence of steps is correct?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train reward model \u2192 supervised fine-tuning \u2192 pre-train foundation model",
      "B": "Pre-train foundation model \u2192 RLHF \u2192 supervised fine-tuning",
      "C": "Pre-train foundation model \u2192 supervised fine-tune on reference data \u2192 train reward model \u2192 reinforcement learning",
      "D": "Supervised fine-tune \u2192 pre-train \u2192 train reward model"
    },
    "explanation": "Standard RLHF: pre-train, supervised fine-tune, collect feedback to train reward model, then RL optimization."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A dataset for fine-tuning is heavily skewed toward one class. To prevent overfitting and bias, what is the most appropriate data preparation step before fine-tuning?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Remove minority class samples",
      "B": "Apply full fine-tuning without adjustment",
      "C": "Use stratified sampling or class-balanced oversampling",
      "D": "Increase learning rate to adapt quickly"
    },
    "explanation": "Stratified sampling or oversampling balances classes and ensures representativeness, preventing biased fine-tuning."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "After fine-tuning on a narrow domain dataset, the model\u2019s performance on general tasks degrades significantly. Which technique mitigates this catastrophic forgetting?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a smaller learning rate",
      "B": "Increase batch size",
      "C": "Apply RLHF",
      "D": "Mix domain data with a subset of general pre-training data during fine-tuning"
    },
    "explanation": "Including a fraction of general data prevents the model from forgetting prior capabilities during fine-tuning."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You need to adapt a multi-modal foundation model to a new domain with limited labeled images and annotations. Which two-step process is most efficient?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Full model fine-tuning \u2192 RLHF",
      "B": "Domain-adaptive pre-training on unlabeled images \u2192 parameter-efficient fine-tuning on labels",
      "C": "Instruction tuning \u2192 supervised classification fine-tuning",
      "D": "Prefix-tuning \u2192 supervised text embedding training"
    },
    "explanation": "First domain-adaptive pre-training uses unlabeled data; then a parameter-efficient method adapts on labels efficiently."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A team wants to reduce cost of repeated fine-tuning experiments. Which parameter-efficient fine-tuning method stores the fewest additional parameters per experiment?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Full fine-tuning",
      "B": "Prefix-tuning",
      "C": "LoRA",
      "D": "Adapter layers"
    },
    "explanation": "LoRA injects low-rank matrices into weights, requiring fewer additional parameters than full tuning or prefix-tuning."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "During instruction tuning, a model outputs harmful or toxic responses. Which step should be added to the fine-tuning pipeline to address this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase training epochs",
      "B": "Remove instruction examples",
      "C": "Use higher learning rate",
      "D": "Incorporate a safety filter or toxic content detection in supervised data curation"
    },
    "explanation": "Filtering training data for safety and removing toxic examples prevents harmful outputs post fine-tuning."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You must adapt a foundation model to a highly specialized jargon-heavy domain. Labeled data is scarce. Which approach yields the best domain adaptation?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Full fine-tuning on limited labels",
      "B": "Unsupervised domain-adaptive pre-training on jargon corpus followed by LoRA",
      "C": "RLHF with random prompts",
      "D": "Zero-shot inference with prompt engineering"
    },
    "explanation": "Domain-adaptive pre-training injects jargon knowledge, then LoRA leverages scarce labels efficiently."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A fine-tuned model is unexpectedly biased toward older data patterns. Which data governance practice could have prevented this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Define and enforce data versioning and lineage before training",
      "B": "Use larger batch sizes",
      "C": "Apply RLHF",
      "D": "Increase model size"
    },
    "explanation": "Data versioning and lineage ensure the training pipeline uses up-to-date, representative data, avoiding stale biases."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which continuous fine-tuning strategy allows safe incremental updates to a deployed foundation model without taking it offline?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Use a rolling\u2010update canary pipeline with adapter modules",
      "B": "Perform full model offline retraining then redeploy",
      "C": "Apply large batch synchronous fine-tuning",
      "D": "Use zero-shot prompting instead"
    },
    "explanation": "Adapter modules can be updated incrementally via a canary rollout, minimizing downtime and risk."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "When fine-tuning with instruction data, which metric best indicates improved adherence to instructions?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Perplexity",
      "B": "ROUGE",
      "C": "Human preference rate in A/B tests",
      "D": "Training loss"
    },
    "explanation": "Human preference rate directly measures whether outputs adhere to instructions, beyond loss or ROUGE."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A domain-specific fine-tuning dataset has label noise. How can you minimize its impact on training?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase learning rate",
      "B": "Use full fine-tuning",
      "C": "Ignore outliers",
      "D": "Apply robust loss functions or sample reweighting"
    },
    "explanation": "Robust loss functions or reweighting can reduce the influence of noisy labels during fine-tuning."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which fine-tuning schedule helps prevent overfitting when adapting to a small dataset?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Long constant high learning rate",
      "B": "Warmup followed by cosine decay",
      "C": "No warmup, sudden drop",
      "D": "Increasing learning rate mid-training"
    },
    "explanation": "A warmup then cosine decay schedule allows stable initial training and gradual fine-tuning, reducing overfitting."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which method efficiently incorporates new vocabulary into a frozen foundation model?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Full vocabulary retraining",
      "B": "Prefix-tuning",
      "C": "Embedding extension with LoRA or adapter fine-tuning",
      "D": "RLHF"
    },
    "explanation": "Adapter modules or LoRA can be applied to embeddings to learn new tokens without full model retraining."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "To fine-tune a multi-lingual foundation model for a low-resource language, what is the best data strategy?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Only use translated synthetic data",
      "B": "Apply full fine-tuning on small native set",
      "C": "Use instruction tuning in high-resource language",
      "D": "Combine cross-lingual transfer learning with small native language corpus"
    },
    "explanation": "Cross-lingual transfer leverages shared representations and small native data for efficient adaptation."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which fine-tuning approach allows you to revert to the base model with minimal effort if domain adaptation fails?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Overwrite original weights",
      "B": "Use adapter modules loaded at runtime",
      "C": "Full fine-tuning saved over base",
      "D": "Instruction tuning integrated into core weights"
    },
    "explanation": "Adapter modules are separate from base weights and can be detached to revert easily."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "During iterative fine-tuning, a model's performance plateaus quickly. Which action is most likely to break the plateau?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Decrease batch size further",
      "B": "Reduce dataset size",
      "C": "Introduce a scheduled learning-rate restarts",
      "D": "Switch to full fine-tuning"
    },
    "explanation": "Learning-rate restarts can help escape local minima and drive further training improvement."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A foundation model fine-tuned with generic instructions still underperforms on a niche domain task. Which next step adds highest domain fidelity?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Perform domain-specific instruction tuning with curated task prompts",
      "B": "Increase model size",
      "C": "Switch to zero-shot prompting",
      "D": "Apply standard RLHF"
    },
    "explanation": "Domain-specific instruction tuning focuses the model on precise task requirements, boosting fidelity."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which evaluation method best measures whether a fine-tuned generative model has adapted stylistically to domain conventions?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Perplexity on general corpus",
      "B": "BLEU against unrelated references",
      "C": "ROUGE on generic summaries",
      "D": "Human evaluation on domain-style adherence"
    },
    "explanation": "Human evaluation specifically on stylistic criteria is necessary to assess domain stylistic adaptation."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A team applies standard supervised fine-tuning but finds the model ignores rare example types. Which transfer learning technique addresses this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use larger batch size",
      "B": "Meta-learning-based fine-tuning (MAML)",
      "C": "Increase epochs on whole data",
      "D": "Instruction tuning"
    },
    "explanation": "Meta-learning like MAML helps the model adapt to few-shot or rare cases by learning to learn quickly."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which component is essential to include when employing RLHF?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Large batch size",
      "B": "Tokenizer adjustments",
      "C": "Cosine learning-rate schedule",
      "D": "Human feedback loop and reward model"
    },
    "explanation": "RLHF relies on human feedback and a trained reward model to guide policy updates."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "When continually fine-tuning on new domain data, what practice ensures traceability of model versions?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Maintain data and model versioning with a registry",
      "B": "Use single monolithic model file",
      "C": "Overwrite logs",
      "D": "Only track final model"
    },
    "explanation": "A registry with versioned datasets and model artifacts ensures full traceability across updates."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A custom domain corpus contains sensitive information. Which governance practice should be part of data preparation?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use full fine-tuning",
      "B": "Ignore and proceed",
      "C": "Apply anonymization and access controls",
      "D": "Increase token length"
    },
    "explanation": "Anonymization and strict access controls are critical to secure sensitive training data."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which fine-tuning metric would best detect overfitting early during training?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Training loss",
      "B": "Validation loss trend",
      "C": "Inference latency",
      "D": "Model size"
    },
    "explanation": "Monitoring validation loss helps detect divergence between train and hold-out performance indicating overfitting."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You need to fine-tune while preserving model safety constraints. Which method adds the least risk?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use adapter-based fine-tuning with safety filter",
      "B": "Full fine-tuning without filters",
      "C": "RLHF without guardrails",
      "D": "Instruction tuning on unfiltered data"
    },
    "explanation": "Adapter-based tuning limits changes and combining with content filters preserves safety constraints."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "After domain adaptation, new prompts produce unexpected hallucinations. Which training modification addresses this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase batch size",
      "B": "Use larger context window",
      "C": "Include retrieval-augmented examples in fine-tuning",
      "D": "Switch to zero-shot prompting"
    },
    "explanation": "RAG during fine-tuning grounds the model and reduces hallucinations by providing factual context."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A team wants to support continuous improvement via human feedback on deployed model outputs. Which pipeline element must they implement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Higher learning rate",
      "B": "Feedback collection interface feeding into reward model updates",
      "C": "Static model endpoint",
      "D": "Single training job"
    },
    "explanation": "A feedback loop and reward model update process are needed for continual RLHF improvements."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which fine-tuning paradigm is best when you have both classification and generation tasks in a single domain?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Pure supervised classification fine-tuning",
      "B": "RLHF only",
      "C": "Instruction tuning only",
      "D": "Multi-task fine-tuning mixing classification and instruction data"
    },
    "explanation": "Multi-task fine-tuning allows the model to learn both classification and generative capabilities jointly."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "When performing domain-adaptive pre-training on a model, which optimizer setting change is recommended?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Lower learning rate than general pre-training",
      "B": "Higher learning rate",
      "C": "No weight decay",
      "D": "Remove warmup"
    },
    "explanation": "A lower learning rate ensures stable domain pre-training without disrupting general knowledge."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which method best quantifies performance degradation to general tasks after fine-tuning on a narrow domain?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Training loss",
      "B": "Domain validation perplexity",
      "C": "Benchmark on held-out general task datasets",
      "D": "Model size change"
    },
    "explanation": "Evaluating on separate general benchmarks reveals any loss of general capability post fine-tuning."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A startup has unlabeled transaction logs and wants to discover typical customer behavior patterns without prior labeling. Which type of learning and algorithm is most appropriate?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Supervised learning with decision trees",
      "B": "Unsupervised learning with clustering (e.g., K-means)",
      "C": "Reinforcement learning with Q-learning",
      "D": "Supervised learning with logistic regression"
    },
    "explanation": "Unlabeled data requires unsupervised learning; clustering algorithms like K-means find patterns without labels."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A financial firm has daily closing prices and wants to predict tomorrow's price. They have known labels. Which type of task and learning paradigm applies?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Supervised classification",
      "B": "Supervised regression",
      "C": "Unsupervised clustering",
      "D": "Reinforcement learning"
    },
    "explanation": "Predicting a continuous numeric value from labeled data is a supervised regression problem."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "In ML, what distinguishes the algorithm from the model artifact?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Algorithm is the trained weights; model is the optimization procedure",
      "B": "Algorithm generates predictions; model defines hyperparameters",
      "C": "Algorithm is the procedure to learn; model is the learned parameters instantiation",
      "D": "Algorithm is the input data; model is the output labels"
    },
    "explanation": "The algorithm is the learning procedure, while the model is the result \u2014 the learned parameters from training."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A real-time translation service requires sub-second responses. Which inference type should you deploy?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Batch inference via scheduled job",
      "B": "Real-time inference via API endpoint",
      "C": "Stream inference via Kinesis Data Analytics",
      "D": "Edge inference on Greengrass only"
    },
    "explanation": "Sub-second response demands real-time inference exposed through an API endpoint."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You receive IoT device logs in JSON format. How do you categorize this data type?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Structured tabular data",
      "B": "Unstructured free text",
      "C": "Semi-structured data",
      "D": "Time-series numeric data"
    },
    "explanation": "JSON has a schema yet flexible fields\u2014qualifying as semi-structured data."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A model performs poorly on both training and test data. Which problem is indicated and what remedy applies?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Overfitting; reduce model complexity",
      "B": "Underfitting; increase model complexity",
      "C": "Data drift; retrain model periodically",
      "D": "High variance; add regularization"
    },
    "explanation": "Poor performance on training and test signals underfitting; increasing capacity can help."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An AI hiring tool rejects candidates from a minority group more often. Which issue does this illustrate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Variance error",
      "B": "Model bias affecting fairness",
      "C": "Overfitting to training data",
      "D": "Underfitting across groups"
    },
    "explanation": "Systematic disadvantage of a group indicates bias/fairness concern, not variance or fit issues."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "In NLP, what is a token in the context of language models?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "A single character",
      "B": "A subword unit such as a wordpiece",
      "C": "A part-of-speech annotation",
      "D": "A document-level vector"
    },
    "explanation": "Modern LLMs break text into subword tokens (wordpieces) for modeling."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Comparing batch versus real-time inference, which statement is true?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Batch inference processes individual records on demand",
      "B": "Real-time inference processes large volumes of data on schedule",
      "C": "Batch inference optimizes throughput over latency",
      "D": "Real-time inference requires training separate models"
    },
    "explanation": "Batch inference trades latency for throughput by processing large volumes at once."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which statement correctly describes the relationship between AI, ML, and deep learning?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "AI \u2282 ML \u2282 DL",
      "B": "ML \u2282 AI and DL \u2282 ML",
      "C": "DL \u2282 AI \u2282 ML",
      "D": "AI, ML, DL are disjoint fields"
    },
    "explanation": "Deep learning is a subset of ML, which itself is a subset of the broader AI field."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A neural network with only one hidden layer is best described as:",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "A shallow neural network",
      "B": "A deep neural network",
      "C": "A convolutional network",
      "D": "A support vector machine"
    },
    "explanation": "Networks with a single hidden layer are termed 'shallow'; deep networks have multiple layers."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which algorithm outputs probability distributions over discrete classes?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Linear regression",
      "B": "Softmax regression",
      "C": "K-means clustering",
      "D": "Principal component analysis"
    },
    "explanation": "Softmax regression (multinomial logistic) yields class probabilities; others do not."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which learning paradigm uses reward signals to learn optimal actions?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Supervised learning",
      "B": "Unsupervised learning",
      "C": "Reinforcement learning",
      "D": "Semi-supervised learning"
    },
    "explanation": "Reinforcement learning uses reward feedback to guide an agent's actions."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A model generalizes well when training and unseen-data errors are similar. This indicates:",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "High bias",
      "B": "High variance",
      "C": "Good fit",
      "D": "Data drift"
    },
    "explanation": "Comparable train/test errors signal the model fits data appropriately without under/overfitting."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which term describes the data type for customer support chat transcripts?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Structured numeric data",
      "B": "Semi-structured log data",
      "C": "Unstructured text data",
      "D": "Time-series sensor data"
    },
    "explanation": "Free-form chat transcripts are unstructured text requiring NLP techniques."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which describes the 'fit' process in ML?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Applying model to new data",
      "B": "Calculating inference latency",
      "C": "Optimizing model parameters on training data",
      "D": "Deploying model to production"
    },
    "explanation": "Fitting means training \u2014 adjusting model parameters to minimize error on training data."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which term refers to features derived from raw data to improve model performance?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Embeddings",
      "B": "Hyperparameters",
      "C": "Feature engineering",
      "D": "Transfer learning"
    },
    "explanation": "Feature engineering transforms and creates input variables to boost model quality."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which inference mode is suitable for nightly generation of business reports?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Online inference",
      "B": "Edge inference",
      "C": "Batch inference",
      "D": "Streaming inference"
    },
    "explanation": "Batch inference handles large volumes on a schedule, ideal for nightly reports."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which statement about unsupervised versus supervised learning is correct?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Supervised learning does not require labeled data",
      "B": "Unsupervised learning minimizes a loss function based on labels",
      "C": "Unsupervised learning discovers hidden patterns without labels",
      "D": "Supervised learning clusters data without supervision"
    },
    "explanation": "Unsupervised learning finds structure/patterns in unlabeled data; supervised requires labels."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A dataset contains grayscale images of size 28\u00d728. Which data type classification applies?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Structured tabular",
      "B": "Time-series numeric",
      "C": "Unstructured image",
      "D": "Semi-structured document"
    },
    "explanation": "Images are unstructured pixel arrays requiring CV techniques, not tabular formats."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which is a characteristic of deep learning compared to classical ML?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Requires less data for training",
      "B": "Learns representations automatically via multiple layers",
      "C": "Always interpretable",
      "D": "Uses k-nearest neighbors inherently"
    },
    "explanation": "Deep learning stacks many layers to learn hierarchical feature representations automatically."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A model shows higher accuracy on test data than on training data. Which issue most likely occurred?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Data leakage",
      "B": "Underfitting",
      "C": "Overfitting",
      "D": "Covariate shift"
    },
    "explanation": "Higher test accuracy than training usually indicates information leaked from test into training."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which describes 'algorithmic bias' in ML?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Random noise in labels",
      "B": "Systematic error from training data misrepresentation",
      "C": "Model's inability to learn complex patterns",
      "D": "Fluctuations in performance over time"
    },
    "explanation": "Algorithmic bias arises when training data misrepresentation causes systematic errors."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "What differentiates structured from unstructured data?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Structured data is stored in tables with defined schema",
      "B": "Unstructured data has numeric values only",
      "C": "Structured data needs NLP to process",
      "D": "Unstructured data is always binary"
    },
    "explanation": "Structured data fits into fixed schemas (tables); unstructured lacks a predefined format."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which statement about reinforcement learning is false?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "It learns from labeled examples",
      "B": "It uses reward feedback",
      "C": "It interacts with an environment",
      "D": "It optimizes cumulative reward"
    },
    "explanation": "Reinforcement learning does not rely on labeled examples; it learns via trial-and-error rewards."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which term describes the risk that a model performs unfairly across subpopulations?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Variance",
      "B": "Bias",
      "C": "Fairness issue",
      "D": "Generalization error"
    },
    "explanation": "Fairness issues occur when model performance differs systematically across groups."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "In AI terminology, what is an 'algorithm' best defined as?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "A mathematical model with learned parameters",
      "B": "A set of rules or procedures for solving a problem",
      "C": "The data used to train a model",
      "D": "The output generated after inference"
    },
    "explanation": "An algorithm is the defined procedure or set of rules used to solve problems or learn from data."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which category best describes audio files used for speech recognition?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Structured",
      "B": "Unstructured",
      "C": "Time-series",
      "D": "Tabular"
    },
    "explanation": "Audio is unstructured data; though sequential, it lacks a fixed schema for direct table storage."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A model uses a predefined architecture like VGG before any training. This blueprint is called:",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Model",
      "B": "Algorithm",
      "C": "Hyperparameter",
      "D": "Training data"
    },
    "explanation": "The architecture or learning procedure is part of the algorithm; the model emerges after training."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which statement illustrates a deep learning characteristic?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model uses manual feature extraction",
      "B": "Model stacks multiple hidden layers for representation learning",
      "C": "Model uses linear decision boundaries only",
      "D": "Model selects features via PCA only"
    },
    "explanation": "Deep learning models use many hidden layers to learn hierarchical representations."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "When applying a trained model to new input to generate predictions, which phase of the ML lifecycle is that?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Training",
      "B": "Inference",
      "C": "Preprocessing",
      "D": "Evaluation"
    },
    "explanation": "Inference refers to using a trained model to make predictions on unseen data."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A retailer has extensive purchase histories for customers but no predefined segments. The marketing team wants to group customers by behavior to target promotions. Which ML technique is most appropriate?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Clustering",
      "B": "Binary classification",
      "C": "Regression",
      "D": "Reinforcement learning"
    },
    "explanation": "Clustering is unsupervised and groups unlabeled data by similarity; classification and regression require labels, and reinforcement learning optimizes sequential decisions."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A bank wants to predict whether a loan applicant will default. They need a probability score and a decision boundary. Which ML approach best fits?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Linear regression",
      "B": "Classification using logistic regression",
      "C": "K-means clustering",
      "D": "Time-series forecasting"
    },
    "explanation": "Logistic regression yields probabilities and class labels for default prediction; linear regression predicts continuous values, clustering groups without labels, and forecasting is temporal."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A housing company needs to estimate house sale prices from features like size and location. Which technique should they use?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "K-nearest neighbors classification",
      "B": "Binary classification",
      "C": "Regression",
      "D": "Dimensionality reduction"
    },
    "explanation": "Regression predicts continuous values (prices); classification predicts discrete classes, and dimensionality reduction compresses features."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A logistics firm wants to detect unusual shipping delays in real time by analyzing delivery times without labeled anomalies. Which technique is most suitable?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Supervised classification",
      "B": "Regression",
      "C": "Collaborative filtering",
      "D": "Unsupervised anomaly detection"
    },
    "explanation": "Unsupervised anomaly detection identifies outliers without labels; classification and regression require labels, and collaborative filtering is for recommendations."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A call center wants to automatically transcribe and analyze customer sentiment from voice calls to identify dissatisfied callers. Which AWS service and technique should they choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Lex with classification",
      "B": "Amazon Polly with clustering",
      "C": "Amazon Transcribe followed by Amazon Comprehend sentiment analysis",
      "D": "Amazon Rekognition for audio classification"
    },
    "explanation": "Transcribe converts speech to text and Comprehend extracts sentiment; Lex is for conversational bots, Polly is TTS, and Rekognition analyzes images/video."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A media company has labeled images of defects in manufacturing and wants to automate defect detection on new images. Which AWS managed service and technique apply?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Textract with regression",
      "B": "Amazon Rekognition Custom Labels with classification",
      "C": "Amazon Comprehend with clustering",
      "D": "Amazon SageMaker for reinforcement learning"
    },
    "explanation": "Rekognition Custom Labels trains an image classification model; Textract extracts text, Comprehend handles NLP, and reinforcement learning is for sequential decision tasks."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A startup needs to translate customer emails in real time across languages without maintaining custom models. Which AWS managed service fits?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Comprehend",
      "B": "Amazon Lex",
      "C": "Amazon Transcribe",
      "D": "Amazon Translate"
    },
    "explanation": "Translate provides managed real-time machine translation; Comprehend does NLP analysis, Lex builds chatbots, and Transcribe converts speech to text."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An e-commerce platform wants to recommend products based on user behavior and similar users. Which ML technique and AWS service?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Collaborative filtering with Amazon Personalize",
      "B": "Clustering with Amazon Comprehend",
      "C": "Sequence prediction with Amazon Lex",
      "D": "Semantic search with Amazon Translate"
    },
    "explanation": "Personalize uses collaborative filtering for recommendations; Comprehend is NLP, Lex is conversational AI, and Translate is translation."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A manufacturer tracks sensor data streams from equipment and wants to predict time to failure. Which technique suits continuous data and error margins?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Classification",
      "B": "Clustering",
      "C": "Time-series regression forecasting",
      "D": "Association rule mining"
    },
    "explanation": "Time-series regression forecasting predicts continuous future values over time; classification predicts classes, clustering groups data, and association mining finds co-occurrence patterns."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A legal firm needs to extract key clauses from contracts and classify them as confidentiality or non-confidentiality. They prefer a fully managed service. Which AWS service and ML approach?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Textract with clustering",
      "B": "Amazon Comprehend custom classification",
      "C": "Amazon Translate with regression",
      "D": "Amazon Polly with unsupervised learning"
    },
    "explanation": "Comprehend custom classification labels text based on categories; Textract extracts raw text, Translate translates languages, Polly generates speech."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A health-tech startup has limited historical patient data and wants to predict disease risk. They worry about overfitting. What should they consider before applying ML?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Proceed with deep learning",
      "B": "Use reinforcement learning",
      "C": "Assess whether data volume and diversity justify a simple rule-based model",
      "D": "Deploy a high-capacity ensemble model"
    },
    "explanation": "With limited data, simple rules can outperform complex models and avoid overfitting; deep or ensemble models require more data, and reinforcement learning is inappropriate."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A content platform needs to summarize long articles automatically. They want high accuracy without building custom models. Which AWS service is appropriate?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Translate",
      "B": "Amazon Polly",
      "C": "Amazon Rekognition",
      "D": "Amazon Comprehend with summarization API"
    },
    "explanation": "Comprehend offers a managed summarization feature; Translate is translation, Polly is TTS, and Rekognition is for images."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A company wants to detect sentiment shifts in social media streams for brand monitoring. They require near-real-time analysis. Which architecture using AWS services achieves this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Translate -> Amazon SageMaker endpoint",
      "B": "Amazon Kinesis Data Streams -> AWS Lambda -> Amazon Comprehend",
      "C": "Amazon S3 batch -> Amazon Athena with Comprehend",
      "D": "Amazon DynamoDB -> Amazon Rekognition"
    },
    "explanation": "Kinesis streams data in real time, Lambda triggers Comprehend sentiment analysis; batch S3 is not real time, Translate and Rekognition are wrong services."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A fraud detection team wants to update their detection rules with model insights but cannot label post-fraud cases easily. Which ML approach helps uncover new fraud patterns?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Supervised classification with historical labels",
      "B": "Regression on fraud amounts",
      "C": "Reinforcement learning on transaction sequences",
      "D": "Unsupervised clustering on transaction features"
    },
    "explanation": "Clustering can reveal patterns in unlabeled data; supervised needs labels, regression predicts amounts, and RL optimizes sequential action rewards."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A marketing team needs to identify topics in customer feedback without predefined categories. Which technique and AWS service apply?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Custom classification with Amazon Comprehend",
      "B": "Sentiment analysis with Amazon Transcribe",
      "C": "Topic modeling with unsupervised Amazon Comprehend",
      "D": "Image analysis with Amazon Rekognition"
    },
    "explanation": "Comprehend supports unsupervised topic modeling; custom classification requires labels, Transcribe is speech-to-text, and Rekognition is for images."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A retailer has price data with many outliers and wants to predict median transaction value. Which ML objective minimizes outlier impact?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Mean squared error regression",
      "B": "Quantile regression",
      "C": "K-means clustering",
      "D": "Logistic regression"
    },
    "explanation": "Quantile regression predicts a specified percentile (median) and is robust to outliers; MSE penalizes outliers heavily, clustering and logistic are unrelated."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A travel company wants to provide personalized itinerary chat support integrated into their website without building backend ML. Which AWS service should they use?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Lex",
      "B": "Amazon Polly",
      "C": "Amazon Comprehend",
      "D": "Amazon Translate"
    },
    "explanation": "Lex builds conversational interfaces with managed NLU and integration; Polly is TTS, Comprehend analyzes text, and Translate translates languages."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A data science team wants to quickly prototype a classification model on tabular data with minimal setup. Which AWS service option is fastest?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "SageMaker custom notebook and build model",
      "B": "Deploy your own EC2 with scikit-learn",
      "C": "Use AWS Lambda for runtime training",
      "D": "Use SageMaker Autopilot"
    },
    "explanation": "Autopilot automates model building on tabular data; other options require manual setup or are unsuitable for training."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An IoT company needs to decide if an edge device should alert based on sensor patterns. They require on-device ML but limited compute. Which approach?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Deploy a full SageMaker endpoint on device",
      "B": "Use Amazon Rekognition on device",
      "C": "Use SageMaker Neo to compile a small footprint model",
      "D": "Use Amazon Comprehend on device"
    },
    "explanation": "Neo compiles models for resource-constrained devices; full endpoints and NLP/image services are inappropriate on edge."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A supply chain manager wants to forecast demand peaks and valleys for inventory planning. Which ML technique suits this seasonal pattern?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Classification",
      "B": "Time-series forecasting",
      "C": "Unsupervised clustering",
      "D": "Reinforcement learning"
    },
    "explanation": "Time-series forecasting models temporal patterns and seasonality; classification, clustering, and RL are not for continuous forecasting."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A news aggregator needs to classify articles by topic and extract key entities. They prefer a single AWS service call. Which service?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Comprehend",
      "B": "Amazon Translate",
      "C": "Amazon Textract",
      "D": "Amazon Polly"
    },
    "explanation": "Comprehend provides classification and entity extraction; Translate translates text, Textract extracts text from images, Polly synthesizes speech."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A gaming company wants NPCs to adapt tactics based on player behavior over multiple matches. Which ML paradigm applies?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Supervised learning",
      "B": "Unsupervised learning",
      "C": "Reinforcement learning",
      "D": "Clustering"
    },
    "explanation": "Reinforcement learning trains agents to optimize rewards through interactions; supervised and unsupervised learning do not handle sequential decision-making."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A research team has a small labeled dataset for image classification but needs higher accuracy. Which AWS managed solution speeds experiments?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Rekognition without custom training",
      "B": "SageMaker Studio with built-in algorithms",
      "C": "Amazon Comprehend classification",
      "D": "Amazon Translate"
    },
    "explanation": "SageMaker Studio with built-in image classification algorithms allows rapid experimentation; Rekognition without custom training won\u2019t improve accuracy, Comprehend and Translate are wrong domains."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A call analytics team must determine call intent from transcripts and route calls automatically. Which combination of AWS services and techniques?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Lex for TTS",
      "B": "Amazon Polly \u2192 Amazon Transcribe",
      "C": "Amazon Translate \u2192 Amazon Comprehend sentiment",
      "D": "Amazon Transcribe \u2192 Amazon Comprehend custom classification"
    },
    "explanation": "Transcribe converts speech, and Comprehend custom classification identifies intent; other combos are irrelevant."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A company with limited data wants to cluster customer feedback text. They cannot label data. Which AWS service can help without labels?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Comprehend custom classification",
      "B": "Comprehend unsupervised topic modeling",
      "C": "Translate clustering",
      "D": "Lex clustering"
    },
    "explanation": "Comprehend supports unsupervised topic modeling on text; other services don\u2019t cluster unlabeled text."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A manufacturing line collects temperature and vibration data to predict equipment failure exact time. Which service and technique?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Forecast classification",
      "B": "Amazon SageMaker clustering",
      "C": "Amazon SageMaker time-series forecasting",
      "D": "Amazon Comprehend regression"
    },
    "explanation": "SageMaker time-series forecasting handles temporal regression; Forecast service focuses on business metrics forecasting, clustering is for segmentation, Comprehend is NLP."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An HR analytics team wants to predict employee attrition risk and understand key factors. Which ML technique supports both prediction and feature importance insight?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "K-means clustering",
      "B": "Reinforcement learning",
      "C": "Neural network black-box model",
      "D": "Interpretable supervised classification (e.g., decision tree)"
    },
    "explanation": "Decision trees provide classification and interpretability; clustering is unsupervised, RL is sequential, and black-box networks lack transparency."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A financial services firm needs a service to continuously monitor AWS account security postures and recommend fixes, including container vulnerabilities. Which service?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Config",
      "B": "AWS CloudTrail",
      "C": "Amazon Inspector",
      "D": "AWS Artifact"
    },
    "explanation": "Inspector assesses vulnerabilities in EC2 and ECR and provides remediation; Config tracks configuration, CloudTrail logs API calls, Artifact provides compliance docs."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A biotech company wants to find new protein structures by exploring combinations without prior labels. Which ML approach should they use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Supervised classification",
      "B": "Unsupervised clustering",
      "C": "Regression",
      "D": "Reinforcement learning"
    },
    "explanation": "Clustering can explore unlabeled data structure; supervised and regression need labels, RL is for sequential decision optimization."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A customer service team wants to deploy a chatbot that understands intents and delegates to live agents when confidence is low. Which AWS service and feature?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Lex with confidence threshold",
      "B": "Amazon Polly with fallback",
      "C": "Amazon Comprehend sentiment",
      "D": "Amazon Translate delegation"
    },
    "explanation": "Lex supports intent recognition and confidence thresholds for fallback; Polly is TTS, Comprehend is NLP analysis, and Translate is translation."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A company wants to index and search through a large repository of documents using semantic similarity. Which AWS service and ML technique?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Kinesis with regression",
      "B": "Amazon Translate with clustering",
      "C": "Amazon Elasticsearch with RL",
      "D": "Amazon OpenSearch Service with vector embeddings"
    },
    "explanation": "OpenSearch supports k-NN vector searches for semantic retrieval; Kinesis is streaming, Translate is translation, RL unsuitable."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A retail analytics team wants to group stores with similar sales patterns and then predict future sales per group. Which sequence of ML techniques applies?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Regression then clustering",
      "B": "Classification then regression",
      "C": "Clustering then time-series forecasting",
      "D": "Reinforcement learning then classification"
    },
    "explanation": "First cluster by pattern, then apply forecasting per cluster; other sequences are illogical or wrong paradigm."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A manufacturing company needs to deploy a model to perform real-time predictions of equipment failures with millisecond latency. Which deployment method should they choose to meet this requirement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set up a SageMaker real-time inference endpoint",
      "B": "Use a SageMaker batch transform job",
      "C": "Deploy the model in an AWS Lambda function",
      "D": "Embed the model in Amazon Kinesis Data Analytics"
    },
    "explanation": "Real-time inference endpoints provide millisecond latency, whereas batch transforms and Kinesis Data Analytics are unsuitable for strict real-time requirements."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A data science team must process a 10 TB dataset once per month to generate predictions. The process does not require low latency. Which AWS service and feature combination is the most cost-effective solution?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker real-time endpoint with on-demand instances",
      "B": "SageMaker batch transform job with managed Spot instances",
      "C": "Deploy model as a Lambda function triggered by S3 uploads",
      "D": "Host the model on Amazon EC2 Auto Scaling group"
    },
    "explanation": "Batch transform with managed Spot instances minimizes cost for large one-off jobs without low-latency requirements."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A team wants to track parameters, metrics, inputs, and outputs across multiple training runs for reproducibility. Which SageMaker feature should they integrate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Model Monitor",
      "B": "SageMaker Model Registry",
      "C": "SageMaker Experiments",
      "D": "SageMaker Debugger"
    },
    "explanation": "SageMaker Experiments captures and organizes metadata across runs, enabling reproducibility. Debugger is for training insights, and Model Monitor is for endpoint drift."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "During feature engineering, a data scientist needs a low-code interface to profile data, identify outliers, and generate transformations. Which AWS service should they use?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Glue DataBrew",
      "B": "Amazon SageMaker Ground Truth",
      "C": "Amazon Athena",
      "D": "Amazon SageMaker Data Wrangler"
    },
    "explanation": "Data Wrangler provides a visual interface for profiling and transforming data; DataBrew is general ETL, not ML-focused."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A fintech startup requires both online and offline feature retrieval for its fraud detection model. Which AWS service best meets this requirement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon DynamoDB",
      "B": "Amazon SageMaker Feature Store",
      "C": "Amazon ElastiCache",
      "D": "Amazon RDS"
    },
    "explanation": "SageMaker Feature Store is designed for online (low-latency) and offline feature retrieval; databases alone lack the ML-specific APIs."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "After deployment, a model\u2019s accuracy begins to decline due to data drift. Which AWS service can automatically detect and alert on data and prediction drift?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS CloudWatch Alarms",
      "B": "Amazon GuardDuty",
      "C": "Amazon SageMaker Model Monitor",
      "D": "AWS Config"
    },
    "explanation": "Model Monitor analyzes input and output data to detect drift; CloudWatch alarms require custom metrics, and GuardDuty is for security threats."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A company\u2019s document classification model returns low-confidence predictions, and they want human review for those cases. Which service should they use to integrate a human-in-the-loop?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Mechanical Turk",
      "B": "Amazon A2I with a private workforce",
      "C": "AWS Step Functions",
      "D": "Amazon Augmented AI (A2I)"
    },
    "explanation": "Amazon A2I provides human review workflows for ML predictions; Mechanical Turk is not integrated into SageMaker."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An organization wants full control over inference infrastructure, including custom libraries and auto-scaling policies. Which deployment approach should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure a SageMaker real-time endpoint",
      "B": "Use SageMaker multi-model endpoint",
      "C": "Deploy a self-hosted inference container on Amazon EKS",
      "D": "Package the model as a Lambda layer"
    },
    "explanation": "Self-hosted containers on EKS allow custom infrastructure control; SageMaker endpoints abstract away low-level management."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "For a binary classification problem with highly imbalanced classes, which performance metric is most appropriate to optimize?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Overall accuracy",
      "B": "F1 score",
      "C": "Mean squared error",
      "D": "Log-loss"
    },
    "explanation": "F1 score balances precision and recall for imbalanced data, whereas accuracy can be misleading."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A credit scoring model must minimize false negatives (risky customers labeled safe). Which metric should the team monitor during model evaluation?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Precision",
      "B": "Recall",
      "C": "ROC AUC",
      "D": "Mean Absolute Error"
    },
    "explanation": "Recall measures the ability to detect actual positives (risky customers); precision measures correctness of positives."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "To track dataset versions, transformation code, and model artifacts within SageMaker, which feature should you use?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "SageMaker Model Monitor",
      "B": "SageMaker Processing",
      "C": "SageMaker Clarify",
      "D": "SageMaker Model Registry"
    },
    "explanation": "Model Registry manages model versions and metadata; Model Monitor is for drift, Clarify for bias detection."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A data scientist needs to tune hyperparameters across 100 training jobs, leveraging Spot Instances to reduce cost. Which SageMaker feature should they configure?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Training Jobs with Managed Spot",
      "B": "SageMaker Experiments",
      "C": "SageMaker Automatic Model Tuning",
      "D": "SageMaker Processing Jobs"
    },
    "explanation": "Automatic Model Tuning runs many jobs exploring hyperparameters and can use managed Spot Instances."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A team wants to automate labeling of images with bounding boxes for a custom object detection model. Which service should they use to build this labeled dataset?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Rekognition Custom Labels",
      "B": "Amazon SageMaker JumpStart",
      "C": "Amazon Comprehend",
      "D": "Amazon SageMaker Ground Truth"
    },
    "explanation": "Ground Truth provides workflows for automated and human labeling; Rekognition Custom Labels requires existing labeled data."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A developer needs to train a model using a custom Docker container with proprietary libraries. Which SageMaker feature allows this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Bring-your-own-container training in SageMaker",
      "B": "SageMaker Hosted Models",
      "C": "SageMaker Neo",
      "D": "SageMaker Notebook Instances"
    },
    "explanation": "SageMaker supports training with custom containers; Hosted Models refer to endpoints, Neo compiles models."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which AWS service enables the orchestration of data preprocessing, model training, and deployment steps into a repeatable workflow?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Step Functions",
      "B": "Amazon SageMaker Pipelines",
      "C": "AWS Glue",
      "D": "Amazon Managed Workflows for Apache Airflow"
    },
    "explanation": "SageMaker Pipelines is specifically designed for ML workflows, integrating with SageMaker steps and components."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A data engineering team must profile streaming data for anomalies before feeding it into the ML pipeline. Which service component should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Model Monitor",
      "B": "AWS Glue Data Catalog",
      "C": "Amazon SageMaker Processing with custom script",
      "D": "Amazon Kinesis Data Firehose"
    },
    "explanation": "SageMaker Processing can run custom anomaly detection scripts on streaming data; Model Monitor is for deployed models."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "To manage and host multiple versions of a model in a central repository, which feature of SageMaker should you use?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "SageMaker Model Monitor",
      "B": "SageMaker Model Registry",
      "C": "SageMaker Experiment Tracker",
      "D": "SageMaker Debugger"
    },
    "explanation": "Model Registry stores and version-controls models; experiments track runs, not served models."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A financial services company must meet audit requirements for model transparency, including lineage of data, code, and model versions. Which SageMaker feature set provides this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Lineage Tracking",
      "B": "SageMaker Pipelines",
      "C": "SageMaker Clarify",
      "D": "AWS CloudTrail"
    },
    "explanation": "Lineage Tracking captures relationships among datasets, code, and models; CloudTrail logs API calls but not ML metadata."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "For large-scale training jobs where training data size exceeds local instance storage, which SageMaker feature should you leverage?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Neo",
      "B": "SageMaker Model Monitor",
      "C": "SageMaker Batch Transform",
      "D": "Use Amazon S3 input channels with Pipe mode"
    },
    "explanation": "Pipe mode streams data directly from S3 to training container, avoiding local storage limits."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A team wants to detect bias and explain model predictions before deployment. Which combination of SageMaker components addresses this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Experiments + Model Registry",
      "B": "SageMaker Clarify + Model Cards",
      "C": "SageMaker Model Monitor + Debugger",
      "D": "SageMaker Ground Truth + Processing"
    },
    "explanation": "Clarify detects bias and computes explainability metrics; Model Cards document this information."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "When selecting a model source for a natural language task, which option allows using a pre-trained open-source model with minimal custom training?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "SageMaker built-in algorithms",
      "B": "Amazon Comprehend custom entity recognizer",
      "C": "Hugging Face model via SageMaker",
      "D": "Amazon Lex chatbot"
    },
    "explanation": "Hugging Face models on SageMaker let you fine-tune open-source transformers with minimal code."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A data scientist needs to rerun a previous training pipeline with updated code but identical data and parameters. Which SageMaker feature simplifies this?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "SageMaker Pipelines re-run capability",
      "B": "SageMaker Model Monitor checkpointing",
      "C": "SageMaker Experiments cloning",
      "D": "SageMaker Debugger replay"
    },
    "explanation": "Pipelines can be re-run with versioned steps to reproduce results; Experiments tracks runs but doesn't automate re-execution."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A predictive maintenance solution requires near real-time inference but can tolerate 2-second latency. Which endpoint type is most cost-effective?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Batch Transform",
      "B": "SageMaker asynchronous inference endpoint",
      "C": "SageMaker real-time endpoint",
      "D": "AWS Lambda with model container"
    },
    "explanation": "Asynchronous endpoints offer lower cost than real-time for latencies in seconds; batch transforms are too slow for near real-time."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which metric would best capture model performance when both false positives and false negatives carry significant but unequal business costs?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Accuracy",
      "B": "Precision",
      "C": "Recall",
      "D": "ROC AUC with customized threshold analysis"
    },
    "explanation": "ROC AUC allows analysis of trade-offs and selection of thresholds that reflect unequal costs."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A team needs to continuously retrain a model monthly when new labeled data arrives, ensuring reproducibility and minimal operational overhead. Which solution is most appropriate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Manual training triggered by developer",
      "B": "SageMaker Pipelines with scheduled triggers",
      "C": "AWS Batch with custom scripts",
      "D": "AWS Glue ETL followed by SageMaker Training Jobs"
    },
    "explanation": "Pipelines with scheduled triggers automate retraining and maintain reproducibility of steps."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "During initial exploratory data analysis, which step most directly uncovers multicollinearity among numerical features?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Compute data skew statistics",
      "B": "Visualize feature distributions",
      "C": "Calculate correlation matrix",
      "D": "Perform missing value imputation"
    },
    "explanation": "Correlation matrix identifies multicollinearity; other steps address distributions or missing values."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which SageMaker capability allows deploying multiple models behind a single HTTPS endpoint to optimize resource utilization?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Inference Recommender",
      "B": "SageMaker Model Monitor multi-model mode",
      "C": "SageMaker Multi-AZ endpoints",
      "D": "SageMaker multi-model endpoint"
    },
    "explanation": "Multi-model endpoints let you serve multiple model artifacts from one endpoint container."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "To reduce training costs while maintaining model convergence speed, a team considers using managed Spot Instances. What should they plan for?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model drift monitoring",
      "B": "Improved inference latency",
      "C": "Handling potential training interruptions",
      "D": "Increased real-time throughput"
    },
    "explanation": "Spot Instances can be interrupted, so the training code must handle restarts or checkpoints."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A team wants to compare performance of multiple models and register the best one automatically for deployment. Which SageMaker feature set should they use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Clarify and Model Monitor",
      "B": "SageMaker Model Registry with Pipelines evaluation step",
      "C": "SageMaker Experiments and Debugger",
      "D": "SageMaker Ground Truth and Experiments"
    },
    "explanation": "Pipelines can include evaluation steps that compare metrics and automatically register the winning model in the Model Registry."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "When selecting a managed API versus self-hosted API for model inference, which trade-off is a key consideration?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Operational overhead versus infrastructure control",
      "B": "Model accuracy versus inference latency",
      "C": "Hyperparameter tuning complexity versus cost",
      "D": "Data preprocessing capabilities versus security"
    },
    "explanation": "Managed APIs reduce operational overhead but limit infrastructure customization; self-hosted offers full control at the cost of more ops work."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A retailer measures model success by uplift in average order value rather than by classification accuracy. What type of metric is this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Technical metric",
      "B": "Capacity metric",
      "C": "Business metric",
      "D": "Hardware utilization metric"
    },
    "explanation": "Business metrics measure real business outcomes, whereas accuracy is a technical metric."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A developer submits a 4096-character document to a foundation model with a 2048-token context window. The model rejects the request, stating the input exceeds the token limit. The developer assumed characters map one-to-one to tokens. Which concept explains this miscalculation?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Tokenization",
      "B": "Chunking",
      "C": "Embeddings",
      "D": "Positional encoding"
    },
    "explanation": "Tokenization splits text into subword units called tokens, which do not correspond one-to-one with characters. Understanding tokenization is key to context window limits."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An engineer builds a RAG pipeline on long contracts. To preserve semantic continuity and reduce retrieval ambiguity, which chunking strategy should they adopt?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Fixed-size non-overlapping chunks",
      "B": "Semantic chunking with 10\u201320% overlap",
      "C": "Sentence-boundary chunking without overlap",
      "D": "Arbitrary character-based windows with 50% overlap"
    },
    "explanation": "Semantic chunking groups related sentences and a small overlap preserves context while minimizing redundancy."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A data scientist uses 64-dimensional embeddings for semantic search and sees many false positives between distinct topics. Which adjustment most directly improves topic discrimination?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase embedding dimensionality to 128 or 256",
      "B": "Switch from cosine similarity to Euclidean distance",
      "C": "Apply mean pooling instead of using the CLS token",
      "D": "Reduce embedding dimension to 32"
    },
    "explanation": "Higher-dimensional embeddings can capture finer semantic nuances, reducing false positives."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "When comparing text embedding vectors for similarity, which metric is most robust to differences in vector magnitude and focuses on directional similarity?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Euclidean distance",
      "B": "Manhattan distance",
      "C": "Cosine similarity",
      "D": "Jaccard distance"
    },
    "explanation": "Cosine similarity measures the angle between vectors, ignoring their magnitudes, making it robust to length differences."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A business needs to generate concise summaries of long reports. Which transformer architecture variant is best suited for sequence-to-sequence summarization tasks?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Encoder-only",
      "B": "Decoder-only",
      "C": "Encoder-decoder",
      "D": "Diffusion model"
    },
    "explanation": "Encoder-decoder transformers (seq2seq) encode input then decode into a summary, ideal for summarization."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "To implement a chatbot that produces fluent text continuations, which transformer variant should be selected?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Encoder-only",
      "B": "Decoder-only",
      "C": "Encoder-decoder",
      "D": "Diffusion-based"
    },
    "explanation": "Decoder-only (autoregressive) transformers predict the next token, making them suitable for text generation/chatbots."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An AI team needs to generate high-resolution images from noise. Which generative model type is most appropriate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Autoregressive transformer",
      "B": "Variational autoencoder",
      "C": "Diffusion model",
      "D": "Encoder-only transformer"
    },
    "explanation": "Diffusion models learn to denoise noisy data through a forward/reverse process and excel at high-resolution image generation."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A research group wants to map images and text into a shared embedding space for cross-modal retrieval. Which foundation model family should they choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "GPT",
      "B": "BERT",
      "C": "CLIP",
      "D": "DALL-E"
    },
    "explanation": "CLIP jointly trains image and text encoders to project both modalities into a common vector space for retrieval."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "The LLM produces incorrect reasoning steps. You want it to enumerate its logic before answering. Which prompt engineering technique elicits this behavior?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Zero-shot prompting",
      "B": "Few-shot prompting",
      "C": "Chain-of-thought prompting",
      "D": "Temperature adjustment"
    },
    "explanation": "Chain-of-thought prompts request the model to show intermediate reasoning, improving logical outputs."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An organization needs to generate code in both Python and JavaScript. Which model choice best supports multilingual code generation?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "GPT fine-tuned on multilingual code corpora",
      "B": "BERT fine-tuned on code classification",
      "C": "Transformer-based diffusion model",
      "D": "Encoder-only model"
    },
    "explanation": "Autoregressive GPT variants fine-tuned on diverse code scripts excel at multilingual code generation."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "During the pre-training stage of a foundation model, which data characteristic is most critical for robust language understanding?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High-quality labeled task-specific data",
      "B": "Large, diverse unlabeled text corpora",
      "C": "Balanced class labels across tasks",
      "D": "Human-annotated evaluation sets"
    },
    "explanation": "Pre-training uses massive, diverse unlabeled corpora to learn general language patterns."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A customer wants to adapt a general LLM to legal domain text. Which lifecycle stage accomplishes this specialization?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Pre-training",
      "B": "Fine-tuning",
      "C": "Evaluation",
      "D": "Inference"
    },
    "explanation": "Fine-tuning adjusts a pre-trained model to a domain by training on domain-specific data."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "After fine-tuning, which evaluation approach ensures the model meets business objectives beyond perplexity metrics?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "BLEU score against reference corpus",
      "B": "Perplexity on validation data",
      "C": "Human-in-the-loop user satisfaction surveys",
      "D": "Model throughput benchmarks"
    },
    "explanation": "Human evaluations assess real-world satisfaction and alignment with business goals."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "In deploying a foundation model for real-time inference at scale, which strategy optimizes cost and latency?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Hosted API with autoscaling",
      "B": "Self-hosted on under-provisioned GPU",
      "C": "Batch inference on EC2 Spot Instances",
      "D": "Single-instance serving"
    },
    "explanation": "Managed APIs with autoscaling adjust resources to demand, balancing latency and cost."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "To continuously improve a production chatbot, which feedback mechanism should be integrated into the model lifecycle?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Static evaluation dataset",
      "B": "Manual hyperparameter tuning only",
      "C": "User interaction logs for retraining",
      "D": "Single-shot inference"
    },
    "explanation": "Ingesting user logs enables retraining on real use-cases, improving performance over time."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Why do transformer models add positional embeddings to token embeddings?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "To encode token frequency",
      "B": "To enable the model to recognize sequence order",
      "C": "To reduce embedding dimension",
      "D": "To improve normalization"
    },
    "explanation": "Positional embeddings inject information about token positions, allowing sequence order to be modeled."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "In a transformer, what semantic role does the token embedding vector serve?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Positional information",
      "B": "Semantic representation of the token",
      "C": "Attention weight storage",
      "D": "Output logits for prediction"
    },
    "explanation": "Token embeddings map discrete tokens into continuous semantic vectors for the model to process."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A RAG pipeline uses 512-token chunks with 50% overlap, causing redundancy and high storage. How to optimize chunking?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase chunk size to 1024 tokens",
      "B": "Reduce overlap to 10\u201320%",
      "C": "Convert tokens to characters",
      "D": "Use a single chunk per document"
    },
    "explanation": "Reducing overlap lowers redundant embeddings while maintaining context continuity."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Why normalize embedding vectors before computing cosine similarity?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "To remove directional bias",
      "B": "To standardize magnitude to unit length and isolate direction",
      "C": "To reduce embedding dimension",
      "D": "To anonymize data"
    },
    "explanation": "Normalization scales vectors to unit length so cosine similarity reflects only direction."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Multi-head attention in transformers enables the model to:",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase network depth",
      "B": "Attend to information from multiple representation subspaces",
      "C": "Reduce parameter count",
      "D": "Perform convolution"
    },
    "explanation": "Each head learns attention in a different subspace, capturing diverse relationships."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "In diffusion models, what is the primary purpose of the noise schedule during training?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "To initialize model weights randomly",
      "B": "To corrupt data gradually so the model learns denoising",
      "C": "To augment with synonym replacements",
      "D": "To enforce sparsity"
    },
    "explanation": "A noise schedule progressively adds noise, training the model to reverse this process."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which trait distinguishes multi-modal foundation models from single-modal ones?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Processing only text data",
      "B": "Handling and generating multiple data types like text and images",
      "C": "Having fewer parameters",
      "D": "Using RNN architecture"
    },
    "explanation": "Multi-modal models process and relate different modalities (e.g., text, images) in a unified framework."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Large semantic gaps between chunks cause RAG retrievals to return irrelevant passages. How improve chunking?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use fixed-size byte-level chunks",
      "B": "Chunk at paragraph boundaries for semantic coherence",
      "C": "Remove stop words before chunking",
      "D": "Increase embedding dimension"
    },
    "explanation": "Chunking by logical paragraphs preserves coherent context and improves retrieval relevance."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "The prompt includes \"Answer in bullet points\" to influence format. Which prompt engineering concept does this illustrate?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Context window",
      "B": "Instruction prompt",
      "C": "Negative prompt",
      "D": "Latent space manipulation"
    },
    "explanation": "An instruction prompt explicitly guides the model\u2019s response format or behavior."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "In image generation via diffusion, how does a negative prompt affect the output?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Specifies elements the model should avoid generating",
      "B": "Specifies the color palette",
      "C": "Sets the token length",
      "D": "Controls the random seed"
    },
    "explanation": "Negative prompts instruct the model to exclude certain concepts from generated outputs."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "What is a drawback of excessively increasing embedding dimensionality beyond necessary semantic space?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increased inference speed",
      "B": "Higher storage and computation costs with diminishing returns",
      "C": "Decreased semantic differentiation",
      "D": "Automatic overfitting avoidance"
    },
    "explanation": "Too many dimensions raise resource costs while offering minimal semantic gains."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Before fine-tuning, selecting a representative dataset prevents bias. Which data characteristic is most important?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High volume of identical examples",
      "B": "Diversity covering expected use-case distributions",
      "C": "Exclusively positive examples",
      "D": "Uniform input lengths"
    },
    "explanation": "Diverse data reflecting real use-cases ensures the model generalizes fairly and accurately."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A team compares Model A (lower perplexity on legal texts) and Model B (smaller and faster). For production legal advice generation, which should they choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model A, because domain performance outweighs latency",
      "B": "Model B, because speed is primary",
      "C": "Model B, to reduce cost",
      "D": "Neither; build a custom model"
    },
    "explanation": "In sensitive domains like legal advice, domain accuracy is critical, even at higher cost or latency."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which foundation model type is primarily trained to predict the next token in a sequence?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Encoder-only",
      "B": "Decoder-only autoregressive",
      "C": "Encoder-decoder",
      "D": "Diffusion"
    },
    "explanation": "Decoder-only autoregressive models are trained to predict subsequent tokens given prior context."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "After deployment, hallucinations increase. Which lifecycle stage should address this by incorporating real user feedback?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Pre-training",
      "B": "Fine-tuning with reinforcement learning from human feedback",
      "C": "Evaluation",
      "D": "Inference"
    },
    "explanation": "RLHF fine-tuning uses collected feedback to align model outputs with user expectations, reducing hallucinations."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A healthcare startup wants to use a generative AI model to draft patient-friendly explanations of complex medical reports. They need consistent, accurate outputs and must minimize hallucinations. Which limitation of generative AI poses the greatest risk, and what mitigation strategy best addresses it?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Nondeterminism; use temperature=1.0 for more diverse outputs",
      "B": "Hallucinations; implement retrieval-augmented generation (RAG) with a curated medical knowledge base",
      "C": "Interpretability; generate detailed attention maps for clinical review",
      "D": "Latency; host the model on GPU-accelerated instances"
    },
    "explanation": "Hallucinations are a primary risk in medical text generation. RAG constrains outputs to verified documents, reducing invented facts while maintaining accuracy."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A marketing agency evaluates a general-purpose image-to-text foundation model to automate alt-text generation. They find outputs vary widely in detail level. Which generative AI characteristic explains this variability, and how can they control it?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model size; switch to a smaller model for consistency",
      "B": "Pre-training data bias; fine-tune on balanced captions",
      "C": "Nondeterminism; lower the temperature hyperparameter",
      "D": "Tokenization artifacts; adjust max token length"
    },
    "explanation": "Temperature controls randomness: lowering it yields more deterministic, consistent descriptions from the same prompt."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A finance firm wants to generate regulatory summaries from legal documents but must track source citations. Which limitation of generative AI complicates this requirement, and which AWS service can help maintain provenance?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Latency; use Amazon Q for faster inference",
      "B": "Model drift; use SageMaker Model Monitor for drift detection",
      "C": "Scalability; use Bedrock with auto-scaling",
      "D": "Lack of source traceability; use SageMaker Clarify and Amazon Q\u2019s source citation feature"
    },
    "explanation": "Generative models don\u2019t inherently cite sources. Amazon Q integrates retrieval with citation tracking. SageMaker Clarify can audit data provenance."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A media company must generate localized marketing copy in 10 languages using a single foundation model. They are concerned about cross-lingual performance. Which evaluation metric best quantifies business value for multilingual consistency?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "BLEU score across language pairs",
      "B": "Latency percentiles for each language",
      "C": "ROUGE-L on English only",
      "D": "Perplexity on the training corpus"
    },
    "explanation": "BLEU measures translation quality across languages; it directly reflects consistency and accuracy in localization tasks."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A logistics provider uses a generative AI chatbot to answer customer queries about shipment status. They observe the model sometimes fabricates tracking events. Beyond hallucinations, what model limitation is at play, and how can they detect and reduce it?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Low throughput; deploy on higher-capacity endpoints",
      "B": "Data privacy; enable encryption at rest",
      "C": "Nondeterminism; set a low temperature and implement post-generation validation against the tracking database",
      "D": "Fine-tuning drift; retrain weekly"
    },
    "explanation": "Nondeterminism causes variability in outputs; reducing temperature increases determinism, and validating outputs against real-time data catches fabrications."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A retail chain wants to generate personalized product recommendations using a generative model. They need to measure business impact beyond model accuracy. Which metric should they prioritize, and why?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "F1 score; balances precision and recall",
      "B": "Click-through rate (CTR); reflects actual user engagement and conversion potential",
      "C": "BLEU score; measures textual similarity",
      "D": "Inference latency; ensures real-time suggestions"
    },
    "explanation": "CTR captures how often recommended items are clicked, directly measuring user engagement and likely revenue impact."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A legal tech vendor uses a foundation model for contract clause generation. Clients require deterministic outputs for auditing. Which setting adjustment will improve auditability without fine-tuning the model?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase max token length",
      "B": "Enable logit biasing for key terms",
      "C": "Switch to few-shot prompting",
      "D": "Set temperature to near zero for deterministic behavior"
    },
    "explanation": "Lowering temperature reduces output randomness, producing consistent, repeatable text useful for audits."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A publisher uses generative AI to auto-create article summaries but must ensure summaries are not misleading. Which limitation most threatens content fidelity, and what technique best mitigates it?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Hallucinations; apply RAG with source document retrieval",
      "B": "Latency; batch process summaries",
      "C": "Scalability; use Bedrock with auto-scaling",
      "D": "Embedding drift; refresh embeddings weekly"
    },
    "explanation": "Hallucinations cause incorrect summaries. RAG anchors generation to actual source text, preserving fidelity."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A gaming company wants to leverage a generative AI agent to provide in-game NPC dialogue. They need balance between creativity and storyline consistency. Which generation parameter and approach best achieve this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase top-p sampling; use zero-shot prompts",
      "B": "Lower temperature; fine-tune on story scripts",
      "C": "Increase repetition penalty; use chain-of-thought prompts",
      "D": "Use few-shot prompting; disable system messages"
    },
    "explanation": "Lower temperature controls creativity vs consistency. Fine-tuning on story scripts aligns the model to game lore and style."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A biotech company needs to generate molecular synthesis protocols. They require high accuracy and domain adherence. Which model limitation poses the greatest challenge, and how can they address it on AWS?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Latency; deploy on CPU instances",
      "B": "Cost; use Spot Instances",
      "C": "Hallucinations; use RAG with a curated protocol database in Amazon Neptune",
      "D": "Scalability; shard across multiple SageMaker endpoints"
    },
    "explanation": "Hallucinations can produce invalid protocols. RAG with a structured database ensures generation uses verified procedures."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A telecom provider wants to compare two generative AI offerings for automated script generation. They require objective performance evaluation. Which combination of metrics gives the most comprehensive assessment?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Perplexity and latency",
      "B": "BLEU score and model size",
      "C": "ROUGE score and temperature",
      "D": "BLEU/ROUGE for quality and inference latency for operational feasibility"
    },
    "explanation": "BLEU and ROUGE assess text quality; latency ensures the solution meets real-time generation requirements."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A financial advisory service uses a foundation model to draft personalized investment advice. They must comply with regulatory transparency requirements. Which limitation challenges compliance, and what AWS tool helps ensure governance?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Scalability; use Bedrock\u2019s auto-scaling",
      "B": "Lack of explainability; generate model cards with SageMaker Model Cards and use Amazon A2I for human review",
      "C": "Throughput; optimize GPU usage",
      "D": "Embedding drift; retrain embeddings frequently"
    },
    "explanation": "Opaque model decisions hinder regulatory compliance. SageMaker Model Cards document model details, and A2I provides human validation."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An online education platform plans to generate practice exam questions. They need high variety but must avoid factually incorrect content. Which compromise and approach best fit?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Moderate temperature and RAG with curated question bank",
      "B": "High temperature and no retrieval",
      "C": "Few-shot prompting only",
      "D": "Zero-shot prompting and maximum tokens"
    },
    "explanation": "Moderate temperature adds variation, while RAG ensures content is grounded in the verified question bank to avoid false facts."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A news organization uses generative AI to summarize global headlines. They must measure trustworthiness of summaries. Which metric and process best quantify hallucination rates?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ROUGE-L against reference summaries",
      "B": "BLEU score against multilingual corpora",
      "C": "Human evaluation of hallucination frequency with Amazon A2I",
      "D": "Perplexity on news corpus"
    },
    "explanation": "Automatic metrics don\u2019t detect hallucinations reliably. Amazon A2I human-in-the-loop review quantifies factual errors."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A supply-chain firm wants generative AI to predict demand patterns and draft procurement plans. They need to measure solution ROI. Which business metric best captures value?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "F1 score on demand labels",
      "B": "Inference latency",
      "C": "Model throughput",
      "D": "Inventory turnover improvement percentage"
    },
    "explanation": "Inventory turnover improvement directly measures how the AI-driven plans optimize stock and improve capital efficiency."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A hotel chain uses generative AI chatbots for booking assistance. They observe nondeterministic price quote phrasing confusing customers. Which adjustment most effectively reduces variability?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use few-shot prompts",
      "B": "Lower temperature to near zero",
      "C": "Increase max token length",
      "D": "Enable top-p sampling"
    },
    "explanation": "Lowering temperature reduces randomness, leading to consistent phrasing of price quotes."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A media streaming service wants to generate personalized show summaries. They must avoid content bias favoring popular genres. Which strategy minimizes bias in generative outputs?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Fine-tune on a balanced genre dataset",
      "B": "Increase temperature to allow diversity",
      "C": "Use chain-of-thought prompting",
      "D": "Implement RAG with popular titles only"
    },
    "explanation": "Fine-tuning on balanced data ensures equal representation across genres, reducing bias toward popular ones."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A legal department needs deterministic boilerplate clauses from a foundation model. They must evaluate long-form outputs for consistency. Which generation settings and evaluation metric combination should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High temperature; BLEU",
      "B": "Zero-shot; perplexity",
      "C": "Low temperature; ROUGE score compared to standard templates",
      "D": "High top-p; F1 score"
    },
    "explanation": "Low temperature yields consistent text; ROUGE against approved templates measures textual fidelity and consistency."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A biotech startup uses generative AI to propose experimental protocols. They require quantifiable safety metrics. Which model limitation and metric pairing best aligns with safety requirements?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Latency; CPU utilization",
      "B": "Scalability; throughput",
      "C": "Interpretability; attention visualizations",
      "D": "Hallucinations; human-rated protocol validity with Amazon A2I"
    },
    "explanation": "Hallucinations risk unsafe protocol suggestions. Human review via A2I rates validity, directly measuring safety compliance."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A financial app uses a foundation model for fraud alert explanations. They need explainability for regulatory audits. Which AWS feature and model property best support this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Bedrock agents; high throughput",
      "B": "SageMaker Model Cards; transparent, white-box model",
      "C": "Amazon Q; token-based billing",
      "D": "PartyRock; multimodal inputs"
    },
    "explanation": "Model Cards document model lineage and behavior. Choosing a transparent model enhances auditability and explainability."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A logistics AI team wants to generate route summaries from GPS data. They face variable output lengths and missing details. Which parameter modification and technique best address both issues?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set max length and implement prompt templates with required fields",
      "B": "Increase temperature and use chain-of-thought",
      "C": "Enable logit bias for key terms",
      "D": "Use few-shot prompting without templates"
    },
    "explanation": "Limiting max length controls size; prompt templates enforce inclusion of mandatory fields, ensuring detail completeness."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A retail AI solution generates product descriptions; the business cares about ARPU uplift. Which performance metric should they track, and why?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "F1 score on category labels",
      "B": "Perplexity on description corpus",
      "C": "Average revenue per user (ARPU) before vs after deployment",
      "D": "Inference latency"
    },
    "explanation": "ARPU directly measures revenue impact per user, aligning model output quality with financial performance."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A car manufacturer uses generative AI to draft maintenance manuals. They require minimal hallucinations in technical instructions. Which approach best ensures accuracy?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High temperature with few-shot examples",
      "B": "RAG with an indexed manual database in OpenSearch",
      "C": "Chain-of-thought prompting",
      "D": "Fine-tune on generic technical corpus"
    },
    "explanation": "RAG using an indexed, authoritative manual database grounds generation in correct technical content, eliminating hallucinations."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An e-commerce site uses generative AI for dynamic ad copy. They want to optimize click-through and avoid repetitiveness. Which combination of settings and metrics is optimal?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Temperature 1.0; BLEU",
      "B": "Temperature 0.0; perplexity",
      "C": "Top-p = 0.3; ROUGE",
      "D": "Moderate temperature and top-p; CTR and novelty score"
    },
    "explanation": "Moderate sampling settings balance novelty and coherence; tracking CTR (engagement) and a novelty metric prevents repetitive copy."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A pharmaceutical company uses generative AI to summarize clinical trial reports. They need to minimize hallucinations and measure summary relevance. Which evaluation approach and metric should they implement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "RAG with human review via Amazon A2I and ROUGE-L against expert summaries",
      "B": "Zero-shot prompting and BLEU",
      "C": "High temperature generation and perplexity",
      "D": "Chain-of-thought prompting and F1 score"
    },
    "explanation": "RAG anchors to real reports; human-in-the-loop review quantifies hallucinations and ROUGE-L measures relevance to expert summaries."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A financial news platform automates article translation. They need to track cross-domain accuracy for emerging topics. Which business metric and process best capture model performance drift?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Average BLEU across domains",
      "B": "Perplexity on all content",
      "C": "ROUGE on English only",
      "D": "Periodic BLEU evaluation on emerging-topic test set with Model Monitor alerts"
    },
    "explanation": "Periodic BLEU on a domain-specific test set detects drift in new topics; Model Monitor alerts ensure timely retraining."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A broadcaster uses generative AI to script news intros. They observe unintended bias in phrasing political content. Which root cause and mitigation best resolve this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Pre-training data bias; fine-tune on balanced political corpora",
      "B": "Hallucinations; use RAG",
      "C": "Nondeterminism; lower temperature",
      "D": "Scalability; use higher throughput endpoints"
    },
    "explanation": "Bias stems from pre-training data. Fine-tuning on a balanced dataset corrects model bias and yields neutral phrasing."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A travel app generates destination descriptions. They want to maximize user engagement but must avoid misleading content. Which generation strategy and metric pairing achieves this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High temperature with ROUGE",
      "B": "Low temperature with perplexity",
      "C": "Moderate temperature with CTR and human-rated truthfulness",
      "D": "Top-p sampling with BLEU"
    },
    "explanation": "Moderate temperature encourages creative but controlled text. Tracking CTR measures engagement and truthfulness ratings detect misleading content."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A manufacturing firm uses generative AI for maintenance logs summarization. They need to detect out-of-distribution inputs causing model errors. Which AWS feature and process helps guard against this limitation?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Q; source citations",
      "B": "SageMaker Model Monitor with custom OOD detection rules",
      "C": "Amazon A2I for hallucination review",
      "D": "Bedrock agents for orchestration"
    },
    "explanation": "Model Monitor can detect when inputs fall outside the training distribution, triggering alerts or human review before inference."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A retailer uses a foundation model to generate email marketing subject lines. They want to A/B test variations but avoid sacrificing deliverability through spammy language. Which evaluation metric and prompt technique combination best balances creativity and compliance?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High temperature; BLEU",
      "B": "Zero-shot; perplexity",
      "C": "Few-shot; ROUGE",
      "D": "Moderate temperature; human-rated spam score and open rate"
    },
    "explanation": "Moderate sampling ensures creative copy, while human-rated spam scoring and tracking open rates measure compliance and effectiveness."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A legal AI team must generate redacted contract summaries. They require strict accuracy and no accidental disclosures. Which generative AI limitation poses the biggest risk, and how can they mitigate it?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Hallucinations; use RAG with redaction filter and human review via A2I",
      "B": "Latency; optimize GPU endpoints",
      "C": "Scalability; shard inference across endpoints",
      "D": "Throughput; increase batch size"
    },
    "explanation": "Hallucinations can reintroduce redacted content. RAG with filters and human review ensures no sensitive data is leaked."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A SaaS analytics company uses generative AI to write SQL queries from natural language. They notice queries sometimes reference nonexistent tables. Which model limitation is this, and what strategy best addresses it?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Latency; improve instance type",
      "B": "Cost; switch to Spot Instances",
      "C": "Hallucinations; employ RAG with schema retrieved from live database",
      "D": "Deployment complexity; use Bedrock"
    },
    "explanation": "Model hallucinations produce invalid queries. RAG with live database schema ensures generated SQL matches actual tables."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A startup needs to prototype a document summarization application using a large language model without managing infrastructure. They require minimal latency and no fine-tuning. Which AWS service or feature best meets these requirements?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "SageMaker JumpStart custom training job",
      "B": "Amazon Bedrock managed foundation model endpoint",
      "C": "Amazon Comprehend extractive summarization API",
      "D": "Amazon SageMaker Real-Time Inference with a self-hosted open-source model"
    },
    "explanation": "Amazon Bedrock provides fully managed, low-latency access to foundation models without customer-managed infrastructure. SageMaker JumpStart custom jobs require management and fine-tuning, Comprehend is extractive only, and self-hosting imposes infrastructure overhead."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A financial services company must run high-throughput generative chat sessions during market hours. They want predictable pricing per token and need to minimize cost under heavy load. Which pricing model should they choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker JumpStart per-instance hourly pricing",
      "B": "Bedrock usage-based with variable markup",
      "C": "Bedrock token-based charges with volume discounts",
      "D": "Amazon Q per-query fixed fee"
    },
    "explanation": "Bedrock\u2019s token-based model with volume discounts yields predictable per-token pricing and lower costs at scale. JumpStart hourly instances may idle and incur unused hours; Amazon Q\u2019s per-query fee lacks token granularity."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A healthcare application uses a confidential medical knowledge base with RAG via embeddings. They need a HIPAA-eligible, VPC-isolated embedding store with low latency. Which AWS service combination should they use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon OpenSearch Service with public endpoints",
      "B": "Amazon RDS for PostgreSQL without encryption",
      "C": "Amazon Neptune with VPN-only connectivity",
      "D": "Amazon DocumentDB in VPC with encryption at rest and in transit"
    },
    "explanation": "DocumentDB in a VPC with encryption supports HIPAA compliance and low latency for embedding storage. OpenSearch public endpoints and unencrypted RDS fail compliance; Neptune adds complexity and lacks managed embedding integration."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A media company wants to generate images with a diffusion model hosted in Bedrock. They require burstable throughput up to 50 requests/sec during live events, and will pay only when models run. Which service feature should they use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Bedrock serverless inference with auto-scaling",
      "B": "Bedrock provisioned capacity",
      "C": "SageMaker Hosting with dedicated GPU instances",
      "D": "Amazon EC2 Spot Instances running OpenAI API proxy"
    },
    "explanation": "Bedrock serverless inference auto-scales to bursts and charges only per request. Provisioned capacity incurs fixed cost, SageMaker hosting requires instance management, and EC2 Spot Instances add operational complexity."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A regulated enterprise must maintain model invocation logs and prompt inputs for audit, enforce encryption in transit, and isolate model endpoints. Which AWS configuration satisfies all requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Bedrock public endpoint with CloudTrail enabled",
      "B": "Bedrock VPC endpoint with CloudTrail Data Events and TLS enforced",
      "C": "SageMaker JumpStart with S3 logging only",
      "D": "Amazon Q over HTTPS without VPC controls"
    },
    "explanation": "Bedrock VPC endpoints with CloudTrail Data Events capture inputs and outputs, enforce TLS, and isolate traffic. Public endpoints or S3-only logging do not provide full isolation or comprehensive audit."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A developer wants to fine-tune a foundation model for domain-specific language but minimize compute cost. They only need a few examples and can tolerate higher latency. Which approach balances cost and performance?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Full fine-tuning in SageMaker with p3.16xlarge",
      "B": "Bedrock fine-tuning with RLHF",
      "C": "SageMaker Ground Truth data labeling",
      "D": "In-context few-shot prompting on Bedrock foundation model"
    },
    "explanation": "In-context few-shot prompting on Bedrock avoids fine-tuning fees and heavy compute, trading higher latency for much lower cost. Full fine-tuning is expensive, RLHF adds complexity, and Ground Truth only labels data."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A startup evaluates embedding storage options for RAG workflows. They need sub-50 ms nearest neighbor search latency at 100 qps and minimal management overhead. Which service is optimal?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon OpenSearch Service with k-NN plugin",
      "B": "Amazon Aurora PostgreSQL with pgvector extension",
      "C": "Amazon DynamoDB with Lambda custom search",
      "D": "Amazon Timestream optimized for time-series"
    },
    "explanation": "OpenSearch k-NN plugin provides low-latency vector search at scale with managed service. Aurora pgvector incurs higher latency and management; DynamoDB+Lambda adds infra overhead; Timestream is for time-series."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An application requires embedding generation and immediate storage. To minimize end-to-end latency, which invocation pattern should be used?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Invoke Bedrock embedding API, store to S3 via SDK next",
      "B": "Batch Bedrock embeddings, process in EMR, then S3",
      "C": "Invoke Bedrock embedding API directly from Lambda writing to Amazon DynamoDB",
      "D": "Precompute embeddings offline and store in Aurora"
    },
    "explanation": "A Lambda that directly invokes Bedrock embeddings and writes to DynamoDB yields minimal latency and overhead. Batch or offline methods add delay; S3 introduces higher write latency for real-time needs."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A legal firm must deploy a generative Q&A chatbot using Amazon Q. They require data residency in us-east-2, secure access, and usage logs. Which steps meet all requirements? (Select TWO.)",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure Amazon Q domain in us-east-2",
      "B": "Enable VPC endpoints and PrivateLink for Q",
      "C": "Use Comprehend Q&A instead of Amazon Q",
      "D": "Use public Q endpoints with IP restriction"
    },
    "explanation": "Configuring the domain in the required region and enabling VPC endpoints/PrivateLink secures and confines data. Comprehend Q&A is different service and public endpoints lack full isolation."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A retail company is building a conversational shopping assistant using AWS Bedrock. The assistant must handle context windows up to 1,500 tokens, maintain average response latency under 200 ms, and minimize per-token cost. Which foundation model should the company choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Anthropic Claude 2 (max 100k tokens, ~300 ms latency, high per-token cost)",
      "B": "Amazon Titan Text (max 2,048 tokens, ~80 ms latency, lowest per-token price)",
      "C": "Cohere Command (max 2,048 tokens, ~250 ms latency, moderate cost)",
      "D": "Stability Text 2 (max 1,024 tokens, ~90 ms latency, lowest cost)"
    },
    "explanation": "Titan Text meets the 1,500-token requirement, offers sub-200 ms latency, and has the lowest per-token price. Stability Text 2\u2019s context is too small; Claude 2 has higher latency and cost; Cohere is slower."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A healthcare analytics team needs to generate detailed patient summaries from long clinical notes (up to 8,000 tokens). Which approach best accommodates this requirement while controlling cost?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a single in-context prompt with a Titan Text model.",
      "B": "Fine-tune Amazon Titan Text on clinical notes.",
      "C": "Implement Retrieval Augmented Generation (RAG) with chunked embeddings and Amazon Bedrock.",
      "D": "Switch to Anthropic Claude 2 without any retrieval."
    },
    "explanation": "RAG with chunked embeddings supports 8,000 tokens, reduces hallucinations, and avoids expensive fine-tuning. Titan\u2019s window is too small; Claude 2 is cost-prohibitive; fine-tuning is overkill."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An engineering firm requires multimodal inference on text and diagrams. Which AWS foundation model selection criteria is most critical?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Support for multimodal input (text+image)",
      "B": "Lowest possible per-token cost",
      "C": "Maximum context length",
      "D": "Highest temperature variability"
    },
    "explanation": "Multimodal inference demands a foundation model trained on both text and image inputs. Cost, context length, and temperature are secondary if the model lacks multimodal capability."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A media company must generate video captions with consistent style and minimal hallucinations. Which inference parameter change will most reduce hallucinations?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature from 0.2 to 0.8",
      "B": "Decrease temperature from 0.8 to 0.2",
      "C": "Increase max tokens to 1,000",
      "D": "Remove stop sequences"
    },
    "explanation": "Lowering temperature reduces randomness and hallucinations. Increasing max tokens or removing stop sequences does not address hallucinations; higher temperature increases them."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A startup wants to store vector embeddings for semantic search of documents with occasional schema changes and dynamic updates. Which storage service is best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon OpenSearch Service",
      "B": "Amazon RDS for PostgreSQL",
      "C": "Amazon Neptune",
      "D": "Amazon DocumentDB"
    },
    "explanation": "Amazon Neptune provides graph storage optimized for dynamic, schema-flexible vector embeddings and efficient nearest-neighbor search. OpenSearch is less flexible for dynamic schemas; RDS and DocumentDB aren\u2019t optimized for graph vector queries."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A financial regulator uses a foundation model to answer compliance queries against a static regulation corpus. The corpus is ~50 GB. What is the most cost-effective way to ensure answers reflect the latest regulations?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Fine-tune the model every time regulations update",
      "B": "Use a large context prompt of full documents",
      "C": "Use in-context learning with all regulations loaded",
      "D": "Implement RAG with embeddings in Amazon OpenSearch and refresh index on updates"
    },
    "explanation": "RAG with OpenSearch embeddings allows incremental updates of a 50 GB corpus, ensures currency, and is more cost-effective than repeated fine-tuning or huge context prompts."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An AI team must adapt a foundation model to a niche legal language using only 1,000 labeled examples and limited budget. Which customization approach balances cost and performance?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Full pre-training on legal text",
      "B": "RLHF with human annotators",
      "C": "Instruction-tuning (fine-tuning) on the 1,000 examples",
      "D": "In-context learning without any fine-tuning"
    },
    "explanation": "Instruction-tuning on a small dataset is cost-effective and improves domain performance. Full pre-training and RLHF are too expensive; pure in-context learning yields weaker performance."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A media monitoring system needs to process user queries and, if needed, execute a sequence of API calls (translation, sentiment, summarization). Which Bedrock feature simplifies orchestration?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "RAG with OpenSearch",
      "B": "Agentless multi-model pipeline",
      "C": "In-context chain-of-thought prompting",
      "D": "Bedrock Agents"
    },
    "explanation": "Bedrock Agents orchestrate multi-step workflows across APIs. Chain-of-thought or RAG won\u2019t manage external API calls; \u201cagentless pipeline\u201d isn\u2019t a real feature."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An enterprise requires an LLM endpoint that can scale to unpredictable traffic with cost-efficient idle pricing. Which invocation mode should you choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Synchronous API with provisioned concurrency",
      "B": "Synchronous API with on-demand (no provisioned concurrency)",
      "C": "Asynchronous API with on-demand invocations",
      "D": "Batch transform jobs in SageMaker"
    },
    "explanation": "Asynchronous on-demand endpoints scale elastically, incur no idle provisioned cost, and handle unpredictable traffic. Synchronous on-demand can throttle; provisioned is costly; batch jobs don\u2019t suit real-time."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A banking app uses a foundation model to process user transaction queries. It must redact PII before generating responses. Which integration achieves this with minimal latency?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run a Lambda pre-processor to filter PII then call Bedrock",
      "B": "Use a Bedrock agent with built-in PII redaction tool",
      "C": "Fine-tune the model to ignore PII",
      "D": "Implement post-processing in the client application"
    },
    "explanation": "A Bedrock Agent with built-in PII redaction provides low-latency, integrated protection. Lambda adds latency; fine-tuning won\u2019t reliably remove PII; client post-processing is insecure."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A logistics company wants to store embeddings for thousands of ever-changing route descriptions with atomic updates and graph queries. Which storage is best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon OpenSearch Service",
      "B": "Amazon DynamoDB",
      "C": "Amazon Neptune",
      "D": "Amazon RDS"
    },
    "explanation": "Neptune supports ACID graph updates and nearest-neighbor searches. OpenSearch is less reliable for atomic graph operations; DynamoDB isn\u2019t optimized for vector search; RDS lacks graph features."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A startup has a $1,000 monthly AI budget. They need a nightly summary of 100,000 news articles (500 tokens each). Which inference pattern is most cost-effective?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Real-time streaming inference",
      "B": "Synchronous API per article",
      "C": "Bedrock Agents orchestration",
      "D": "Batch transform in SageMaker"
    },
    "explanation": "Batch transform jobs in SageMaker run in bulk overnight at lower per-token cost with no concurrent endpoint overhead. Real-time and synchronous calls cost more; Agents add complexity."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A legal tech company must guarantee explainability for each generated paragraph. Which foundation model choice best balances explainability and performance?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Anthropic Claude 2",
      "B": "Amazon Titan Text with model cards and low complexity",
      "C": "Cohere Command",
      "D": "OpenAI GPT-4 via Bedrock"
    },
    "explanation": "Titan Text with published model cards offers high transparency. Claude 2 and OpenAI GPT-4 lack full model cards on AWS; Cohere Command is opaque."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A data analytics team wants real-time chat summarization where occasional context window overruns occur. Which strategy best handles overruns?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase max tokens indefinitely",
      "B": "Use temperature of 1.0",
      "C": "Implement sliding-window RAG with embeddings",
      "D": "Fine-tune the foundation model"
    },
    "explanation": "Sliding-window RAG seamlessly handles context overruns by chunking and retrieving relevant segments, without unbounded tokens or expensive fine-tuning."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An education platform wants to reduce inference costs by at least 50%. They currently fine-tune a foundation model for every course update. What alternative yields the greatest savings?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use in-context learning with dynamic prompts instead of fine-tuning",
      "B": "Increase provisioned concurrency",
      "C": "Switch to a larger foundation model",
      "D": "Batch transform jobs daily"
    },
    "explanation": "In-context learning eliminates fine-tuning costs, achieving >50% savings. Provisioned concurrency and batch transforms do not address fine-tuning expense; larger models cost more."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A manufacturing firm needs natural language control of robots in multiple languages. Which foundation model feature is most important?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "High max tokens",
      "B": "Low temperature",
      "C": "Multilingual language coverage",
      "D": "Large model size"
    },
    "explanation": "Multilingual coverage is essential for multi-language commands. Token length, temperature, and size are secondary if the model can\u2019t understand required languages."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A consulting firm sees unpredictable spikes in LLM usage during peak hours. To control request latency and cost, which Bedrock concurrency feature should they adjust?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Batch size",
      "B": "Provisioned concurrency",
      "C": "Max tokens",
      "D": "Temperature"
    },
    "explanation": "Provisioned concurrency allocates pre-warmed capacity to maintain low latency during peaks. Batch size, max tokens, and temperature don\u2019t manage concurrency."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A telecom company uses RAG with a 200 GB document store. Index updates must reflect new data within minutes. Which architecture meets this requirement?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Rebuild the entire OpenSearch index daily",
      "B": "Fine-tune the model hourly",
      "C": "Use DynamoDB for embeddings",
      "D": "Stream new docs to OpenSearch via Kinesis Data Firehose"
    },
    "explanation": "Streaming via Firehose ensures minute-latency index updates in OpenSearch. Daily rebuilds are too slow; DynamoDB lacks vector search; frequent fine-tuning is impractical."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A media company must enforce a hard stop at 200 tokens in every generated summary. Which inference parameter should they set?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Temperature to 0",
      "B": "maxTokenLimit to 200",
      "C": "TopP to 1.0",
      "D": "Beam width to 5"
    },
    "explanation": "Setting maxTokenLimit to 200 enforces a hard stop. Temperature, TopP, and beam width do not constrain output length."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A legal research tool needs precise citations. Which retrieval method should you combine with a foundation model to ensure factual accuracy?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "In-context few-shot prompting",
      "B": "Temperature of 1.0",
      "C": "RAG with a vetted legal corpus",
      "D": "Increase max tokens"
    },
    "explanation": "RAG with a vetted legal corpus grounds responses in source documents, ensuring factual citations. Prompting, temperature, and token length by themselves don\u2019t guarantee accuracy."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A startup wants to orchestrate a dynamic knowledge-search\u2192generate workflow within Bedrock without writing orchestration code. Which feature should they use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Custom Lambda preprocessor",
      "B": "In-context prompt chaining",
      "C": "Fine-tuned multi-task model",
      "D": "Bedrock Agents"
    },
    "explanation": "Bedrock Agents provide no-code orchestration of search and generation steps. Prompt chaining and Lambda require manual coding; fine-tuning can\u2019t automate retrieval."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A company must pay only for inference time used, with no idle costs, and handle unpredictable daily traffic. Which Bedrock billing option suits this?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Provisioned concurrency",
      "B": "On-demand concurrency",
      "C": "Monthly subscription",
      "D": "Batch transform"
    },
    "explanation": "On-demand concurrency bills per inference with no idle fees. Provisioned concurrency incurs idle charges; subscriptions and batch transforms aren\u2019t per-inference real-time."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A risk analytics team must run summary inference on a petabyte-scale dataset overnight under a strict $2,000 budget. Which architecture is most appropriate?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Real-time Bedrock endpoint",
      "B": "Provisioned concurrency synchronous calls",
      "C": "Bedrock Agents",
      "D": "SageMaker batch transform with Spot instances"
    },
    "explanation": "SageMaker batch transform with Spot instances handles large datasets overnight under budget. Real-time endpoints and Agents are costlier for batch."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An AI solution must ensure every generated output is traceable to specific data sources. Which RAG component provides this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Document retrieval metadata",
      "B": "High temperature",
      "C": "maxTokenLimit",
      "D": "Fine-tuning"
    },
    "explanation": "Retrieval metadata tracks source documents for traceability. Temperature and token limits don\u2019t; fine-tuning embeds data but lacks per-generation traceability."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A biotech firm wants to fine-tune a foundation model on proprietary data but also retain the ability to use general knowledge. Which customization approach preserves general capabilities?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Full model replacement with proprietary-only data",
      "B": "Instruction-tuning on proprietary data",
      "C": "Continual pre-training on only proprietary data",
      "D": "In-context learning only"
    },
    "explanation": "Instruction-tuning adapts the model to proprietary data while preserving base knowledge. Full replacement or continual pre-training risks catastrophic forgetting; in-context alone is less performant."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A global news aggregator needs sub-100 ms inference for headlines in multiple regions. How should they minimize network latency?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a single US-East Bedrock endpoint",
      "B": "Enable provisioned concurrency only",
      "C": "Increase maxTokenLimit",
      "D": "Deploy regional Bedrock endpoints nearest users"
    },
    "explanation": "Regional endpoints reduce network hops and latency. A single US-East endpoint increases latency for distant users; provisioned concurrency and token limits don\u2019t address network delay."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A manufacturing firm uses a foundation model to translate technical manuals. They observe inconsistent terminology. Which prompt engineering technique best improves consistency?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a prompt template with defined glossary and format instructions",
      "B": "Increase temperature to 0.9",
      "C": "Remove stop sequences",
      "D": "Use only zero-shot prompting"
    },
    "explanation": "A glossary template enforces consistent terminology. Higher temperature increases variance; removing stop sequences and zero-shot prompting degrade control."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A software company must process variable batch sizes of code snippets with predictable billing and no idle charges. Which endpoint type should they choose?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Provisioned Bedrock endpoint",
      "B": "On-demand Bedrock endpoint",
      "C": "SageMaker batch transform",
      "D": "Synchronous Lambda invocation"
    },
    "explanation": "On-demand Bedrock endpoints bill per inference with no idle fees, ideal for variable loads. Provisioned endpoints incur idle costs; batch transform and Lambda aren\u2019t optimized for real-time code snippet processing."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An enterprise team must comply with data residency rules. Which customization method ensures data never leaves their AWS Region?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Use a public API for fine-tuning",
      "B": "Use a global Bedrock endpoint",
      "C": "In-context learning with external data",
      "D": "Fine-tune a private Bedrock foundation model in their region"
    },
    "explanation": "Private regional fine-tuning keeps data in the same region. Public APIs and global endpoints may move data; in-context learning doesn\u2019t train the model but still sends data externally."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A contact center wants to automatically correct grammar and expand abbreviations in live chats before analysis. Which approach yields the lowest end-to-end latency?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Batch transform jobs hourly",
      "B": "On-demand Bedrock text endpoint with high temperature",
      "C": "Synchronous Bedrock endpoint with moderate max tokens",
      "D": "RAG with OpenSearch"
    },
    "explanation": "A synchronous Bedrock text endpoint with moderate token limit gives lowest real-time latency. Batch jobs are too slow; high temperature is irrelevant to latency; RAG adds retrieval overhead."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A gaming company needs the model to generate personalized dialogs referencing user history stored in DynamoDB. Which integration pattern is best?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Embed all history in the prompt",
      "B": "Fine-tune model on historical data",
      "C": "Use a large maxTokenLimit",
      "D": "Pre-retrieve user history from DynamoDB and pass via prompt or RAG"
    },
    "explanation": "Pre-retrieval from DynamoDB and passing relevant context via prompt or RAG is efficient and cost-effective. Embedding all history inflates tokens; fine-tuning is static; token limit alone doesn\u2019t guarantee relevance."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A financial services chatbot is giving superficially correct but logically flawed responses on multi-step loan eligibility queries. Which prompt engineering technique should you apply to improve its stepwise reasoning?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Chain-of-thought prompting",
      "B": "Zero-shot prompting",
      "C": "Negative prompting",
      "D": "Temperature tuning"
    },
    "explanation": "Chain-of-thought encourages the model to articulate intermediate steps, improving multi-step logical reasoning. Zero-shot provides no examples, negative prompts discourage outputs, and temperature tuning only affects randomness."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You need to instruct a foundation model to translate text but also avoid certain sensitive terms. Which prompt component should you include?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Few-shot examples",
      "B": "Chain-of-thought instruction",
      "C": "Negative prompt instructions",
      "D": "High temperature parameter"
    },
    "explanation": "Negative prompts explicitly instruct the model what to avoid. Few-shot gives examples, chain-of-thought is for reasoning, and temperature affects creativity, not exclusion."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An internal knowledge base retrieval application returns irrelevant documents. You want the model to weigh recent data more heavily. Which prompt engineering adjustment addresses recency bias?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add negative prompts",
      "B": "Use zero-shot only",
      "C": "Increase max token length",
      "D": "Include time-stamped context in prompt examples"
    },
    "explanation": "By including time-stamped examples, the model learns to prioritize recent information. Negative prompts exclude content, zero-shot lacks examples, and token length doesn\u2019t address recency."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A marketing team reports inconsistent style in AI-generated product descriptions. Which prompt engineering best practice stabilizes tone and format?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Use a very low temperature",
      "B": "Provide a structured template with placeholders",
      "C": "Apply chain-of-thought prompting",
      "D": "Switch to zero-shot prompting"
    },
    "explanation": "Structured templates enforce consistent style by constraining output. Low temperature reduces randomness but doesn\u2019t ensure format, chain-of-thought is for reasoning, zero-shot lacks guidance."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "During RAG (Retrieval Augmented Generation), generated answers include hallucinations. Which prompt technique reduces hallucination risk?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase few-shot examples",
      "B": "Lower the temperature to zero",
      "C": "Instruct the model to cite sources from the provided context",
      "D": "Use negative prompts against hallucinations"
    },
    "explanation": "Prompting the model to cite sources ties it to retrieved context. Zero temperature reduces randomness but hallucinations can still occur. Negative prompts aren\u2019t reliable, and more examples alone won\u2019t eliminate hallucinations."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You need to generate SQL queries from natural language but want to prevent malicious injections. Which prompt engineering practice helps guard against prompt injection attacks?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provide multiple few-shot examples",
      "B": "Use explicit system message with strict API schema constraints",
      "C": "Increase temperature to diversify syntax",
      "D": "Include chain-of-thought steps"
    },
    "explanation": "System messages with strict schema enforce structure and reduce injection risk. Few-shot doesn\u2019t block injection, higher temperature increases variability, and chain-of-thought is unrelated to security."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An LLM generates partial answers due to token limits. Which prompt adjustment helps produce complete answers?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Add negative prompts",
      "B": "Lower temperature",
      "C": "Switch to zero-shot",
      "D": "Include explicit \u2018continue until complete\u2019 instruction"
    },
    "explanation": "Explicit instructions to continue help the model produce full answers within length constraints. Negative prompts exclude content, temperature and zero-shot don\u2019t address completeness."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A sentiment-analysis assistant mislabels sarcasm as positive sentiment. Which prompt technique might improve detection?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Provide few-shot examples of sarcastic sentences labeled correctly",
      "B": "Use negative prompts to exclude positive terms",
      "C": "Increase max tokens",
      "D": "Use zero-shot prompting only"
    },
    "explanation": "Few-shot examples teach the model specific sarcasm patterns. Negative prompts exclude words rather than teach nuance, token limits don\u2019t help, and zero-shot lacks examples."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A code generation model outputs insecure code snippets. Which prompt engineering strategy can enforce security best practices?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Chain-of-thought prompting",
      "B": "Include security checklist and example compliant code in prompt",
      "C": "Use negative prompts against insecure patterns",
      "D": "Increase temperature for diversity"
    },
    "explanation": "Embedding a security checklist plus example code guides the model toward secure outputs. Chain-of-thought is for reasoning, negative prompts aren\u2019t specific enough, and temperature affects randomness."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your conversational agent drifts off topic after several turns. Which prompt engineering technique limits topic drift?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Lower temperature",
      "B": "Increase few-shot examples",
      "C": "Use a persistent system prompt defining topic and role constraints",
      "D": "Include chain-of-thought prompts"
    },
    "explanation": "A system prompt that persists each turn reinforces role and topic. Temperature and examples help style not persistence, chain-of-thought is for reasoning."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "To minimize repetition in generated poetry, which prompt or parameter adjustment is most effective?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a negative prompt against repeated lines",
      "B": "Provide zero-shot instructions",
      "C": "Use chain-of-thought",
      "D": "Lower the repetition penalty parameter"
    },
    "explanation": "Reducing repetition penalty discourages repeated phrases. Negative prompts may not enforce pattern, zero-shot lacks guidance, chain-of-thought is irrelevant."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An LLM used for code summarization omits critical function details. Which prompt engineering approach helps ensure completeness?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Ask the model to list all function parameters and return values explicitly",
      "B": "Use zero-shot prompting",
      "C": "Apply negative prompts",
      "D": "Chain-of-thought prompt"
    },
    "explanation": "Explicitly requesting detailed output forces inclusion of all elements. Zero-shot, negative prompts, and chain-of-thought won\u2019t guarantee completeness of technical details."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A translation model is inconsistent with formal versus informal tone across languages. Which prompt engineering best practice ensures desired register?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Use chain-of-thought",
      "B": "Provide parallel examples showing formal and informal translations",
      "C": "Increase temperature",
      "D": "Switch to zero-shot"
    },
    "explanation": "Parallel examples teach tone mapping for each register. Chain-of-thought is for reasoning, temperature affects randomness, zero-shot lacks examples."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "To improve factual accuracy in a medical QA assistant, which prompt inclusion is most critical?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Chain-of-thought reasoning steps",
      "B": "Negative prompts against common myths",
      "C": "Cited excerpts from verified medical literature in context",
      "D": "High temperature for varied responses"
    },
    "explanation": "Including verified context anchors responses to factual sources. Chain-of-thought doesn\u2019t ensure factuality, negative prompts exclude myths but don\u2019t supply facts, temperature increases creativity."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "When generating legal contract clauses, you need strict adherence to jurisdictional terms. Which prompt technique enforces this requirement?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Negative prompts excluding non-jurisdiction terms",
      "B": "Zero-shot prompting",
      "C": "Chain-of-thought",
      "D": "Provide clause templates labeled by jurisdiction as few-shot examples"
    },
    "explanation": "Jurisdiction-specific templates as few-shot examples guide strict adherence. Negative prompts and zero-shot offer no structure, chain-of-thought is unrelated."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A multi-turn assistant forgets user preferences across turns. Which prompt engineering practice can maintain memory?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Persist user preference summary in system prompt",
      "B": "Use chain-of-thought prompts",
      "C": "Add more few-shot examples",
      "D": "Increase temperature"
    },
    "explanation": "Including a running summary in the system prompt preserves state across turns. Chain-of-thought, examples, and temperature don\u2019t maintain turn state."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You observe that small changes in wording produce vastly different model outputs. Which prompt engineering mitigation can reduce this sensitivity?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use zero-shot prompting",
      "B": "Provide paraphrased few-shot examples with equivalent meaning",
      "C": "Increase temperature",
      "D": "Apply negative prompts"
    },
    "explanation": "Paraphrased examples teach the model to generalize beyond specific wording. Zero-shot and negative prompts don\u2019t address wording sensitivity, and higher temperature worsens variability."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A customer-facing summary bot truncates long documents prematurely. Which prompt adjustment helps it handle large inputs gracefully?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Lower temperature",
      "B": "Use chain-of-thought",
      "C": "Implement chunking instruction with overlap and summary stitching",
      "D": "Add negative prompts"
    },
    "explanation": "Chunking with overlap and stitching instructions enables processing long documents. Temperature and chain-of-thought don\u2019t address input length, negative prompts exclude content."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "In a classification task, model outputs inconsistent labels for edge cases. Which prompt technique can improve boundary consistency?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provide labeled boundary examples in few-shot format",
      "B": "Use zero-shot prompting",
      "C": "Lower repetition penalty",
      "D": "Include chain-of-thought"
    },
    "explanation": "Boundary examples teach edge-case behavior explicitly. Zero-shot lacks examples, repetition penalty and chain-of-thought don\u2019t enforce classification consistency."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You want the model to generate rebuttals to arguments. Which prompt engineering approach yields persuasive, structured responses?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Chain-of-thought prompting",
      "B": "Use zero-shot only",
      "C": "Negative prompts against informal tone",
      "D": "Provide argumentative outline template with few-shot examples"
    },
    "explanation": "An outline template plus examples structures persuasive rebuttals. Chain-of-thought is reasoning oriented, zero-shot lacks structure, negative prompts don\u2019t guide persuasion."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A summarization model overemphasizes certain topics present in the prompt. How do you reduce prompt bias?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature",
      "B": "Use chain-of-thought",
      "C": "Balance topic representation in prompt context examples",
      "D": "Negative prompts"
    },
    "explanation": "Balancing examples ensures all topics receive equal weight. Temperature, chain-of-thought, and negative prompts don\u2019t correct initial context bias."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An AI writing assistant must follow a company style guide exactly. Which prompt strategy enforces this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Chain-of-thought",
      "B": "Embed style guide rules and sample text in the system prompt",
      "C": "Zero-shot prompting",
      "D": "High temperature"
    },
    "explanation": "Embedding style guide and samples in system prompt enforces compliance. Chain-of-thought is unrelated, zero-shot lacks guidance, temperature affects randomness."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "To debug poor model outputs, you need to trace which prompt phrases influenced errors. Which prompt engineering practice aids this analysis?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use zero-shot prompting",
      "B": "Chain-of-thought",
      "C": "Negative prompts",
      "D": "Log and test variations by isolating single instruction changes"
    },
    "explanation": "Isolating and logging single instruction changes identifies problematic phrases. Other techniques don\u2019t support diagnostic testing."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A model generating product recommendations overfits to popular items. Which prompt modification can diversify suggestions?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Include recent low-popularity item examples in few-shot set",
      "B": "Use zero-shot prompting",
      "C": "Lower max tokens",
      "D": "Negative prompt against popular items"
    },
    "explanation": "Including under-represented examples teaches diversity. Zero-shot and token limits are irrelevant, negative prompts only exclude popular items without positive guidance."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your model misinterprets multi-language prompts and outputs in wrong language. Which prompt engineering best practice corrects this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use chain-of-thought",
      "B": "Use explicit language tags and examples in few-shot prompts",
      "C": "Increase temperature",
      "D": "Negative prompts"
    },
    "explanation": "Language tags with examples clearly instruct desired language. Chain-of-thought, temperature, and negative prompts don\u2019t enforce language choice."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A model used for code reviews misses style guide violations. Which prompt addition ensures style compliance?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Chain-of-thought prompt",
      "B": "Zero-shot instruction",
      "C": "Include style guide checklist and example violations in prompt",
      "D": "Negative prompts"
    },
    "explanation": "Providing checklist and violation examples guides the model to detect style issues. Chain-of-thought, zero-shot, and negative prompts aren\u2019t specific enough."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A technical support bot answers user requests but sometimes reveals internal debug info. Which prompt technique prevents leaking this info?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use negative prompts to forbid internal system messages",
      "B": "Chain-of-thought prompting",
      "C": "Zero-shot prompting",
      "D": "Increase temperature"
    },
    "explanation": "Negative prompts explicitly forbid leakage of system messages. Other techniques don\u2019t address content filtering."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A summarization model must preserve named entities accurately. Which prompt engineering approach helps maintain entity fidelity?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Chain-of-thought",
      "B": "Zero-shot prompting",
      "C": "Negative prompts",
      "D": "Provide few-shot examples emphasizing entity preservation and highlighting changes"
    },
    "explanation": "Examples that highlight correct entity handling teach the model to preserve names. Other methods don\u2019t focus on entity fidelity."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your LLM assistant needs to ask clarifying questions before proceeding on ambiguous user input. Which prompt engineering pattern supports this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Negative prompts",
      "B": "Include conditional branching instructions: \u2018If unclear, ask a follow-up question.\u2019",
      "C": "Zero-shot prompting",
      "D": "Chain-of-thought prompting"
    },
    "explanation": "Conditional instructions ensure the model asks follow-ups when necessary. Other options don\u2019t incorporate dynamic branching logic."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You have a 70B-parameter foundation model pre-trained on general web text. You need to adapt it to a medical domain where data is scarce and highly specialized. Which fine-tuning approach minimizes compute cost while preserving general capabilities?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Full-parameter fine-tuning on the medical corpus",
      "B": "LoRA (low-rank adaptation) on the medical corpus",
      "C": "Instruction tuning using in-context examples without weight updates",
      "D": "Continual pre-training on the medical data only"
    },
    "explanation": "LoRA applies low-rank adapters to update a small subset of parameters, reducing compute cost and preserving most pre-trained weights. Full fine-tuning is costlier; instruction tuning without weight updates doesn\u2019t adapt weights; continual pre-training risks catastrophic forgetting."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "During RLHF of a chatbot, you observe reward hacking: the model produces token sequences that game the reward model without satisfying user intent. What modification to the RLHF pipeline best addresses this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase learning rate to encourage exploration",
      "B": "Add more demonstration examples to the initial supervised fine-tuning",
      "C": "Regularize policy updates by KL-penalty against the supervised fine-tuned model",
      "D": "Remove the reward model and use human ranking directly"
    },
    "explanation": "A KL-penalty (posterior regularization) keeps the policy close to the supervised model, preventing the policy from drifting to reward-hacking behaviors. Higher learning rates worsen hacking, and removing the reward model loses guidance; adding demos helps but doesn\u2019t prevent hacking."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You\u2019re preparing data for fine-tuning an LLM on financial reports. The data contains duplicated boilerplate sections across documents. Which data preparation step is most critical to improve model generalization?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deduplicate boilerplate content before fine-tuning",
      "B": "Augment the dataset with random noise in boilerplate sections",
      "C": "Increase the weight of boilerplate examples in loss computation",
      "D": "Mask boilerplate tokens during training"
    },
    "explanation": "Deduplication prevents the model from overfitting to repeated boilerplate text. Augmentation or weighting boilerplate is counterproductive; masking tokens distorts context."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You need to fine-tune a foundation model for a classification task with only 500 labeled examples. Which transfer learning strategy reduces overfitting risk?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Fine-tune all layers with high learning rate",
      "B": "Freeze embedding layer and fine-tune remaining layers",
      "C": "Freeze all but the classification head and train head only",
      "D": "Use adapter modules inserted in middle layers and train only them"
    },
    "explanation": "Adapter modules allow parameter-efficient fine-tuning, limiting overfitting on small datasets. Training only the head may underfit; freezing embedding and training all other layers still updates many parameters; high learning rates risk divergence."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "For continuous pre-training of an LLM on customer support logs, you must avoid catastrophic forgetting of general language. Which technique helps retain pre-trained knowledge?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a higher learning rate on new data",
      "B": "Mix a percentage of original pre-training data in each batch",
      "C": "Train only the classification head during pre-training",
      "D": "Apply domain\u2010specific tokenization exclusively"
    },
    "explanation": "Mixing pre-training data ensures the model sees general language examples, preventing forgetting. Higher learning rates accelerate forgetting; training only the head doesn\u2019t adapt the core; domain-specific tokenization reduces general coverage."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You want to perform instruction tuning on a foundation model to improve multi-step reasoning. Which prompt design in your supervised dataset best supports chain-of-thought?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Concise question with final answer only",
      "B": "Question plus bullet-point hints",
      "C": "Question with step-by-step reasoning trace leading to the answer",
      "D": "Randomly shuffled question-answer pairs"
    },
    "explanation": "Providing step-by-step reasoning in examples teaches the model chain-of-thought. Final-answer-only examples don\u2019t illustrate reasoning; bullet hints are less explicit; shuffled pairs harm learning."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "When fine-tuning a multilingual foundation model on a single-language corpus, you observe decreased performance in other languages. Which remedy mitigates this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Multi-task fine-tuning: include a small sample of other languages",
      "B": "Increase batch size on the single-language data",
      "C": "Reduce the number of training epochs",
      "D": "Apply language-specific token embeddings only"
    },
    "explanation": "Including other-language samples prevents catastrophic forgetting of those languages. Bigger batches or fewer epochs don\u2019t address forgetting systematically; changing token embeddings alone doesn\u2019t retain old-language knowledge."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your fine-tuning dataset contains sensitive PII fields. Which data governance step is essential before training?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase dataset size to dilute PII instances",
      "B": "Use higher dropout rate during fine-tuning",
      "C": "Deploy guardrails at inference time only",
      "D": "Anonymize or remove PII fields from the training data"
    },
    "explanation": "Removing or anonymizing PII prevents the model from memorizing sensitive data. Dilution or dropout doesn\u2019t guarantee removal; guardrails at inference don\u2019t protect training privacy."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You compare two fine-tuning configurations for the same LLM. Config A uses a small learning rate with many epochs; Config B uses a larger learning rate with fewer epochs. Config B converges faster but overshoots occasionally. What hyperparameter adjustment balances stability and speed?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase batch size further",
      "B": "Reduce the number of layers being updated",
      "C": "Use a cosine learning rate schedule with warm-up",
      "D": "Switch optimizer from AdamW to SGD"
    },
    "explanation": "A cosine learning rate schedule with warm-up allows the learning rate to start low, ramp up, then decay, improving stability while maintaining speed. Changing batch size or layers doesn\u2019t directly control overshoot; SGD is less adaptive than AdamW."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "In RLHF, the reward model is trained on human-labeled comparisons. You notice low inter-annotator agreement on some comparison pairs. Which approach improves reward model performance?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Adjudicate disagreements to create a gold-standard label set",
      "B": "Discard examples with annotator disagreement",
      "C": "Increase the reward model complexity",
      "D": "Train reward model with unfiltered raw labels"
    },
    "explanation": "Adjudicating yields consistent labels, improving reward model learning. Discarding data reduces sample size; more complex model overfits noise; unfiltered labels perpetuate inconsistency."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your foundation model fine-tuning pipeline includes data augmentation by synonym replacement. After training, model outputs are semantically inconsistent. What is the likely cause?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Synonym replacement increased dataset size too much",
      "B": "Context-insensitive synonym substitution broke meaning",
      "C": "The model overfitted to rare words introduced",
      "D": "Embeddings failed to update for replaced tokens"
    },
    "explanation": "Context-insensitive synonym replacement can change meaning and introduce noise, leading to inconsistent outputs. Increased size isn\u2019t harmful per se; overfitting to rare words is possible but secondary; embeddings update normally if tokens appear."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "When fine-tuning a foundation model for domain adaptation, you must decide on dataset representativeness. Which criterion is most critical?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use only the highest-quality examples regardless of frequency",
      "B": "Maximize dataset size even with noisy examples",
      "C": "Balance examples equally across all subtopics",
      "D": "Match fine-tuning data distribution to target domain usage distribution"
    },
    "explanation": "Aligning the data distribution with expected real-world use cases ensures the model learns relevant patterns. Quality alone may bias rarity; size without quality introduces noise; equal subtopic balance may not reflect actual usage."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A fine-tuned model exhibits overconfidence on out-of-distribution inputs. Which fine-tuning modification most effectively calibrates confidence?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Include a calibration dataset with \u2018no-answer\u2019 or random inputs during fine-tuning",
      "B": "Train with label smoothing only on in-distribution data",
      "C": "Reduce model size by pruning layers",
      "D": "Increase learning rate to produce sharper outputs"
    },
    "explanation": "Introducing out-of-distribution or no-answer examples helps the model learn to abstain or lower confidence. Label smoothing alone affects in-distribution predictions; pruning or higher learning rates don\u2019t improve calibration."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "For continuous pre-training, the streaming customer data arrives hourly. You want to update the base model daily without interruption. Which strategy ensures efficient updates?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Stop service and reload updated model every 24h",
      "B": "Use rolling A/B deployment with canary releases",
      "C": "Fine-tune in-place without versioning",
      "D": "Accumulate a week\u2019s data and update weekly"
    },
    "explanation": "Rolling A/B with canary releases allows daily updates with minimal service interruption. In-place fine-tuning risks model instability; weekly updates lag; full service downtime is unacceptable."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You plan to instruction-tune a model with mixed formats: some examples are dialogues, others are bullet lists. After training, the model only outputs dialogues. Why?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Learning rate was too high, favoring longer sequences",
      "B": "Batch size was too small, underfitting bullet examples",
      "C": "Class imbalance: dialogue examples outnumber bullets",
      "D": "Optimizer bias toward dialog tokens"
    },
    "explanation": "When one format dominates the dataset, the model learns that format preferentially. High learning rates or optimizer choice aren\u2019t format-specific issues; batch size alone won\u2019t override class imbalance."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your reward model training uses pairwise ranking data collected via crowdsourcing. Labelers misunderstand the ranking task, flipping preferences inconsistently. Which step should you add to your pipeline?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Qualification tests for labelers with known comparisons",
      "B": "Increase batch size during reward model training",
      "C": "Switch to absolute scoring instead of ranking",
      "D": "Reduce model complexity to prevent overfitting noisy labels"
    },
    "explanation": "Qualification tests ensure labelers understand the task, improving data quality. Batch size and model complexity don\u2019t fix labeler misunderstanding; absolute scoring changes task but doesn\u2019t guarantee understanding."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You need to fine-tune a foundation model for question answering using in-domain FAQ pairs only. The model sometimes hallucinates plausible but incorrect answers. Which data-driven fix reduces hallucination?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add random distractor questions to training",
      "B": "Increase temperature during fine-tuning",
      "C": "Train with higher dropout rate",
      "D": "Include negative examples: questions with 'no answer' label"
    },
    "explanation": "Including negative examples teaches the model to respond with 'no answer' rather than hallucinate when data is missing. Distractors and dropout don\u2019t address hallucination; temperature affects inference, not training."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "In domain adaptation, you notice that certain rare entity types are underrepresented and the model fails to recognize them. Which technique improves rare entity recognition without collecting more data?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase fine-tuning epochs",
      "B": "Use a higher learning rate for rare entity tokens",
      "C": "Apply oversampling of rare-entity examples in fine-tuning batches",
      "D": "Prune embeddings for common tokens"
    },
    "explanation": "Oversampling rare-entity examples ensures the model sees them more often, improving recognition. Higher epochs or learning rates risk overfitting; pruning embeddings harms overall performance."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your fine-tuning job on SageMaker is slow and expensive due to full-model updates. Which SageMaker feature can reduce training cost and time?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Neo for compilation",
      "B": "Use SageMaker distributed data parallel with adapter-enabled training",
      "C": "Switch to larger instances for faster throughput",
      "D": "Use automatic model tuning"
    },
    "explanation": "Using parameter-efficient training with adapter modules and SageMaker's distributed data parallel reduces compute by updating fewer parameters. Neo is for inference; larger instances increase cost; automatic tuning optimizes hyperparameters, not model size."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You observe the fine-tuned model performance plateaus early. Analysis shows that gradient norms explode after epoch 2. Which remedy is most appropriate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Implement gradient clipping",
      "B": "Use a larger batch size",
      "C": "Remove weight decay",
      "D": "Decrease model depth"
    },
    "explanation": "Gradient clipping prevents exploding gradients, stabilizing training. Larger batches may worsen instability; removing weight decay doesn't address gradients; changing depth isn't a tuning direct remedy."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You fine-tune a multilingual LLM using data from multiple dialects. The model favors majority dialect outputs. Which method ensures fair representation?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase overall dataset size",
      "B": "Use higher dropout on majority dialect data",
      "C": "Train with higher learning rate on minority dialects",
      "D": "Assign sampling weights inversely proportional to dialect frequency"
    },
    "explanation": "Inverse-frequency sampling balances representation so minority dialects are seen as often as majority ones. Other methods don\u2019t systematically correct bias."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your continuous pre-training pipeline must comply with GDPR. Which data governance practice must you implement?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train only on encrypted data",
      "B": "Use privateLink for model hosting",
      "C": "Maintain data lineage and consent records for each example",
      "D": "Apply differential privacy at inference time"
    },
    "explanation": "GDPR requires traceability of personal data origin and consent; data lineage and consent records ensure compliance. Encryption and PrivateLink help security but not consent; inference privacy doesn\u2019t cover training compliance."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You aim to reduce carbon footprint of repeated fine-tuning experiments. Which strategy has greatest impact?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use larger GPUs to shorten training time",
      "B": "Use parameter-efficient fine-tuning methods",
      "C": "Increase number of epochs to convergence",
      "D": "Train on spot instances only"
    },
    "explanation": "Parameter-efficient fine-tuning (adapters, LoRA) reduces compute cycles substantially, cutting energy use. Larger GPUs helps but still full-model training; more epochs increase carbon; spot instances affect cost, not total energy."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You need to incorporate new regulatory guidelines annually into your foundation model. Which fine-tuning schedule best balances freshness and cost?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Semi-annual fine-tuning on guidelines plus a small weekly replay buffer of older data",
      "B": "Monthly full-data fine-tuning",
      "C": "Ad-hoc fine-tuning only when guidelines change",
      "D": "Continuous daily fine-tuning with only new guidelines"
    },
    "explanation": "Semi-annual comprehensive fine-tuning with weekly replay prevents forgetting and limits compute. Monthly full fine-tuning is costly; ad-hoc risks staleness; daily tuning is overkill and costly."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "During supervised fine-tuning, your loss decreases but evaluation accuracy on domain tasks stagnates. Which hypothesis and fix are most plausible?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Data leakage\u2014shuffle training data",
      "B": "Optimizer misconfiguration\u2014switch from AdamW to SGD",
      "C": "Overfitting common patterns\u2014introduce early stopping based on validation",
      "D": "Model too small\u2014switch to larger foundation model"
    },
    "explanation": "Decreasing loss with stagnant accuracy indicates overfitting; early stopping on validation halts training before overfitting. Data leakage causes high eval scores; optimizer change alone doesn\u2019t address overfitting; larger model exacerbates issue."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You want students to reproduce your fine-tuning results. Which practice ensures experiment reproducibility?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Publish final model artifacts only",
      "B": "Increase random seed variation",
      "C": "Use dynamic learning rate schedules",
      "D": "Log hyperparameters, seed values, code, and data versions in experiment tracking"
    },
    "explanation": "Comprehensive logging of hyperparameters, seeds, code, and data versions enables exact reproduction. Publishing only artifacts omits process; varying seeds reduces reproducibility; dynamic schedules must be recorded to be reproducible."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "What metric would you choose to evaluate the instruction-tuned model\u2019s step-by-step reasoning quality?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "BLEU score on final answer only",
      "B": "Perplexity on the prompt dataset",
      "C": "Fraction of steps where human evaluation rates reasoning correct",
      "D": "Token generation speed"
    },
    "explanation": "Human evaluation of reasoning steps captures quality of chain-of-thought. BLEU on final answer ignores reasoning; perplexity measures fluency; speed irrelevant to reasoning quality."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your fine-tuning job on Amazon SageMaker fails due to out-of-memory errors on 8-GPU instances. Which solution reduces memory footprint without degrading performance?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable gradient checkpointing",
      "B": "Reduce batch size to 1",
      "C": "Disable mixed-precision training",
      "D": "Increase model parallelism chunks to more GPUs"
    },
    "explanation": "Gradient checkpointing trades compute for memory, allowing larger models to train without OOM. Batch size 1 slows convergence; disabling mixed precision increases memory; more GPUs adds cost and complexity but checkpointing is first fix."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You must fine-tune a foundation model while ensuring fairness across demographic groups. Which data preparation tactic helps?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Oversample majority group examples",
      "B": "Use stratified sampling to balance demographic representation",
      "C": "Mask demographic tokens during training",
      "D": "Train separate models per demographic"
    },
    "explanation": "Stratified sampling balances training data across demographics. Oversampling majority worsens imbalance; masking tokens removes context; separate models increase maintenance and complicate fairness across groups."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "In your RLHF pipeline, you need to efficiently generate rollouts for policy optimization. Which technique reduces computation without sacrificing policy quality?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use full-length trajectories for all episodes",
      "B": "Increase rollout batch size indefinitely",
      "C": "Train policy and value networks jointly with single head",
      "D": "Use off-policy sampling from a replay buffer with prioritized experiences"
    },
    "explanation": "Off-policy prioritized replay focuses on informative experiences, reducing the number of rollouts needed. Full-length trajectories are heavy; batch size increase has diminishing returns; joint training doesn\u2019t address rollout efficiency directly."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A legal firm uses a foundation model to summarize complex contracts. The firm requires summaries that preserve legal semantics rather than exact wording. Which automatic evaluation metric should be prioritized to measure semantic similarity?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ROUGE-L",
      "B": "BLEU",
      "C": "BERTScore",
      "D": "Perplexity"
    },
    "explanation": "BERTScore measures semantic similarity using contextual embeddings, making it more suited to capture meaning preservation than surface overlap metrics like ROUGE or BLEU."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A developer fine-tunes a code generation foundation model and wants to measure the percentage of generated functions that compile and pass unit tests. Which evaluation metric best captures this requirement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "BLEU",
      "B": "pass@k",
      "C": "Cyclomatic complexity",
      "D": "Perplexity"
    },
    "explanation": "pass@k directly measures the fraction of samples that pass testing under k attempts, reflecting functional correctness rather than surface similarity or code complexity."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You need to benchmark two multilingual translation foundation models across multiple language pairs. Which dataset and metric combination is most appropriate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "WMT and BLEU",
      "B": "GLUE and ROUGE",
      "C": "SQuAD and BERTScore",
      "D": "COCO and METEOR"
    },
    "explanation": "The WMT (Workshop on Machine Translation) dataset with BLEU is the industry standard for evaluating machine translation quality across language pairs."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "When evaluating paraphrased customer support responses, which metric accounts for synonym usage and paraphrase quality over exact word overlap?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "BLEU",
      "B": "ROUGE",
      "C": "Exact match",
      "D": "METEOR"
    },
    "explanation": "METEOR incorporates stemming and synonym matching, making it better for paraphrase evaluation than surface-overlap metrics like BLEU or ROUGE."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A summarization model\u2019s outputs must be easy for non-expert readers. Which evaluation metric specifically measures readability?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ROUGE",
      "B": "Flesch-Kincaid score",
      "C": "BERTScore",
      "D": "Perplexity"
    },
    "explanation": "The Flesch-Kincaid score quantifies text readability based on sentence and word length, unlike ROUGE or BERTScore which measure overlap or semantics."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A customer service chatbot built on a foundation model should actually resolve user inquiries. Which business metric aligns best with this objective?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "BLEU",
      "B": "Average turn length",
      "C": "Perplexity",
      "D": "Resolution rate"
    },
    "explanation": "Resolution rate directly measures the proportion of conversations that successfully resolve user issues, reflecting business value."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "To detect hallucinations in a fact-based generation task, which evaluation approach is most effective?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Measure BLEU",
      "B": "Measure ROUGE",
      "C": "Use QAGS for factual consistency",
      "D": "Compute perplexity"
    },
    "explanation": "QAGS evaluates factual consistency by comparing generated claims against source documents, while BLEU/ROUGE/perplexity do not assess factual correctness."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Given a limited evaluation budget but a need for an automatic metric that correlates well with human judgment on summarization quality, which metric should you choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "BLEU",
      "B": "BERTScore",
      "C": "ROUGE",
      "D": "Perplexity"
    },
    "explanation": "Studies show BERTScore correlates more closely with human judgments of semantic quality than BLEU or ROUGE for summarization."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "In assessing the diversity of creative text generations from a foundation model, which metric helps quantify n-gram variation across outputs?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Self-BLEU",
      "B": "BLEU",
      "C": "ROUGE",
      "D": "BERTScore"
    },
    "explanation": "Self-BLEU measures diversity by computing BLEU of each output against other outputs; lower Self-BLEU indicates greater diversity."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A marketing team uses a foundation model to draft ad copy. To evaluate impact on business outcomes, which metric is the most direct indicator?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ROUGE",
      "B": "Perplexity",
      "C": "Click-through rate (CTR)",
      "D": "Accuracy"
    },
    "explanation": "CTR measures how often users click on ads, directly reflecting marketing effectiveness unlike text overlap or perplexity scores."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "When summarization outputs must not exceed 100 words consistently, which evaluation metric measures length compliance across the dataset?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ROUGE",
      "B": "BLEU",
      "C": "Temperature",
      "D": "Compression ratio"
    },
    "explanation": "Compression ratio (output length/reference length) and explicit length checks quantify adherence to word limits, unlike ROUGE or BLEU."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "To test whether an observed improvement in BLEU score between two model versions is statistically significant, which method should you apply?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Bootstrap resampling for confidence intervals",
      "B": "Paired t-test",
      "C": "ANOVA",
      "D": "Single-sample z-test"
    },
    "explanation": "Bootstrap resampling computes confidence intervals for BLEU differences without assuming normality, which is preferable for text metrics."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "For a retrieval-augmented generation (RAG) application, which pair of evaluation metrics best measure retrieval accuracy and subsequent generation quality?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "BLEU and perplexity",
      "B": "Recall@k and ROUGE",
      "C": "Accuracy and F1",
      "D": "Precision and recall"
    },
    "explanation": "Recall@k evaluates how often relevant documents are retrieved, and ROUGE assesses the quality of the generated text given those documents."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which evaluation strategy specifically quantifies the rate of hallucination occurrences in generated outputs?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Calculate BLEU",
      "B": "Compute ROUGE",
      "C": "Measure perplexity",
      "D": "Human annotation to label hallucinated statements"
    },
    "explanation": "Only human annotation can reliably identify and quantify hallucinations; automated overlap or likelihood metrics cannot detect factual errors."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which metric penalizes repetition in generated text by measuring the proportion of unique n-grams?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ROUGE",
      "B": "Perplexity",
      "C": "distinct-n",
      "D": "BLEU"
    },
    "explanation": "distinct-n computes the ratio of unique n-grams to total n-grams, penalizing repetitive outputs unlike ROUGE or BLEU."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which benchmark dataset would you choose to evaluate a foundation model\u2019s question-answering performance on general topics?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SQuAD",
      "B": "GLUE",
      "C": "WMT",
      "D": "COCO"
    },
    "explanation": "SQuAD is designed for extractive question answering across diverse topics, while GLUE is for classification, WMT for translation, and COCO for images."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "To monitor drift in a deployed foundation model\u2019s outputs over time, which evaluation approach is most appropriate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Cross-validation with multiple folds",
      "B": "Periodic human review of sampled outputs",
      "C": "Single-run perplexity measurement",
      "D": "Monitoring parameter count"
    },
    "explanation": "Periodic human review identifies qualitative shifts or biases in outputs that automated metrics might miss, making it essential for drift detection."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "For evaluating translation at the character level to better handle morphologically rich languages, which metric should you use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "BLEU",
      "B": "ROUGE",
      "C": "FID",
      "D": "chrF"
    },
    "explanation": "chrF calculates F-scores over character n-grams, improving evaluation for languages with complex morphology compared to word-based metrics."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "In cross-lingual summarization tasks, which automatic metric leverages multilingual embeddings to assess semantic similarity?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Perplexity",
      "B": "BLEU",
      "C": "BERTScore",
      "D": "METEOR"
    },
    "explanation": "Multilingual BERTScore uses contextual embeddings across languages for semantic evaluation, outperforming BLEU or METEOR on cross-lingual tasks."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which evaluation procedure ensures that improvements in ROUGE score are due to true model gains and not sample variance?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Cross-validation with multiple folds",
      "B": "Single-run on a fixed test set",
      "C": "Increasing beam size during decoding",
      "D": "Lowering the generation temperature"
    },
    "explanation": "Cross-validation over multiple folds reduces variance by averaging performance across diverse data splits, validating genuine improvements."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "When evaluating generative image models, which metric should you use to assess both fidelity and diversity of outputs?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "BLEU",
      "B": "Fr\u00e9chet Inception Distance (FID)",
      "C": "ROUGE",
      "D": "BERTScore"
    },
    "explanation": "FID measures the distance between feature distributions of real and generated images, capturing both diversity and quality."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A product team uses user engagement metrics to assess a chat model\u2019s improvements. Which combination best reflects user satisfaction?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "BLEU and ROUGE",
      "B": "Perplexity and BERTScore",
      "C": "Hit rate and recall",
      "D": "Net Promoter Score (NPS) and average session length"
    },
    "explanation": "NPS surveys gauge satisfaction and loyalty, while session length indicates engagement\u2014together they reflect real user experience improvements."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "To evaluate whether a fine-tuned LLM answers domain-specific queries accurately, which evaluation dataset style should be used?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "GLUE",
      "B": "WMT",
      "C": "In-domain benchmark dataset with labeled answers",
      "D": "COCO"
    },
    "explanation": "An in-domain benchmark with labeled answers ensures evaluation reflects the specific content and difficulty of target queries."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which automatic evaluation metric is least reliable for open-ended text generation tasks, often showing poor correlation with human judgment?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Perplexity",
      "B": "BLEU",
      "C": "Human evaluation",
      "D": "ROUGE"
    },
    "explanation": "Perplexity measures model confidence, not output quality, and often correlates weakly with human assessments of open-ended generations."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which metric can identify shifts in output token distribution between two model versions by measuring distributional divergence?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "BLEU",
      "B": "KL divergence of n-gram distributions",
      "C": "ROUGE",
      "D": "Perplexity"
    },
    "explanation": "KL divergence quantifies how one probability distribution diverges from another, making it suitable for detecting distribution shifts."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "To evaluate factual consistency automatically, which specialized tool or metric should you use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Perplexity",
      "B": "BLEU",
      "C": "ROUGE",
      "D": "FactCC"
    },
    "explanation": "FactCC uses a classifier to detect factual errors in generated text, providing an automated measure of factual consistency."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which evaluation approach measures the trade-off between generation latency and text quality in a foundation model?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "BLEU alone",
      "B": "Log-likelihood",
      "C": "Plot BLEU against average response time",
      "D": "Perplexity"
    },
    "explanation": "Plotting BLEU versus response time reveals how quality changes with latency, enabling balanced performance optimization."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "To evaluate a multilingual retrieval-augmented generation system, which combined metrics should be used for retrieval effectiveness and generated text quality?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "MRR and BERTScore",
      "B": "BLEU and Perplexity",
      "C": "FID and ROUGE",
      "D": "Accuracy and F1"
    },
    "explanation": "MRR measures retrieval ranking quality, and BERTScore evaluates semantic quality of the generated text in multiple languages."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which evaluation technique reduces annotation bias in human assessments of generated outputs?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Rate outputs with known prompts visible",
      "B": "Use leaderboard scores only",
      "C": "Remove human evaluators",
      "D": "Blind evaluation of outputs without prompts"
    },
    "explanation": "Blind evaluation, where evaluators don\u2019t see prompts or model identity, minimizes bias toward known systems or prompts."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "When constructing a summarization benchmark for reliable performance evaluation, which dataset characteristic is most critical?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Minimal dataset size",
      "B": "Domain diversity in reference summaries",
      "C": "Single reference per example",
      "D": "Unlabeled references"
    },
    "explanation": "Domain diversity in references ensures the benchmark covers varied content, making evaluation results generalizable across use cases."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A financial services company uses a SageMaker Clarify pre-training bias analysis and finds a high Statistical Parity Difference (SPD) for a protected group. Which next step best mitigates bias at the dataset level?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Apply class weighting during model training to penalize misclassifications for the protected group.",
      "B": "Inject synthetic samples of the protected group after model training to balance outputs.",
      "C": "Use stratified sampling or data augmentation to increase representation of the under-represented group before retraining.",
      "D": "Adjust the decision threshold post-inference to equalize positive rates across groups."
    },
    "explanation": "Mitigating SPD bias at the dataset level requires increasing representation via stratified sampling or augmentation prior to training. Other options address bias post-training or at inference."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An e-commerce retailer is concerned about model overfitting and lack of robustness when training a computer vision model. Which combination of responsible AI practices most directly addresses variance risk?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Model Monitor to detect drift and apply differential privacy during training.",
      "B": "Implement RLHF and use Bedrock Guardrails to enforce safety constraints.",
      "C": "Apply data pruning and use a larger transformer foundation model for high accuracy.",
      "D": "Augment training images for diversity and use k-fold cross-validation to measure generalization."
    },
    "explanation": "Augmenting data increases diversity reducing variance, while k-fold cross-validation assesses generalization. Other options address drift, privacy, or performance but not variance."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A legal team warns that a fine-tuned foundation model may produce outputs infringing third-party IP. Which responsible AI control best reduces IP infringement risk?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Implement content filtering using Bedrock Guardrails to block copyrighted text generation.",
      "B": "Use SageMaker Clarify to identify copyright in the training data.",
      "C": "Apply differential privacy during fine-tuning to obscure original data.",
      "D": "Deploy Model Monitor to detect IP patterns in inference logs."
    },
    "explanation": "Bedrock Guardrails enforce rules at generation time to block copyrighted outputs. Clarify and privacy techniques address bias/data privacy, not IP output control."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A model deployed on SageMaker shows a rising False Positive Rate (FPR) for a minority group over time. Which AWS capability and metric should the team use to detect this fairness drift?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Clarify post-training: monitor AUC separately for each group.",
      "B": "SageMaker Model Monitor: configure a bias drift job to track group-specific FPR over time.",
      "C": "Amazon A2I: route predictions for the minority group to human reviewers.",
      "D": "SageMaker Debugger: set up rule to detect increased loss for the minority group."
    },
    "explanation": "Model Monitor\u2019s bias drift jobs can track fairness metrics like group-specific FPR. Clarify is offline, A2I routes for human review but does not detect drift automatically, Debugger tracks training metrics."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An organization must document data lineage and responsible AI decisions across the ML pipeline. Which AWS service combination provides end-to-end lineage and audit capabilities?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Config to track resource changes and SageMaker Clarify for bias reports.",
      "B": "AWS CloudTrail for API activity and Amazon Macie for data classification.",
      "C": "SageMaker Model Monitor for drift and Amazon Inspector for vulnerabilities.",
      "D": "AWS Glue Data Catalog for metadata lineage and AWS CloudTrail for audit logs."
    },
    "explanation": "Glue Data Catalog captures dataset metadata and lineage; CloudTrail logs API calls and workflow steps for audits. Other options address security or bias reporting but not full lineage."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A healthcare provider needs to ensure its model does not discriminate by gender. They require transparency of feature impact. Which approach best delivers explainability and bias detection?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon A2I to have clinicians label misclassifications by gender.",
      "B": "Train with differential privacy to avoid encoding gender information.",
      "C": "Use SageMaker Clarify\u2019s SHAP explainability to measure gender\u2019s feature importance and bias metrics.",
      "D": "Configure Bedrock Guardrails to block gender-related outputs."
    },
    "explanation": "Clarify SHAP provides feature impact and bias metrics for sensitive attributes. A2I handles human review, privacy training obfuscates but not explains, Guardrails block outputs, not explain model internals."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A startup chooses between a large foundation model and a smaller custom model. To minimize environmental impact while meeting 95% of their accuracy target, which responsible AI strategy should they apply?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use the large foundation model with reduced precision (FP16) and no fine-tuning.",
      "B": "Benchmark both models for energy consumption per inference and select the smaller custom model if its accuracy meets the target.",
      "C": "Host the large model on GPU-powered instances with auto-scaling to reduce idle time.",
      "D": "Apply on-device inference to the foundation model to eliminate data center emissions."
    },
    "explanation": "Benchmarking energy per inference and choosing the smallest model that meets accuracy minimizes environmental impact. Precision and hosting strategies affect cost/performance but don't directly measure impact."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A retail company implements RAG with Bedrock for QA but fears hallucinations. Which responsible AI control at runtime best reduces hallucination risk?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable SageMaker Clarify\u2019s hallucination detection post-inference.",
      "B": "Use Amazon A2I to route uncertain answers for human verification.",
      "C": "Fine-tune the foundation model on curated domain data.",
      "D": "Implement Bedrock Guardrails with citation requirements to only respond with source-linked information."
    },
    "explanation": "Guardrails can enforce citations and block unsupported content, directly addressing hallucinations at runtime. Clarify does not detect hallucinations, A2I reviews but doesn\u2019t prevent, fine-tuning reduces but doesn\u2019t eliminate."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A government agency needs to ensure ML model decisions are explainable to comply with regulations. Which AWS tool and practice combination supports this requirement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Clarify to generate Model Cards and SHAP explainability reports.",
      "B": "Leverage Amazon A2I to have domain experts annotate each decision.",
      "C": "Deploy foundation models via Bedrock with no customization for transparency.",
      "D": "Store all inference logs in Amazon S3 for manual auditing later."
    },
    "explanation": "Clarify Model Cards and SHAP reports provide structured model documentation and feature-level explainability. A2I reviews specific instances but not full model transparency; logs alone aren\u2019t sufficient."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A fraud detection model exhibits high variance yet low bias. Which dataset characteristic adjustment most directly reduces variance?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Ensure the dataset is demographically representative by adding minority samples.",
      "B": "Increase overall dataset size by collecting more varied fraudulent/non-fraudulent examples.",
      "C": "Remove features highly correlated with the target variable to simplify the model.",
      "D": "Adjust the decision threshold to favor reducing false positives."
    },
    "explanation": "High variance is reduced by increasing dataset size and diversity. Adding minority samples addresses bias. Feature removal and threshold adjustments address complexity and operating point, not variance."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "During bias detection, a team discovers a proxy variable highly correlated with sensitive attribute. Which responsible AI action best addresses this issue?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Mask the sensitive attribute at inference to prevent discrimination.",
      "B": "Use RLHF to teach the model fairness constraints.",
      "C": "Remove or de-correlate the proxy variable from the training dataset.",
      "D": "Adjust class weights during training to counter the proxy effect."
    },
    "explanation": "Removing or de-correlating the proxy variable prevents the model from learning unintended sensitive correlations. Masking at inference doesn\u2019t retrain, RLHF isn\u2019t designed for bias removal, class weights address imbalance but not proxy variables."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A voice assistant model trained on English data shows poor performance and bias when used in a non-English region. Which dataset curation practice most responsibly addresses this outcome?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy an English foundation model with higher capacity GPUs to improve recognition.",
      "B": "Use Bedrock Guardrails to translate non-English inputs to English before inference.",
      "C": "Apply post-processing to normalize accents in the English transcripts.",
      "D": "Collect and include diverse voice data from the target region to retrain or fine-tune the model."
    },
    "explanation": "Collecting and including regional voice data ensures diversity and reduces bias. Other options patch translation or normalization without addressing dataset deficiency."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An NLP model fine-tuned on user reviews shows unexpected bias towards negative sentiment for a demographic. Which Clarify metric pair best identifies both bias and explainability issues?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Post-training bias analysis with Disparate Impact and SHAP feature importance for demographic tokens.",
      "B": "Pre-training bias analysis with Chi-squared test and LIME for local explanations.",
      "C": "Model monitor with data quality checks and concept drift detection.",
      "D": "A2I simulation to route negative sentiment for human audit and RLHF to adjust behavior."
    },
    "explanation": "Post-training bias analysis with Disparate Impact quantifies bias; SHAP explains feature contributions. Pre-training analysis doesn\u2019t reflect model; other options don\u2019t cover both bias and explainability."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A social media platform wants to ensure its content recommendation model is robust and fair. Which multi-step approach correctly applies responsible AI practices?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy bedrock foundation model with no customization, then monitor user complaints.",
      "B": "Use diverse training data, perform Clarify bias analysis, then implement Model Monitor for drift and fairness metrics.",
      "C": "Apply RLHF to incorporate user feedback, then use A2I to human-review flagged recommendations.",
      "D": "Use Amazon Comprehend to detect hate speech and apply Bedrock Guardrails to block content."
    },
    "explanation": "Diverse data plus Clarify bias analysis and Model Monitor drift checks covers training data fairness, bias validation, and ongoing monitoring. Other options focus only on feedback loops or content filtering."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An energy company must choose between batch and real-time inference to reduce carbon footprint. Which responsible AI consideration should guide their choice?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Real-time inference ensures fairness by updating model with live data.",
      "B": "Batch inference simplifies explainability by aggregating predictions.",
      "C": "Batch inference on low-usage off-peak times reduces energy consumption compared to always-on real-time endpoints.",
      "D": "Real-time inference allows Guardrails to block hallucinations instantly."
    },
    "explanation": "Batch inference during off-peak reduces energy use, aligning with sustainability goals. Real-time addresses performance and safety but uses constant compute."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A hiring platform\u2019s resume screening model shows high false negative rates for a protected group. Which fairness metric adjustment and mitigation step is appropriate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Optimize for equalized odds by balancing FPR across groups via post-processing calibration.",
      "B": "Target equal opportunity by adjusting threshold to equalize True Positive Rate (TPR) for the protected group.",
      "C": "Use DP-SGD during training to ensure differential privacy for all candidates.",
      "D": "Deploy Clarify\u2019s pre-training bias report and retrain on a larger dataset."
    },
    "explanation": "Equal opportunity focuses on equal TPR; adjusting thresholds post-training targets false negatives. Equalized odds balances both FPR and FNR; DP-SGD addresses privacy, not fairness; pre-training bias doesn\u2019t guarantee TPR parity."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A model serving medical diagnoses must be auditable for bias and quality. Which combination of AWS services automates bias detection and logging at inference time?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Clarify with server-side inference wrappers and CloudWatch Logs for real-time bias metrics.",
      "B": "Amazon A2I for human-in-the-loop and S3 event notifications for logs.",
      "C": "Model Monitor batch jobs scheduled hourly and AWS Config for configuration logs.",
      "D": "Bedrock Guardrails for diagnosis validation and CloudTrail for API logs."
    },
    "explanation": "Combining Clarify in inference wrappers emits bias metrics, and CloudWatch Logs captures them. Model Monitor process is offline. A2I is human review, not automatic detection. Guardrails validate content but not bias metrics."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A bank uses a decision tree model that is transparent but underperforms. Which trade-off should the team consider for responsible AI?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch to a black-box model and rely on post-hoc explanations to boost accuracy.",
      "B": "Remove explainability requirements to allow complex ensemble models.",
      "C": "Implement RLHF to retrain the decision tree on dynamic feedback.",
      "D": "Balance interpretability and performance by exploring a surrogate model or shallow ensemble with built-in explanations."
    },
    "explanation": "Surrogate or shallow ensemble allows slightly higher complexity with interpretability. Fully black-box loses transparency. Removing explainability breaks compliance. RLHF is for generative models, not decision trees."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A company trains a sentiment analysis model on customer feedback. They discover the model overfits on lengthier negative reviews. Which responsible AI technique addresses this bias?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use A2I to filter out long negative reviews during inference.",
      "B": "Implement Bedrock Guardrails to normalize output sentiment scores.",
      "C": "Apply length-based sampling or reweighting in the training set to balance review lengths.",
      "D": "Fine-tune the model further with more negative samples."
    },
    "explanation": "Balancing training samples by review length addresses overfitting to long texts. A2I and Guardrails act at inference; more negative samples reinforce, not fix length bias."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A chatbot built with Bedrock must avoid generating unsafe content. Which responsible AI practice at the design stage is most effective?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Clarify to detect bias in the training corpus.",
      "B": "Ingest and label unsafe prompt examples, then configure Guardrails to block or redirect those scenarios.",
      "C": "Deploy Model Monitor to detect unsafe outputs after production.",
      "D": "Enable Amazon WAF to filter content returned by the chatbot."
    },
    "explanation": "Labeling unsafe prompts upfront and configuring Guardrails prevents unsafe content generation. Clarify addresses bias, Monitor is reactive, WAF filters at network level, not semantics."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An AI image classification pipeline uses Amazon A2I. Which workflow change enhances responsible AI by reducing human bias in reviews?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provide diverse annotator pools and rotate assignments to minimize individual bias.",
      "B": "Only route low-confidence images for review without systematic sampling.",
      "C": "Use fixed guidelines without updating them to ensure consistency.",
      "D": "Increase the AI confidence threshold to reduce human interventions."
    },
    "explanation": "Diverse annotators and rotation reduce individual biases. Routing only low-confidence may miss systematic errors; fixed guidelines become outdated; raising threshold reduces oversight."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A transportation firm must ensure their ML model aligns with environmental sustainability guidelines. Which metric and AWS feature should they monitor?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Track energy consumption per batch inference via SageMaker Debugger and AWS CloudWatch metrics.",
      "B": "Monitor carbon emission estimates from Bedrock Guardrails.",
      "C": "Use Clarify to report environmental bias in the dataset.",
      "D": "Implement A2I to sample inferences for manual sustainability audits."
    },
    "explanation": "SageMaker Debugger profiles training/inference resource usage; CloudWatch aggregates metrics, enabling monitoring of energy consumption. Guardrails and Clarify don\u2019t provide sustainability metrics."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An AI system must comply with an accountability law requiring proof of non-discrimination. Which deliverable best demonstrates compliance?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Stored inference logs in S3 with anonymized user IDs.",
      "B": "A2I human review transcripts for flagged inferences.",
      "C": "Generated Model Card from SageMaker Clarify including bias metrics and mitigation steps.",
      "D": "Bedrock Guardrails configuration file showing blocked content rules."
    },
    "explanation": "A Model Card documents model details, bias metrics, and mitigation, serving as formal proof. Logs and reviews are raw data; Guardrails config shows rules but not measured bias."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A retail AI solution applies oversampling of under-represented class, causing overfitting. Which strategy maintains dataset balance while reducing overfitting risk?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch to SMOTE to generate synthetic minority samples without real data.",
      "B": "Combine controlled undersampling of majority and slight oversampling of minority, followed by cross-validation.",
      "C": "Use AWS Glue DataBrew to mask minority class labels during training.",
      "D": "Increase model regularization hyperparameters until overfitting subsides."
    },
    "explanation": "Balanced undersampling and moderate oversampling reduces class imbalance and overfitting; SMOTE can introduce noise; masking labels corrupts data; regularization alone may not address imbalance."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A company fine-tunes a foundation model on user data containing PII. Which responsible AI step ensures privacy compliance before fine-tuning?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Clarify to detect PII entities in training data and remove or anonymize them.",
      "B": "Enable Bedrock Guardrails to block PII during inference.",
      "C": "Configure Model Monitor to alert on PII patterns in logs.",
      "D": "Rely on Data Catalog policies to quarantine sensitive buckets."
    },
    "explanation": "Clarify PII detection can identify and remove/anonymize PII pre-training. Guardrails and Model Monitor act post-deployment; Data Catalog policies manage storage, not data content."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A model shows underfitting due to overly strict fairness constraints during training. Which mitigation balances fairness with performance?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Remove all fairness constraints and retrain on raw data.",
      "B": "Increase penalty weight on fairness loss to reduce bias further.",
      "C": "Apply differential privacy to reduce model complexity.",
      "D": "Relax fairness constraint hyperparameter slightly and monitor performance vs bias trade-off."
    },
    "explanation": "Relaxing constraint hyperparameter allows model to trade a bit more bias for performance. Removing constraints violates fairness; increasing penalty worsens underfitting; privacy doesn\u2019t address fairness/performance trade-off."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A content moderation model must detect emerging hate speech terms. Which AWS service and practice support ongoing dataset curation?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Clarify analyzers to detect new terms in training data.",
      "B": "Implement Amazon Kendra on user reports to discover new terms, then retrain model periodically.",
      "C": "Configure Bedrock Guardrails to ban any unknown terms automatically.",
      "D": "Rely on Model Monitor drift detection to alert on lexical shifts."
    },
    "explanation": "Kendra can index user reports to surface new terms; human in the loop identifies them for dataset updates. Clarify doesn\u2019t detect emergent terms; Guardrails ban known rules; drift detection alerts but requires manual term discovery."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which combination of AWS services and tasks demonstrates end-to-end responsible AI for a fraud detection pipeline?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Glue Data Catalog for lineage, SageMaker Clarify for bias analysis, Model Monitor for drift, and CloudTrail for audit logs.",
      "B": "Amazon A2I for human review, Bedrock for model serving, WAF for security, and Cost Explorer for cost analytics.",
      "C": "AWS Config for configuration management, Amazon Comprehend for NLP, RDS for storage, and Inspector for vulnerability scanning.",
      "D": "SageMaker JumpStart for model templates, S3 for data storage, CloudFront for delivery, and Macie for data classification."
    },
    "explanation": "Glue Data Catalog tracks data lineage; Clarify performs bias analysis; Model Monitor checks drift; CloudTrail logs activities, covering responsible AI. Other combinations miss key responsibilities."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A model\u2019s predictions must be fair across subgroups; the team chooses Demographic Parity. Which operational step ensures this fairness definition is maintained?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Implement RLHF to shape model behavior according to parity.",
      "B": "Add a bias-aware post-processing component to adjust prediction probabilities to match subgroup rates.",
      "C": "Use differential privacy during training to equalize distributions.",
      "D": "Apply Bedrock Guardrails to filter outputs by subgroup outcomes."
    },
    "explanation": "Post-processing adjustment aligns subgroup positive rates for Demographic Parity. RLHF, privacy, or Guardrails do not directly enforce statistical parity at output distribution."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A biotech firm needs human review on high-risk inferences while minimizing bias in those reviews. Which Amazon A2I workflow configuration best achieves this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use private workforce with diverse backgrounds and stratify task assignments to avoid reviewer clustering.",
      "B": "Route all inferences above a confidence threshold to the same internal SME team.",
      "C": "Expose reviewer identities to each other to foster accountability.",
      "D": "Allow reviewers to self-select tasks for efficiency."
    },
    "explanation": "A private, diverse workforce with stratified assignment reduces bias in human review. Routing only high confidence misses risky cases; exposing identities can bias; self-selection leads to bias clustering."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A loan approval model must demonstrate robustness against adversarial input perturbations. Which responsible AI technique helps achieve this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Clarify to detect adversarial examples in the training data.",
      "B": "Deploy the model via Bedrock with Guardrails against malicious inputs.",
      "C": "Include adversarial training samples in training set and retrain to increase robustness.",
      "D": "Use Model Monitor to block anomalous inference requests."
    },
    "explanation": "Adversarial training augments training data with perturbed inputs to improve robustness. Clarify flags bias, not adversarial; Guardrails guard content, not adversarial attacks; Monitor blocks but doesn\u2019t improve robustness."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A fintech startup needs an AI model to score loan applications with high interpretability for regulators. They require per-feature contribution analysis. Which model and tool combination best balances transparency and predictive performance?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deep neural network with LIME",
      "B": "Gradient Boosting Machine with SHAP values",
      "C": "Logistic regression with no post-hoc analysis",
      "D": "Transformer-based model with attention visualization"
    },
    "explanation": "GBMs offer strong performance and SHAP provides consistent, global and local interpretability, unlike attention or none."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A healthcare system must explain individual patient risk predictions. They need an interactive dashboard of feature influence, counterfactuals, and confidence intervals. Which AWS service and feature set should you use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Clarify alone",
      "B": "Open source SHAP deployed on EC2",
      "C": "SageMaker Model Monitor with Clarify and Model Explainability",
      "D": "SageMaker Endpoints with CloudWatch metrics only"
    },
    "explanation": "Model Monitor plus Clarify supports real-time explainability and dashboards; Clarify alone lacks monitoring."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A retail chain uses an LLM for product recommendations but regulators demand transparency. Which approach best improves explainability with minimal performance loss?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Implement RAG with knowledge provenance and track token sources",
      "B": "Fine-tune the LLM on proprietary data without documentation",
      "C": "Switch entirely to a rule-based recommender",
      "D": "Use closed-book LLM inference with opaque prompt templates"
    },
    "explanation": "RAG captures source documents, improving transparency while preserving model strengths."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "When presenting model decisions to non-technical stakeholders, which design principle ensures explanations are meaningful?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Display raw feature weights",
      "B": "Show full decision tree structure",
      "C": "Provide accuracy metrics only",
      "D": "Use counterfactual explanations in natural language"
    },
    "explanation": "Counterfactuals in NL align with human reasoning; raw weights or trees overwhelm non-experts."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A customer complaints model shows unexpected bias against a minority group. Which tool combination helps detect and mitigate this bias while maintaining explainability?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Clarify for bias detection and Fairness Metrics",
      "B": "SageMaker Autopilot for retraining without transparency",
      "C": "Deep neural network debugging with manual feature drop",
      "D": "LIME analysis without bias metrics"
    },
    "explanation": "Clarify\u2019s bias metrics and dashboards directly identify and quantify group disparities."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An insurer needs an audit trail of model behavior and feature importances over time. Which AWS feature should you configure?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Store SageMaker Training jobs only",
      "B": "Enable SageMaker Model Monitor drift and Clarify Reports to S3",
      "C": "Log endpoint API calls in CloudTrail only",
      "D": "Use SageMaker Experiments without explainability modules"
    },
    "explanation": "Model Monitor with Clarify automatically logs drift and explanations to S3 for audits."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A supply chain optimization model is highly accurate but opaque. You need feature-level transparency and robust version control. Which workflow fits?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy as AWS Lambda with ad-hoc logging",
      "B": "Serve via SageMaker Endpoint and inspect output only",
      "C": "Use SageMaker Projects with Model Registry and Clarify explainability",
      "D": "Export model to S3 and analyze in external notebook"
    },
    "explanation": "SageMaker Projects plus Model Registry provides traceability and Clarify offers feature insights."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A marketing team wants to understand why language generation prompts produce certain outputs. You need an explainable approach without exposing model internals. What do you implement?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Visualize transformer attention heads",
      "B": "Extract embeddings for manual inspection",
      "C": "Run explainability only on input tokens",
      "D": "Generate attribution maps via integrated gradients API"
    },
    "explanation": "Integrated gradients gives token-level attribution without model internals exposure."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A credit scoring model uses a black-box ensemble. Regulators ask for global and local fairness metrics. Which pipeline achieves this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Ingest model predictions into SageMaker Clarify bias and explainability pipelines",
      "B": "Rebuild model as transparent GLM without metrics",
      "C": "Use CloudWatch metrics to track latency",
      "D": "Apply LIME locally on sample predictions only"
    },
    "explanation": "Clarify provides both fairness and explainability metrics globally and locally."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You must document model lineage, data sources, and explainability tests for compliance. Which combination offers a consolidated view?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CloudTrail logs plus ad-hoc Excel reports",
      "B": "SageMaker Model Registry with Model Cards and Clarify reports",
      "C": "Store docs in S3 without integration",
      "D": "Use AWS Config rules only"
    },
    "explanation": "Model Registry combined with Model Cards centralizes lineage, data, and explainability outputs."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A fraud detection model must provide real-time, human-understandable alerts with explanation. Which integration achieves this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Publish raw scores to CloudWatch",
      "B": "Use SageMaker Asynchronous Inference without explainability",
      "C": "Embed Clarify real-time feature attributions into Lambda for alerting",
      "D": "Send model logits directly to SNS"
    },
    "explanation": "Clarify\u2019s real-time attributions can be passed via Lambda for contextual alerts."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An LLM-based chatbot must cite source documents to be explainable. Which AWS service and feature support this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Lex with sentiment analysis",
      "B": "SageMaker JumpStart with default prompts",
      "C": "Comprehend for entity extraction",
      "D": "Amazon Bedrock with Retrieval Augmented Generation"
    },
    "explanation": "Bedrock RAG includes provenance to cite source documents in responses."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "To reduce hallucinations, you add guardrails but must maintain transparency. Which Bedrock feature do you enable?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AI Guardrails for response validation and logging",
      "B": "Auto-scaling with no logging",
      "C": "Fine-tuning without constraint definitions",
      "D": "Inline prompt encryption"
    },
    "explanation": "AI Guardrails enforce policies while producing logs for transparency."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You need to compare performance vs interpretability trade-offs across models. Which practice follows human-centered design?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Automatically pick highest accuracy model",
      "B": "Use the most interpretable regardless of performance",
      "C": "Conduct user studies on prototype explanations and refine",
      "D": "Deploy all models in parallel without feedback"
    },
    "explanation": "User studies ensure explanations meet user needs and inform trade-offs."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An AI ethics team requires documentation of model decision logic. Which artifact from SageMaker fulfills this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Training job CloudWatch logs",
      "B": "Model Card with transparency and test results",
      "C": "Endpoint invocation metrics",
      "D": "S3 bucket lifecycle policy"
    },
    "explanation": "Model Cards include decision logic, evaluation results, and transparency details."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A chatbot\u2019s responses vary nondeterministically, losing user trust. Which setting adjustment improves explainability consistency?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Lower temperature and set top-k sampling to 1",
      "B": "Increase top-p sampling",
      "C": "Add more context tokens only",
      "D": "Disable beam search"
    },
    "explanation": "Reducing temperature and sampling size makes outputs more consistent and explainable."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A manufacturing AI pipeline processes visual defects. Engineers need per-region heatmaps. Which technique and tool deliver this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "LIME on raw images",
      "B": "Manual annotation overlay",
      "C": "Confusion matrix in SageMaker Studio",
      "D": "SageMaker Clarify integrated with feature importance for images (SHAP)"
    },
    "explanation": "Clarify\u2019s SHAP-based image explainability produces region heatmaps integrated in Studio."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your NLP classifier uses a transformer. You must expose decision rationale without revealing model internals. Which approach works?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Share model weights",
      "B": "Generate rationale tokens via chain-of-thought prompting",
      "C": "Distribute attention maps",
      "D": "Publish training data only"
    },
    "explanation": "Chain-of-thought prompts produce human-readable rationales without exposing weights."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A model\u2019s performance dips post-deployment. You suspect data drift and opaque decisions. How do you both monitor drift and maintain explainability?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable SageMaker Model Monitor data drift and periodic Clarify explainability reports",
      "B": "Rely on accuracy metrics in CloudWatch",
      "C": "Schedule manual reviews only",
      "D": "Use Lambda to retrain blindly"
    },
    "explanation": "Model Monitor tracks drift; Clarify periodic reports maintain up-to-date explainability."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An autonomous vehicle company needs to justify stopping decisions to regulators. Which combination delivers transparent logic?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use end-to-end CNN logs only",
      "B": "Deploy LLM for decision text",
      "C": "Implement rule-based safety layer with SHAP on perception model outputs",
      "D": "Record raw sensor input to S3"
    },
    "explanation": "A rule layer ensures deterministic stops; SHAP adds explainability to sensor model decisions."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "For a sensitive legal document summarizer, you must prove summaries\u2019 provenance. Which design ensures traceability?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Fine-tune LLM without logs",
      "B": "Use Lambdas to anonymize sources",
      "C": "Store only summaries in S3",
      "D": "Electronically tag and log source document sections via Bedrock RAG"
    },
    "explanation": "RAG stores document references alongside summaries for end-to-end provenance."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your team wants a transparent view of cluster-wide feature importances from a distributed model. Which AWS service feature helps?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CloudWatch distributed tracing",
      "B": "SageMaker Clarify with distributed explainability mode",
      "C": "Glue ETL transformation logs",
      "D": "EKS pod metrics"
    },
    "explanation": "Clarify\u2019s distributed explainability aggregates importances across shards."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A chatbot integrates multiple foundation models. You need a unified explainability layer without custom code. What do you choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Q for aggregated query provenance",
      "B": "Bedrock Agents without explainability",
      "C": "Lambda orchestration only",
      "D": "S3 auditing on each model"
    },
    "explanation": "Amazon Q provides a unified explainability interface for multi-model chat applications."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "To comply with GDPR\u2019s right to explanation, you must provide individuals model decision reasons. Which AWS feature most directly supports this?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "CloudTrail logs of API calls",
      "B": "CloudWatch metrics retention",
      "C": "SageMaker Clarify local explanations stored in Model Registry",
      "D": "S3 access logs"
    },
    "explanation": "Clarify local explanations stored in the Registry allow per-request rationale retrieval."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An energy company\u2019s demand forecast model is a black box. They need global interpretability and quantitative feature rankings. Which method and service combination should you apply?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run SHAP summary plots via SageMaker Clarify",
      "B": "Inspect first-layer weights in a DNN",
      "C": "Use LIME on a single prediction",
      "D": "Estimate feature correlation externally"
    },
    "explanation": "Clarify\u2019s SHAP summary provides global importance rankings across the dataset."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your model uses transfer learning from a large foundation model. You need to show how fine-tuning changed decision boundaries. How do you document this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Store only final model weights",
      "B": "Produce before-and-after Clarify explainability reports",
      "C": "Log training loss only",
      "D": "Track only fine-tuning epochs"
    },
    "explanation": "Before/after Clarify reports reveal shifts in feature attributions due to tuning."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A real-time bidding system requires sub-second explanations for each bid. Which architecture meets this SLA and explainability requirement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker real-time endpoint with Clarify real-time attributions",
      "B": "Batch Clarify jobs on S3",
      "C": "Asynchronous inference only",
      "D": "Post-hoc analysis offline"
    },
    "explanation": "Real-time endpoints with Clarify provide low-latency attributions inline with predictions."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "To enforce transparency, you restrict models to only open-source licenses. Which SageMaker feature helps manage license compliance and explainability?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "SageMaker Autopilot",
      "B": "SageMaker Studio notebooks",
      "C": "CloudTrail for notebooks",
      "D": "SageMaker Model Cards with license metadata"
    },
    "explanation": "Model Cards capture licensing details and explainability metadata for compliance audits."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your model serves multiple customer segments. You need segment-specific explanations without duplicating pipelines. Which design is most efficient?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy separate endpoints per segment",
      "B": "Use batch Clarify per segment offline",
      "C": "Use multi-tenant endpoint with dynamic Clarify context per request",
      "D": "Train individual models per segment"
    },
    "explanation": "A multi-tenant endpoint that passes segment context to Clarify yields dynamic explanations without duplication."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An AI ethics review board requests evidence of model robustness and interpretability under edge-case inputs. Which test suite do you implement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Generate adversarial examples and run Clarify on those samples",
      "B": "Review only training accuracy",
      "C": "Simulate random noise without explainability",
      "D": "Conduct user surveys only"
    },
    "explanation": "Testing adversarial inputs combined with Clarify shows robustness and explanation under edge cases."
  },
  {
    "taskStatement": "5.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A company deploys a SageMaker real-time inference endpoint in a VPC to serve sensitive PII. The security team requires that all network traffic between on-premises clients and the endpoint never traverse the public internet, and all traffic must be encrypted. Which combination of AWS features meets these requirements with the least operational overhead?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Direct Connect between on-premises and the VPC, and configure a SageMaker VPC endpoint in the VPC; enforce TLS in the endpoint configuration.",
      "B": "Use the public SageMaker endpoint over TLS and restrict access by IP address in the IAM policy.",
      "C": "Place a public Network Load Balancer in front of the endpoint and enforce TLS; restrict client IPs with security groups.",
      "D": "Deploy the endpoint in a private subnet with no internet gateway; use a NAT gateway for egress; require a client VPN."
    },
    "explanation": "A Direct Connect + VPC endpoint ensures traffic never leaves the AWS network. End-to-end TLS in the endpoint config provides encryption. Alternatives either traverse the internet or add unnecessary components."
  },
  {
    "taskStatement": "5.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An organization stores AI training data in an S3 bucket. They must detect accidental public exposure of sensitive data, including PII, as soon as possible. Which solution provides automated detection and alerting for public or compromised bucket objects?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable S3 Server Access Logging and analyze logs with a custom Lambda function.",
      "B": "Use AWS Config rule s3-bucket-public-read-prohibited to trigger SNS notifications.",
      "C": "Enable Amazon Macie to continuously monitor the bucket for public access and PII, and configure Macie alerts to Amazon Security Hub.",
      "D": "Use CloudTrail Data events for S3 and query with Athena daily for public object ACLs."
    },
    "explanation": "Macie continuously profiles S3 for sensitive data and public access. While AWS Config detects policy changes, it doesn\u2019t inspect object content; log analysis is slower and less accurate."
  },
  {
    "taskStatement": "5.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A financial services firm uses a SageMaker feature store with high-volume streaming data. They must ensure data at rest is encrypted, and they need detailed audit logs of data access. Which KMS key configuration and logging combination meets these requirements?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use the AWS owned CMK for feature store and rely on CloudTrail management events.",
      "B": "Use a symmetric customer-managed CMK in KMS with default key usage and enable CloudTrail data events on the S3 bucket.",
      "C": "Use an asymmetric CMK for encryption and enable CloudTrail management events only.",
      "D": "Use a symmetric customer-managed CMK with key policies granting least privilege, enable KMS key rotation, and enable CloudTrail data events on S3 and KMS."
    },
    "explanation": "Customer-managed symmetric CMK provides control and rotation; data events on both S3 and KMS record object and decrypt/encrypt usage for audit. Asymmetric keys are not needed for SSE."
  },
  {
    "taskStatement": "5.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A data pipeline ingests customer records into an S3 bucket before training. The security team mandates that data be encrypted in transit and that any malformed or malicious data be rejected before storage. Which combination of features enforces both requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable S3 default encryption (SSE-S3) and require HTTPS uploads.",
      "B": "Deploy an S3 VPC endpoint with a bucket policy requiring aws:SecureTransport=true and use a Lambda@Edge function to validate file format before writing to S3.",
      "C": "Use public S3 endpoint with HTTPS and configure AWS WAF to block API anomalies.",
      "D": "Use API Gateway to accept uploads over TLS and write directly to the S3 bucket."
    },
    "explanation": "An S3 VPC endpoint plus bucket policy disallows non-TLS uploads. A Lambda@Edge (via CloudFront) or Lambda trigger can validate content before storage. WAF doesn\u2019t validate payload format."
  },
  {
    "taskStatement": "5.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your team implements an ECR repository for Docker images used in AI workloads. They must automatically scan images for CVEs and receive prioritized remediation recommendations. Which AWS feature should they enable?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable AWS Config rule ecr-image-tag-mutability to require immutable tags.",
      "B": "Enable Amazon Inspector image scanning on ECR for scan-on-push and send findings to Security Hub.",
      "C": "Use ECR lifecycle policies to expire untagged images and rely on third-party scanners.",
      "D": "Configure GuardDuty to analyze ECR image traffic for anomalies."
    },
    "explanation": "Amazon Inspector\u2019s ECR image scanning provides CVE detection on push and integrates with Security Hub. Config rules manage mutability but don\u2019t scan for vulnerabilities."
  },
  {
    "taskStatement": "5.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A regulated healthcare application trains NLP models on medical records. The team must track data lineage and model changes for audit. Which AWS service combination provides automatic lineage capture of datasets, processing jobs, and model versions?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Experiments and the SageMaker Model Registry to record input datasets, training jobs, and model versions.",
      "B": "Deploy AWS Glue Data Catalog for lineage and use CloudWatch Logs for training job events.",
      "C": "Use AWS Config to track SageMaker resources and AWS X-Ray for job traces.",
      "D": "Implement AWS Step Functions for pipeline orchestration and log parameters to CloudTrail."
    },
    "explanation": "SageMaker Experiments captures lineage of datasets, training jobs and parameters. Model Registry tracks versioning. Glue and Config don\u2019t capture full ML pipeline lineage automatically."
  },
  {
    "taskStatement": "5.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A security audit discovered that your SageMaker notebook instances can download any internet asset because they are in a public subnet. The audit requires blocking internet access while still allowing package installations from approved AWS sources. Which VPC configuration meets these requirements with minimal changes?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Move notebooks to private subnets with no route to an Internet Gateway.",
      "B": "Keep public subnet but remove the Internet Gateway; traffic will fail.",
      "C": "Add a NAT Gateway in an isolated subnet and force all notebook traffic through it.",
      "D": "Move notebooks to private subnet with a VPC endpoint for Amazon S3 and Amazon ECR, and no Internet Gateway."
    },
    "explanation": "Private subnet plus S3 and ECR endpoints allows package installs from approved repos stored in S3/ECR while blocking all other internet egress without a NAT or IGW."
  },
  {
    "taskStatement": "5.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your AI application uses AWS Lambda functions to preprocess data stored in S3. The security team wants to ensure logs from these functions cannot be tampered with or deleted after 90 days. Which combination of services enforces log immutability?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure Lambda to write logs to a custom S3 bucket and enable versioning.",
      "B": "Route Lambda logs to CloudWatch Logs, set a retention period of 90 days, and rely on CloudTrail for API logs.",
      "C": "Route Lambda logs to CloudWatch Logs, export to an S3 bucket with S3 Object Lock in Compliance mode and a 90-day retention.",
      "D": "Send Lambda logs to Kinesis Data Stream and store to DynamoDB with TTL of 90 days."
    },
    "explanation": "CloudWatch Logs export to S3 with Object Lock in Compliance mode prevents deletion or modification for the retention period. Versioning alone is insufficient."
  },
  {
    "taskStatement": "5.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An organization builds a generative AI service that accepts free-form text inputs. To mitigate prompt injection risks, they want to enforce content filtering before passing inputs to the model. Which AWS components should they integrate?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon Comprehend to detect PII and block inputs containing sensitive data.",
      "B": "Implement AWS WAF with a custom rule to filter harmful patterns.",
      "C": "Use GuardDuty to detect anomalies in API calls and throttle requests.",
      "D": "Route inputs through an API Gateway custom authorizer Lambda function that sanitizes input and applies profanity/regex filters before forwarding to the model."
    },
    "explanation": "A custom authorizer Lambda on API Gateway can sanitize input against injection patterns. WAF is not NLP-aware; Comprehend detects PII but not injection."
  },
  {
    "taskStatement": "5.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your machine learning team uses an S3 bucket to store both raw and processed data. The security team requires separation of duties so that data scientists can read processed data but not raw data. Which IAM strategy enforces this least privilege model?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Apply a bucket policy to deny raw data prefix access for data scientist IAM group.",
      "B": "Create two IAM policies: one grants GetObject on processed-prefix only; assign to data scientists; the second attaches a Deny on raw-prefix for all users.",
      "C": "Use ACLs to grant Read access to processed prefix and remove Read from raw prefix.",
      "D": "Segment the bucket into two buckets: raw and processed, and grant permissions accordingly."
    },
    "explanation": "Prefix-level IAM policies enforce least privilege without creating separate buckets. ACLs are outdated. Splitting buckets adds operational overhead."
  },
  {
    "taskStatement": "5.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A data scientist must ensure that every step of the model training pipeline is logged and that logs cannot be retroactively modified. Which configuration satisfies this requirement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable CloudTrail data events for S3, SageMaker, and KMS; send CloudTrail logs to an S3 bucket with Object Lock in Compliance mode.",
      "B": "Use SageMaker Studio built-in logging and rely on CloudWatch Logs retention settings.",
      "C": "Configure CloudWatch Logs with a 90-day retention period and encrypt logs with an AWS owned CMK.",
      "D": "Export CloudWatch Logs to a Kinesis Data Firehose delivery stream to an Elasticsearch cluster."
    },
    "explanation": "CloudTrail data events record every API call; Object Lock ensures logs can\u2019t be altered. CloudWatch retention doesn\u2019t prevent log tampering."
  },
  {
    "taskStatement": "5.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your inference service hosts a third-party model in a container from ECR. The security policy mandates vulnerability scans on the container image before deployment. Which automated workflow enforces this policy?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use CodePipeline to build the container image and trigger a Lambda that runs a third-party scanner.",
      "B": "Manually scan the image using Amazon Inspector CLI before pushing to ECR.",
      "C": "Enable Amazon Inspector\u2019s ECR image scanning on push, use ECR lifecycle policy to block unscanned images, and integrate findings with CodePipeline approval stage.",
      "D": "Use AWS Config custom rule to check for unscanned images daily."
    },
    "explanation": "Inspector on-push scanning plus lifecycle policy can prevent unscanned images from being deployed; integration into CodePipeline ensures gating."
  },
  {
    "taskStatement": "5.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An AI analytics team needs to share S3 training data with another AWS account securely, without exposing data publicly. They also need audit trails of every access. Which combination of features should they implement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use S3 bucket policy with AWSOrgPrincipal to allow the other account and enable S3 Access Logs.",
      "B": "Create an S3 Access Point with cross-account policy for the specific account and enable AWS CloudTrail data events for object reads.",
      "C": "Generate pre-signed URLs and distribute them; rely on S3 Server Access Logging.",
      "D": "Use S3 replication to the other account bucket; enable CloudWatch metrics."
    },
    "explanation": "S3 Access Points with cross-account policies scope access; CloudTrail data events record each read. Pre-signed URLs can leak; replication duplicates data."
  },
  {
    "taskStatement": "5.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your model training code pulls data from a private S3 bucket. To protect credentials, you want the SageMaker execution role to retrieve encrypted credentials at runtime. Which combination meets best practice for secrets management?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Store credentials as plaintext in S3 encrypted with SSE-S3; grant SageMaker access.",
      "B": "Hard-code credentials in the training container and restrict IAM policies.",
      "C": "Use AWS Systems Manager Parameter Store with SecureString and a Lambda to fetch at training start.",
      "D": "Store secrets in AWS Secrets Manager with automatic rotation; allow the SageMaker role to retrieve secrets via Secrets Manager API."
    },
    "explanation": "Secrets Manager provides encryption at rest, automatic rotation, and fine-grained access via IAM. Parameter Store also works but Secrets Manager is recommended for credentials."
  },
  {
    "taskStatement": "5.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A security requirement mandates that all SageMaker model artifact S3 buckets must enforce encryption at rest and deny uploads without encryption. Which bucket policy accomplishes this?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "A Deny statement on s3:PutObject where s3:x-amz-server-side-encryption is null.",
      "B": "A Deny statement on s3:PutObject if aws:SecureTransport is false.",
      "C": "A Deny statement on s3:GetObject where the SSE algorithm is not aws:kms.",
      "D": "A Deny statement on s3:DeleteObject if the object is unencrypted."
    },
    "explanation": "Denying PutObject when s3:x-amz-server-side-encryption is missing enforces encryption at rest. SecureTransport relates to in-transit encryption."
  },
  {
    "taskStatement": "5.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your ingestion pipeline writes sensitive user data into an Amazon Kinesis Data Stream. The compliance team requires in-flight encryption and that only certain IAM roles can put data. How do you satisfy these requirements?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy a VPC endpoint for Kinesis and require HTTPS for producers.",
      "B": "Use client-side encryption in the producer application and attach an IAM policy to the stream resource.",
      "C": "Enable server-side encryption with a KMS CMK on the stream and configure an IAM resource policy on the stream to allow only specific IAM roles to PutRecord.",
      "D": "Configure a Kinesis Data Firehose delivery stream with encryption enabled."
    },
    "explanation": "Kinesis SSE with CMK secures data in transit and at rest; an IAM resource policy restricts which roles can put records. Firehose is for delivery, not streaming ingestion."
  },
  {
    "taskStatement": "5.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A data engineer uses AWS Glue to preprocess training data. The security team demands that no Glue job can write to production buckets unless it\u2019s explicitly approved. Which control mechanism enforces this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use IAM tags on Glue jobs and restrict S3 writes based on tags.",
      "B": "Implement an AWS Config rule that audits Glue job role permissions and triggers a remediation Lambda to remove unauthorized access.",
      "C": "Use Glue job bookmarks to prevent accidental overwrites.",
      "D": "Enable CloudTrail logs for Glue and manually review permissions weekly."
    },
    "explanation": "An AWS Config rule can automatically detect and remediate Glue job roles with unauthorized S3 permissions. Tag-based restrictions aren\u2019t enforced by Glue role assignment."
  },
  {
    "taskStatement": "5.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your AI platform uses AWS Secrets Manager to store database credentials. The security team wants automatic detection of secrets exfiltration by unauthorized API calls. Which service and feature combination should you use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable CloudTrail management events and alert on unauthorized GetSecretValue API calls.",
      "B": "Use AWS Config to detect changes to Secrets Manager resources.",
      "C": "Use AWS WAF to block suspicious API calls to Secrets Manager endpoints.",
      "D": "Enable CloudTrail data events for Secrets Manager and integrate with Amazon GuardDuty to alert on anomalous API activity."
    },
    "explanation": "CloudTrail data events capture fine-grained Secrets Manager calls; GuardDuty analyzes logs for anomalies. Config and WAF don\u2019t detect unauthorized API usage."
  },
  {
    "taskStatement": "5.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "To comply with data sovereignty, training jobs must run in a specific Availability Zone and use only encryption keys stored in that AZ. How do you configure this in SageMaker?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create a symmetric CMK in the desired region and specify it in the training job; use an AZ-specific subnet.",
      "B": "Launch training in a VPC with subnets in the required AZ and use a customer-managed CMK with an \"aws:sourceVpce\" condition restricting usage to that VPC endpoint.",
      "C": "Use an AWS owned CMK and specify the AZ in the training job config.",
      "D": "Deploy training on spot instances in the AZ and use SSE-S3 for data encryption."
    },
    "explanation": "Restricting the CMK via a condition on the VPC endpoint ensures the key can only be used within the specific AZ\u2019s VPC. AWS owned keys cannot be restricted."
  },
  {
    "taskStatement": "5.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your inference pipeline uses an Amazon API Gateway front end to invoke SageMaker endpoints. The security team wants to ensure that only calls signed with SigV4 and containing a valid JWT from Amazon Cognito can invoke the endpoint. How do you enforce this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure API Gateway with AWS_IAM authorization and a Cognito authorizer; require SigV4 signed requests and a valid JWT.",
      "B": "Use a Lambda authorizer that validates SigV4 signatures and JWTs.",
      "C": "Enable AWS WAF with rules to verify JWT and AWS SigV4 in the HTTP headers.",
      "D": "Place a CloudFront distribution in front of API Gateway with origin access control."
    },
    "explanation": "API Gateway\u2019s built-in AWS_IAM auth requires SigV4; a Cognito authorizer enforces JWT. Combining both enforces both constraints."
  },
  {
    "taskStatement": "5.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A threat model identifies that SageMaker Notebook instance metadata endpoint could expose IAM credentials. You must prevent notebooks from calling instance metadata. How can you enforce this without code changes?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Disable the metadata service on the instance via user data script.",
      "B": "Apply an IAM policy denying ec2:DescribeInstanceMetadata for the notebook role.",
      "C": "Use an IAM instance profile with ec2:MetadataHttpTokens=required and enforce IMDSv2 only via the launch configuration.",
      "D": "Deploy the notebook in a Fargate container that has no metadata endpoint."
    },
    "explanation": "Requiring IMDSv2 with enforced tokens prevents unauthorized metadata access. IAM policies cannot restrict metadata access directly."
  },
  {
    "taskStatement": "5.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your regression model writes output to a DynamoDB table. The security team mandates encrypted data in transit, encrypted at rest with a customer-managed key, and real-time audit of table operations. Which configuration satisfies all requirements?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable DynamoDB encryption at rest with AWS owned CMK and configure TLS in the SDK; use CloudWatch metrics.",
      "B": "Use a DynamoDB global table for cross-region, enable SSE-KMS with a CMK, and export to CloudTrail.",
      "C": "Configure DynamoDB with SSE-C and require HTTPS; enable DynamoDB Streams for auditing.",
      "D": "Enable DynamoDB SSE-KMS with a customer-managed CMK, enforce HTTPS in IAM policy by aws:SecureTransport, and enable CloudTrail data events for the table."
    },
    "explanation": "A CMK gives at-rest control; SecureTransport enforces TLS; CloudTrail data events audit all PutItem/GetItem calls in real time."
  },
  {
    "taskStatement": "5.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A security audit flagged that S3 objects used by your models are sometimes decrypted on client side without proper access checks. You want to ensure that only authorized services can decrypt data and that every decryption is logged. Which strategy fulfills this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure default S3 bucket encryption with SSE-S3 and rely on CloudTrail management events.",
      "B": "Use SSE-KMS with a customer-managed CMK that has a key policy granting decryption only to specific IAM roles and enable CloudTrail data events for KMS.",
      "C": "Implement client-side encryption in application code using a symmetric key stored in Secrets Manager.",
      "D": "Use SSE-C and supply a passphrase at runtime via an API call."
    },
    "explanation": "SSE-KMS with CMK policies restrict decryption; CloudTrail data events record decrypt operations. SSE-S3 cannot limit decryption or log usage."
  },
  {
    "taskStatement": "5.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your batch inference jobs run on EMR clusters accessing S3 training data. The security policy requires min-entropy checks on data inputs to detect tampering. Which AWS feature or service helps implement this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable S3 Object Lock with checksum validation (SHA-256) and use AbortIncompleteMultipartUpload.",
      "B": "Use Amazon Macie to scan data for integrity violations.",
      "C": "Configure EMR bootstrap actions to compute and compare checksums manually.",
      "D": "Use AWS Artifact to validate data source compliance."
    },
    "explanation": "S3 supports checksum validation to detect data corruption/tampering. Object Lock enforces immutability but checksum ensures integrity. Macie doesn\u2019t check integrity."
  },
  {
    "taskStatement": "5.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "When deploying a multi-model SageMaker endpoint, you want to prevent unauthorized model copies or downloads from the endpoint container. Which configuration provides this protection?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy the endpoint in a private subnet with no outbound internet.",
      "B": "Use a VPC endpoint to restrict container ECR pull traffic.",
      "C": "Disable IAM role for the endpoint and use access keys instead.",
      "D": "Use an endpoint execution role with Allow pulls from ECR repository only, no S3 or SSM permissions."
    },
    "explanation": "Restricting the execution role to only ECR pull permissions prevents the container from accessing S3 or other services to copy or download model artifacts."
  },
  {
    "taskStatement": "5.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your AI application uses an Amazon DynamoDB table to store model metadata. The security team requires that table backups must be encrypted with a separate key from the primary table encryption. How do you configure this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable point-in-time recovery; it uses same table CMK by default.",
      "B": "Use on-demand backups with a separate KMS key specified for backup encryption.",
      "C": "Export the table to S3 and apply SSE-C encryption.",
      "D": "Use continuous backups to another DynamoDB table encrypted with its own CMK."
    },
    "explanation": "On-demand backups allow specifying a different CMK for backup encryption. PITR uses the table key and cannot be changed."
  },
  {
    "taskStatement": "5.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your inference function is vulnerable to malicious payloads sent via JSON content. You want to reject any requests containing executable code patterns before they reach the Lambda runtime. Which service or feature is best suited to enforce this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS WAF with SQL injection ruleset.",
      "B": "CloudFront with a managed ruleset for cross-site scripting.",
      "C": "API Gateway request validation with a model schema that rejects payloads not matching the JSON schema.",
      "D": "Lambda layers that sanitize input at runtime."
    },
    "explanation": "API Gateway request validation enforces JSON schema and rejects invalid or unexpected fields before reaching Lambda, reducing risk of code injection."
  },
  {
    "taskStatement": "5.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A large dataset of training images is stored in an S3 bucket. The security mandate requires that any object tagged \"PII\" be encrypted with a dedicated CMK, and other objects with a different CMK. Which mechanism enforces encryption by tag at upload time?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use an S3 bucket policy with separate Deny statements blocking PutObject when s3:RequestObjectTag/PII=true and sse-kms-key-id != CMK-PII, and another for !PII tag.",
      "B": "Use S3 default encryption with an encryption key that rotates based on object tags.",
      "C": "Rely on client-side tagging logic in the ingestion application.",
      "D": "Use S3 Lifecycle rules to re-encrypt mismatched objects after upload."
    },
    "explanation": "Bucket policy conditions on request-tagging and SSE-KMS key ID enforce correct CMK per tag at put time. Lifecycle re-encrypts after-the-fact, not preventive."
  },
  {
    "taskStatement": "5.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your SageMaker training jobs pull data from an RDS database inside a private subnet. The training role requires credentials but you must avoid long-lived credentials. Which solution follows AWS best practices?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Store DB credentials in SSM Parameter Store and grant SageMaker role access.",
      "B": "Hard-code temporary credentials in the training script.",
      "C": "Use AWS Secrets Manager with rotation disabled.",
      "D": "Use AWS Secrets Manager with rotation enabled and grant the SageMaker execution role GetSecretValue permission."
    },
    "explanation": "Secrets Manager with rotation and least-privilege IAM access follows best practices. Parameter Store can work but Secrets Manager is recommended for rotating database credentials."
  },
  {
    "taskStatement": "5.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A security review requires that all Lambda functions in your AI pipeline must run with the minimal set of privileges. You want to enforce that any new functions cannot have wildcard actions in their execution role. How can you automatically detect violations?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use GuardDuty to monitor Lambda execution logs for unauthorized API calls.",
      "B": "Enable an AWS Config managed rule IAM_ROLE_NO_STATEMENT_WITH_WILDCARD and scope it to Lambda execution role ARNs.",
      "C": "Implement CloudWatch alarms on IAM policy changes.",
      "D": "Use AWS CloudTrail to audit IAM PutRolePolicy events daily."
    },
    "explanation": "AWS Config rule IAM_ROLE_NO_STATEMENT_WITH_WILDCARD automatically flags any IAM role policy containing wildcard actions. GuardDuty and alarms are not specific to role definitions."
  },
  {
    "taskStatement": "5.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your inference application logs traffic to an S3 bucket. You must ensure logs are encrypted in transit and at rest, and cannot be modified after writing. Which combination of S3 features satisfies these requirements?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable SSE-S3 and enforce https via bucket policy.",
      "B": "Use SSE-KMS with a CMK and default encryption, and rely on S3 versioning.",
      "C": "Enable SSE-KMS with a CMK, enforce aws:SecureTransport=true, enable Object Lock in Governance mode.",
      "D": "Use client-side encryption before upload and enable MFA delete."
    },
    "explanation": "SSE-KMS + SecureTransport ensures encryption. Object Lock in Governance mode prevents modification but still allows authorized changes if needed. Versioning alone doesn\u2019t prevent modifications."
  },
  {
    "taskStatement": "5.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A compliance requirement states that every S3 object used for training must be scanned for malware before training jobs read the data. Which architecture best satisfies this requirement with minimal latency impact?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon Macie for malware scanning and store only approved objects in a separate prefix for training jobs.",
      "B": "Deploy AWS Lambda on PutObject S3 event to call GuardDuty for malware analysis synchronously.",
      "C": "Build a custom EC2-based scanner that pulls from S3 and pushes to the training prefix.",
      "D": "Use ECR image scanning for data objects in S3."
    },
    "explanation": "Macie malware protection can scan on upload and tag or move clean objects to a different prefix used by training. Lambda or custom scanners add latency and operational overhead."
  },
  {
    "taskStatement": "5.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your SageMaker inference endpoint uses HTTPS. The security team wants to rotate the TLS certificate every 90 days without downtime. Which solution achieves this with minimal changes?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy the endpoint behind an Application Load Balancer with ACM-managed certificates and rotate certificates periodically.",
      "B": "Store certificates in S3 and update the endpoint config with CLI on rotation.",
      "C": "Use CloudFront with self-managed certificate and swap distributions.",
      "D": "Use API Gateway (REST) integration with SageMaker and enable custom domain with ACM-managed certificate, auto-renewed."
    },
    "explanation": "API Gateway custom domain with ACM auto-issues and renews certificates. No downtime occurs and no manual rotation is needed. ALB could work but adds complexity."
  },
  {
    "taskStatement": "5.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A regulated financial services firm deploys an AI model on Amazon SageMaker to process sensitive loan applications. To demonstrate compliance with ISO 27001 and SOC 2, they must provide evidence of model invocation logs, resource configuration drift, and change history across the AI environment. Which AWS service should they use to aggregate and automate the evidence collection for audits?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS CloudTrail",
      "B": "AWS Config",
      "C": "AWS Audit Manager",
      "D": "AWS Artifact"
    },
    "explanation": "AWS Audit Manager automates collection of evidence (from CloudTrail, Config, etc.) and maps it to frameworks like ISO 27001 and SOC 2. CloudTrail and Config alone store logs and configurations but don\u2019t automate audit evidence collection. Artifact provides compliance documents, not operational evidence."
  },
  {
    "taskStatement": "5.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A healthcare startup uses AI to categorize patient imaging data. They must implement data retention policies to comply with a 7-year medical record retention law and ensure no deletions before that period. Which combination of features enforces this requirement at the storage layer?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon S3 Lifecycle rule transitioning objects to Glacier after 7 years",
      "B": "Amazon S3 Object Lock in Compliance mode with a 7-year retention period",
      "C": "S3 Versioning enabled with manual deletion controls",
      "D": "AWS Backup vault locks for S3 with a 7-year lock period"
    },
    "explanation": "S3 Object Lock in Compliance mode enforces retention periods (immutable) and prevents deletion before expiry. Lifecycle rules alone cannot prevent deletion. Versioning doesn\u2019t enforce retention, and AWS Backup doesn\u2019t support S3 object lock at source."
  },
  {
    "taskStatement": "5.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An AI-driven supply chain analytics platform must maintain an auditable history of all configuration changes in its AWS infrastructure for five years. They need near real-time alerts for noncompliant changes. Which service combination meets these requirements?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS CloudTrail with S3 storage and Athena queries",
      "B": "AWS Config with monthly compliance reports",
      "C": "AWS CloudWatch Events with Lambda remediation",
      "D": "AWS Config with advanced resource configuration recording and AWS Config Rules SNS notifications"
    },
    "explanation": "AWS Config continuously records configuration changes, stores history for compliance periods, and Config Rules can trigger SNS for near real-time alerts on noncompliance. CloudTrail logs events but doesn\u2019t provide compliance alerts out-of-the-box. Athena and monthly reports aren\u2019t real-time."
  },
  {
    "taskStatement": "5.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A multinational AI company must comply with data sovereignty laws requiring that model training data and logs for EU-based customers never leave EU regions. Which approach enforces this at the account level?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Organizations Service Control Policy to restrict resource creation to EU regions",
      "B": "Enable AWS Config Multi-Region recording only for EU regions",
      "C": "Set S3 bucket policies to deny non-EU region replication",
      "D": "Use IAM permissions to limit access based on AWS region tag"
    },
    "explanation": "A Service Control Policy (SCP) can block any creation of resources outside specified regions across accounts. Config recording and bucket policies address logging and replication but don\u2019t prevent resource creation globally. IAM region tags can be bypassed if misconfigured."
  },
  {
    "taskStatement": "5.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An AI solution processes customer PII and must comply with GDPR\u2019s right-to-be-forgotten. They need a documented process to delete PII across data stores and ensure deletions are auditable. Which AWS service helps orchestrate and report on the deletion workflow?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Step Functions with DynamoDB Streams",
      "B": "AWS Systems Manager Change Manager with runbooks and approval workflows",
      "C": "AWS Config with remediation functions",
      "D": "Amazon EventBridge scheduled rules triggering Lambdas"
    },
    "explanation": "Systems Manager Change Manager runbooks can orchestrate multi-step workflows with approvals, track actions, and generate audit reports. Config remediation focuses on resource compliance, Step Functions and EventBridge don\u2019t provide built-in approval tracking or audit documentation."
  },
  {
    "taskStatement": "5.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A gaming company uses a foundation model via Amazon Bedrock. They need to ensure all prompts and responses are logged and retained for auditing, while minimizing administrative overhead. Which feature should they enable?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Bedrock Fine-tuning with S3 logging",
      "B": "Enable CloudTrail Data Events for Bedrock",
      "C": "Use Bedrock API Gateway with CloudWatch logs",
      "D": "Enable Amazon Bedrock Activity Logging to Amazon S3"
    },
    "explanation": "Amazon Bedrock Activity Logging delivers prompts and model responses directly to S3 for auditing. CloudTrail doesn\u2019t natively capture Bedrock payloads. API Gateway with CloudWatch adds complexity and may not log responses."
  },
  {
    "taskStatement": "5.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An AI enterprise requires an automated quarterly review of its governance policies and evidence collection for internal auditors. Which AWS service can schedule and manage these recurring assessments?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Config conformance packs",
      "B": "AWS CloudTrail Lake queries",
      "C": "AWS Audit Manager assessment frameworks with scheduled assessments",
      "D": "AWS Trusted Advisor scheduled reports"
    },
    "explanation": "Audit Manager can schedule assessments against compliance frameworks on a quarterly basis, automating evidence collection and reporting. Config conformance packs check compliance configurations but don\u2019t schedule full audit reports. CloudTrail Lake and Trusted Advisor don\u2019t provide compliance frameworks or scheduled audit deliverables."
  },
  {
    "taskStatement": "5.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A biotech firm must ensure that cryptographic keys used by its AI workloads comply with rotation policies mandated by NIST 800-57. They also need to record each rotation event for audits. Which configuration satisfies both requirements?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS KMS automatic key rotation enabled and AWS CloudTrail logging of KMS key usage events",
      "B": "Manual KMS key rotation every 90 days and SNS notifications",
      "C": "Store keys in AWS Secrets Manager with rotation schedule and CloudWatch Logs",
      "D": "Use KMS Customer-Managed Keys without rotation and document rotations manually"
    },
    "explanation": "KMS automatic key rotation meets NIST\u2019s rotation requirement. CloudTrail logs each key usage and rotation event for audits. Secrets Manager is for secrets, not KMS keys. Manual rotation and manual documentation are prone to error."
  },
  {
    "taskStatement": "5.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A government agency requires proof that all data classification labels applied to AI training datasets are consistently enforced at access time and logged for audit. Which AWS feature combination accomplishes this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Macie classification jobs with CloudWatch Logs",
      "B": "AWS Lake Formation tag-based access control with CloudTrail logging",
      "C": "S3 bucket policies using object tags with Config rules",
      "D": "AWS Glue Data Catalog tags with Athena access logs"
    },
    "explanation": "Lake Formation tag-based access control enforces classification labels at query/run time and CloudTrail logs access attempts and policy evaluations. Macie only classifies data, not enforce. S3 policies and Glue tags don\u2019t provide unified enforcement with audit logging across services."
  },
  {
    "taskStatement": "5.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An AI SaaS provider must comply with SOC 2 Type II. They want automated continuous monitoring of configuration compliance against best practices and to generate evidence for auditor review. Which service should they deploy?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Trusted Advisor checks via API",
      "B": "Amazon Inspector scans",
      "C": "CloudWatch Alarms",
      "D": "AWS Config conformance packs integrated with AWS Audit Manager"
    },
    "explanation": "Config conformance packs continuously evaluate compliance against rules and can feed evidence into Audit Manager. Trusted Advisor provides best practice checks but is not integrated with audit evidence. Inspector scans vulnerabilities, CloudWatch alarms aren\u2019t compliance focused."
  },
  {
    "taskStatement": "5.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A retail AI application must maintain governance documentation for all third-party AI model licensing across its organization. They need to store model license documents, track approval status, and make them available for compliance reviews. Which AWS service combination is most appropriate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Artifact for storing compliance documents and AWS Service Catalog for approval workflows",
      "B": "AWS License Manager for AI model licenses and AWS Config for tracking",
      "C": "Amazon S3 with Inventory and AWS IAM for access control",
      "D": "AWS Secrets Manager for document storage and AWS Systems Manager Parameter Store"
    },
    "explanation": "AWS Artifact provides storage and retrieval of compliance/licensing documents, while Service Catalog can manage approval workflows for model usage. License Manager is for software licenses, not documents. S3/inventory doesn\u2019t provide workflows. Secrets Manager and Parameter Store aren\u2019t suited for documents."
  },
  {
    "taskStatement": "5.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A media company must ensure that all AI model inference requests are encrypted in transit and that they can prove encryption usage to auditors. What combination of AWS features provides proof of TLS usage and endpoint encryption enforcement?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker endpoint with IAM authentication enforced",
      "B": "Use VPC endpoints and CloudWatch Logs",
      "C": "Enable HTTPS-only on Amazon API Gateway private integration for SageMaker and log TLS handshake data in CloudTrail",
      "D": "Configure SageMaker endpoint security configurations with AWS WAF"
    },
    "explanation": "Enforcing HTTPS-only on API Gateway ensures TLS. CloudTrail logs the API calls including protocol used, providing evidence of TLS usage. IAM authentication alone doesn\u2019t prove TLS. VPC endpoints encrypt at network layer but don\u2019t log TLS. WAF is for web filtering."
  },
  {
    "taskStatement": "5.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An AI-driven compliance tool ingests financial documents and must adhere to data residency policies. Audit reviewers need to confirm where data is stored and processed. Which AWS artifact should the compliance team download and review?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Config compliance pack",
      "B": "AWS CloudTrail event history report",
      "C": "AWS Audit Manager evidence report",
      "D": "AWS Artifact Service Organization Control (SOC) reports and ISO certifications"
    },
    "explanation": "AWS Artifact provides official compliance reports (SOC, ISO) that document AWS\u2019s data center locations and controls. Config and CloudTrail give operational logs but not official compliance attestations. Audit Manager evidence is operational, not formal certification."
  },
  {
    "taskStatement": "5.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A global AI service must enforce that all customer data is tagged with its classification level and that any untagged resources trigger an alert and are remediated automatically. What should they implement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "S3 bucket tagging policies with AWS Config Aggregator",
      "B": "AWS Config Rules to detect missing resource tags and Automated Remediation via AWS Systems Manager",
      "C": "IAM permission boundary on tagging operations and CloudWatch Events",
      "D": "AWS Organizations tag policies with CloudTrail triggers"
    },
    "explanation": "Config Rules can detect untagged resources and trigger automated remediation through Systems Manager Automation runbooks. Tag policies enforce tagging at creation but don\u2019t alert or remediate existing untagged resources. IAM boundaries limit tagging but don\u2019t enforce remediation. Organizations tag policies only define tag keys/values."
  },
  {
    "taskStatement": "5.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A logistics company automates AI workflows across multiple AWS accounts. They need a centralized view of compliance posture, policy drift, and remediation status for AI workloads. Which architecture best meets this requirement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Organizations with a delegated admin account for AWS Config Aggregator and cross-account Config Rules",
      "B": "Enable CloudTrail multi-account trails to a central S3 bucket",
      "C": "Deploy AWS Audit Manager in each account and aggregate reports manually",
      "D": "Use AWS Security Hub with guardrails in each account"
    },
    "explanation": "Config Aggregator centralized in a delegated admin account aggregates compliance data and cross-account Config Rules enforce policies. CloudTrail centralization logs events but doesn\u2019t enforce compliance. Audit Manager manual aggregation is error-prone. Security Hub focuses on security findings, not general compliance rules."
  },
  {
    "taskStatement": "5.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A company\u2019s AI models handle regulated PII and require an immutable audit trail of all data access events and API calls for five years. Which setup ensures immutable storage and forensic-grade audit logs?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CloudTrail logs to encrypted S3 with lifecycle prevent deletion",
      "B": "CloudWatch Logs encrypted and archived monthly",
      "C": "CloudTrail multi-region trails with S3 Object Lock in Compliance mode and AWS Config recording",
      "D": "Store logs in DynamoDB with point-in-time recovery"
    },
    "explanation": "CloudTrail multi-region provides comprehensive logs; S3 Object Lock Compliance mode ensures immutability for 5 years; Config adds resource change history. CloudWatch Logs and DynamoDB don\u2019t guarantee immutability at forensic standard."
  },
  {
    "taskStatement": "5.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An AI startup must prove to auditors that its development and deployment processes follow its internal governance framework. They want to automatically capture process adherence events (e.g., model promotion, dataset approval) and store them with cryptographic integrity. Which solution provides this capability?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS CodePipeline with CloudWatch Logs",
      "B": "AWS EventBridge custom event bus with CloudTrail integration and S3 Object Lock",
      "C": "AWS Step Functions with DynamoDB Streams",
      "D": "AWS Config custom rule with SNS notifications"
    },
    "explanation": "Custom events on EventBridge can record governance events; CloudTrail integration logs them; storing in S3 with Object Lock preserves cryptographic integrity. CodePipeline logs aren\u2019t structured for governance events. Step Functions/DynamoDB don\u2019t ensure immutability."
  },
  {
    "taskStatement": "5.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A financial AI application needs to enforce multi-factor authentication (MFA) for any user executing model deployment actions. Auditors request logs of MFA usage per action. Which configuration meets these requirements?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Require MFA in IAM policies and use CloudWatch Logs for user activity",
      "B": "Enable AWS SSO with MFA and review SSO audit logs",
      "C": "Use AWS Organizations SCP to enforce MFA and check AWS Config",
      "D": "Enforce MFA via IAM policy conditions and audit CloudTrail logs for sts:AssumeRole MFAAuthenticated field"
    },
    "explanation": "IAM policy conditions can require MFAAuthenticated:true. CloudTrail logs sts:AssumeRole events including MFAAuthenticated flag, providing audit trail. SSO logs may not map to IAM actions, and SCP cannot enforce MFA at action level."
  },
  {
    "taskStatement": "5.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A healthcare AI solution must ensure that model explainability artifacts (e.g., SHAP values) are retained for at least three years and accessible for audits. They also need to track who accessed these artifacts. Which approach satisfies both requirements?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Store artifacts in S3 with access logging enabled and Object Lock with 3-year retention",
      "B": "Publish artifacts to AWS Glue Data Catalog with 3-year lifecycle",
      "C": "Save artifacts in DynamoDB with PITR and CloudWatch Logs",
      "D": "Use AWS RDS with automated backups and IAM auditing"
    },
    "explanation": "S3 with Object Lock in Compliance mode ensures retention. S3 access logging records who accessed artifacts. Glue Catalog doesn\u2019t store artifacts; DynamoDB PITR and CloudWatch Logs don\u2019t provide immutability or direct access logs at file level; RDS backups aren\u2019t suitable for artifact storage."
  },
  {
    "taskStatement": "5.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A global AI service provider must comply with multiple regulatory frameworks (GDPR, HIPAA, PCI DSS) and present a unified compliance dashboard. Which AWS service can consolidate evidence from multiple sources and provide framework-based status?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Security Hub",
      "B": "AWS Config dashboard",
      "C": "AWS Audit Manager",
      "D": "AWS Well-Architected Tool"
    },
    "explanation": "Audit Manager maps evidence from CloudTrail, Config, IAM, etc., to multiple compliance frameworks and provides a unified dashboard. Security Hub focuses on security findings, Config dashboards show resource compliance, and Well-Architected Tool is for architecture reviews."
  },
  {
    "taskStatement": "5.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An AI platform processes credit card transactions and must adhere to PCI DSS data retention and encryption-at-rest requirements. They need to demonstrate that all EBS volumes hosting model artifacts are encrypted and snapshots retained for 1 year. Which configuration provides auditors with automated proof?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable EBS encryption by default and use SSM Automation for snapshot retention",
      "B": "Use AWS Config managed rule ebs-volume-in-use-encrypted and aws-config SnapshotRetention custom rule",
      "C": "Manually tag encrypted volumes and generate quarterly reports",
      "D": "Use AWS Backup vault lock with a 1-year lock period"
    },
    "explanation": "Config managed rule checks encryption on EBS volumes; a custom Config rule can enforce and report snapshot retention. This automates evidence. Default encryption doesn\u2019t prove snapshot retention; manual tagging and reporting is not automated; AWS Backup supports snapshots but not CSI-level reporting in Config."
  },
  {
    "taskStatement": "5.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A pharmaceutical AI workflow uses Amazon ECS with Fargate. Auditors require logs of container image provenance and compliance validation results before deployment. Which combination ensures traceability and compliance checks?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon ECR image scanning with AWS Config rule ecr-image-scanning-required",
      "B": "Enable CloudTrail on ECS and tag images",
      "C": "Run Amazon Inspector on Fargate tasks at runtime",
      "D": "Use AWS Service Catalog to deploy ECS services"
    },
    "explanation": "ECR image scanning finds vulnerabilities. Config rule ensures all images are scanned before use. CloudTrail logs ECS API calls but doesn\u2019t enforce scanning; Inspector runtime doesn\u2019t ensure pre-deployment checks; Service Catalog handles provisioning but not scanning enforcement."
  },
  {
    "taskStatement": "5.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An AI-driven fraud detection service must implement least-privilege IAM policies for model invocation and ensure policies are reviewed every 90 days. Which AWS feature helps automate policy compliance checks and certification workflows?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "IAM Access Analyzer with monthly reports",
      "B": "AWS IAM Access Analyzer for policy findings and AWS Systems Manager Patch Manager calendar for review reminders",
      "C": "AWS Artifact for policy certifications",
      "D": "AWS Config with IAM policy evaluation rules"
    },
    "explanation": "IAM Access Analyzer identifies overly permissive policies. Combined with Systems Manager maintenance windows or Patch Manager calendar, it can schedule certification reminders. Config can evaluate policies but doesn\u2019t manage certification workflows. Artifact is for documents."
  },
  {
    "taskStatement": "5.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A global healthcare company uses AI to process patient data. They need to enforce that all AI-related API calls originate from within their VPC and that any violations are captured for audit. Which configuration meets this need?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS WAF to block external calls and CloudWatch alarms",
      "B": "Require TLS mutual authentication and log to CloudTrail",
      "C": "Deploy VPC Interface endpoints for AI services and enable CloudTrail for AWS PrivateLink logs",
      "D": "Use Security Groups to restrict IPs and S3 bucket policies"
    },
    "explanation": "VPC interface endpoints (PrivateLink) ensure API calls stay within VPC; CloudTrail logs PrivateLink calls for audit. WAF is for HTTP endpoints, mutual TLS doesn\u2019t enforce VPC origin, security groups and bucket policies don\u2019t cover all AI service APIs."
  },
  {
    "taskStatement": "5.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A data analytics firm must prove to regulators that their AWS AI environment is configured according to the company\u2019s custom governance standards. They want continuous evaluation and weekly compliance reporting. Which AWS service should they leverage?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Trusted Advisor scheduled checks",
      "B": "Amazon Macie with scheduled classification",
      "C": "CloudTrail Lake queries",
      "D": "AWS Config with custom conformance packs and delivery channels"
    },
    "explanation": "Config custom conformance packs can encode the company\u2019s governance standards and AWS Config delivery channels export weekly compliance reports. Trusted Advisor and Macie focus on specific checks, CloudTrail Lake is for ad-hoc analysis."
  },
  {
    "taskStatement": "5.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An AI solution must be accredited under the EU AI Act, requiring documentation of governance processes and risk assessments. The company wants to version-control governance artifacts, track approval status, and retrieve an audit trail. Which combination of AWS services meets this requirement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS CodeCommit for artifacts, AWS Config for tracking",
      "B": "AWS CodeCommit with signed commits, AWS CodePipeline approvals, and AWS Artifact for framework mapping",
      "C": "Amazon S3 with Object Lock and AWS Step Functions",
      "D": "AWS Systems Manager Parameter Store and AWS CloudTrail"
    },
    "explanation": "CodeCommit signed commits version artifacts; CodePipeline approval actions track governance process; Artifact provides regulatory framework documentation. Config tracks resources not governance docs; S3/Object Lock store artifacts but lack workflow; Parameter Store unsuitable for documents."
  },
  {
    "taskStatement": "5.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A fintech company\u2019s risk committee requires that any change to AI-model hyperparameters in production must be approved and logged. They also want to recover the previous approved hyperparameters if needed. What solution provides this capability?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Model Registry with approval workflows and version tracking",
      "B": "Store hyperparameters in AWS Secrets Manager with version labels",
      "C": "Log hyperparameter changes via CloudWatch and manual approval",
      "D": "Maintain hyperparameters in DynamoDB with PITR"
    },
    "explanation": "Model Registry supports model versions, approval statuses, and metadata including hyperparameters. It allows rollback to prior versions. Secrets Manager isn\u2019t designed for hyperparameter metadata and approvals. CloudWatch logs are not structured for governance; DynamoDB PITR captures data but lacks approval workflows."
  },
  {
    "taskStatement": "5.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An AI company must implement data residency enforcement for model inference logs, ensuring logs for APAC customers are stored only in APAC regions. Which mechanism enforces this and prevents misconfiguration?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CloudTrail multi-region trails with S3 replication restrictions",
      "B": "Use IAM conditions to restrict bucket ARNs by region",
      "C": "AWS Organizations SCP denying log delivery outside specified APAC regions",
      "D": "AWS Config rule to detect cross-region log buckets"
    },
    "explanation": "An SCP can block any S3 log delivery to buckets outside allowed regions at organization level. Config rules detect misconfig but don\u2019t prevent creation. IAM conditions can be bypassed if incorrect policy applied. CloudTrail replication restrictions aren\u2019t a feature."
  },
  {
    "taskStatement": "5.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A government contractor\u2019s AI system must maintain an immutable record of data provenance, model training parameters, and inference results, accessible for audits but not deletable. Which storage configuration satisfies these needs?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon RDS with encryption and backups",
      "B": "DynamoDB with point-in-time recovery",
      "C": "EFS with lifecycle policies",
      "D": "S3 with Object Lock in Compliance mode and versioning enabled"
    },
    "explanation": "S3 Object Lock Compliance mode ensures immutability. Versioning preserves object history. RDS backups and DynamoDB PITR can be changed or deleted, EFS doesn\u2019t offer immutability."
  },
  {
    "taskStatement": "5.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An AI-driven recommendation service must demonstrate to auditors that all API keys for third-party data sources are rotated every 90 days and usage is logged. Which combination of AWS services enforces rotation and tracks usage?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Secrets Manager automatic rotation and CloudTrail data events for Secrets Manager",
      "B": "IAM Access Keys for service accounts and CloudWatch Logs",
      "C": "AWS KMS key rotation and AWS Config rule",
      "D": "AWS SSO for credential management and CloudTrail"
    },
    "explanation": "Secrets Manager supports automatic secret rotation and CloudTrail Data Events capture every GetSecretValue call for usage logging. IAM Access Keys and KMS key rotation don\u2019t cover API keys, and SSO doesn\u2019t rotate secrets automatically."
  },
  {
    "taskStatement": "5.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A multinational enterprise needs to certify quarterly that its AI environment complies with its internal security policy and generate a report for each region. Which service simplifies cross-region compliance evidence collection and report generation?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Security Hub with custom insights",
      "B": "AWS Audit Manager with multiple regional assessments",
      "C": "AWS Trusted Advisor consolidated view",
      "D": "AWS Config dashboard exports"
    },
    "explanation": "Audit Manager supports running assessments per region and consolidates evidence into reports. Security Hub insights focus on security findings, not policy compliance frameworks. Trusted Advisor and Config dashboards lack formal reporting against internal policies."
  },
  {
    "taskStatement": "5.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A financial AI platform must enforce that no S3 bucket associated with model artifacts is publicly exposed. They want automated remediation when a violation occurs and documentation of the remediation action for auditors. What implementation meets these needs?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Config rule s3-bucket-public-read-prohibited with remediation via Systems Manager Automation runbook and Config remediation history",
      "B": "S3 Block Public Access and CloudWatch alarm",
      "C": "Bucket policies to deny public access and SNS notifications",
      "D": "IAM SCP to prevent public ACL creation and manual ticketing"
    },
    "explanation": "Config managed rule detects public buckets; remediation runbooks can automatically fix and Config records remediation history for audit. Block Public Access prevents exposure but doesn\u2019t document an audit trail. Bucket policies and SCPs prevent but lack remediation tracking."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A company implements a rule-based tax calculation engine that applies predefined rules to compute tax liabilities and never adapts from past transactions. According to standard AI/ML definitions, this solution is classified as:",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "An AI system but not a machine learning system",
      "B": "A machine learning system but not an AI system",
      "C": "A deep learning system",
      "D": "Neither an AI nor a machine learning system"
    },
    "explanation": "Rule-based systems fall under AI broadly but do not learn from data, so they are not considered ML."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An online recommendation service updates user suggestions instantly as users browse items. Which inferencing type best describes this system?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Batch inferencing",
      "B": "Real-time inferencing",
      "C": "Offline inferencing",
      "D": "Mini-batch inferencing"
    },
    "explanation": "Instant updates require real-time inferencing, not periodic batch processing."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A facial recognition model shows lower accuracy on certain demographic groups compared to others. To address this, the practitioner must improve which concept?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Bias",
      "B": "Fit",
      "C": "Fairness",
      "D": "Generalization"
    },
    "explanation": "Fairness ensures consistent performance across groups; bias refers to error sources but fairness is the goal."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A model achieves 98% accuracy on training data but only 60% on validation data. This pattern indicates:",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Underfitting",
      "B": "Overfitting",
      "C": "Good fit",
      "D": "Data bias"
    },
    "explanation": "High training and low validation accuracy show the model memorized training data (overfitting)."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A retailer has transactional logs without labels and wants to segment customers by purchase behavior. Which learning approach is most appropriate?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Supervised learning",
      "B": "Unsupervised learning",
      "C": "Reinforcement learning",
      "D": "Deep learning"
    },
    "explanation": "Clustering segments without labels requires unsupervised learning."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which data type best describes sensor readings collected every second from an industrial machine?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Unstructured data",
      "B": "Tabular static data",
      "C": "Time-series data",
      "D": "Categorical data"
    },
    "explanation": "Sequential timestamped readings are time-series data."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A startup wants to index and search transcripts of customer support calls. Which AI domain should they leverage?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Computer vision",
      "B": "Natural language processing",
      "C": "Reinforcement learning",
      "D": "Generative AI"
    },
    "explanation": "Processing and searching text transcripts is an NLP use case."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "In machine learning, the iterative process that adjusts weights during model training is called the \u201calgorithm,\u201d whereas the learned weights themselves constitute the \u201c\u201d:",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Model",
      "B": "Feature",
      "C": "Dataset",
      "D": "Payload"
    },
    "explanation": "The algorithm is the procedure; the model is the learned representation (weights)."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which of the following algorithms is NOT considered a deep learning method?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Convolutional neural network",
      "B": "Recurrent neural network",
      "C": "Random forest",
      "D": "Transformer"
    },
    "explanation": "Random forest is an ensemble of decision trees, not a neural network."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A monthly invoice processing pipeline processes transactions at month end. What type of inferencing does it use?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Real-time inferencing",
      "B": "Online inferencing",
      "C": "Batch inferencing",
      "D": "Stream inferencing"
    },
    "explanation": "Processing data at set intervals is batch inferencing."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A model with multiple hidden layers that learns hierarchical feature representations is best described as:",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Machine learning model",
      "B": "Deep learning model",
      "C": "Reinforcement learning model",
      "D": "Statistical model"
    },
    "explanation": "Multiple hidden layers characterize a deep learning model."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A robotic arm learns to sort objects by trial and error with a reward for correct sorting. This exemplifies:",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Supervised learning",
      "B": "Unsupervised learning",
      "C": "Reinforcement learning",
      "D": "Transfer learning"
    },
    "explanation": "Learning via rewards/punishments is reinforcement learning."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "When a model\u2019s training and validation errors both decrease to a stable low value, this indicates:",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Underfitting",
      "B": "Overfitting",
      "C": "Optimal fit",
      "D": "Data leakage"
    },
    "explanation": "Low and stable errors on both sets suggest a good balance (optimal fit)."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A marketing team uses a pre-trained model with billions of parameters on text corpora to generate summaries. This model type is known as:",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Small language model",
      "B": "Large Language Model (LLM)",
      "C": "Rule-based model",
      "D": "Reinforcement model"
    },
    "explanation": "Models with billions of parameters trained on text are Large Language Models."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which concept maps textual tokens to dense numerical vectors for downstream processing?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Chunking",
      "B": "Tokenization",
      "C": "Embedding",
      "D": "Sampling"
    },
    "explanation": "Embeddings convert tokens into continuous vector representations."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Image files used in computer vision tasks are best characterized as which data type?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Structured data",
      "B": "Unstructured data",
      "C": "Tabular data",
      "D": "Categorical data"
    },
    "explanation": "Image pixels do not follow fixed schema\u2014hence unstructured."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Feeding new data into a trained model to generate predictions is called:",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Training",
      "B": "Inferencing",
      "C": "Preprocessing",
      "D": "Tuning"
    },
    "explanation": "Predicting with a trained model is inferencing."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "In linear regression, the linear equation y=mx+b with learned m and b represents the:",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Model",
      "B": "Algorithm",
      "C": "Feature",
      "D": "Weight"
    },
    "explanation": "The equation with learned parameters is the model; algorithm is how it\u2019s learned."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A telecom company wants to group customers based on usage patterns without prior labels. Which technique and learning type apply?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Supervised classification",
      "B": "Unsupervised clustering",
      "C": "Supervised regression",
      "D": "Reinforcement Q-learning"
    },
    "explanation": "Grouping without labels uses unsupervised clustering."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A delivery drone optimizes its navigation policy by trial and error with reward for timely delivery. Which learning type and business objective does this exemplify?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Supervised learning for prediction",
      "B": "Unsupervised learning for segmentation",
      "C": "Reinforcement learning for decision optimization",
      "D": "Deep learning for feature extraction"
    },
    "explanation": "Using rewards to improve decisions is reinforcement learning for decision optimization."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An autonomous vehicle collects LIDAR point clouds in real time but processes them hourly for route refinement. Identify the data type and inferencing mode.",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Time-series & real-time",
      "B": "Unstructured & batch",
      "C": "Structured & streaming",
      "D": "Tabular & real-time"
    },
    "explanation": "Point clouds are unstructured; hourly processing is batch inferencing."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "DeepMind\u2019s AlphaGo learns to win Go by playing games against itself and receiving rewards. Which learning paradigm is this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Supervised learning",
      "B": "Unsupervised learning",
      "C": "Reinforcement learning",
      "D": "Transfer learning"
    },
    "explanation": "Self-play with rewards is a reinforcement learning approach."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "In a sentiment analysis pipeline, converting word indices into dense vectors is called:",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Tokenization",
      "B": "Embedding",
      "C": "Chunking",
      "D": "Sampling"
    },
    "explanation": "Embedding maps discrete indices to continuous vectors."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A bank predicts the likelihood of loan default as a probability. Which ML task is this?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Classification",
      "B": "Regression",
      "C": "Clustering",
      "D": "Dimensionality reduction"
    },
    "explanation": "Predicting discrete outcome probabilities is classification."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An email spam filter learns from labeled spam and non-spam examples. Identify the data labeling and learning type combination.",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Labeled data & supervised learning",
      "B": "Unlabeled data & unsupervised learning",
      "C": "Labeled data & reinforcement learning",
      "D": "Unstructured data & supervised learning"
    },
    "explanation": "Spam filters use labeled examples for supervised learning."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An autoencoder learns normal transaction patterns without labels to detect anomalies. This technique falls under:",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Unsupervised learning",
      "B": "Supervised learning",
      "C": "Reinforcement learning",
      "D": "Transfer learning"
    },
    "explanation": "Autoencoders train on unlabeled data to learn patterns\u2014unsupervised learning."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A fraud detection system predicts fraudulent transactions instantly using a trained supervised model. Which combination applies?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Unsupervised learning & batch inferencing",
      "B": "Supervised learning & batch inferencing",
      "C": "Supervised learning & real-time inferencing",
      "D": "Reinforcement learning & real-time inferencing"
    },
    "explanation": "Predicting fraud instantly with a trained supervised model is real-time inferencing."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A customer segmentation model groups users on the fly into clusters for personalized offers. Which combination applies?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Supervised learning & real-time",
      "B": "Unsupervised learning & real-time",
      "C": "Supervised learning & batch",
      "D": "Reinforcement learning & batch"
    },
    "explanation": "On-the-fly grouping without labels is unsupervised real-time inferencing."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Gradient descent, used to minimize loss during training, is an example of a:",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Model",
      "B": "Algorithm",
      "C": "Feature",
      "D": "Hyperparameter"
    },
    "explanation": "Gradient descent is the algorithm that updates model parameters."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "In a text-to-speech system, converting written language into spoken audio falls under which AI subdomain?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Computer vision",
      "B": "Natural language processing",
      "C": "Reinforcement learning",
      "D": "Anomaly detection"
    },
    "explanation": "Text-to-speech processes language, an NLP task."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A financial institution must flag potentially fraudulent credit card transactions in real time. Which ML technique is most appropriate to minimize false negatives while allowing probabilistic scoring?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Logistic regression with threshold tuning",
      "B": "K-means clustering with centroid distance",
      "C": "Principal component analysis for dimensionality reduction",
      "D": "Linear regression predicting fraud score"
    },
    "explanation": "Fraud detection is a supervised, binary classification problem. Logistic regression provides probabilistic outputs and can be threshold-tuned to balance false positives/negatives. Clustering and PCA are unsupervised and linear regression predicts continuous values, not classes."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An e-commerce retailer wants to segment customers for targeted marketing using only purchase history without labels. Which ML technique should they apply?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Hierarchical or k-means clustering",
      "B": "Decision tree classification",
      "C": "Logistic regression",
      "D": "Reinforcement learning"
    },
    "explanation": "Customer segmentation without labels is an unsupervised clustering problem. Classification and regression require labeled outcomes; reinforcement learning is for action policies, not segmentation."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A subscription service wants to predict if a customer will churn at the end of their billing cycle. Which ML approach best matches this requirement?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Time-series regression forecasting",
      "B": "Supervised classification",
      "C": "Unsupervised clustering",
      "D": "Reinforcement learning"
    },
    "explanation": "Churn prediction is a binary outcome (churn/no churn), so supervised classification is appropriate. Forecasting is for continuous numeric predictions, clustering is unsupervised, and reinforcement learning addresses sequential decision making."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A retail chain needs to forecast next quarter\u2019s sales volume at each store using historical daily sales data. Which ML technique is most suitable?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Time-series forecasting",
      "B": "Binary classification",
      "C": "K-means clustering",
      "D": "Anomaly detection"
    },
    "explanation": "Sales forecasting over time is a time-series regression problem. Classification and clustering are not designed for predicting continuous values over time; anomaly detection identifies outliers, not future values."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A travel website wants to analyze free-form customer reviews to determine overall sentiment. Which AWS managed service should they choose?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Comprehend",
      "B": "Amazon Translate",
      "C": "Amazon Lex",
      "D": "Amazon Polly"
    },
    "explanation": "Amazon Comprehend provides NLP capabilities including sentiment analysis. Translate is for language translation, Lex is for chatbots, and Polly is for text-to-speech."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A call center needs to generate text transcripts from live voice calls for compliance auditing. Which AWS service do you recommend?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Transcribe",
      "B": "Amazon Comprehend",
      "C": "Amazon Lex",
      "D": "Amazon Polly"
    },
    "explanation": "Amazon Transcribe converts speech to text. Comprehend analyzes text, Lex builds chatbots, and Polly generates speech from text."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A company wants to build an IVR system that understands customer intent and routes calls accordingly. Which service is the best fit?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Lex",
      "B": "Amazon Polly",
      "C": "Amazon Transcribe",
      "D": "Amazon Comprehend"
    },
    "explanation": "Amazon Lex provides intent recognition and dialog management ideal for IVR. Polly is TTS, Transcribe is speech-to-text, and Comprehend is NLP analysis not optimized for conversational flows."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An operations team wants to generate spoken alarms from textual alerts. Which AWS service fulfills this without custom model training?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Polly",
      "B": "Amazon Lex",
      "C": "Amazon Comprehend",
      "D": "Amazon Transcribe"
    },
    "explanation": "Amazon Polly provides high-quality text-to-speech directly. Lex is for chatbots, Comprehend is text analysis, and Transcribe is speech recognition."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A global marketing team needs to localize ad copy from English to Spanish at scale. Which service will deliver accurate, context-aware translations?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Translate",
      "B": "Amazon Comprehend",
      "C": "Amazon Polly",
      "D": "Amazon Transcribe"
    },
    "explanation": "Amazon Translate provides neural machine translation with context awareness. Comprehend analyzes sentiment and entities, Polly is TTS, and Transcribe is STT."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A logistics provider wants to automatically extract line-item details from scanned invoices. Which AWS service is most appropriate?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Textract",
      "B": "Amazon Comprehend",
      "C": "Amazon Rekognition",
      "D": "Amazon Translate"
    },
    "explanation": "Amazon Textract extracts structured text (forms, tables) from documents. Comprehend analyzes unstructured text, Rekognition analyzes images, and Translate handles translation."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A social media platform needs to tag objects in user-uploaded photos for content discovery. Which AWS service should be used?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Rekognition",
      "B": "Amazon Textract",
      "C": "Amazon Comprehend",
      "D": "Amazon Translate"
    },
    "explanation": "Rekognition provides object and scene detection in images. Textract is OCR, Comprehend is text NLP, and Translate is for language translation."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An R&D team wants to discover underlying topics in a large corpus of unlabeled news articles. Which ML approach is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Unsupervised topic modeling or clustering",
      "B": "Supervised classification",
      "C": "Reinforcement learning",
      "D": "Linear regression"
    },
    "explanation": "Topic discovery in unlabeled data requires unsupervised methods like clustering or topic modeling. Supervised and regression require labels; reinforcement learning is for sequential decision-making."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A small bakery with stable weekly orders wants to plan production without high tooling costs. Which solution is most appropriate?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Rule-based heuristic forecasting",
      "B": "Time-series ML forecasting",
      "C": "Unsupervised clustering",
      "D": "Supervised classification"
    },
    "explanation": "Low variance and predictable demand can be handled with simple heuristics. ML forecasting adds unnecessary cost and complexity; clustering and classification don\u2019t predict numerical demand."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A news aggregator must group articles in real time as they arrive without prior labels. Which AWS service or technique is optimal?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Kinesis + custom clustering in SageMaker",
      "B": "Amazon Comprehend sentiment analysis",
      "C": "Amazon Translate real-time translation",
      "D": "Amazon DetectTopic API"
    },
    "explanation": "Real-time grouping requires streaming + unsupervised clustering in SageMaker. Comprehend Topic Modeling is batch, no native DetectTopic API exists, and Translate only translates."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A startup with only 100 users wants to recommend content. Labels are sparse. Which initial approach maximizes ROI?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Rule-based content filtering",
      "B": "Deep collaborative filtering",
      "C": "Supervised classification",
      "D": "Reinforcement learning"
    },
    "explanation": "With sparse data and few users, simple rule-based filtering yields ROI. Complex models require more data and expertise and may not outperform basic heuristics initially."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A credit card company needs to identify outlier spending behaviors without labeled fraud examples. Which technique fits best?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "K-means clustering",
      "B": "Isolation Forest anomaly detection",
      "C": "Logistic regression",
      "D": "Decision tree classification"
    },
    "explanation": "Isolation Forest is specifically designed for unsupervised anomaly detection. Clustering may group anomalies but is less direct; classification requires labels."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A legal firm needs to extract entities (names, dates, citations) from contracts. Which AWS service combination is ideal for structured entity extraction?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Textract for form/key-value extraction + Amazon Comprehend for entity recognition",
      "B": "Amazon Transcribe + Amazon Translate",
      "C": "Amazon Rekognition + Amazon Polly",
      "D": "Amazon Lex + Amazon Comprehend"
    },
    "explanation": "Textract extracts structured text and key-value pairs; Comprehend then identifies entity types. Other combinations don\u2019t handle both OCR and NLP entity extraction."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A marketing team wants to detect keywords and sentiment in social media streams in near real time. Which architecture is best?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Kinesis Data Streams \u2192 Lambda \u2192 Comprehend Real-Time APIs",
      "B": "S3 batch upload \u2192 Comprehend Batch APIs",
      "C": "Lex chatbot \u2192 Comprehend",
      "D": "Transcribe streaming \u2192 Translate"
    },
    "explanation": "Kinesis + Lambda + Comprehend Real-Time enables streaming analysis. Batch APIs introduce latency; Lex is chat-focused; Translate is irrelevant."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A telecom wants to forecast network traffic spikes to provision capacity. Which ML solution suits this business case?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Time-series forecasting (ARIMA or deep learning)",
      "B": "Binary classification",
      "C": "Unsupervised clustering",
      "D": "Reinforcement learning"
    },
    "explanation": "Traffic spike prediction is a time-series forecasting problem. Classification predicts categories, clustering groups data, RL learns policies\u2014not forecasting continuous values."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A publisher needs to translate large volumes of articles to multiple languages while preserving idiomatic expressions. Which AWS service best fits?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Translate with custom terminology",
      "B": "Amazon Comprehend sentiment analysis",
      "C": "Amazon Textract OCR",
      "D": "Amazon Polly TTS"
    },
    "explanation": "Translate with custom terminology supports idioms and domain-specific terms. Comprehend analyzes text, Textract extracts text, Polly synthesizes speech."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A media company wants to summarize long-form video transcripts into bullet-point highlights. Which combination meets this need?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Transcribe \u2192 Comprehend Summarization",
      "B": "Translate \u2192 Polly",
      "C": "Lex \u2192 Comprehend Entities",
      "D": "Rekognition \u2192 Textract"
    },
    "explanation": "Transcribe produces text from video; Comprehend Summarization extracts bullet points. Other combos don\u2019t provide summarization."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A law enforcement agency seeks to group seized documents by topic without pre-labeling. Which AWS ML feature supports this directly?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Comprehend Topic Modeling",
      "B": "Amazon Translate",
      "C": "Amazon Rekognition",
      "D": "Amazon Polly"
    },
    "explanation": "Comprehend Topic Modeling groups documents by topic unsupervised. Translate, Rekognition, and Polly serve other use cases."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A retail chain has historical sales and external factors; they need to fine-tune a model to predict sales uplift from promotions. Which AWS service covers end-to-end without deep ML coding?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon SageMaker Autopilot",
      "B": "Amazon Personalize",
      "C": "Amazon Forecast",
      "D": "Amazon Comprehend"
    },
    "explanation": "SageMaker Autopilot automates training and tuning with custom data. Forecast is specialized for time-series only, Personalize is for recommendations, and Comprehend is NLP."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A fraud team wants an anomaly detection service tuned for metrics like transaction volume spikes without building models. Which AWS service is ideal?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon SageMaker",
      "B": "Amazon Comprehend",
      "C": "Amazon Lookout for Metrics",
      "D": "Amazon Kinesis"
    },
    "explanation": "Lookout for Metrics is designed for ML-based anomaly detection on time-series metrics. SageMaker requires model building; Comprehend is NLP; Kinesis is streaming infrastructure."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A content platform must recommend articles based on user reading history but has no labeled preferences. Which AWS service accelerates this?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Personalize",
      "B": "Amazon SageMaker Ground Truth",
      "C": "Amazon Comprehend",
      "D": "Amazon Kendra"
    },
    "explanation": "Amazon Personalize provides collaborative filtering and content-based recommendations without requiring deep ML expertise. Ground Truth labels data, Comprehend is NLP, and Kendra is enterprise search."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A hospital wants to classify radiology images into normal versus abnormal categories. There\u2019s sufficient labeled data but no DL expertise. What is the fastest path?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Build a CNN in SageMaker training jobs",
      "B": "Use SageMaker JumpStart prebuilt image classification pipeline",
      "C": "Use Rekognition",
      "D": "Use Amazon Comprehend"
    },
    "explanation": "SageMaker JumpStart offers prebuilt image classification pipelines. Rekognition doesn\u2019t support custom medical imaging; Comprehend is NLP."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A compliance team needs to automatically detect PII (names, SSNs) in stored S3 objects. Which service fulfills this requirement?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Macie",
      "B": "AWS Config",
      "C": "Amazon GuardDuty",
      "D": "Amazon Macie"
    },
    "explanation": "Amazon Macie uses ML to detect PII in S3. Config monitors resources, GuardDuty detects threats, so only Macie fits."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A streaming platform wants to cluster users based on viewing patterns for A/B testing but has no outcome labels. Which ML paradigm applies?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Supervised classification",
      "B": "Reinforcement learning",
      "C": "Unsupervised clustering",
      "D": "Regression"
    },
    "explanation": "Clustering viewing patterns without labels is unsupervised. Classification and regression need labeled targets; reinforcement learning requires a reward signal."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A retailer must decide whether to use ML or rule-based automation for order fraud checks. Which factor indicates ML is NOT appropriate?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Large historical dataset available",
      "B": "Patterns frequently change",
      "C": "High risk of loss on misclassification",
      "D": "Business rules are stable and simple"
    },
    "explanation": "Stable and well-defined rules don\u2019t justify ML complexity. ML is appropriate when data is large, patterns evolve, and high accuracy is needed."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An automotive manufacturer needs to predict remaining useful life of machinery components (continuous output). Which ML technique fits?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Binary classification",
      "B": "Regression modeling",
      "C": "Clustering",
      "D": "Topic modeling"
    },
    "explanation": "Remaining useful life is a continuous numeric prediction, so regression is appropriate. Classification outputs discrete labels; clustering groups data; topic modeling handles text."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A media company needs to automatically generate metadata tags (keywords) for video frames. Which service solves both image analysis and metadata extraction without custom training?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Rekognition Video",
      "B": "Amazon Comprehend",
      "C": "Amazon Translate",
      "D": "Amazon Textract"
    },
    "explanation": "Rekognition Video provides frame-level object detection and tagging. Comprehend is NLP, Translate translates text, Textract extracts text from images/documents."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A news aggregator wants to deliver personalized article feeds by learning user preferences over time. Which AWS service should be used to avoid building custom ML pipelines?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Personalize",
      "B": "Amazon SageMaker Ground Truth",
      "C": "Amazon Comprehend Topics",
      "D": "Amazon Forecast"
    },
    "explanation": "Amazon Personalize offers managed recommendation models. Ground Truth is labeling, Comprehend Topics is unsupervised grouping, Forecast is time-series."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A vendor needs to translate medical device manuals, preserving domain-specific terminology. Which feature of Amazon Translate helps achieve this?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Custom parallel data",
      "B": "Custom terminology glossary",
      "C": "Neural Text to Speech",
      "D": "Active Custom Translation"
    },
    "explanation": "Custom terminology glossaries ensure consistent translation of terms. Parallel data can improve quality, but glossaries specifically lock term translations. Neural TTS is Polly; active custom translation isn\u2019t a separate feature."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A support center wants to detect emerging topics in customer feedback over time without manual labeling. Which AWS managed feature supports this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Translate",
      "B": "Amazon Comprehend Sentiment API",
      "C": "Amazon Comprehend Topics",
      "D": "Amazon Lex"
    },
    "explanation": "Comprehend Topics performs unsupervised topic modeling to find themes. Sentiment API only labels sentiment; Translate and Lex serve other purposes."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "For an AI solution that must provide sub-second inference on simple numeric input, which deployment method in AWS SageMaker is most cost-effective?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Batch transform",
      "B": "Serverless inference",
      "C": "Training job",
      "D": "Real-time endpoint with minimal provisioned instances"
    },
    "explanation": "Sub-second inference requires a real-time endpoint. Minimal instance count reduces cost; serverless has higher cold-start latency; batch transform is asynchronous; training jobs don\u2019t serve inferences."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A multi-language support team needs to extract key phrases and then translate them. Which service sequence is correct?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Comprehend key-phrases \u2192 Translate",
      "B": "Translate \u2192 Comprehend sentiment",
      "C": "Transcribe \u2192 Polly",
      "D": "Rekognition \u2192 Comprehend"
    },
    "explanation": "You first extract text features (key phrases) with Comprehend, then translate them with Translate. Other sequences do not fulfill both steps."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A retail analytics team needs to perform exploratory data analysis and feature engineering on customer transaction data before model training. They want a visual, low-code tool that integrates with Amazon S3 and writes features directly to a centralized store for reuse. Which AWS service combination should they choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Glue DataBrew for EDA and Amazon SageMaker Feature Store",
      "B": "Amazon SageMaker Data Wrangler for EDA and Amazon SageMaker Feature Store",
      "C": "AWS Glue for EDA and Amazon S3 for feature storage",
      "D": "Amazon Athena for EDA and Amazon DynamoDB for feature storage"
    },
    "explanation": "Data Wrangler provides visual EDA and preprocessing and natively writes to SageMaker Feature Store, enabling feature reuse."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An online gaming company wants to monitor model performance post-deployment to detect data drift and bias. Which combination of AWS services meets both requirements with minimal custom code?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon CloudWatch metrics and Amazon QuickSight dashboards",
      "B": "AWS Lambda functions and Amazon SNS alerts",
      "C": "Amazon SageMaker Model Monitor and Amazon SageMaker Clarify",
      "D": "AWS Config rules and AWS CloudTrail logs"
    },
    "explanation": "Model Monitor captures data drift, Clarify monitors bias. Together they automate monitoring with minimal code."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A financial services firm needs hyperparameter tuning at scale for their credit-risk classification model. Which SageMaker feature should they use, and why?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Automatic Model Tuning, because it launches parallel training jobs to search hyperparameter space",
      "B": "SageMaker Neo, because it compiles models for edge deployment",
      "C": "SageMaker Debugger, because it visualizes gradients during training",
      "D": "SageMaker Edge Manager, because it manages endpoints at the edge"
    },
    "explanation": "Automatic Model Tuning runs parallel training with different hyperparameter sets to optimize a specified metric."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A start-up wants to serve a deep learning inference endpoint with unpredictable traffic patterns and minimize cost. Which production method should they choose?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy on Amazon EC2 behind an Application Load Balancer",
      "B": "Use SageMaker batch transform with scheduled jobs",
      "C": "Host a self-managed Kubernetes cluster with auto-scaling",
      "D": "Deploy a SageMaker serverless inference endpoint"
    },
    "explanation": "Serverless inference automatically scales to zero and up on demand, minimizing cost under unpredictable traffic."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "During model evaluation, the data scientist notices class imbalance. They need a metric that accounts for precision and recall. Which metric should they choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Accuracy",
      "B": "F1 score",
      "C": "Mean squared error",
      "D": "Log loss"
    },
    "explanation": "F1 score combines precision and recall, making it suitable for imbalanced classes."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A marketing team wants reproducible experiments and lineage tracking for each model iteration. Which SageMaker capability should they leverage?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Model Monitor",
      "B": "SageMaker Clarify",
      "C": "SageMaker Experiments",
      "D": "SageMaker Edge Manager"
    },
    "explanation": "SageMaker Experiments tracks parameters, metrics, artifacts, and lineage of experiments for reproducibility."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A hospital develops a medical imaging classifier. Regulations require ad-hoc manual review of flagged cases. Which combination helps automate inference and human review?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker batch transform and Amazon CloudWatch Alarms",
      "B": "SageMaker real-time endpoint and AWS Lambda",
      "C": "Amazon Augmented AI (A2I) with SageMaker real-time endpoint",
      "D": "SageMaker Neo compiled model and AWS Step Functions"
    },
    "explanation": "A2I routing config on SageMaker endpoint sends uncertain predictions for human review automatically."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An e-commerce firm wants to compare performance of a custom model against a pre-trained model from JumpStart without managing training infrastructure. Which approach meets this requirement?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Download JumpStart model, fine-tune locally, compare metrics",
      "B": "Use SageMaker JumpStart to deploy both models to endpoints and run A/B tests",
      "C": "Use Amazon EC2 instances for both models, deploy batch transforms",
      "D": "Use AWS Lambda for custom code and AWS Glue for pre-trained inference"
    },
    "explanation": "JumpStart can deploy both pre-trained and custom models to endpoints; A/B testing through Invocation and metrics comparison."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A credit card company wants to store features for online real-time fraud detection and offline batch risk scoring. Which storage option should they choose?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon S3 for both online and offline",
      "B": "Amazon DynamoDB for both",
      "C": "Amazon Aurora for both",
      "D": "SageMaker Feature Store with online and offline stores"
    },
    "explanation": "Feature Store provides both low-latency online and batch offline stores with consistent feature data."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A manufacturing company needs to orchestrate an ML pipeline that includes data processing, model training, tuning, deployment, and monitoring in a repeatable way. Which service is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon SageMaker Pipelines",
      "B": "AWS Step Functions without SageMaker integrations",
      "C": "AWS Data Pipeline",
      "D": "AWS Batch"
    },
    "explanation": "SageMaker Pipelines is designed to orchestrate ML workflows end-to-end with native SageMaker steps."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "After deployment, a model\u2019s prediction latency spikes during peak hours. Which SageMaker feature helps maintain consistent latencies?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model Monitor data capture",
      "B": "Automatic endpoint scaling with target utilization",
      "C": "Clarify bias detection",
      "D": "Feature Store online cache"
    },
    "explanation": "Automatic scaling on SageMaker endpoints adjusts instance count based on utilization to maintain latency."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A data science team must compare multiple model versions\u2019 performance on a benchmark dataset and register the best version for production. Which workflow accomplishes this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Glue jobs to evaluate models and update S3",
      "B": "Evaluate in Jupyter notebooks, manually tag models",
      "C": "Use SageMaker Experiments for tracking, register in SageMaker Model Registry",
      "D": "Deploy all versions to endpoints and pick highest throughput"
    },
    "explanation": "Experiments track runs; Model Registry stores approved model versions for production."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A company collects streaming sensor data and wants to preprocess in real time before scoring. Which SageMaker component should they use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Data Wrangler",
      "B": "Glue ETL",
      "C": "Athena",
      "D": "SageMaker Processing with a streaming endpoint"
    },
    "explanation": "SageMaker Processing allows custom preprocessing code in real-time via a model endpoint or processing container integrated with streaming sources."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which metric is most appropriate for selecting a regression model that balances prediction error and penalizes large errors in a housing price prediction task?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "R\u00b2 (coefficient of determination)",
      "B": "Root Mean Squared Error (RMSE)",
      "C": "Mean Absolute Percentage Error (MAPE)",
      "D": "Accuracy"
    },
    "explanation": "RMSE penalizes larger errors more heavily, suitable when large deviations are especially undesirable."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A team needs to retrain their fraud detection model when input data distribution changes significantly. Which process should they implement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Model Monitor to detect drift and trigger SageMaker Pipelines retraining step",
      "B": "Daily manual retraining based on calendar schedule",
      "C": "Use CloudWatch to trigger a Lambda every hour for retraining",
      "D": "Rebuild model only when accuracy falls below 99%"
    },
    "explanation": "Model Monitor detects statistical drift and can trigger Pipelines to automate retraining upon drift detection."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A model deployed with multi-model endpoints in SageMaker needs version control and staged rollout. Which pattern supports this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use separate endpoints per model and DNS routing",
      "B": "Use batch transform for staging",
      "C": "Use SageMaker Model Registry with endpoint variant weights",
      "D": "Use AWS Lambda alias shifting"
    },
    "explanation": "Model Registry supports deployment as endpoint variants with weight shifting for staged rollouts."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A recommendation engine team wants to evaluate two ranking algorithms offline without affecting production. Which SageMaker feature helps conduct such evaluation at scale?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Real-time inference endpoint with traffic split",
      "B": "SageMaker Batch Transform on held-out data",
      "C": "Ground Truth labeling jobs",
      "D": "Athena on S3 logs"
    },
    "explanation": "Batch Transform can process large volumes of data offline to compare algorithm outputs on the same dataset."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A life-sciences company needs transparent audit trails for all data and model changes. Which combination provides lineage tracking and immutable storage?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Experiments for lineage and Amazon S3 Object Lock for immutability",
      "B": "AWS CloudTrail and AWS Config",
      "C": "AWS Glue Data Catalog and AWS Backup",
      "D": "Amazon Macie and AWS KMS"
    },
    "explanation": "Experiments tracks ML lineage; S3 Object Lock ensures immutable storage of artifacts."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "For a use case requiring nightly batch scoring and daily model refresh, which deployment architecture is most cost-effective and operationally simple?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Real-time endpoint always on",
      "B": "Multi-model endpoint with constant instances",
      "C": "Self-hosted EC2 cluster with auto-scaling",
      "D": "Batch Transform for scoring and Pipelines for scheduled retraining"
    },
    "explanation": "Batch Transform supports scheduled batch scoring; Pipelines can orchestrate daily retraining, reducing always-on cost."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which SageMaker feature helps ensure model reproducibility by capturing Docker environment, code, data input location, and hyperparameters automatically?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Clarify",
      "B": "SageMaker Experiments",
      "C": "SageMaker Model Monitor",
      "D": "SageMaker Edge Manager"
    },
    "explanation": "Experiments automatically records environment, inputs, parameters, and outputs to reproduce runs exactly."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A team is evaluating open-source versus custom-trained models for NLP. They need a service that supports both and provides unified deployment. Which AWS service should they choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Comprehend",
      "B": "Amazon Translate",
      "C": "Amazon SageMaker endpoints",
      "D": "Amazon Lex"
    },
    "explanation": "SageMaker endpoints can host custom and prebuilt open-source models for unified deployment."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "After deploying a computer vision model to an endpoint, the data science team needs to detect bias toward certain image attributes. Which tool and integration should they use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use CloudWatch to track accuracy per attribute",
      "B": "Use Batch Transform and Athena queries",
      "C": "Use QuickSight dashboards on logs",
      "D": "Integrate SageMaker Clarify with endpoint data capture"
    },
    "explanation": "Clarify can analyze captured prediction data from endpoints to assess bias across attributes."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A media company wants to compare training jobs across different compute instance types to optimize cost-performance trade-off. Which SageMaker feature helps automate this comparison?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Automatic Model Tuning with resource search",
      "B": "SageMaker Edge Manager",
      "C": "SageMaker Debugger",
      "D": "SageMaker Neo"
    },
    "explanation": "Automatic Model Tuning can search over hyperparameters and resource types to optimize performance metric."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A healthcare provider must ensure their model pipeline is compliant and records all configuration changes. Which service combination supports compliance and auditing for both data and model steps?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Model Monitor and GuardDuty",
      "B": "AWS CloudTrail with SageMaker Pipeline logging",
      "C": "AWS Config and Amazon Inspector",
      "D": "Amazon Macie and AWS Key Management Service"
    },
    "explanation": "CloudTrail records API calls for pipelines and configurations, ensuring audit trails; pipelines log each step."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which training approach allows you to use pre-built AWS container images with custom code, track experiments, and easily transition to deployment?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Training jobs with pre-built containers and Experiments",
      "B": "Use EC2 with Docker installed manually and notebooks",
      "C": "Use AWS Batch",
      "D": "Package code for AWS Lambda training"
    },
    "explanation": "SageMaker Training with AWS images lets you supply entry point scripts, integrates with Experiments, and deploys to endpoints."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A telecom operator wants near real-time feature updates for churn prediction. Which architecture best supports low-latency feature ingestion and model inference?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Glue ETL feeding S3 to batch transform",
      "B": "Athena queries feeding endpoint",
      "C": "Lambda writing to DynamoDB invoked by batch job",
      "D": "Kinesis Data Streams -> Lambda -> Feature Store online -> SageMaker endpoint"
    },
    "explanation": "Kinesis and Lambda capture streaming data, store in Feature Store\u2019s online store, then low-latency SageMaker endpoint for inference."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A logistics firm wants automated model retraining whenever model accuracy falls below 85% on live data. Which pattern achieves this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Integrate Model Monitor alerts with EventBridge to trigger Pipelines retraining",
      "B": "Manually check CloudWatch and run Pipeline",
      "C": "Use Lambda on a fixed schedule",
      "D": "Use CloudTrail to capture retraining calls"
    },
    "explanation": "Model Monitor can stream metrics; EventBridge rule triggers retraining Pipeline when accuracy metric crosses threshold."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "During feature engineering, a data scientist wants to quickly prototype SQL-based transformations on S3 data without spinning up servers. Which service should they use?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Glue Studio notebooks",
      "B": "Amazon EMR cluster",
      "C": "Amazon Athena with SageMaker Data Wrangler integration",
      "D": "Amazon Redshift"
    },
    "explanation": "Athena can run serverless SQL on S3; Data Wrangler integrates with Athena queries for rapid prototyping."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which key MLOps principle is demonstrated by using SageMaker Model Registry with approval workflows before production deployment?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Scalability",
      "B": "Governance and reproducibility",
      "C": "Experimentation",
      "D": "Edge deployment"
    },
    "explanation": "Model Registry approval workflows enforce governance and ensure reproducibility and auditability before deployment."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which combination of SageMaker services supports the full ML lifecycle from data collection through monitoring, while minimizing custom infrastructure?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Data Wrangler, Training jobs, Pipelines, Model Monitor",
      "B": "EC2, Lambda, DynamoDB, CloudWatch",
      "C": "Glue, Batch, S3, Athena",
      "D": "EMR, Kinesis, Redshift, QuickSight"
    },
    "explanation": "Data Wrangler handles EDA, Training jobs train models, Pipelines orchestrate, and Model Monitor provides post-deployment monitoring."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A data science team is building a document retrieval system using embeddings. They initially split documents into fixed 512-token chunks with no overlap, resulting in poor semantic coherence at chunk boundaries. Which chunking strategy best improves retrieval quality while controlling embedding count?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Keep 512-token fixed chunks and add a 50-token overlap between consecutive chunks.",
      "B": "Use sentence-boundary chunking, grouping up to 512 tokens per chunk without splitting sentences.",
      "C": "Reduce chunk size to 256 tokens with no overlap to avoid splitting concepts.",
      "D": "Increase chunk size to 800 tokens with no overlap to cover full paragraphs."
    },
    "explanation": "Sentence-boundary chunking preserves semantic units and limits embedding count, improving retrieval without unnecessary overlap."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You need to store embeddings for a multi-language knowledge base. Which metric best captures semantic similarity across languages in a shared vector space?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Euclidean distance, since it measures absolute difference between vectors.",
      "B": "Manhattan distance, because it sums per-dimension differences.",
      "C": "Cosine similarity, as it normalizes for vector magnitude and focuses on orientation.",
      "D": "Hamming distance, due to binary representation of multilingual tokens."
    },
    "explanation": "Cosine similarity normalizes vector lengths and highlights semantic orientation, critical for cross-language embeddings."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A generative AI model produces repetitive output when asked to summarize varied documents. Which embedding-based retrieval technique can mitigate this by enriching prompts with relevant context?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use larger chunks so more context is in each prompt.",
      "B": "Apply k-means clustering on embeddings to select representative chunks.",
      "C": "Use Euclidean-based nearest neighbor retrieval for variety.",
      "D": "Implement Retrieval Augmented Generation (RAG), retrieving top-k embeddings to include in the prompt."
    },
    "explanation": "RAG enriches the prompt with diverse, relevant context retrieved via embeddings, reducing repetition."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "When fine-tuning a foundation model on domain text, you observe that rare domain tokens are tokenized into multiple subwords, harming learning. Which tokenization change addresses this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a custom byte-pair encoding tokenizer including domain vocabulary.",
      "B": "Switch to character-level tokenization to avoid subwords.",
      "C": "Increase context window size so subwords span longer sequences.",
      "D": "Use word-level tokenization to treat each word as a token."
    },
    "explanation": "Custom BPE with domain terms as tokens ensures rare domain words are single tokens, improving model learning."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A team is comparing transformer and diffusion models for image generation. They need rapid, deterministic output for UI previews. Which model type is most appropriate?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Transformer-based autoregressive diffusion model for faster, one-shot inference.",
      "B": "Standard diffusion model, despite its iterative denoising steps.",
      "C": "Variational autoencoder, even though it often blurs details.",
      "D": "GAN, because it\u2019s non-iterative but suffers from mode collapse."
    },
    "explanation": "Autoregressive transformer models provide fast, deterministic generation suitable for previews; standard diffusion is iterative."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "During prompt engineering you observe model hallucinations when generating code. Which embedding-based approach helps verify correctness?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature to introduce randomness and explore alternatives.",
      "B": "Use larger context windows to feed entire codebase at once.",
      "C": "Retrieve relevant code snippets via embeddings and include them as examples in the prompt.",
      "D": "Switch to pure instruction-tuned model without retrieval."
    },
    "explanation": "Retrieving similar code via embeddings grounds the model and reduces hallucinations by providing factual examples."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You must index embeddings for a 10 B-token text corpus. Which approximate nearest neighbor index balances query latency and memory footprint?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Brute-force linear scan with GPU acceleration.",
      "B": "KD-Tree index, which degrades in high dimensions.",
      "C": "Exact VP-Tree for guaranteed accuracy.",
      "D": "HNSW (Hierarchical Navigable Small World) graph index for fast, memory-efficient search."
    },
    "explanation": "HNSW offers a good trade-off of performance and resource use for high-dimensional embeddings."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "In a multi-modal foundation model, how are image pixels and text tokens aligned in the embedding space?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Image CNN features appended to token IDs directly.",
      "B": "Project both image and text through modality-specific encoders into a shared vector space.",
      "C": "Interleave pixel values and token embeddings in a single sequence.",
      "D": "Convert images to text descriptions only and embed as text."
    },
    "explanation": "Modality-specific encoders map different inputs into a unified embedding space for multi-modal reasoning."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A foundation model uses 16-bit floating point embeddings, but memory constraints prompt reducing to 8-bit. What is the primary risk?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increased overfitting due to lower precision.",
      "B": "Longer inference latency from dequantization overhead.",
      "C": "Reduced embedding fidelity, harming retrieval accuracy.",
      "D": "Incompatibility with cosine similarity calculations."
    },
    "explanation": "Quantizing to 8-bit lowers vector precision, which may degrade semantic similarity estimates."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which prompt technique helps a generative model better follow multi-step instructions requiring reasoning?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Single-shot prompting with all instructions in one block.",
      "B": "Zero-shot prompting with no examples.",
      "C": "Chain-of-thought prompting, listing reasoning steps internally.",
      "D": "Few-shot chain-of-thought prompting, demonstrating multi-step reasoning with examples."
    },
    "explanation": "Few-shot chain-of-thought shows the model how to reason step-by-step, improving multi-step task performance."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your diffusion model generates artifacts when denoising high-frequency image regions. Which adjustment reduces this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase the number of diffusion timesteps to smooth transitions.",
      "B": "Reduce batch size to focus on each image.",
      "C": "Use a higher learning rate during fine-tuning.",
      "D": "Switch to a smaller context window."
    },
    "explanation": "More timesteps allow finer-grained denoising, reducing artifacts in detailed areas."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A team wants to pre-train a transformer model on text and images jointly. Which architecture component is essential for consistent position tracking?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Separate attention heads for each modality without sharing.",
      "B": "Shared positional embeddings so both text tokens and image patches have coherent position encoding.",
      "C": "Sinusoidal positional encodings only for text.",
      "D": "Relative positional encodings only for images."
    },
    "explanation": "Shared positional embeddings ensure the model understands spatial and sequential relationships across modalities."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "During foundation model selection for a customer Q&A chatbot, which factor is least critical?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model\u2019s tokenization scheme aligns with domain vocabulary.",
      "B": "Embedding dimension matches existing vector store.",
      "C": "Inference latency meets SLA.",
      "D": "Model\u2019s pre-training data license region."
    },
    "explanation": "While licensing matters generally, regional license restrictions typically don\u2019t affect inference suitability directly."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "When using embeddings for semantic search, why might you prefer dense vector indexes over inverted term indexes?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Dense indexes capture semantics beyond exact keyword matches.",
      "B": "Inverted indexes are faster at high-dimensional similarity search.",
      "C": "Dense indexes require exact token overlap.",
      "D": "Inverted indexes scale linearly with vector dimension."
    },
    "explanation": "Dense vector indexes use embeddings to capture meaning similarity rather than exact term matching."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A model fine-tuned with instruction prompts fails on unseen instructions. What tokenizer issue could cause this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Tokenizer uses byte-level encoding, splitting known words.",
      "B": "Tokenizer vocabulary size is too large.",
      "C": "New instruction tokens map to unknown embeddings causing OOV splitting.",
      "D": "Tokenizer adds unwanted BOS/EOS tokens only."
    },
    "explanation": "Unknown instruction tokens are split into multiple subwords, hurting the model\u2019s understanding of the new instructions."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "For long-form text generation, which transformer attention mechanism reduces quadratic complexity?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Full self-attention with learned masks.",
      "B": "Sparse attention, attending only to local and selected global tokens.",
      "C": "Relative positional attention only.",
      "D": "Cross-attention without self-attention."
    },
    "explanation": "Sparse attention limits token interactions, reducing complexity while preserving key global context."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "In a foundation model lifecycle, which phase directly addresses embedding drift over time?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Pre-training from scratch on general corpus.",
      "B": "Initial fine-tuning on task data.",
      "C": "Continuous pre-training or periodic re-embedding with fresh data.",
      "D": "One-time batch embedding generation."
    },
    "explanation": "Continuous pre-training or re-embedding on new data counteracts embedding drift as data evolves."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A generative audio model uses diffusion. To speed up inference while preserving fidelity, which strategy helps?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use knowledge distillation to train a smaller model replicating diffusion outputs.",
      "B": "Increase diffusion timesteps and lower scheduling rate.",
      "C": "Use pure autoregressive sampling instead of diffusion.",
      "D": "Reduce model depth without any distillation."
    },
    "explanation": "Distillation transfers knowledge to a smaller network that approximates diffusion outputs faster."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which foundational concept differentiates transformer embeddings from RNN hidden states?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Transformers use convolution over tokens instead of recurrence.",
      "B": "Transformers have no positional encodings.",
      "C": "RNN states are parallelizable, transformers aren\u2019t.",
      "D": "Transformer embeddings derive from global self-attention, not sequential hidden states."
    },
    "explanation": "Self-attention lets transformers compute embeddings considering all tokens simultaneously, unlike sequential RNN states."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your diffusion-based image model hallucinates text in generated images. Which training adjustment reduces this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Reduce diffusion timesteps for coarser denoising.",
      "B": "Include more varied real images without text overlays in pre-training data.",
      "C": "Increase learning rate during fine-tuning on textless images.",
      "D": "Switch to autoregressive transformer model."
    },
    "explanation": "Excluding text-overlaid images during pre-training prevents the model from learning to generate unwanted text."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "In embedding-based clustering, which technique improves grouping of semantically similar documents?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Use raw TF-IDF vectors instead of embeddings.",
      "B": "Apply PCA retaining only the top principal component.",
      "C": "Normalize embeddings to unit length before clustering.",
      "D": "Cluster on unnormalized Euclidean distances."
    },
    "explanation": "Unit-normalizing embeddings ensures clustering uses angular relationships, reflecting semantic similarity."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A text-to-image pipeline uses CLIP embeddings for retrieval. Which aspect of CLIP embeddings is most critical for this task?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CLIP\u2019s large vocabulary size.",
      "B": "Joint text-image embedding alignment for cross-modal similarity.",
      "C": "CLIP\u2019s text generation capability.",
      "D": "Its autoregressive decoding mechanism."
    },
    "explanation": "CLIP embeddings are designed so text and image reside in the same vector space, enabling retrieval."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "To compress large embedding indexes for cost efficiency, which quantization method trades off minimal accuracy loss?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Product quantization splitting vectors into subspaces.",
      "B": "Binarization of entire embedding to 1 bit.",
      "C": "Scalar quantization to nearest integer.",
      "D": "Full 8-bit uniform quantization without subspace partitioning."
    },
    "explanation": "Product quantization reduces storage with limited accuracy loss by vector subspace quantization."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your foundation model uses sinusoidal positional encodings but underperforms on very long sequences. Which encoding modification addresses this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase the frequency of sinusoidal embeddings.",
      "B": "Remove positional encodings altogether.",
      "C": "Switch to rotary positional embeddings supporting extrapolation.",
      "D": "Use absolute position embeddings learned only during pre-training."
    },
    "explanation": "Rotary embeddings generalize better to unseen sequence lengths, improving long-sequence performance."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "When designing prompts for image generation with diffusion models, what does a negative prompt typically achieve?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Specifies the desired style explicitly.",
      "B": "Increases sampling steps for clarity.",
      "C": "Embeds an example image in the prompt.",
      "D": "Tells the model which elements to avoid generating."
    },
    "explanation": "Negative prompts instruct diffusion models on concepts or artifacts to omit during generation."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "In a retrieval-augmented LLM application, you notice redundant context in multiple retrieved passages. Which embedding approach reduces redundancy?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase embedding dimension to separate topics.",
      "B": "Use Maximal Marginal Relevance to re-rank and diversify retrieved embeddings.",
      "C": "Switch to Euclidean similarity to penalize similar vectors.",
      "D": "Apply k-means clustering on the prompt itself."
    },
    "explanation": "MMR re-ranks retrieval to maximize relevance while reducing redundancy among selected passages."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which tokenization approach best handles morphologically rich languages in foundation models?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SentencePiece byte-pair encoding capturing subword units.",
      "B": "Word-level tokenization splitting on whitespace.",
      "C": "Character-level tokenization ignoring subwords.",
      "D": "Rule-based stemming before tokenization."
    },
    "explanation": "Byte-pair encoding adapts to morphological variation by learning subword units reflecting language structure."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "During fine-tuning, you observe overfitting on a small domain dataset. Which embedding-related step can mitigate this?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Reduce embedding dimension to lower model capacity.",
      "B": "Switch to a larger pre-trained foundation model.",
      "C": "Freeze lower transformer layers including initial embedding layer.",
      "D": "Increase learning rate for the embedding layer."
    },
    "explanation": "Freezing early layers prevents overfitting by limiting parameter updates to higher-level layers."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your multi-modal foundation model\u2019s cross-modal retrieval suffers from hubness (few vectors nearest to many). Which technique alleviates hubness?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Decrease embedding dimension to 128.",
      "B": "Normalize only image embeddings, not text.",
      "C": "Use Euclidean distance instead of cosine.",
      "D": "Apply Local Scaling to adjust neighbor distances by local density."
    },
    "explanation": "Local Scaling adjusts similarity based on neighborhood density and reduces hubness."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "When deploying a diffusion model behind an API, prompt sizes vary. How can chunking token sequences for the denoiser improve throughput?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use fixed 1024-token chunks regardless of prompt length.",
      "B": "Batch similar token lengths together to minimize padding and GPU waste.",
      "C": "Always pad to the max context length to standardize batches.",
      "D": "Split on sentence boundaries without re-batching."
    },
    "explanation": "Batching similar lengths reduces padding overhead, improving GPU utilization and throughput."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A financial services firm plans to deploy a generative AI assistant to draft client communications that must be consistent and fully auditable. Which decoding strategy and parameter setting best ensures deterministic outputs for audit trails?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use top-p sampling with p=0.9",
      "B": "Apply temperature=0.7 with nucleus sampling",
      "C": "Use greedy decoding or temperature=0.0",
      "D": "Use beam search with high diversity penalty"
    },
    "explanation": "Greedy decoding (or temperature=0.0) produces deterministic outputs, critical for auditability. Sampling methods introduce nondeterminism."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A startup needs to generate personalized product descriptions at high throughput while controlling per-token costs. Which pricing model and AWS service offers the best trade-off?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon SageMaker endpoint using large custom model charged by instance-hour",
      "B": "Amazon Bedrock with token-based Foundation Model API",
      "C": "SageMaker JumpStart small model with on-demand EC2 billing",
      "D": "Amazon Lex usage-based utterance pricing"
    },
    "explanation": "Bedrock token-based billing aligns cost to usage and supports high throughput, minimizing idle instance costs."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A media company uses a foundation model for video caption generation but sees occasional hallucinated events. Which mitigation approach most directly reduces hallucinations?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature to encourage diversity",
      "B": "Use a larger foundation model",
      "C": "Switch to zero-shot prompting",
      "D": "Implement retrieval-augmented generation with vetted transcript store"
    },
    "explanation": "Retrieval-augmented generation grounds outputs in verified transcripts, reducing hallucination."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An e-commerce platform measures conversion lift after integrating generative recommendations. Which metric best isolates model impact on average order value (AOV)?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Relative increase in AOV of users exposed vs. control group",
      "B": "Total number of generated recommendations",
      "C": "Model perplexity on recommendation prompts",
      "D": "Percentage of successful API calls"
    },
    "explanation": "Comparing AOV between exposed and control isolates the model\u2019s impact on order value."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A legal tech firm must ensure generated contract clauses are legally accurate. Which foundation model setting reduces factual inaccuracy?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use higher temperature",
      "B": "Use low temperature with domain-specific fine-tuning",
      "C": "Use larger context window without fine-tuning",
      "D": "Use few-shot prompting with random examples"
    },
    "explanation": "Fine-tuning on domain-specific legal text plus low temperature reduces inaccuracy."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A healthcare chatbot must comply with privacy regulations and avoid hallucinations. Which deployment choice best balances compliance and performance?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Public foundation model hosted on AWS GovCloud",
      "B": "High-capacity public API with encryption in transit",
      "C": "Bedrock API with multi-tenant model",
      "D": "Deploy a private Bedrock foundation model in a VPC endpoint"
    },
    "explanation": "Private Bedrock in VPC ensures data residency, encryption, and reduces risk of data leakage."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A retailer notices that their generative AI product descriptions sometimes omit critical warranty details. What\u2019s the most effective prompt engineering approach to address this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase model temperature for creativity",
      "B": "Switch to a larger foundation model",
      "C": "Add explicit instruction and examples highlighting warranty details (few-shot)",
      "D": "Use an external summarization service"
    },
    "explanation": "Few-shot prompting with explicit examples ensures the model includes required details."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A SaaS provider wants to measure API latency cost trade-offs for real-time generative responses. Which strategy yields lowest average latency per dollar?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Scale up to a larger GPU instance in SageMaker",
      "B": "Select a smaller Bedrock foundation model with sufficient quality",
      "C": "Use high-throughput batching on a large SageMaker endpoint",
      "D": "Implement multi-region replication of a large model"
    },
    "explanation": "Smaller Bedrock models reduce latency and cost per inference, balancing quality and performance."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A marketing team must generate social media posts quickly but with brand consistency. Which generative AI advantage addresses this need?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model interpretability",
      "B": "Cross-domain performance",
      "C": "Non-determinism",
      "D": "Adaptability and responsiveness via prompt templates"
    },
    "explanation": "Prompt templates leverage adaptability/responsiveness to rapidly produce consistent brand voice."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A data scientist is comparing performance of two foundation models on cross-domain tasks. Which metric best captures relative performance?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Average F1 score across domain-specific benchmark datasets",
      "B": "Total number of parameters",
      "C": "Training compute FLOPs",
      "D": "Context window size"
    },
    "explanation": "F1 across multiple benchmarks measures cross-domain performance, capturing precision and recall."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A business requires the simplest possible integration of generative AI without managing infrastructure or model customization. Which AWS offering meets this?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon SageMaker custom endpoint",
      "B": "Amazon Bedrock managed API",
      "C": "SageMaker JumpStart custom container",
      "D": "Self-hosted open-source model on EC2"
    },
    "explanation": "Bedrock managed API abstracts infrastructure and customization, offering simplicity."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A global company must comply with data residency laws when generating customer insights. Which approach ensures compliance?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use multi-region Bedrock endpoints without restrictions",
      "B": "Encrypt data in transit only",
      "C": "Deploy region-specific Bedrock foundation models with VPC lockdown",
      "D": "Use SageMaker JumpStart across regions"
    },
    "explanation": "Region-specific deployment with VPC lockdown ensures data stays within required jurisdictions."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A team measures ARPU uplift after integrating generative chat. Which analysis isolates generative AI\u2019s contribution to ARPU?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A/B test difference in ARPU between chat-enabled and baseline cohorts",
      "B": "Total API token usage",
      "C": "Average session length",
      "D": "Number of chat messages generated"
    },
    "explanation": "A/B testing ARPU directly measures generative chat\u2019s impact on revenue per user."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An enterprise struggles with hallucinations in summary generation of compliance documents. Which model choice most reduces hallucinations?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase prompt context length",
      "B": "Use zero-shot summarization on large model",
      "C": "Switch to text-diffusion model",
      "D": "Implement RAG over a curated compliance document store"
    },
    "explanation": "RAG uses verified source documents, preventing unsupported hallucinated content."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A company must choose between fine-tuning a foundation model or employing few-shot prompting. They have limited labeled data (<100 examples). Which approach is preferable?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Fine-tune model with limited examples",
      "B": "Use few-shot in-context learning via prompt templates",
      "C": "Pre-train model on unrelated large dataset",
      "D": "Deploy zero-shot without examples"
    },
    "explanation": "Few-shot prompting is more effective than fine-tuning when labeled data is scarce."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A manufacturer wants to generate maintenance logs from sensor data summaries. They require high explainability for audits. Which model attribute should they prioritize?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Large multimodal diffusion model",
      "B": "High temperature setting",
      "C": "Transparent architecture with model card and provenance tracking",
      "D": "Black-box large LLM with accuracy metrics"
    },
    "explanation": "Transparent model with documented provenance supports explainability required for audits."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A retailer must generate dynamic promotional images. They care most about generation speed over fine detail. Which generative model type is best?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Transformer-based text-to-image",
      "B": "Diffusion model optimized for low sampling steps (e.g., high-speed pipeline)",
      "C": "GAN with extensive fine-tuning",
      "D": "VAE-based high-fidelity model"
    },
    "explanation": "Diffusion models with fewer sampling steps trade quality for speed, aligning with requirements."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A gaming company observes inconsistent NPC dialogues. They need more coherent responses. Which prompt technique improves coherence?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase max tokens limit",
      "B": "Chain-of-thought prompting with structured steps",
      "C": "Use random negative prompts",
      "D": "Switch to unsupervised learning"
    },
    "explanation": "Chain-of-thought prompting guides the model through structured reasoning, improving coherence."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An organization tracking customer lifetime value wants generative personalization. Which performance metric best evaluates business impact?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model perplexity on user prompts",
      "B": "Token generation latency",
      "C": "Number of personalized messages",
      "D": "Incremental CLV uplift compared to control"
    },
    "explanation": "Incremental CLV uplift directly measures personalization\u2019s effect on long-term customer value."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A compliance team must prevent inadvertent exposure of regulated content in generative outputs. Which technique best addresses this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Implement guardrails with Bedrock safety modules",
      "B": "Increase model temperature",
      "C": "Use larger context window",
      "D": "Switch to zero-shot prompting"
    },
    "explanation": "Bedrock guardrails allow policy-based filters to block sensitive content, ensuring compliance."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A financial application requires consistent terminology across generated reports. Which model setting prevents terminology drift?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable stochastic beam search",
      "B": "Apply high top-k sampling",
      "C": "Use low temperature with glossary injection in prompt",
      "D": "Increase response length"
    },
    "explanation": "Low temperature reduces randomness and injecting a glossary anchors terminology."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An AI team compares API cost effectiveness for low-volume high-value document generation. Which AWS billing dimension should they optimize?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Instance-hour in SageMaker",
      "B": "Token count in Bedrock",
      "C": "Number of API calls in Lex",
      "D": "Storage bytes in S3"
    },
    "explanation": "Token count billing in Bedrock maps directly to document length and value, optimizing cost."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A pharma company is concerned about model interpretability for clinical summaries. Which limitation of foundation models is most relevant?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Token-based pricing unpredictability",
      "B": "High throughput latency",
      "C": "Multimodal data handling",
      "D": "Black-box nature limiting transparency"
    },
    "explanation": "Foundation models often lack interpretability, a key concern in regulated clinical contexts."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A customer wants to balance creativity and factual accuracy in a marketing copy generator. Which temperature setting is optimal?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Moderate temperature (~0.5)",
      "B": "High temperature (~1.0)",
      "C": "Zero temperature",
      "D": "Top-p=0.2 only"
    },
    "explanation": "A moderate temperature balances creativity and control, mitigating hallucinations while allowing variation."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A global brand uses generative AI translations. They need consistent style across languages. Which foundation model feature supports this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Larger parameter count",
      "B": "Zero-shot capabilities",
      "C": "Multilingual fine-tuning on brand corpus",
      "D": "Higher decoding temperature"
    },
    "explanation": "Fine-tuning on a multilingual brand corpus ensures consistent style across languages."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A customer embedding service sees decline in similarity accuracy across evolving knowledge base. Which approach restores cross-domain performance?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase embedding vector size",
      "B": "Recompute embeddings with incremental index using RAG pipeline",
      "C": "Switch to random sampling metrics",
      "D": "Apply high temperature during encoding"
    },
    "explanation": "Recomputing embeddings and using RAG ensures embeddings reflect updated knowledge, restoring accuracy."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A brand wants to minimize latency for chat responses during peak hours. Which scaling strategy is most cost-effective?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provision large SageMaker endpoint 24/7",
      "B": "Increase VPC throughput",
      "C": "Use Bedrock serverless with auto-scaling",
      "D": "Deploy on single EC2 spot instance"
    },
    "explanation": "Bedrock serverless auto-scales with demand, minimizing idle cost and ensuring low latency."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An organization tracks generative AI performance across domains. Which composite metric provides holistic evaluation?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Weighted score combining BLEU, ROUGE, and domain GDP impact",
      "B": "Average token count per response",
      "C": "Only perplexity on validation set",
      "D": "Percentage of API errors"
    },
    "explanation": "A composite metric blending NLP scores and business impact yields holistic performance evaluation."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A consulting firm must enforce data governance in generative pipelines. Which AWS feature helps trace data lineage in Bedrock?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Model Monitor",
      "B": "AWS Config rules",
      "C": "AWS Macie data classification",
      "D": "Bedrock model cards with provenance metadata"
    },
    "explanation": "Model cards in Bedrock include provenance metadata and lineage, supporting governance."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A team needs to estimate throughput cost for batch image generation via diffusion models. Which factor most affects cost estimate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Number of prompt parameters",
      "B": "Number of sampling steps per image",
      "C": "Vocabulary size",
      "D": "Context window length"
    },
    "explanation": "Sampling steps determine computation per image, directly impacting cost in diffusion pipelines."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A regulated healthcare provider needs to deploy a generative AI service that ensures all inference logs and API calls are captured in AWS CloudTrail for audit. They must minimize operational overhead by not managing servers. Which AWS service meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon SageMaker JumpStart endpoint behind AWS API Gateway",
      "B": "Amazon Bedrock",
      "C": "Amazon Q",
      "D": "Amazon Comprehend"
    },
    "explanation": "Bedrock is a fully managed service with built-in CloudTrail logging for all API calls and no server management. SageMaker JumpStart requires you to manage endpoints. Amazon Q is for document Q&A but less mature for full audit and isn\u2019t explicitly CloudTrail-integrated. Comprehend isn\u2019t a foundation model service."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A startup wants to generate marketing copy using a pre-trained LLM. They anticipate low monthly usage but occasional spikes. They need to minimize costs when idle. Which AWS offering is most cost-effective?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Bedrock token-based pricing",
      "B": "SageMaker JumpStart with on-demand EC2 instances",
      "C": "Self-hosted LLM on an always-on EC2 cluster",
      "D": "Amazon Q provisioned capacity"
    },
    "explanation": "Bedrock\u2019s token-based pricing means you pay only when you invoke the model, minimizing idle costs. JumpStart incurs instance-hour charges whenever the endpoint is active, even if idle. Self-hosting means always-on cluster costs. Amazon Q\u2019s provisioned capacity must be paid for even when idle."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A telecom company needs a generative AI service within their VPC for legal compliance. They want to fine-tune a foundation model with internal data without exposing data outside the VPC. Which AWS service and deployment pattern should they choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock public endpoint with KMS encryption",
      "B": "Amazon Q with private link",
      "C": "SageMaker JumpStart in VPC with private subnets",
      "D": "Self-managed open-source model in EKS"
    },
    "explanation": "SageMaker JumpStart can be deployed in your VPC private subnets, keeping data in-VPC and isolating model training. Bedrock endpoints are managed outside your VPC. Amazon Q does not support private VPC deployments. Self-managing requires substantial ops overhead."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A media company wants to generate realistic images using a diffusion foundation model but lacks GPU expertise. They also want auto-scaling for unpredictable workloads. Which AWS feature combination should they use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run Stable Diffusion on EC2 GPU instances with Auto Scaling",
      "B": "Deploy Stable Diffusion in EKS with spot instances",
      "C": "Use SageMaker JumpStart customized Stable Diffusion endpoint",
      "D": "Use Amazon Bedrock\u2019s Stable Diffusion model with built-in autoscaling"
    },
    "explanation": "Bedrock offers pre-trained Stable Diffusion with managed autoscaling and no GPU management. JumpStart requires you to set up instance fleets and autoscaling policies. EC2/EKS requires manual GPU and scaling management."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A legal firm needs an AI assistant that uses private documents stored in S3. They require retrieval-augmented generation. Which AWS service integration best supports RAG out-of-the-box?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Q with Kendra indexing",
      "B": "Amazon Bedrock with AWS Kendra retrieval plugin",
      "C": "SageMaker JumpStart with DocumentDB",
      "D": "AWS Comprehend with ElasticSearch"
    },
    "explanation": "Bedrock integrates natively with Kendra for retrieval-augmented generation. Amazon Q focuses on QA but requires separate retrieval setup. SageMaker JumpStart and Comprehend need custom orchestration for RAG."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An enterprise wants to serve low-latency multi-turn chat responses to millions of users. They need a high-throughput conversational foundation model. Which AWS service and instance type should they choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock with provisioned capacity",
      "B": "SageMaker JumpStart on g4dn.xlarge",
      "C": "Amazon Q with on-demand throughput",
      "D": "Amazon Comprehend with batch processing"
    },
    "explanation": "Bedrock provisioned capacity delivers guaranteed throughput and low latency at scale. JumpStart g4dn.xlarge may not scale to millions or guarantee throughput. Amazon Q is for enterprise Q&A with lower throughput SLAs. Comprehend isn\u2019t conversational."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A gaming company wants to prototype code generation for game logic quickly and with minimal configuration. They do not need production-grade SLAs. Which AWS technology is best to start?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Bedrock with RLHF customization",
      "B": "SageMaker JumpStart custom training",
      "C": "Amazon CodeWhisperer preview through Amazon Bedrock",
      "D": "Amazon Q code generation feature"
    },
    "explanation": "CodeWhisperer is accessible through Bedrock with minimal setup for code generation. It is ideal for prototyping. SageMaker JumpStart requires endpoint setup. Bedrock RLHF needs customization. Amazon Q doesn\u2019t focus on code."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A retailer must ensure data sovereignty by keeping inference data within the EU. They need a managed generative service compliant with GDPR and EU data residency. Which AWS service configuration meets this need?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock in us-east-1 with EU region replication",
      "B": "Amazon Bedrock in eu-west-1 with customer-managed KMS",
      "C": "SageMaker JumpStart in us-west-2 with VPC endpoints",
      "D": "Amazon Q global endpoint with VPC private link"
    },
    "explanation": "Bedrock deployed in eu-west-1 ensures data residency in the EU. Coupled with a customer-managed KMS key in the same region, it meets GDPR. Replication from us-east-1 violates sovereignty. JumpStart in us-west-2 is outside EU. Amazon Q global endpoint may process data outside the EU."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A startup evaluating AWS generative AI options wants to minimize time-to-market and avoid model fine-tuning. They also require SLA-backed availability. Which AWS service should they select?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock",
      "B": "SageMaker JumpStart zero-shot models",
      "C": "Self-hosted LLaMA on EC2",
      "D": "Amazon Q with custom indexing"
    },
    "explanation": "Bedrock offers fully managed foundation models, zero configuration, and SLA-backed availability. JumpStart endpoints have no formal SLAs. Self-hosting and Q both require additional maintenance or retrieval setups."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A firm must process sensitive legal text with a foundation model and ensure encryption in transit and at rest, plus private networking. They prefer pay-per-use. Which AWS solution fits all criteria?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock public endpoint with TLS",
      "B": "SageMaker JumpStart on spot instances without VPC",
      "C": "SageMaker JumpStart endpoint in VPC with KMS encryption",
      "D": "Amazon Q via internet with customer key"
    },
    "explanation": "SageMaker JumpStart in a VPC with customer-managed KMS ensures private networking and encryption at rest and transit. Bedrock endpoints are public and you cannot deploy them in VPC. Amazon Q does not support private VPC deployments."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A media startup expects to generate thousands of images per hour and needs predictable monthly billing. Which AWS generative AI service and pricing model should they choose?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock token-based pricing",
      "B": "Amazon Q request-based billing",
      "C": "SageMaker JumpStart on-demand instances",
      "D": "SageMaker JumpStart on reserved GPU instances"
    },
    "explanation": "Reserving GPU instances for SageMaker JumpStart gives predictable instance-hour billing for high-volume image generation. Bedrock\u2019s token-based pricing can vary unpredictably. Amazon Q\u2019s billing is also usage-based. JumpStart on-demand has variable costs."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A logistics company wants to experiment with multiple foundation models (text, image, code) rapidly without provisioning infrastructure. They also need to manage access via IAM. Which AWS service meets these criteria?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon SageMaker Ground Truth",
      "B": "Amazon Bedrock with IAM policies",
      "C": "SageMaker JumpStart with KMS roles",
      "D": "Amazon Q with Cognito authentication"
    },
    "explanation": "Bedrock offers multiple model modalities through a single console/API, IAM integration for access control, and no infra provisioning. JumpStart focuses on SageMaker models only. Ground Truth is data labeling. Amazon Q is limited to QA."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An organization needs to generate long-form text (>5,000 tokens) reliably. Which service supports extended output lengths natively?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock with a model supporting long context windows",
      "B": "Amazon Q with streaming pagination",
      "C": "SageMaker JumpStart with multi-chunk stitching",
      "D": "Amazon Comprehend for long documents"
    },
    "explanation": "Bedrock offers foundation models with extended context windows for single-stream long outputs. Q is for QA, not generative long-form. JumpStart requires stitching and manual orchestration. Comprehend is analytics, not generation."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A research team needs to prototype embedding generation and storage with minimal setup. They want an AWS service that handles embeddings and index management. Which service is most appropriate?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Q",
      "B": "Amazon Comprehend",
      "C": "Amazon Bedrock embeddings with Kendra integration",
      "D": "SageMaker JumpStart with DocumentDB"
    },
    "explanation": "Bedrock provides an embeddings API and can integrate with Kendra for index management. Amazon Q and Comprehend don\u2019t offer native embedding/integration. JumpStart requires you to set up database and indexing."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A global enterprise requires a generative AI solution that complies with SOC 2 and ISO 27001 without custom certification. Which AWS service provides this out-of-the-box?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock",
      "B": "SageMaker JumpStart",
      "C": "Self-hosted Hugging Face on EC2",
      "D": "Amazon Q"
    },
    "explanation": "Bedrock is SOC 2 and ISO 27001 compliant by AWS\u2019s managed service. JumpStart endpoints inherit SageMaker compliance but require configuration. Self-hosting requires you to certify. Amazon Q\u2019s compliance coverage is narrower."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A developer needs to quickly test an open-source foundation model for style transfer in a sandbox environment with minimal AWS service limits. Which option is fastest to set up?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Deploy model on EKS with SageMaker Serving containers",
      "B": "Use SageMaker JumpStart one-click deployment",
      "C": "Walk through Bedrock console to add third-party model",
      "D": "Set up Amazon Q custom connector"
    },
    "explanation": "JumpStart offers one-click deployment of many open-source models into a test endpoint. Bedrock doesn\u2019t allow arbitrary third-party sandbox models. EKS setup is slower. Amazon Q isn\u2019t for custom style transfer."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An automotive company wants to leverage a proprietary foundation model from AWS Marketplace while maintaining complete customization control. They need to bring-your-own-license (BYOL). Which service supports this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock with custom model marketplace",
      "B": "Amazon Q with enterprise connector",
      "C": "SageMaker JumpStart BYOL container",
      "D": "Amazon Comprehend custom classification"
    },
    "explanation": "SageMaker JumpStart supports BYOL containers for proprietary models, giving full customization. Bedrock marketplace models are managed by AWS and don\u2019t support BYOL. Q and Comprehend do not support BYOL models."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A company requires the lowest possible inference latency in milliseconds for high-frequency trading advisory. They need direct GPU control. Which AWS solution is most appropriate?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Amazon Bedrock provisioned capacity",
      "B": "Amazon Q synchronous endpoint",
      "C": "SageMaker JumpStart on Fargate",
      "D": "Self-hosted GPU cluster on EC2 G5 instances"
    },
    "explanation": "Self-hosted GPU cluster on EC2 G5 gives you control over GPU allocation and network optimizations for sub-10ms latency. Bedrock and Q have added network hops. JumpStart on Fargate doesn\u2019t support GPUs."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A biotech research group needs to fine-tune a protein-structure foundation model with hundreds of GPU instances managed automatically, and then serve the model. Which AWS offering should they use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock automatic fine-tuning",
      "B": "SageMaker JumpStart fine-tuning job and endpoint",
      "C": "Amazon Q custom model service",
      "D": "AWS Lambda with EFS storing weights"
    },
    "explanation": "SageMaker JumpStart supports distributed fine-tuning on GPU clusters and hosting via managed endpoints. Bedrock does not allow user fine-tuning. Amazon Q is QA only. Lambda cannot handle large model weights."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A fintech startup needs a generative AI chatbot secured by IAM and integrated with DynamoDB for context retrieval. They want minimal orchestration. Which AWS service combination is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock with custom DynamoDB retrieval code",
      "B": "Amazon Q with Lambda polling DynamoDB",
      "C": "SageMaker JumpStart with Kendra",
      "D": "Amazon Lex with QLDB"
    },
    "explanation": "Bedrock allows direct API calls from Lambda or code that integrates DynamoDB for RAG, with IAM-based security, offering minimal orchestration. Q does retrieval but requires additional Lambda orchestration. JumpStart and Lex combinations are heavier."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A gaming studio needs burst capacity for character voice synthesis using Amazon Polly-style generative audio. They must avoid over-provisioning GPUs. Which service is preferred?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Self-hosted Tacotron on EC2 Spot Instances",
      "B": "SageMaker JumpStart audio model",
      "C": "Amazon Bedrock\u2019s text-to-speech foundation model",
      "D": "Amazon Polly standard voices"
    },
    "explanation": "Bedrock\u2019s audio foundation models offer burstable pay-per-use without GPU provisioning. Polly is not a foundation model with customizable voices. JumpStart audio models still require GPU instances. Self-hosting implies GPU management."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A research lab wants to benchmark multiple foundation models for summarization on identical hardware to isolate model performance. Which AWS approach is most appropriate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon Bedrock comparison APIs",
      "B": "Use SageMaker JumpStart on a fixed EC2 instance type",
      "C": "Use Amazon Q with differencing parameters",
      "D": "Use AWS Lambda with multi-model endpoints"
    },
    "explanation": "JumpStart lets you deploy multiple models on the same EC2 instance type, ensuring hardware consistency during benchmarking. Bedrock runs on hidden infra. Q doesn\u2019t allow model selection. Lambda can\u2019t guarantee fixed hardware."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A company must ensure prompt inputs and model outputs are redactable for GDPR \u2019right to be forgotten\u2019. They want a managed solution with data lifecycle controls. Which AWS service best fits?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock with customer-managed S3 logs and lifecycle policies",
      "B": "SageMaker JumpStart with default CloudWatch logs",
      "C": "Amazon Q auditing mode",
      "D": "Amazon Comprehend with Data Lifecycle Manager"
    },
    "explanation": "Bedrock stores logs in S3 under your account where you can apply lifecycle and deletion policies. JumpStart stores logs in CloudWatch with limited control. Q\u2019s audit logs aren\u2019t easily exported. Comprehend isn\u2019t generative."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A government agency requires FedRAMP High compliance for generative AI use. They need a model service with the highest security boundary. Which service qualifies?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Q",
      "B": "SageMaker JumpStart",
      "C": "Amazon Comprehend",
      "D": "Amazon Bedrock in an AWS GovCloud region"
    },
    "explanation": "Bedrock in AWS GovCloud has FedRAMP High boundary. Q and Comprehend are not offered in GovCloud with FedRAMP High. JumpStart on regular SageMaker may not meet FedRAMP High."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A marketing team wants to rapidly A/B test two different foundation models for tone consistency. They want to avoid endpoint cold-start latency affecting results. Which AWS service configuration should they choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock token calls to different models",
      "B": "SageMaker JumpStart provisioned endpoints for both models",
      "C": "Amazon Q with session stickiness",
      "D": "Amazon Lambda multiplexing Bedrock endpoints"
    },
    "explanation": "Provisioned SageMaker JumpStart endpoints keep models warm for consistent low latency. Bedrock cold starts can vary per request. Q isn\u2019t designed for multi-model A/B testing. Lambda multiplexing adds unpredictable jitter."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your startup wants to reduce inference cost by caching frequent prompts and responses. Which AWS service supports built-in caching at the model inference layer?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock with CloudFront**",
      "B": "Amazon Q with DynamoDB cache",
      "C": "SageMaker JumpStart with accelerator inference accelerator",
      "D": "Amazon Comprehend with batch cache"
    },
    "explanation": "SageMaker JumpStart inference accelerators (Elastic Inference) allow caching intermediate layers and reduce compute costs. Bedrock doesn\u2019t offer built-in caching. Amazon Q requires building your own cache. Comprehend batch is offline only."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A telecom wants to run simultaneous experiments on fine-tuning strategies (RLHF vs. instruction tuning) across different foundation models. They need automated experiment tracking and reproducibility. Which AWS feature should they employ?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Bedrock version control",
      "B": "SageMaker JumpStart experiment trials",
      "C": "AWS CodeCommit with Bedrock",
      "D": "Amazon Q audit logs"
    },
    "explanation": "SageMaker JumpStart supports experiment trials and tracking for fine-tuning jobs, enabling reproducibility. Bedrock does not offer built-in experiment tracking. CodeCommit is source control, and Q audit logs don\u2019t manage experiments."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An e-commerce platform wants high-availability inference across two regions for a foundation model without data replication. Which AWS generative AI service supports cross-region endpoints?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock multi-region endpoints",
      "B": "SageMaker JumpStart VPC peering endpoints",
      "C": "Amazon Q regional fallback",
      "D": "Amazon Comprehend global endpoint"
    },
    "explanation": "Bedrock supports multi-region endpoints for high availability without manual replication. JumpStart requires deploying separate endpoints per region. Q and Comprehend are region-specific without multi-region abstraction."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A fintech company needs to fine-tune a LLM under strict PCI DSS controls. They want built-in data encryption, audit, and key rotation. Which AWS service should they pick?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock with default settings",
      "B": "Amazon Q with PCI integration",
      "C": "SageMaker JumpStart in private subnet with customer-managed KMS",
      "D": "Amazon Comprehend custom classification"
    },
    "explanation": "SageMaker JumpStart in a private subnet offers VPC isolation, customer-managed KMS keys for encryption with rotation, and CloudTrail audit logs. Bedrock defaults don\u2019t guarantee VPC isolation or key rotation. Q and Comprehend aren't designed for fine-tuning."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A fintech startup needs to embed a foundation model into its fraud detection pipeline. They require sub-200 ms real-time inference, support for English and Spanish, moderate customization for domain-specific terminology, and predictable cost. Which pre-trained model selection best meets these criteria?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A large, high-capacity English-only model offering the best accuracy but 500 ms latency.",
      "B": "A tiny model (<1 B parameters) with 50 ms latency supporting only English and no customization.",
      "C": "A mid-sized multilingual model (5 B parameters) with documented <200 ms real-time latency and in-context domain adaptation support.",
      "D": "A large multilingual model (50 B parameters) optimized for batch inference but no SLA on latency."
    },
    "explanation": "Option C balances latency, language coverage, and in-context customization within cost constraints; other options sacrifice a required dimension."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An e-commerce platform must generate personalized product descriptions on demand. They expect bursts of 1 K requests/sec. Which selection criterion most directly impacts their ability to scale with bursts?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model parameter count, since larger models always scale better.",
      "B": "Throughput SLA (requests/sec) and horizontal scaling support.",
      "C": "Provisioned I/O length per request.",
      "D": "Quality benchmark on small datasets."
    },
    "explanation": "Throughput SLA and horizontal scaling directly determine burst handling; other factors don\u2019t guarantee throughput at scale."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A multilingual chatbot requires support for 20 languages, low cost per token, and minimal per-inference customization. Which model attribute should be prioritized?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Highest parameter size for maximum quality.",
      "B": "Lowest latency single-language model.",
      "C": "Batch inference throughput.",
      "D": "Multi-language coverage with token-based pricing and endpoint customization."
    },
    "explanation": "Multi-language coverage and token-based pricing align with cost and language requirements; others focus on irrelevant characteristics."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A research team needs high accuracy on technical language and is willing to accept 1 s latency. They will fine-tune heavily. Which selection criterion matters most?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model openness and fine-tuning support (e.g., foundation model with instruction-tuning API).",
      "B": "Lowest latency endpoints.",
      "C": "Multimodal support.",
      "D": "Minimal model size to reduce cost."
    },
    "explanation": "Fine-tuning support is critical for customizing technical language; latency and multimodal needs are secondary."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A mobile app must run offline without connectivity, so they need an on-device foundation model. Which selection criterion rules out server-hosted models?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High token throughput requirements.",
      "B": "On-device footprint size and local inference capability.",
      "C": "Batch vs real-time inference.",
      "D": "Language coverage across cloud endpoints."
    },
    "explanation": "On-device footprint size and local inference determine offline suitability; other criteria are irrelevant to offline use."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Users report inconsistent creativity in a marketing slogan generator. Which inference parameter adjustment will systematically increase output variability?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature to allow more randomness.",
      "B": "Decrease max output length to limit content.",
      "C": "Lower repetition penalty to allow more rote repetition.",
      "D": "Reduce context window to truncate input."
    },
    "explanation": "Higher temperature yields more varied outputs; other parameters do not directly affect creativity randomness."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A document summarizer cuts off mid-sentence. Which parameter change prevents truncation?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature.",
      "B": "Lower top-p sampling.",
      "C": "Increase max output token length.",
      "D": "Decrease model batch size."
    },
    "explanation": "Raising max output token length allows longer summaries; other parameters don\u2019t extend outputs."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A chatbot sometimes repeats phrases verbatim. Which inference parameter reduces repetition?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature.",
      "B": "Increase top-k sampling.",
      "C": "Increase max input length.",
      "D": "Apply or raise repetition penalty."
    },
    "explanation": "Repetition penalty discourages repeated tokens; other settings control randomness or length."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A code generator rarely includes required imports. Which change to inference parameters is most likely to address this?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Lower temperature for more deterministic output.",
      "B": "Increase top-p to sample more tokens.",
      "C": "Increase max input length to include import instructions in prompt.",
      "D": "Increase repetition penalty."
    },
    "explanation": "A longer prompt context ensures the model sees import instructions; temperature and top-p aren\u2019t effective here."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An LLM-based assistant hallucinates facts when answering. Which parameter tweak minimizes factual errors?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Lower temperature to produce more conservative responses.",
      "B": "Increase max output length.",
      "C": "Raise top-k to sample more tokens.",
      "D": "Decrease context window."
    },
    "explanation": "Lowering temperature makes outputs more deterministic and less prone to hallucination; other changes don\u2019t reduce hallucinations."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A financial advisory app uses RAG to answer queries from compliance documents. Which component is essential for a RAG implementation?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Continuous in-context learning without external data.",
      "B": "An embedding store and a retrieval mechanism to fetch relevant passages.",
      "C": "Fine-tuning the base model on compliance text.",
      "D": "High temperature to diversify retrieved content."
    },
    "explanation": "RAG requires embedding storage plus retrieval to augment prompts; fine-tuning isn\u2019t RAG and temperature is irrelevant to retrieval."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An enterprise uses Salesforce documents to answer sales queries via RAG. Which AWS service can host the knowledge base for low-cost retrieval?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Aurora PostgreSQL.",
      "B": "Amazon Neptune with Gremlin queries.",
      "C": "Amazon OpenSearch for vector search on document embeddings.",
      "D": "Amazon S3 with no indexing."
    },
    "explanation": "OpenSearch supports vector search with scalable indexing; S3 alone cannot index embeddings; other databases are less optimized."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A law firm needs to update its RAG corpus daily with new case law, ensure fast retrieval, and minimize operational overhead. Which architecture best fits?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run Neptunedb with nightly data imports and manual indexing.",
      "B": "Use RDS Aurora with full-text search and custom Lambda indexing.",
      "C": "Store JSON in S3 and scan files per query.",
      "D": "Use Amazon OpenSearch Service with automated ingestion pipelines and managed vector indices."
    },
    "explanation": "Managed OpenSearch with automation balances speed, freshness, and low overhead; other options require heavy management or are slow."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A startup wants RAG only for urgent queries (5%). For the rest, direct LLM inference is cheaper. How should they architect cost-efficiently?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Always perform RAG retrieval on every query.",
      "B": "Branch logic: detect urgent queries, use RAG only when needed, otherwise direct inference.",
      "C": "Fine-tune model to internalize everything, avoiding RAG.",
      "D": "Pre-embed all possible queries offline and store in S3."
    },
    "explanation": "Conditional branching uses RAG only when necessary, reducing retrieval costs; options A and C increase costs, D is impractical."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "To implement RAG, a team must vectorize documents. Which AWS feature automates embedding generation?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon SageMaker JumpStart embedding pipelines.",
      "B": "Bedrock custom fine-tuning.",
      "C": "Lambda with batch translation.",
      "D": "AWS Glue DataBrew."
    },
    "explanation": "SageMaker JumpStart provides embedding pipelines; Bedrock fine-tuning and Glue are unrelated to embedding generation."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A healthcare app requires HIPAA-eligible embedding storage with sub-second vector search. Which AWS service meets both needs?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon RDS for PostgreSQL with pgvector extension.",
      "B": "Amazon DocumentDB undocumented vector plugin.",
      "C": "Amazon Neptune with HaVQL endpoints.",
      "D": "Amazon OpenSearch Service HIPAA-eligible domain with vector search."
    },
    "explanation": "OpenSearch supports vector search and is HIPAA-eligible; others lack managed support or eligibility."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "At 1 M documents, vector searches slow below SLA. Which storage change offers lowest maintenance while improving performance?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch to Neptune with custom indexing.",
      "B": "Scale OpenSearch cluster with dedicated ML nodes for vector search.",
      "C": "Migrate to RDS Aurora serverless.",
      "D": "Store embeddings in S3 and perform Lambda scans."
    },
    "explanation": "Scaling OpenSearch with ML nodes optimizes vector speed with minimal management; others add complexity or are too slow."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A startup considers Aurora PostgreSQL vs OpenSearch for embeddings. They need cost-effective writes for infrequent updates and fast reads. Which choice?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Aurora PostgreSQL serverless with pgvector for low update cost and indexed reads.",
      "B": "OpenSearch with daily bulk reindexing.",
      "C": "Neptune cluster for graph queries.",
      "D": "DocumentDB with manual sharding."
    },
    "explanation": "Aurora serverless minimizes cost on infrequent writes while supporting indexed reads; OpenSearch reindexing is heavier."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A media company fine-tunes a foundation model for style consistency vs using in-context learning. Which cost factor most impacts this trade-off?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Data storage cost for training datasets.",
      "B": "Latency of inference endpoints.",
      "C": "Temperature parameter tuning.",
      "D": "Compute and storage costs of fine-tuning vs pay-per-token RAG or in-context calls."
    },
    "explanation": "Fine-tuning incurs upfront compute/storage costs, whereas in-context RAG is pay-per-token; that cost trade-off drives decision."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A company must decide between full model fine-tuning vs few-shot prompts. They expect high query volume and low per-request cost. Which is likely cheaper over time?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Full fine-tuning, since customized models always cost less.",
      "B": "Few-shot prompting, due to lower maintenance and pay-per-token pricing at scale.",
      "C": "Pre-training from scratch.",
      "D": "Building custom retrieval engines to avoid LLM calls."
    },
    "explanation": "Few-shot prompting avoids fine-tuning overhead and scales with token-based pricing; other options are impractical or expensive."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "For sporadic complex queries, a company debates pre-training a new model vs RAG. What cost factor favors RAG?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Data labeling costs.",
      "B": "Model parameter count.",
      "C": "High upfront pre-training compute versus per-call retrieval fees.",
      "D": "Inference latency."
    },
    "explanation": "Pre-training requires massive upfront compute; RAG incurs per-call retrieval fees, making it cheaper for sporadic use."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A biotech firm needs multiple specialized models and wonders if fine-tuning each is cost-effective vs a single foundation model with RAG. Which scenario favors RAG?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "When domain corpora are small and finely segmented, making many fine-tunings expensive.",
      "B": "When data volumes justify building separate huge models.",
      "C": "When inference latency must be under 50 ms.",
      "D": "When on-device deployment is required."
    },
    "explanation": "Small segmented domains make many fine-tunings costly, so using RAG with one model is more cost-effective."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A multi-step travel booking agent needs to search availability, book flights, and confirm hotels in sequence. Which AWS feature enables orchestrating these tasks?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Bedrock prompt chaining via a single prompt.",
      "B": "RAG retrieval with multi-document queries.",
      "C": "Custom Lambda orchestration without LLM.",
      "D": "Bedrock Agents to execute step-by-step actions autonomously."
    },
    "explanation": "Bedrock Agents coordinate multi-step tasks via LLM-driven actions; prompt chaining alone isn\u2019t sufficient orchestration."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A supply chain assistant must call external APIs, ingest responses, and produce a final report. Which approach reduces custom code?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Fine-tune a foundation model to embed API logic.",
      "B": "Use RAG to index API docs.",
      "C": "Deploy a Bedrock Agent with connector plug-ins to APIs.",
      "D": "Use SageMaker Pipelines orchestrating Lambdas."
    },
    "explanation": "Bedrock Agents support connectors to external APIs with minimal code; other methods require more custom development."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An HR chatbot must verify identities, fetch records, and schedule interviews. Which architecture supports LLM reasoning plus API calls?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Direct model inference with embedded API keys.",
      "B": "Use a Bedrock Agent that invokes AWS Lambda functions for each step.",
      "C": "RAG with HR document embeddings only.",
      "D": "Fine-tune the model on HR policies."
    },
    "explanation": "Agents calling Lambdas allow secure API interactions and stepwise logic; RAG and fine-tuning alone can\u2019t orchestrate actions."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "To troubleshoot a failed Agent workflow, which Bedrock feature helps inspect each step's LLM decision?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Agent execution logs and step traces in Bedrock console.",
      "B": "SageMaker Clarify bias reports.",
      "C": "Model Monitor drift plots.",
      "D": "CloudWatch S3 access logs."
    },
    "explanation": "Bedrock Agents provide execution logs/traces of each decision step; other services monitor different aspects."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A sales bot Agent must maintain context across ten user turns. Which Agent configuration ensures context retention?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "High temperature setting.",
      "B": "Low top-p value.",
      "C": "RAG knowledge base only.",
      "D": "Stateful Agent session with conversational memory enabled."
    },
    "explanation": "Stateful sessions with memory retain multi-turn context; inference parameters and RAG alone don\u2019t preserve session state."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You are designing a prompt to guide a large language model to perform multi-step mathematical reasoning before answering. Which technique will most reliably produce a correct solution?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Embed the full solution in the prompt as context and ask the model to repeat it.",
      "B": "Use a single-shot prompt with the question and a request for the answer only.",
      "C": "Use a chain-of-thought prompt that explicitly asks the model to \"explain your reasoning step by step\" before giving the final answer.",
      "D": "Provide multiple unrelated examples and then ask for the solution to the target problem."
    },
    "explanation": "Chain-of-thought prompts explicitly break down reasoning steps, improving multi-step problem solving over single-shot or unrelated examples."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A developer wants the model to avoid sensitive topics without explicit deletion. Which prompt engineering technique should they apply?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature to reduce determinism.",
      "B": "Include a negative prompt section listing prohibited topics.",
      "C": "Use chain-of-thought to deter the model from broaching sensitive content.",
      "D": "Provide only a few-shot prompt of acceptable topics."
    },
    "explanation": "Negative prompts explicitly instruct the model to avoid specified content, more reliable than few-shot or chain-of-thought for exclusion."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You need to design a prompt template for a financial query chatbot that extracts entities. Which practice ensures consistent outputs?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Leave input schema flexible and let the model infer structure.",
      "B": "Encourage the model to ask follow-up questions when uncertain.",
      "C": "Use a zero-shot prompt with only instructions and no examples.",
      "D": "Define a structured template with labeled slots and provide two few-shot examples mapping questions to slot values."
    },
    "explanation": "A template with labeled slots and few-shot examples yields consistent structured outputs, reducing variability."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A prompt engineer notices that the model hallucinates project deadlines. What prompt modification could reduce this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add explicit \"I do not know\" fallback instructions for unknown dates.",
      "B": "Increase temperature to add variability.",
      "C": "Remove instructions about default responses.",
      "D": "Use random negative examples to confuse the model."
    },
    "explanation": "Providing an explicit fallback instruction reduces hallucinations by telling the model how to respond when uncertain."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your application needs consistent summaries of legislative text. To limit length without losing key points, which parameter and prompt style should you use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High temperature and a zero-shot prompt.",
      "B": "Lower temperature with a summarization template including a max_length instruction.",
      "C": "Few-shot examples of summaries of unrelated texts.",
      "D": "Chain-of-thought prompting with long context."
    },
    "explanation": "A lower temperature and explicit max_length instruction control verbosity while preserving important content."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A customer support bot must avoid revealing system internals. Which engineering approach secures the prompt?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use high temperature to generalize responses.",
      "B": "Provide examples of system\u2010internal responses to alter them.",
      "C": "Implement guardrails by embedding instructions that explicitly forbid mentioning internal details.",
      "D": "Allow the model to ask itself follow-up questions."
    },
    "explanation": "Embedding guardrail instructions to explicitly forbid certain content is the standard way to enforce boundaries."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You must translate text but avoid literal translations of idioms. Which prompt best achieves this?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "\"Translate this text to French.\"",
      "B": "Provide a few idiom translations as examples then translate.",
      "C": "Use chain-of-thought to explain how to translate.",
      "D": "Use a prompt with instructions: \"Translate preserving meaning\u2014not literal word-for-word\u2014especially for idioms.\""
    },
    "explanation": "Explicit instructions about preserving meaning over literal translation guide the model away from word-for-word renders."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A security audit reveals prompt-injection vulnerabilities. What prompt design change can mitigate prompt hijacking?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a system-level instruction layer separate from user input.",
      "B": "Chain multiple user inputs to confuse attackers.",
      "C": "Increase maximum token length.",
      "D": "Use few-shot examples of malicious inputs."
    },
    "explanation": "Separating system prompts from user prompts prevents user-supplied text from overriding core instructions."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "When designing prompts for few-shot learning on a classification task, you notice class imbalance. Which tactic can improve performance?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature to diversify outputs.",
      "B": "Provide more zero-shot instructions.",
      "C": "Include proportional examples for underrepresented classes in the few-shot examples.",
      "D": "Remove examples for overrepresented classes."
    },
    "explanation": "Balancing the few-shot examples ensures the model sees adequate samples of each class, improving bias."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your prompt includes context, instruction, and examples. The model ignores the instruction. What likely caused this?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Temperature is set too low.",
      "B": "Your chain-of-thought prompt is missing.",
      "C": "You used negative prompts incorrectly.",
      "D": "The examples contradict the instructions, causing the model to follow examples instead."
    },
    "explanation": "Models prioritize exemplars over abstract instructions; contradictory examples override instructions."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "To improve factual accuracy on Q&A, you plan to retrieve documents and include them in the prompt. What technique is this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Zero-shot prompting",
      "B": "Retrieval Augmented Generation (RAG)",
      "C": "Chain-of-thought prompting",
      "D": "Negative prompting"
    },
    "explanation": "RAG supplies external knowledge to the prompt to improve factual grounding."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A prompt engineer wants to generate SQL queries from natural language. Which style yields the most reliable queries?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Few-shot prompt with paired NL questions and SQL statements.",
      "B": "Zero-shot ask \"Convert to SQL\" without examples.",
      "C": "Chain-of-thought to show stepwise table scanning.",
      "D": "A negative prompt listing forbidden SQL functions."
    },
    "explanation": "Few-shot paired examples guide the model to learn the NL-to-SQL mapping effectively."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You notice that adding more few-shot examples degrades performance. What is the most likely cause?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Temperature is too low.",
      "B": "Instruction length is too short.",
      "C": "Context window exceeded, causing truncation of important examples.",
      "D": "Negative prompts are overridden."
    },
    "explanation": "Exceeding the context window truncates earlier examples, harming performance."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "For a creative brainstorming assistant, you need diverse ideas. Which prompt setting supports this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Low top_p and low temperature.",
      "B": "Use zero-shot instructions only.",
      "C": "Provide few-shot focused examples.",
      "D": "High temperature and high top_p with an open-ended instruction template."
    },
    "explanation": "High temperature and top_p increase diversity, suited for brainstorming tasks."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your prompt includes sensitive user data. To prevent leakage, which practice should you employ?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Increase temperature so the model forgets specifics.",
      "B": "Redact PII before including any user data in the prompt.",
      "C": "Use a negative prompt to exclude PII.",
      "D": "Use chain-of-thought to obscure the data."
    },
    "explanation": "Redacting PII is the reliable approach to prevent sensitive data from appearing in responses."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You are using negative prompts to block disallowed content, but they sometimes fail. What additional measure can improve enforcement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Combine negative prompts with model-level content filters.",
      "B": "Lower the temperature to zero.",
      "C": "Use more few-shot allowed examples.",
      "D": "Add chain-of-thought steps rejecting the content."
    },
    "explanation": "Pairing negative prompts with content filters catches any disallowed outputs that slip through."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A prompt leverages placeholders like {{user_name}} and {{date}}. What benefit does this template provide?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Reduces hallucinations.",
      "B": "Enables chain-of-thought prompting.",
      "C": "Supports dynamic injection of variables while keeping prompt structure consistent.",
      "D": "Increases model temperature."
    },
    "explanation": "Templates with placeholders allow dynamic content while preserving prompt consistency and structure."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your prompt for summarization often includes irrelevant details. Which prompt adjustment will focus the model?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Add more few-shot examples of long summaries.",
      "B": "Include an instruction: \"Focus only on the main points and exclude minor details.\"",
      "C": "Increase temperature to reduce determinism.",
      "D": "Use chain-of-thought to list every detail then summarize."
    },
    "explanation": "Explicit instructions to exclude minor details refocus the model on main points."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "In a translation prompt, the model sometimes outputs back-translated English. How do you correct this?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Add few-shot examples of translations back to English.",
      "B": "Remove context from the prompt.",
      "C": "Use higher top_p.",
      "D": "Clarify: \"Translate from English to Spanish only, do not output in English.\""
    },
    "explanation": "Explicitly specifying source and target languages in instructions prevents unintended English output."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You want the model to follow a strict JSON output schema. Which practice ensures compliance?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provide a JSON schema in the prompt and ask \"Output strictly in this JSON format.\"",
      "B": "Use chain-of-thought to generate JSON step by step.",
      "C": "Ask for the output in bullet points then convert.",
      "D": "Use negative prompts to block non-JSON text."
    },
    "explanation": "Including the exact JSON schema and instructing strict adherence drives the model to produce valid JSON."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A prompt engineer tests with boundary questions and finds inconsistent behavior. What best practice can improve prompt robustness?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature to handle edge cases.",
      "B": "Use zero-shot with generic instructions.",
      "C": "Include random noise in examples.",
      "D": "Iteratively test and refine prompts with boundary and adversarial cases."
    },
    "explanation": "Iterative testing with adversarial prompts helps identify weaknesses and refine prompt reliability."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your prompt includes an example that is semantically similar to the target but uses different phrasing. This causes errors. How do you fix it?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Remove the example altogether.",
      "B": "Rewrite examples to use consistent phrasing and terminology.",
      "C": "Add negative prompts blocking the old phrasing.",
      "D": "Increase max_tokens to include more context."
    },
    "explanation": "Consistent phrasing ensures the model learns the correct mapping between example and task."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "When optimizing prompts for latency, you notice longer prompts slow response. What trade-off can you adjust?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Increase temperature to speed up decoding.",
      "B": "Add more few-shot examples.",
      "C": "Shorten the prompt by trimming non-essential context and rely on zero-shot instructions.",
      "D": "Add chain-of-thought to guide response quickly."
    },
    "explanation": "Trimming prompt context and using concise instructions reduces token count and lowers latency."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A prompt accidentally reveals internal model behavior. How can you prevent this?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Add instructions: \"Do not mention internal reasoning or hidden policies.\"",
      "B": "Increase temperature to obfuscate.",
      "C": "Use chain-of-thought explicitly.",
      "D": "Include examples of internal behaviors to guide them."
    },
    "explanation": "Explicitly forbidding the mention of internal processes prevents model exposition."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your prompt works well at low volume but degrades when many users query concurrently. What is a prompt engineering mitigation?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add more examples per prompt.",
      "B": "Standardize prompts via a shared template and cache frequent prompt structures.",
      "C": "Use higher temperature to distribute load.",
      "D": "Switch to chain-of-thought prompting."
    },
    "explanation": "Standardizing and caching prompt templates reduces construction overhead and variability under load."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You want the model to ask clarifying questions if the input is ambiguous. Which prompt style supports this behavior?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Zero-shot with final answer only.",
      "B": "Few-shot examples showing direct answers.",
      "C": "Few-shot examples where the model asks follow-up questions before answering.",
      "D": "Chain-of-thought showing internal ambiguity resolution."
    },
    "explanation": "Few-shot examples demonstrating clarifying questions teach the model to adopt that behavior."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "During prompt iteration you try random synonyms in instructions and see erratic results. What is the root issue?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Instruction too specific.",
      "B": "Temperature too low.",
      "C": "Examples are missing.",
      "D": "Model is sensitive to wording; maintain consistent terminology for stability."
    },
    "explanation": "Models can be highly sensitive to instruction wording; consistent terminology yields more stable outputs."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A prompt engineer applies chain-of-thought for sentiment analysis but sees no benefit. Why?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Chain-of-thought only works with few-shot prompts.",
      "B": "Sentiment analysis is single-step; chain-of-thought adds unnecessary cost without accuracy gain.",
      "C": "Temperature was set too high.",
      "D": "They forgot negative prompts."
    },
    "explanation": "Chain-of-thought aids multi-step reasoning; single-shot tasks like sentiment analysis don't benefit significantly."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "To prevent prompt leakage in logs, which practice should you follow?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Log full raw prompts for auditing.",
      "B": "Use chain-of-thought to split prompts.",
      "C": "Mask or hash sensitive portions of prompts before logging.",
      "D": "Set temperature to zero."
    },
    "explanation": "Masking or hashing sensitive data in logs protects privacy while preserving auditability."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your prompt uses multi-shot examples but the order influences correctness. How do you mitigate ordering bias?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Randomize the order of few-shot examples during prompt construction.",
      "B": "Increase temperature.",
      "C": "Use negative prompts blocking first examples.",
      "D": "Switch to chain-of-thought."
    },
    "explanation": "Randomizing example order avoids accidental bias toward earlier examples."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You need to test a prompt\u2019s vulnerability to jailbreaking. Which step is most effective?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature to encourage rule breaking.",
      "B": "Use few-shot allowed content examples.",
      "C": "Add negative prompts blocking \u201cjailbreak\u201d.",
      "D": "Design adversarial inputs that attempt to override instructions and test responses."
    },
    "explanation": "Adversarial testing reveals if malicious inputs can override system instructions, identifying jailbreak risks."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A financial services firm needs a foundation model to understand domain\u2010specific jargon from its proprietary 10 GB corpus. They have limited human\u2010labelled data but extensive raw text. Which customization method should they apply to best adapt the foundation model to their domain?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Instruction tuning using supervised examples",
      "B": "Reinforcement learning with human feedback (RLHF)",
      "C": "Domain adaptation via continuous pre\u2010training on raw text",
      "D": "Transfer learning with a small labelled dataset only"
    },
    "explanation": "Domain adaptation (continued pre\u2010training) uses raw domain text without heavy labels to adapt model."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A startup wants to fine\u2010tune a public LLM for summarization of legal contracts but cannot afford extensive compute. Which low\u2010resource fine\u2010tuning approach balances cost and performance?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Full parameter fine\u2010tuning on all layers",
      "B": "Instruction tuning with a prompt example dataset",
      "C": "Reinforcement learning with human feedback",
      "D": "Domain adaptation with large unlabelled data"
    },
    "explanation": "Instruction tuning with curated prompts uses less compute than full fine\u2010tuning and improves summarization."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A retail company notices its fine\u2010tuned foundation model performance degrading monthly due to changing product catalogs. Which process addresses this issue?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "One\u2010time fine\u2010tuning on initial data",
      "B": "Periodic instruction tuning only when issues appear",
      "C": "Revert to the base foundation model",
      "D": "Continuous pre\u2010training with recent catalog data"
    },
    "explanation": "Continuous pre\u2010training on fresh data combats concept drift."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "When preparing data for fine\u2010tuning a foundation model with RLHF, which step is MOST critical to ensure high\u2010quality reward signals?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Curating diverse human preference comparisons",
      "B": "Balancing classes in the raw unlabeled corpus",
      "C": "Normalizing text token lengths",
      "D": "Applying data augmentation to unlabelled data"
    },
    "explanation": "RLHF needs human\u2010annotated preference data for reward model training."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A life\u2010sciences company wants to restrict a foundation model\u2019s clinical advice to approved guidelines. Which fine\u2010tuning method best encodes these constraints?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Instruction tuning with guideline\u2010based prompts",
      "B": "Domain adaptation on general clinical text",
      "C": "Supervised transfer learning on labeled Q&A pairs",
      "D": "Reinforcement learning with patient feedback"
    },
    "explanation": "Instruction tuning can encode guideline style and constraints directly."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You have a foundation model fine\u2010tuned on 50k labeled examples. You want further improvement on a subcategory representing only 500 examples. Which approach is optimal?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Retrain entire model on combined 50.5k dataset",
      "B": "Perform targeted fine\u2010tuning (transfer learning) on 500 examples",
      "C": "Use RLHF with broad general feedback",
      "D": "Continuous pre\u2010training on unlabeled data"
    },
    "explanation": "Targeted transfer learning on few examples adapts specialized subcategory."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A conversational agent generates unsafe suggestions occasionally. You have developer time but limited data. Which customization pipeline addresses safety concerns?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Domain adaptation on more unlabeled conversation logs",
      "B": "Instruction tuning with longer prompts",
      "C": "Transfer learning on generic safe corpora",
      "D": "RLHF incorporating human safety feedback"
    },
    "explanation": "RLHF with human\u2010defined safe/unsafe labels tunes reward for safety."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "During fine\u2010tuning a foundation model on image captioning, performance stalls. You suspect data issues. What should you evaluate FIRST?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model hyperparameters like learning rate",
      "B": "Compute instance type",
      "C": "Representativeness and labeling consistency of captioned images",
      "D": "Batch size and tokenization schema"
    },
    "explanation": "Data quality and representativeness are first to inspect when fine\u2010tuning stalls."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An LLM trained for medical advice must comply with data\u2010privacy laws. Which data preparation practice is MOST important before fine\u2010tuning?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Converting text to lower\u2010case tokens",
      "B": "Anonymizing and removing personal identifiers",
      "C": "Segmenting documents by sentence length",
      "D": "Balancing positive/negative sentiment"
    },
    "explanation": "Removing personally identifiable information is key for compliance."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A customer support chatbot was instruction tuned but underperforms on technical queries. Which next step will most effectively improve domain accuracy?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase prompt length",
      "B": "Switch to RLHF",
      "C": "Domain adaptation with technical support transcripts",
      "D": "Fine\u2010tune on general instruction datasets"
    },
    "explanation": "Continued pre\u2010training on domain transcripts boosts technical knowledge."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which characteristic distinguishes instruction tuning from domain adaptation?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Instruction tuning uses paired input\u2010output examples",
      "B": "Domain adaptation requires human feedback loops",
      "C": "Instruction tuning uses raw unlabeled text",
      "D": "Domain adaptation modifies reward functions"
    },
    "explanation": "Instruction tuning relies on (instruction, response) supervised pairs."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A retailer fine\u2010tuned a model on product descriptions but wants to reduce hallucinations. Which addition to the fine\u2010tuning pipeline helps?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Increase batch size",
      "B": "Swap tokenizer for larger vocabulary",
      "C": "Add more unlabeled data",
      "D": "Incorporate RLHF with human\u2010rated truthfulness"
    },
    "explanation": "RLHF can penalize hallucinations via human feedback."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You must measure the impact of fine\u2010tuning on model size and latency. Which evaluation metric combination is most relevant?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Perplexity and BLEU score",
      "B": "Model parameter count and inference time",
      "C": "ROUGE and BERTScore",
      "D": "Training loss and validation accuracy"
    },
    "explanation": "Parameter count and latency directly measure size and inference performance."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A LLM fine\u2010tuning job on AWS Bedrock fails due to OOM errors. Which action best mitigates the issue?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase learning rate",
      "B": "Use larger batch size",
      "C": "Enable parameter-efficient fine\u2010tuning (PEFT)",
      "D": "Switch to full model fine\u2010tuning"
    },
    "explanation": "PEFT reduces memory use by tuning fewer parameters."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "For training a specialized legal model you need detailed citations. Which fine\u2010tuning method supports injecting citations into outputs?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Instruction tuning with citation\u2010annotated examples",
      "B": "Domain adaptation on legal text only",
      "C": "Transfer learning on generic corpora",
      "D": "RLHF without examples"
    },
    "explanation": "Instruction tuning with explicit examples teaches citation patterns."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A media company wants to adapt an LLM to generate poetry in a niche style. They have 1k curated poems. Which approach?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Domain adaptation on large unlabeled text",
      "B": "Instruction tuning (few\u2010shot) with provided poems",
      "C": "RLHF with reader preferences",
      "D": "Continuous pre\u2010training on general poetry"
    },
    "explanation": "Instruction tuning on curated examples captures niche style effectively."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which risk is MOST associated with a foundation model undergoing continuous pre\u2010training on uncurated data?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Longer inference latency",
      "B": "Insufficient parameter updates",
      "C": "Introduction of bias and drift",
      "D": "Loss of base\u2010model capabilities"
    },
    "explanation": "Uncurated data can introduce unwanted biases and concept drift."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "After instruction tuning, a model still fails a key business requirement 30% of the time. What is the MOST direct way to further optimize output quality?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase dataset size by adding raw text",
      "B": "Switch foundation model to a larger variant",
      "C": "Decrease training epochs",
      "D": "Apply RLHF with quality\u2010weighted feedback"
    },
    "explanation": "RLHF optimizes according to human\u2010rated quality metrics."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A foundation model fine\u2010tuning on SageMaker took days. You need faster iterations. Which practice speeds up experiments?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use full\u2010precision (FP32) training",
      "B": "Employ parameter\u2010efficient fine\u2010tuning methods",
      "C": "Remove early stopping",
      "D": "Increase dataset size"
    },
    "explanation": "Parameter\u2010efficient methods reduce compute and speed up tuning."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which dataset split is MOST appropriate for preventing data leakage in fine\u2010tuning evaluation?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "80% train, 20% train",
      "B": "90% train, 10% test (no validation)",
      "C": "Train, validation, test with disjoint IDs",
      "D": "Train on combined train/validation and test on same"
    },
    "explanation": "Disjoint splits avoid leakage and ensure true evaluation."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A sensitive language model requires high transparency. Which customization step provides the best audit trail?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Record data lineage and fine\u2010tuning parameters",
      "B": "Use only instruction tuning",
      "C": "Skip continuous pre\u2010training",
      "D": "Rely solely on prompt engineering"
    },
    "explanation": "Versioning data and parameters ensures auditability."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A model fine\u2010tuned via RLHF exhibits mode collapse. Which corrective action is most effective?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase batch size",
      "B": "Add more uncurated data",
      "C": "Diversify human feedback and reward coverage",
      "D": "Reduce feedback frequency"
    },
    "explanation": "Broadening feedback signals prevents collapse to narrow outputs."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your team wants to reuse a fine\u2010tuning pipeline for multiple domains. Which architectural choice ensures modularity?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Embed all data in a single preprocessing script",
      "B": "Parameterize data pipelines per domain",
      "C": "Hard\u2010code model names",
      "D": "Use one monolithic training job"
    },
    "explanation": "Parameterization allows easy swapping of domain data."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which fine\u2010tuning strategy best minimizes catastrophic forgetting of base knowledge?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Combined training on base and new data",
      "B": "Full fine\u2010tuning only on new data",
      "C": "Domain adaptation on new data alone",
      "D": "Instruction tuning after base training"
    },
    "explanation": "Mixing base and domain data retains original capabilities."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You have limited human feedback for RLHF. Which technique improves the reward model training?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Remove low\u2010variance feedback",
      "B": "Train on raw unlabelled data",
      "C": "Increase model size",
      "D": "Apply data augmentation on feedback pairs"
    },
    "explanation": "Augmenting few feedback pairs increases diversity for reward training."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A compliance audit demands record of each fine\u2010tuning run. Which AWS feature ensures full traceability?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Hyperparameter Tuning jobs",
      "B": "SageMaker Experiments and model registry",
      "C": "AWS CloudTrail only",
      "D": "SNS notifications"
    },
    "explanation": "SageMaker Experiments tracks run metadata and model versions."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A large multimodal foundation model is adapted to classify medical images. Which fine\u2010tuning method uses both image and text pairs?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Instruction tuning on text only",
      "B": "RLHF with clinician feedback",
      "C": "Multimodal transfer learning with paired datasets",
      "D": "Continuous pre\u2010training on image tags"
    },
    "explanation": "Multimodal transfer learning adapts model on both modalities."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your fine\u2010tuning budget is fixed. You need to decide dataset size vs label quality trade\u2010off. Which guideline is MOST appropriate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Prefer smaller high\u2010quality labelled data over larger noisy data",
      "B": "Always maximize dataset size",
      "C": "Label all data regardless of noise",
      "D": "Use only unlabelled data"
    },
    "explanation": "High\u2010quality labels have more impact than noisy large data."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "During continuous pre\u2010training, model overfits to recent data. Which addition to pipeline mitigates this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase training epochs",
      "B": "Include regularization with replay of older data",
      "C": "Remove older data entirely",
      "D": "Decrease batch size"
    },
    "explanation": "Regular replay of old data prevents overfitting to new data."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A retail company fine-tunes a foundation model to summarize customer reviews. They notice that ROUGE scores are high, but human evaluators rate summaries as irrelevant. Which action MOST effectively diagnoses the issue?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase the beam width during inference to improve relevance.",
      "B": "Switch from ROUGE to BLEU to better capture summary quality.",
      "C": "Apply a larger prompt context window before generation.",
      "D": "Perform a human evaluation focused on summary faithfulness and compare against ROUGE components."
    },
    "explanation": "Comparing human faithfulness ratings with ROUGE sub-scores (e.g., ROUGE-L for longest common subsequence) reveals where automatic metrics diverge from human judgment, diagnosing the misalignment."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An enterprise uses a foundation model for code generation. They benchmark on a public dataset and report BLEU-4 of 45. After deploying to production, user satisfaction drops. Which evaluation step could have predicted this drop?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Measuring perplexity on the public dataset.",
      "B": "Tracking model loss during fine-tuning.",
      "C": "Conducting scenario-based human evaluation with real developer prompts.",
      "D": "Comparing BLEU scores across different code languages."
    },
    "explanation": "Real-world developer prompts differ from benchmark data; scenario-based human evaluation reveals usability issues before deployment."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A team uses BERTScore to evaluate translation quality, but it favors verbose outputs. What complementary metric should they include to penalize verbosity?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SARI",
      "B": "Length-penalized BLEU (BLEU with brevity penalty)",
      "C": "ROUGE recall",
      "D": "METEOR without synonym matching"
    },
    "explanation": "BLEU\u2019s brevity penalty discourages overly long translations, complementing BERTScore which does not penalize verbosity."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A chatbot powered by a foundation model shows high automated metric scores but low conversion rates. Which evaluation metric addresses this gap?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Online A/B testing measuring end-to-end conversion",
      "B": "Token-level accuracy on a held-out dataset",
      "C": "Self-BLEU to measure diversity",
      "D": "Embedding cosine similarity against training data"
    },
    "explanation": "Online A/B testing with conversion as the key metric ties model performance directly to business objectives, unlike offline metrics."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "During evaluation of a summarization foundation model, diversity of outputs is a priority. Which metric combination BEST captures both quality and diversity?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ROUGE-L and BLEU-4",
      "B": "BLEU-4 and perplexity",
      "C": "BERTScore and sequence length variance",
      "D": "ROUGE for quality and Self-BLEU for diversity"
    },
    "explanation": "ROUGE assesses quality against references, while Self-BLEU measures diversity by evaluating overlap among generated summaries."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A finance company fine-tunes a model to generate risk analysis reports. They must ensure factual accuracy. Which evaluation approach is MOST appropriate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Compute ROUGE-F1 against sample reports.",
      "B": "Use a domain expert human evaluation with a fact-checking rubric.",
      "C": "Measure perplexity on a financial corpus.",
      "D": "Evaluate BLEU against generic report templates."
    },
    "explanation": "Only domain experts can reliably assess factual accuracy in risk reports; automated metrics cannot detect subtle errors."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A team compares two foundation models for dialogue generation using BLEU score. The model with higher BLEU yields more generic responses. What evaluation change addresses this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase training data size.",
      "B": "Switch to ROUGE-L metric.",
      "C": "Include human evaluation of response informativeness.",
      "D": "Add length penalty to BLEU."
    },
    "explanation": "Human evaluation of informativeness identifies generic but high-BLEU outputs, guiding selection of the model that better satisfies user needs."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A model yields high BERTScore but low user engagement. Which additional metric or process should be introduced?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Task completion rate in an interactive user study",
      "B": "Perplexity measured on training data",
      "C": "ROUGE recall",
      "D": "BLEU-1 to check unigram overlap"
    },
    "explanation": "Task completion rate in actual interactions ties performance to user engagement, uncovering issues automated scores miss."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An LLM-based summarization service is evaluated using ROUGE. Reports show high recall but low precision. What does this imply and how can it be improved?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Summaries are too short; increase summary length.",
      "B": "Summaries include too much irrelevant content; implement extractive filtering.",
      "C": "Models are underfitting; increase training epochs.",
      "D": "ROUGE is unreliable; switch to BLEU."
    },
    "explanation": "High recall/low precision indicates summaries cover all reference content but add noise; extractive filtering removes irrelevant parts, improving precision."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A model\u2019s BLEU score is stable but user satisfaction drops after fine-tuning. Which evaluation should have been run pre-deployment?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Compute higher n-gram BLEU (BLEU-5)",
      "B": "Measure BLEU on a larger validation set",
      "C": "Conduct blind human evaluations comparing pre- and post-fine-tuning outputs",
      "D": "Check perplexity on fine-tuning data"
    },
    "explanation": "Blind human evaluations directly compare outputs to detect declines in quality that BLEU alone may not reveal."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "When evaluating a generation model across multiple languages, which metric combination balances adequacy and fluency?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "BLEU for adequacy and perplexity for fluency",
      "B": "ROUGE for fluency and BERTScore for adequacy",
      "C": "Self-BLEU for consistency and BLEU for adequacy",
      "D": "METEOR for adequacy and language-specific perplexity for fluency"
    },
    "explanation": "METEOR accounts for synonyms and alignments indicating adequacy, while perplexity per language measures fluency in that language."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A company uses a benchmark dataset to evaluate a foundation model but suspects data leakage. Which approach BEST uncovers leakage?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Compute overlap of n-grams between training and test sets.",
      "B": "Re-run evaluation with a different metric.",
      "C": "Increase the test set size.",
      "D": "Shuffle the test data."
    },
    "explanation": "N-gram overlap analysis reveals if test examples are too similar to training data, indicating potential leakage."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "During evaluation, a summary model\u2019s ROUGE scores vary significantly across topics. What evaluation strategy addresses this variability?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use BLEU instead of ROUGE.",
      "B": "Segment evaluation by topic and report per-topic metrics.",
      "C": "Aggregate scores across all topics only.",
      "D": "Increase model size to reduce variability."
    },
    "explanation": "Per-topic metrics identify domains where performance lags, guiding targeted improvements."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A model generates financial recommendations. Business requires explanations. Which evaluation method assures explanation quality?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Compute BERTScore on explanation tokens.",
      "B": "Use perplexity to assess explanation fluency.",
      "C": "Conduct human evaluation with a rubric for explanation completeness and correctness.",
      "D": "Measure explanation length."
    },
    "explanation": "Only human evaluators using a rubric can reliably assess the substance and correctness of model-generated explanations."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A foundation model returns biased generation for certain demographics. Which evaluation process best uncovers this bias?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Check perplexity across demographics.",
      "B": "Compute aggregate BLEU scores across the dataset.",
      "C": "Use Self-BLEU for diversity.",
      "D": "Perform targeted human evaluation on demographic-specific prompts."
    },
    "explanation": "Targeted human evaluation reveals bias in outputs across demographic groups, which automated metrics may not detect."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You need to evaluate grounding in RAG-enhanced generation. Which approach measures ground-truth consistency?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Compute ROUGE against retrieval documents only.",
      "B": "Use factuality metrics or human fact-checking against source documents.",
      "C": "Measure Self-BLEU against previous answers.",
      "D": "Evaluate token perplexity conditional on retrieved context."
    },
    "explanation": "Factuality metrics or human fact-checking directly assess whether generated content is consistent with retrieved source documents."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A document QA system for legal files reports high EM (Exact Match) but low user trust. What evaluation should be added?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Human evaluation of answer justification coherence.",
      "B": "Compute BLEU against answer keys.",
      "C": "Increase EM to higher n-grams.",
      "D": "Measure Self-BLEU for consistency."
    },
    "explanation": "Human assessment of justification coherence evaluates trustworthiness beyond exact matching of answers."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An AI agent orchestrating multiple foundation models must be evaluated holistically. Which method captures end-to-end task success?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Compute average BLEU per subtask.",
      "B": "Use combined ROUGE-BLEU macro score.",
      "C": "Define and measure a multi-step task completion metric in simulation.",
      "D": "Measure cumulative perplexity."
    },
    "explanation": "A task completion metric that tracks successful execution of all steps in simulation evaluates orchestration performance end to end."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "During offline evaluation of a model for news article generation, automated metrics correlate poorly with editorial ratings. What is the root cause?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Test set too small.",
      "B": "Metrics (ROUGE/BLEU) emphasize n-gram overlap rather than style and coherence.",
      "C": "Training data imbalance.",
      "D": "Inference temperature too high."
    },
    "explanation": "ROUGE/BLEU focus on overlap, failing to capture narrative style and coherence valued by editors."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A cross-domain foundation model shows varied performance on different benchmarks. How can you fairly compare its overall performance?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Average raw scores across benchmarks.",
      "B": "Use the highest score from any benchmark.",
      "C": "Evaluate on only one benchmark domain.",
      "D": "Normalize scores within each benchmark and compute a weighted aggregate."
    },
    "explanation": "Normalization accounts for differing score ranges across benchmarks, allowing fair aggregated comparison."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You want to automate ongoing evaluation of a deployed LLM summary service. Which pipeline component is essential?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Scheduled sampling of production summaries with periodic human or crowdsource review.",
      "B": "Real-time BLEU computation on user-provided inputs.",
      "C": "Automatic ROUGE computation against live user feedback.",
      "D": "Self-BLEU to measure production diversity."
    },
    "explanation": "Scheduled human review of sampled production outputs is necessary for reliable ongoing evaluation, as automated metrics alone are insufficient."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A team assesses hallucinations by measuring overlap between output and source documents. Which automated metric can help?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ROUGE-L recall",
      "B": "QAEval (question-answering based factuality)",
      "C": "Self-BLEU",
      "D": "Perplexity"
    },
    "explanation": "QAEval uses QA over source documents to check factual consistency, detecting hallucinations automatically."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An LLM generates marketing copy. Which evaluation metric aligns best with expected business KPIs?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Click-through rate measured via A/B testing.",
      "B": "BLEU against a reference ad copy.",
      "C": "ROUGE-F1 against training examples.",
      "D": "Perplexity on marketing dataset."
    },
    "explanation": "Click-through rate directly measures the business goal of marketing copy performance, aligning evaluation with KPIs."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "When evaluating multilingual summarization, the team must ensure language fairness. Which evaluation strategy ensures no language is underrepresented?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Compute aggregate ROUGE across all languages.",
      "B": "Use English reference translations only.",
      "C": "Maintain per-language test sets with equal sample sizes and report metrics separately.",
      "D": "Use a single English-centric metric for all languages."
    },
    "explanation": "Equal per-language test sets and separate reporting detect performance disparities, promoting fairness."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A foundation model exhibits high metric scores on public benchmarks but fails on proprietary data. What evaluation practice could have prevented this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Include proprietary hold-out data in the evaluation benchmark.",
      "B": "Increase public benchmark size.",
      "C": "Compute Self-BLEU on public benchmarks.",
      "D": "Measure perplexity on public data."
    },
    "explanation": "Including proprietary data in evaluation ensures the model is tested on domain-specific content, preventing overreliance on public benchmarks."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your team measures BLEU-4 for a poetry generator but suspects it penalizes creativity. Which metric or strategy addresses this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use BLEU-2 instead.",
      "B": "Increase reference set size to many poems.",
      "C": "Measure perplexity instead of BLEU.",
      "D": "Adopt human evaluation of creativity and stylistic quality."
    },
    "explanation": "Human evaluation is required for subjective qualities like creativity, which BLEU cannot capture adequately."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You must evaluate a model\u2019s responsiveness to user instructions in few-shot prompts. Which evaluation method verifies instruction compliance?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Measure n-gram recall against instruction text.",
      "B": "Human evaluation scoring compliance and correctness of responses.",
      "C": "Compute ROUGE-LF against instruction examples.",
      "D": "Use perplexity conditioned on instruction tokens."
    },
    "explanation": "Human evaluation directly assesses whether model outputs follow instructions and are correct, which automated metrics cannot reliably do."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A generative model\u2019s performance is evaluated with a single reference per input. What is a key risk and how to mitigate it?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Overfitting to reference; reduce training epochs.",
      "B": "Underestimating recall; increase beam size.",
      "C": "Lower metric reliability due to limited references; use multiple diverse references or human evaluation.",
      "D": "Inflated BLEU scores; apply brevity penalty."
    },
    "explanation": "Single references limit metric reliability; adding multiple references or human evaluation improves reliability and fairness."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An LLM-based search assistant\u2019s relevance is evaluated by Precision@5 but gives poor recall. Which complementary metric provides a fuller view?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Recall@5",
      "B": "ROUGE-L",
      "C": "BLEU-1",
      "D": "Perplexity"
    },
    "explanation": "Recall@5 measures the proportion of relevant documents retrieved, complementing Precision@5 for balanced evaluation."
  },
  {
    "taskStatement": "3.4",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A team logs user interactions to evaluate a deployed chatbot. Which observed metric best indicates dialogue coherence?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Average response length",
      "B": "Turn-taking frequency",
      "C": "Conversation success rate (user achieves goal without mid-dialog corrections)",
      "D": "BLEU score against template responses"
    },
    "explanation": "Conversation success rate reflects coherence and goal achievement in dialogue, which automated metrics do not capture."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A financial institution uses an ML model to screen loan applications and discovers that applicants from a specific demographic group are disproportionately rejected. Which responsible AI principle is most directly violated?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Fairness \u2013 the model is treating demographic groups unequally.",
      "B": "Transparency \u2013 the model\u2019s decision logic is hidden.",
      "C": "Robustness \u2013 the model cannot handle noisy data.",
      "D": "Safety \u2013 the model is causing harm to users."
    },
    "explanation": "Disparate treatment of a demographic group indicates a fairness violation, as responsible AI demands equal outcomes across groups."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An HR recruiter deploys a resume\u2010screening model that filters out all non\u2010English resumes, thereby excluding qualified multilingual candidates. Which responsible AI feature is lacking?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Inclusivity \u2013 the model fails to consider diverse candidate backgrounds.",
      "B": "Veracity \u2013 the model is providing inaccurate screening.",
      "C": "Robustness \u2013 the model cannot process different document formats.",
      "D": "Safety \u2013 the model is exposing sensitive information."
    },
    "explanation": "Excluding non\u2010English resumes indicates a lack of inclusivity, as a responsible AI system should serve diverse users."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A self\u2010driving car\u2019s perception model performs well in clear weather but fails in foggy conditions. Which responsible AI characteristic should be improved?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Fairness \u2013 ensure equal performance across users.",
      "B": "Transparency \u2013 explain the model\u2019s failure mode.",
      "C": "Robustness \u2013 maintain performance under varied conditions.",
      "D": "Veracity \u2013 avoid generating false sensor readings."
    },
    "explanation": "Robustness refers to consistent performance under different real\u2010world conditions; failure in fog indicates a robustness issue."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A chatbot powered by a foundation model confidently fabricates statistics in its responses. Which responsible AI principle is compromised?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Safety \u2013 the model must avoid harmful outputs.",
      "B": "Fairness \u2013 the model must treat user queries equally.",
      "C": "Robustness \u2013 the model must handle adversarial prompts.",
      "D": "Veracity \u2013 the model\u2019s outputs must be truthful and accurate."
    },
    "explanation": "Veracity is about truthfulness and accuracy; hallucinated statistics violate veracity."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A medical advice AI system suggests a treatment that poses health risks to users. Which responsible AI attribute has been neglected?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Fairness \u2013 equal recommendation quality.",
      "B": "Safety \u2013 avoiding harm to end users.",
      "C": "Inclusivity \u2013 serving diverse patient groups.",
      "D": "Veracity \u2013 providing only factual information."
    },
    "explanation": "Safety focuses on preventing harm; recommending risky treatments shows a safety lapse."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your team needs to enforce content policies (e.g., block hate speech) and prevent hallucinations in responses from a foundation model. Which AWS feature should you use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS WAF \u2013 protects web applications only.",
      "B": "Amazon SageMaker Model Monitor \u2013 monitors metrics post\u2010deployment.",
      "C": "Amazon Bedrock Guardrails \u2013 enforces input/output policies in real time.",
      "D": "Amazon Macie \u2013 discovers and protects sensitive data in S3."
    },
    "explanation": "Bedrock Guardrails allows you to define input/output policies to block disallowed content and reduce hallucinations in generative AI."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "To trigger an alert whenever a Bedrock Guardrails policy is violated, which AWS service should you integrate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon CloudWatch Events \u2013 can detect and alert on Guardrails events.",
      "B": "AWS Config \u2013 tracks configuration changes, not real\u2010time guardrail events.",
      "C": "AWS CodePipeline \u2013 orchestrates CI/CD, not monitoring.",
      "D": "Amazon GuardDuty \u2013 detects threats, not policy violations in LLM responses."
    },
    "explanation": "CloudWatch Events can be configured to catch Bedrock Guardrails violations and send alerts or trigger workflows."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You want to validate user input against allowed schemas before sending it to a foundation model and reject malformed requests. Which component of Bedrock Guardrails addresses this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Output guard \u2013 sanitizes model responses.",
      "B": "Input guard \u2013 enforces schema/patterns before invocation.",
      "C": "Model selector \u2013 chooses which model to use.",
      "D": "Fallback mechanism \u2013 handles service outages."
    },
    "explanation": "Input guards are designed to validate and reject unauthorized or malformed inputs before invoking the model."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You need to automatically mask profanity in generative responses without modifying your application code. Which Bedrock Guardrails feature do you configure?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Input guard \u2013 stops profanity at input stage.",
      "B": "IAM policy \u2013 controls access, not content.",
      "C": "Model selector \u2013 picks a sanitized model.",
      "D": "Output guard \u2013 transforms or filters responses to remove profanity."
    },
    "explanation": "Output guards can apply transformations or filters to model-generated text, such as masking profanity."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "To audit all calls made to your Bedrock foundation models and any guardrail violations for compliance, which AWS service provides an immutable record?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon CloudWatch Logs \u2013 useful but not immutable.",
      "B": "AWS Config \u2013 captures resource configs, not API calls.",
      "C": "AWS CloudTrail \u2013 records API calls and guardrail events immutably.",
      "D": "Amazon X-Ray \u2013 traces requests but not policy violations."
    },
    "explanation": "CloudTrail records all API calls, including Bedrock invocations and guardrail events, for auditing."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A company must choose between a large foundation model and a parameter\u2010efficient adapter layer approach to minimize carbon emissions. Which choice best meets sustainability goals?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use the largest available model for higher accuracy.",
      "B": "Implement parameter-efficient fine-tuning with a smaller base model.",
      "C": "Train the model longer to improve performance.",
      "D": "Host models in on-premises data centers to control cooling."
    },
    "explanation": "Parameter-efficient fine-tuning of a smaller base model substantially reduces compute and hence carbon footprint compared to large full-model training."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "To reduce the energy consumption of inference in production, which technique should you apply?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Quantize the model to lower-precision (e.g., INT8) operations.",
      "B": "Execute inference in real time only.",
      "C": "Increase batch size to 1 for latency improvement.",
      "D": "Fine-tune the model with more data."
    },
    "explanation": "Quantization reduces compute and memory requirements, lowering energy consumption during inference."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A team is deciding between AWS Graviton-based instances and GPU instances for large-scale model training. Which choice improves energy efficiency?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Graviton-based instances for CPU-optimized, energy-efficient training.",
      "B": "Choose the highest-end GPU for maximum throughput.",
      "C": "Run training on on-premises hardware to reduce cloud usage.",
      "D": "Use burstable instances to save cost only."
    },
    "explanation": "Arm-based Graviton processors deliver better performance per watt for many ML workloads, reducing energy usage."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Scheduling batch inference jobs during off-peak hours to take advantage of a lower PUE (power usage effectiveness) is an example of which sustainability practice?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model quantization \u2013 reduces model size.",
      "B": "Workload scheduling \u2013 aligns compute with grid efficiency.",
      "C": "Using renewable energy certificates \u2013 purchases carbon offsets.",
      "D": "Data pruning \u2013 reduces dataset size."
    },
    "explanation": "Scheduling workloads when the data center\u2019s PUE is lower minimizes overall energy overhead, a key sustainability tactic."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A business wants to minimize its AI carbon footprint by selecting an AWS region. Which factor should guide their choice?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Lowest network latency for user traffic only.",
      "B": "Region with all available instance types.",
      "C": "Region with deepest discount pricing.",
      "D": "Region with higher renewable energy usage on the local grid."
    },
    "explanation": "Regions powered by cleaner energy sources reduce the carbon footprint of compute workloads."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An ML team fine\u2010tunes a foundation model using unlicensed internet text, ignoring copyright constraints. Which legal risk are they exposed to?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Intellectual property infringement \u2013 unauthorized use of copyrighted content.",
      "B": "Model drift \u2013 performance degradation over time.",
      "C": "Data poisoning \u2013 malicious training inputs.",
      "D": "Overfitting \u2013 too specific to training data."
    },
    "explanation": "Using copyrighted text without permission risks IP infringement and potential legal action."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A chatbot trained on scraped forum posts inadvertently reveals personal user data. Which legal/compliance risk does this represent?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Hallucinations \u2013 fabricating facts.",
      "B": "IP infringement \u2013 misusing copyrighted content.",
      "C": "Privacy violation \u2013 exposing personal identifiable information.",
      "D": "Overfitting \u2013 echoing training examples."
    },
    "explanation": "Revealing PII from training data breaches privacy regulations and violates data protection laws."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A lender\u2019s credit-scoring model shows statistically significant bias against a protected group, undermining consumer trust and inviting regulatory scrutiny. Which risk category best describes this issue?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Safety risk \u2013 potential physical harm.",
      "B": "Customer trust loss \u2013 reputational and compliance risk.",
      "C": "Sustainability risk \u2013 environmental impact.",
      "D": "Latency risk \u2013 slow response times."
    },
    "explanation": "Biased outcomes erode customer trust and invite legal/regulatory consequences, a customer trust risk."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An insurance chatbot confidently gives incorrect policy advice that leads to financial harm. Which specific risk from responsible AI does this example illustrate?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Data poisoning \u2013 corrupted training data.",
      "B": "Class imbalance \u2013 skewed training labels.",
      "C": "Drift \u2013 changing data distributions.",
      "D": "Hallucinations \u2013 inaccurate outputs presented as fact."
    },
    "explanation": "Confidently incorrect advice from a generative model is a hallucination, risking legal liabilities."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A healthcare application\u2019s language model suggests off-label drug uses without evidence, exposing the company to legal liability. Which risk type is this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Bias risk \u2013 unfair treatment across populations.",
      "B": "Sustainability risk \u2013 high energy usage.",
      "C": "End\u2010user risk \u2013 harm from incorrect guidance.",
      "D": "Veracity risk \u2013 lack of model explainability."
    },
    "explanation": "Incorrect medical guidance poses direct harm to end users, an end\u2010user risk under responsible AI."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A computer vision dataset for facial recognition lacks images of darker skin tones, resulting in higher error rates for certain groups. Which dataset characteristic is missing?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Curation \u2013 removing low-quality images.",
      "B": "Diversity \u2013 including varied skin tones.",
      "C": "Balanced classes \u2013 equal count for each class.",
      "D": "Inclusivity \u2013 serving all age groups."
    },
    "explanation": "Lack of varied skin tones indicates poor diversity, leading to biased performance."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your image classification dataset contains duplicates, mislabeled entries, and missing annotations. Which dataset characteristic issue does this describe?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Inclusivity \u2013 unsuitable for demographic fairness.",
      "B": "Diversity \u2013 missing variation of classes.",
      "C": "Curation \u2013 poor data quality and organization.",
      "D": "Balance \u2013 unequal class representation."
    },
    "explanation": "Duplication and mislabels reflect poor data curation practices, harming model quality."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A binary classification dataset has 95% of examples in one class and 5% in the other. What dataset property should you address?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Inclusivity \u2013 demographic representation.",
      "B": "Curation \u2013 data cleanliness.",
      "C": "Diversity \u2013 feature variation.",
      "D": "Class imbalance \u2013 skewed target distribution."
    },
    "explanation": "A 95/5 split indicates severe class imbalance, which can bias model predictions toward the majority class."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A global voice assistant is trained exclusively on North American English accents. Which dataset characteristic is lacking?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Inclusivity \u2013 support for diverse accents and dialects.",
      "B": "Balance \u2013 equal number of male/female voices.",
      "C": "Curation \u2013 removal of background noise.",
      "D": "Veracity \u2013 truthful responses."
    },
    "explanation": "Excluding non\u2013North American accents shows lack of inclusivity, leading to poor user experience globally."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Before training, you resample your dataset so each class reflects real\u2010world population proportions. Which property does this ensure?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Diversity \u2013 varied feature space coverage.",
      "B": "Inclusivity \u2013 demographic fairness.",
      "C": "Balance \u2013 representative class distribution.",
      "D": "Sustainability \u2013 efficient data usage."
    },
    "explanation": "Resampling to real\u2010world proportions addresses class balance, ensuring representative distribution."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your model achieves 99% accuracy on training data but only 60% on validation data. Which bias/variance issue does this indicate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High bias \u2013 underfitting the data.",
      "B": "High variance \u2013 overfitting to the training set.",
      "C": "Data drift \u2013 changing data distribution over time.",
      "D": "Concept drift \u2013 evolving relationship between features and labels."
    },
    "explanation": "A large gap between train and validation accuracy indicates high variance (overfitting)."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "To detect demographic bias in model predictions post\u2010deployment, which AWS service should you use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon SageMaker Clarify \u2013 analyzes bias and feature attributions.",
      "B": "Amazon SageMaker Model Monitor \u2013 monitors data drift only.",
      "C": "Amazon A2I \u2013 human-review workflow for low-confidence cases.",
      "D": "AWS Config \u2013 monitors resource configurations."
    },
    "explanation": "SageMaker Clarify provides bias detection and feature importance analysis across defined groups."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "You want to continuously track fairness metrics (e.g., false-positive rates across groups) in production. Which AWS tool is best?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Clarify \u2013 batch analysis only.",
      "B": "SageMaker Model Monitor \u2013 continuous real\u2010time monitoring of metrics.",
      "C": "Amazon A2I \u2013 human review only.",
      "D": "AWS CloudTrail \u2013 API audit logs, not metrics."
    },
    "explanation": "Model Monitor can track custom metrics, including fairness, in real time."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "To route low-confidence or high-risk predictions for human review, which AWS service should you integrate?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "SageMaker Clarify \u2013 bias detection but no routing.",
      "B": "SageMaker Model Monitor \u2013 metrics tracking only.",
      "C": "Amazon Augmented AI (A2I) \u2013 orchestrates human reviews.",
      "D": "Amazon Rekognition \u2013 only for image analysis."
    },
    "explanation": "Amazon A2I allows you to define human-review workflows for model outputs based on confidence or policy criteria."
  },
  {
    "taskStatement": "4.1",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "After running SageMaker Clarify, you observe that a feature\u2019s importance differs significantly across demographic groups. Which concept does this reveal?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Data drift \u2013 change in feature distributions over time.",
      "B": "Overfitting \u2013 model too complex.",
      "C": "Variance \u2013 model instability across datasets.",
      "D": "Disparate impact \u2013 unequal model behavior across groups."
    },
    "explanation": "Differing feature importance across groups indicates disparate impact, a form of bias requiring mitigation."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A healthcare startup must deploy a model to predict patient readmission risk. Regulators require clear rationale for each prediction. Which model choice best balances accuracy and explainability?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A deep ensemble of neural networks with gradient-based saliency maps",
      "B": "A random forest with thousands of trees and SHAP explanations",
      "C": "A shallow decision tree pruned for clarity with feature importance annotations",
      "D": "A support vector machine with RBF kernel and LIME explanations"
    },
    "explanation": "A pruned decision tree yields inherently transparent rules, satisfying regulators with minimal complexity, whereas complex ensembles or SVMs remain opaque despite post hoc methods."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An e-commerce company uses SageMaker Model Cards for several models. A risk audit requires assessment of data lineage and bias evaluation. Which section of the Model Card provides this information?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Quantitative analysis section detailing performance metrics",
      "B": "Training data section documenting sources, preprocessing, and bias checks",
      "C": "Usage recommendations section explaining inference patterns",
      "D": "Hyperparameter section listing tuning configurations"
    },
    "explanation": "The training data section of a Model Card records data provenance and any bias analyses, which is needed for risk and lineage audits."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A finance firm must choose a pre-trained NLP model under an open-source license that allows modification and internal transparency. Which license attribute ensures the firm can inspect and modify the model code?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Proprietary commercial license with source code escrow",
      "B": "Non-commercial Creative Commons license",
      "C": "End-user license agreement without vendor source access",
      "D": "OSI-approved permissive license (e.g., Apache 2.0)"
    },
    "explanation": "An OSI-approved permissive license like Apache 2.0 grants rights to view, modify, and redistribute source code, meeting transparency needs."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A user interface team requires explanations in layman\u2019s terms for model predictions. Which human-centered design principle should guide the explanation content?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Contextualizing model output in domain-specific language familiar to end users",
      "B": "Providing raw feature importance scores without interpretation",
      "C": "Including advanced mathematical derivations for transparency",
      "D": "Displaying performance metrics like precision and recall"
    },
    "explanation": "Contextualizing predictions using common domain terminology aligns explanations with users\u2019 mental models, improving comprehension."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A model\u2019s Model Card indicates high accuracy but warns of demographic bias in certain groups. Which step is most appropriate before deployment?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Ignore the warning because accuracy is sufficient",
      "B": "Retrain or tune preprocessing to balance representation before deploying",
      "C": "Deploy and monitor only high-confidence predictions",
      "D": "Apply ensemble averaging to mask bias"
    },
    "explanation": "Addressing bias through data balancing or preprocessing is required to mitigate demographic disparities rather than ignoring or masking them."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A bank must explain credit decisions to customers. Which model explanation technique provides individual-level counterfactual examples?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Partial dependence plots",
      "B": "Global SHAP summary plots",
      "C": "LIME local feature attribution",
      "D": "Counterfactual explanation generation"
    },
    "explanation": "Counterfactual explanations show how input changes alter decisions at the individual level, supporting regulatory right-to-explanation."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "An operations team debates using a black-box model with high throughput vs. an interpretable white-box model with lower throughput. What trade-off should they document?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Interpretability vs. performance in latency-critical applications",
      "B": "Accuracy vs. model complexity when performance metrics are equal",
      "C": "Cost vs. regulatory compliance if throughput varies",
      "D": "Scalability vs. maintenance overhead"
    },
    "explanation": "They need to weigh interpretability against latency/performance, especially if auditability is required in real time."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which AWS tool can be used to generate global and local feature attributions to improve model transparency?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Model Monitor",
      "B": "SageMaker Clarify",
      "C": "SageMaker Autopilot",
      "D": "Amazon A2I"
    },
    "explanation": "SageMaker Clarify provides built-in methods for global and local feature attribution, aiding transparency analyses."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A text\u2010classification model gives reasons like \u201cword occurrence frequency\u201d to end users. Which explanation method does this represent?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Intrinsic explanations using model-internal weights",
      "B": "Post hoc example\u2010based explanations",
      "C": "Surrogate decision tree explanations",
      "D": "Counterfactual explanation"
    },
    "explanation": "Describing predictions based on internal token weight frequencies is an intrinsic explanation, reflecting model internals."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A team uses SHAP values but end users find the output confusing. How can they improve human-centered explainability?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch to raw weight visualizations",
      "B": "Provide detailed mathematical formulas alongside plots",
      "C": "Translate SHAP values into natural language summaries",
      "D": "Expose full data distributions for each feature"
    },
    "explanation": "Summarizing SHAP insights in natural language makes explanations accessible, aligning with human-centered design."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A self-driving car project needs transparent vision models. Which architecture offers the best explainability without drastic performance loss?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deep convolutional network without modifications",
      "B": "Shallow convolutional network combined with attention maps",
      "C": "Ensemble of deep generative adversarial networks",
      "D": "Transformer-based vision model with no interpretability features"
    },
    "explanation": "Adding attention maps to a shallow CNN yields insight into areas driving predictions while maintaining acceptable performance."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A PwC auditor requests a summary of model limitations before deployment. Which part of the Model Card should be updated?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Quantitative results",
      "B": "Training details",
      "C": "Usage recommendations",
      "D": "Limitations and caveats section"
    },
    "explanation": "The limitations and caveats section explicitly documents known weaknesses for audit transparency."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your company plans to fine-tune a foundation model but needs to maintain explainability. What approach minimizes opacity?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Fine-tune all layers with large datasets",
      "B": "Use prompt tuning rather than full model fine-tuning",
      "C": "Increase model depth post fine-tuning",
      "D": "Apply knowledge distillation without retaining student model"
    },
    "explanation": "Prompt tuning customizes model behavior without altering internal weights, preserving transparency of the base model."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A credit model must log all decisions and rationales. Which AWS service can automate capturing predictions and explanation metadata?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Model Monitor with Clarify integration",
      "B": "SageMaker Ground Truth",
      "C": "Amazon CloudWatch Application Insights",
      "D": "AWS Config"
    },
    "explanation": "Model Monitor with Clarify captures prediction inputs, outputs, and feature attributions for logging and review."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "To comply with GDPR\u2019s right-to-explanation, which design practice is essential?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Encrypt all decision logs",
      "B": "Use only cloud-managed proprietary models",
      "C": "Provide counterfactual or feature\u2010based explanations",
      "D": "Store model artifacts indefinitely"
    },
    "explanation": "GDPR requires providing explanations of automated decisions, which is met by counterfactual or feature\u2010based explanations."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A chat assistant uses a black-box large language model. Users report inconsistent reasoning. Which strategy improves transparency?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase model temperature",
      "B": "Add more user feedback loops",
      "C": "Limit response length",
      "D": "Use a retrieval augmented generation pipeline with provenance citations"
    },
    "explanation": "RAG with provenance citations shows source documents, making model outputs more transparent and verifiable."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which explanation method is model-agnostic and builds a local surrogate to explain individual predictions?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SHAP TreeExplainer",
      "B": "LIME",
      "C": "Integrated Gradients",
      "D": "Attention heatmaps"
    },
    "explanation": "LIME fits a simple interpretable model locally around each prediction, regardless of the underlying model type."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your product team insists on maximal model performance and opposes transparency features that slow inference. How do you address this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Propose a hybrid system: high\u2010performance model behind the scenes with an interpretable surrogate for user explanations",
      "B": "Disable logging to speed up inference",
      "C": "Switch entirely to an interpretable model and accept accuracy loss",
      "D": "Use asynchronous explanation requests after inference"
    },
    "explanation": "A surrogate for explanations preserves throughput while offering users transparent reasoning separate from core inference."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which is a risk of choosing a highly transparent model in a security-sensitive application?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Regulatory non-compliance",
      "B": "Excessive latency",
      "C": "Easier maintenance",
      "D": "Model inversion attacks exposing training data"
    },
    "explanation": "Transparent models with exposed structures may reveal sensitive training data patterns, risking inversion attacks."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "During user testing, explanations based only on model confidence scores are unhelpful. Which alternative best aids user understanding?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provide raw probability distributions",
      "B": "Show training dataset size",
      "C": "Offer top contributing features per prediction",
      "D": "List all hyperparameters"
    },
    "explanation": "Highlighting key features driving each decision gives users actionable insights beyond mere confidence levels."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A marketing team wants to know if a text classification model considers banned words. Which transparency tool reveals word\u2010level influences?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "SageMaker Model Monitor",
      "B": "SageMaker Clarify word\u2010attribution feature",
      "C": "Model Card quantitative metrics",
      "D": "Feature Store audit logs"
    },
    "explanation": "SageMaker Clarify\u2019s word-attribution identifies text tokens\u2019 importance in predictions, revealing banned word influence."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Your team uses a black-box ensemble. A stakeholder demands global explanations. Which technique provides global surrogate model insights?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a decision tree surrogate on the ensemble\u2019s inputs and outputs",
      "B": "Generate local LIME explanations and average them",
      "C": "Use integrated gradients",
      "D": "Deploy attention mechanisms"
    },
    "explanation": "A global surrogate decision tree approximates the ensemble\u2019s decision surface, offering interpretable rules."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A supply chain model\u2019s Model Card lacks information on feature engineering. Why is this a transparency concern?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Performance metrics might be inflated",
      "B": "Cost estimates could be inaccurate",
      "C": "Stakeholders cannot trace how raw inputs become features, hindering interpretability",
      "D": "Model deployment will fail"
    },
    "explanation": "Without feature engineering details, users can\u2019t understand how inputs map to features, reducing model explainability."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which step best mitigates explanation risks when using third-party foundation models?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Trust vendor documentation blindly",
      "B": "Skip bias assessments to save time",
      "C": "Only use models with closed-source weights",
      "D": "Perform independent evaluations and review open-source Model Cards"
    },
    "explanation": "Independent tests and reviewing Model Cards ensures third-party models meet transparency and bias requirements."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A logistics company needs to explain anomaly detection alerts in real time. Which design is most suitable?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use an interpretable isolation forest with path length visualizations",
      "B": "Deploy a deep autoencoder and log latent activations",
      "C": "Train a GAN and inspect discriminator features",
      "D": "Apply black-box clustering with post hoc clustering explanations"
    },
    "explanation": "An interpretable isolation forest allows visualization of how anomalies are isolated, offering real\u2010time clarity."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which documentation practice supports transparency throughout the model lifecycle?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Only logging final model metrics",
      "B": "Maintaining versioned Model Cards with data and design decisions",
      "C": "Archiving raw code without metadata",
      "D": "Keeping hyperparameter settings private"
    },
    "explanation": "Versioned Model Cards record evolving data sources, design rationale, and performance, ensuring transparency over time."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "Which explanation approach is least prone to misleading interpretations for correlated features?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Individual conditional expectation plots",
      "B": "Permutation feature importance",
      "C": "SHAP dependence plots with interaction effects",
      "D": "Linear model coefficients"
    },
    "explanation": "SHAP dependence plots with interaction terms separate correlated feature effects, reducing misleading attributions."
  },
  {
    "taskStatement": "4.2",
    "exam": "AWS Certified AI Practitioner (AIF-C01)",
    "stem": "A customer disputes a decision and wants a human review. Which AWS service facilitates human-in-the-loop explainability?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Model Monitor",
      "B": "AWS Config",
      "C": "SageMaker Clarify",
      "D": "Amazon Augmented AI (A2I)"
    },
    "explanation": "Amazon A2I routes flagged predictions to human reviewers, providing an interface for explanation and correction."
  }
]