[
  {
    "taskStatement": "1.1",
    "stem": "A company processes IoT sensor data streams at 200 MB/s for ML feature engineering. The data arrives in JSON format and must be ingested into SageMaker Data Wrangler in near-real time. The ingestion solution must minimize operational overhead and storage costs while enabling efficient downstream transformations. Which solution meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy Amazon Kinesis Data Streams to buffer JSON records. Configure an AWS Lambda function to batch and convert records into Parquet, then write to S3. Use Data Wrangler to read from S3.",
      "B": "Use Amazon Kinesis Data Firehose to ingest JSON data directly into an S3 bucket with Parquet conversion and GZIP compression. Configure Data Wrangler to read from that S3 bucket.",
      "C": "Set up an Amazon Managed Streaming for Apache Kafka cluster. Use Kafka Connect to stream data into an Amazon Redshift table. Use Data Wrangler to query Redshift.",
      "D": "Ingest streams into an AWS Glue Streaming ETL job. Write output to Amazon DynamoDB. Use Data Wrangler to fetch items from DynamoDB."
    },
    "explanation": "Kinesis Data Firehose natively converts JSON to Parquet with compression and writes to S3, minimizing operational overhead and cost. The other options add complexity or incur higher cost/management burden."
  },
  {
    "taskStatement": "1.1",
    "stem": "A dataset consists of structured CSV transaction logs, semi-structured JSON user event logs, and tens of millions of small binary image files. The ML pipeline uses Spark on EMR for preprocessing and SageMaker for training. The dataset has grown to multi-terabyte scale and ingestion performance is suffering. Which storage configuration should the ML engineer choose to optimize throughput, cost, and simplicity?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Store CSV and JSON in S3. Store images in Amazon FSx for Lustre mounted to EMR. Use Spark on EMR to read both sources.",
      "B": "Import all data into Amazon Redshift with Spectrum. Store CSV/JSON in tables, images as BLOBs. UNLOAD data for SageMaker training.",
      "C": "Store all data in Amazon S3 as partitioned, SNAPPY-compressed Parquet files. Configure EMR to read via the S3A connector and SageMaker to use S3 input mode.",
      "D": "Use Amazon EFS to store all raw files. Mount EFS to EMR clusters and SageMaker training jobs as a unified data source."
    },
    "explanation": "Partitioned Parquet on S3 offers cost-effective, high-throughput storage, simple management, and native support in Spark and SageMaker. FSx adds cost/ops, Redshift BLOBs and Spectrum incur complexity, and EFS suffers lower throughput and higher cost for large-scale data."
  },
  {
    "taskStatement": "1.1",
    "stem": "An ML engineer runs a SageMaker processing job to merge and feature-engineer 50 million small JSON files in Amazon S3. The job fails due to S3 request throttling from excessive LIST and GET operations. Which solution will reduce operational overhead and ensure reliable job completion?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure a VPC endpoint for S3 in the SageMaker processing job to reduce throttling. Increase parallelism to maximize throughput.",
      "B": "Enable S3 Transfer Acceleration on the bucket to speed up GET requests. Use the Transfer Acceleration endpoint in the processing job.",
      "C": "Use AWS Glue to run a job that aggregates the small JSON files into larger SNAPPY-compressed Parquet files in S3. Update the SageMaker processing job to read the Parquet files.",
      "D": "Copy all JSON files to an Amazon EFS file system using AWS DataSync. Configure the SageMaker processing job to read directly from EFS."
    },
    "explanation": "Aggregating small files into larger Parquet files reduces S3 request count and improves throughput with minimal ongoing operations. VPC endpoints and Transfer Acceleration do not change S3 request rate limits; EFS adds complexity and cost."
  },
  {
    "taskStatement": "1.2",
    "stem": "An ML engineer needs to encode a high-cardinality categorical feature (10 000 unique values) for a tree-based model. They must minimize memory footprint, avoid one-hot explosion, and preserve ordering where meaningful. Which approach and AWS tool should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use one-hot encoding in SageMaker Data Wrangler to produce sparse binary columns.",
      "B": "Use label encoding in AWS Glue DataBrew to assign integer codes to categories.",
      "C": "Use binary encoding in SageMaker Data Wrangler to compress categories into \u2308log\u2082(10000)\u2309 binary features.",
      "D": "Use frequency encoding in an AWS Glue ETL job to replace each category with its occurrence count."
    },
    "explanation": "Binary encoding (also called bit hashing) reduces dimensionality to \u2308log\u2082(n)\u2309 columns and preserves some ordinal information. SageMaker Data Wrangler supports a built-in binary encoding transform, minimizing memory and avoiding one-hot explosion."
  },
  {
    "taskStatement": "1.2",
    "stem": "A data scientist is building a cleaning pipeline in SageMaker Data Wrangler for a numerical dataset with extreme outliers and missing values. They need to: (1) cap outliers at the 1st and 99th percentiles, (2) impute remaining missing values with the column median, and (3) standardize features to zero mean and unit variance. Which sequence of Wrangler nodes achieves this with minimal custom code?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Standardize \u2192 percentile clip \u2192 median impute",
      "B": "Median impute \u2192 percentile clip \u2192 standardize",
      "C": "Percentile clip \u2192 median impute \u2192 standardize",
      "D": "Median impute \u2192 standardize \u2192 percentile clip"
    },
    "explanation": "Clipping outliers first avoids imputing capped values. Next, median imputation handles any remaining missing data. Finally, standardization yields zero mean/unit variance on the cleaned data."
  },
  {
    "taskStatement": "1.2",
    "stem": "A real-time recommendation engine requires streaming user events from Kinesis Data Streams to be transformed (JSON flattening, type conversion) and ingested into SageMaker Feature Store with under-second latency. Which solution minimizes operational overhead and meets latency SLAs?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Implement an AWS Lambda function triggered by Kinesis Data Streams that transforms each record and calls the SageMaker Feature Store PutRecord API.",
      "B": "Run an AWS Glue streaming ETL job on Kinesis Data Streams with PySpark to transform and write directly to Feature Store.",
      "C": "Deploy an Apache Flink application on Amazon EMR to consume Kinesis, perform transformations, write to S3, then batch-load into Feature Store.",
      "D": "Use Amazon Kinesis Data Analytics (Flink SQL) to transform the stream and send to a SageMaker Batch Transform job for periodic ingestion."
    },
    "explanation": "A Lambda function on Kinesis provides sub-second processing with minimal infrastructure to manage. It can call PutRecord directly. Glue streaming and EMR/Flink introduce greater complexity and latency; Batch Transform cannot meet real-time SLAs."
  },
  {
    "taskStatement": "1.3",
    "stem": "A health-tech startup has a dataset in Amazon S3 with PII fields (names, SSNs) and binary labels. They must prepare the data for SageMaker training by removing or masking PII, validating data quality, computing pre-training bias metrics (class imbalance, difference in proportions), and mitigating identified bias before training. Which sequence of AWS services and actions fulfills these requirements with minimal operational overhead?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon Macie to discover PII \u2192 apply client-side KMS encryption \u2192 run a SageMaker Processing job with a custom script to compute bias metrics.",
      "B": "Use AWS Glue Data Quality to profile dataset \u2192 use AWS Glue ETL to anonymize PII \u2192 run SageMaker Model Monitor to compute pre-training bias metrics.",
      "C": "Use SageMaker Ground Truth to label PII columns \u2192 use SageMaker Data Wrangler to transform data \u2192 use SageMaker Clarify ModelBias to compute post-training bias.",
      "D": "Use SageMaker Data Wrangler to detect and mask PII with built-in transforms \u2192 launch a SageMaker Clarify DataBias job to calculate pre-training CI and DPL metrics \u2192 apply synthetic oversampling for the minority class via Data Wrangler or a SageMaker Processing job."
    },
    "explanation": "Option D leverages Data Wrangler\u2019s built-in PII masking, uses Clarify\u2019s DataBias monitoring to compute pre-training bias metrics, and then mitigates imbalance via a native transform or Processing job, minimizing custom code and meeting compliance and bias-mitigation requirements."
  },
  {
    "taskStatement": "1.3",
    "stem": "During preprocessing, an ML engineer discovers a significant class imbalance: one protected class comprises only 1% of the training data. They need to generate additional synthetic samples for that class without introducing label noise and then validate the augmented dataset\u2019s quality. Which approach aligns best with AWS-recommended bias mitigation and data validation practices?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Clarify\u2019s synthetic data generator to produce minority-class examples and merge with original data.",
      "B": "Launch a SageMaker Processing job that uses scikit-learn\u2019s SMOTE to create synthetic minority-class samples, then run AWS Glue Data Quality jobs to validate data integrity.",
      "C": "Use AWS Glue DataBrew\u2019s \u201cGenerate rows\u201d transform to duplicate minority-class records until balance is achieved.",
      "D": "Ingest data into SageMaker Feature Store and use record augmentation in Feature Store to rebalance classes."
    },
    "explanation": "Option B applies SMOTE in a managed Processing job to generate realistic synthetic samples for the minority class, then validates the augmented dataset with Glue Data Quality, aligning with best practices for bias mitigation and data integrity."
  },
  {
    "taskStatement": "1.3",
    "stem": "An ML engineer must deliver preprocessed, bias-mitigated training data to a SageMaker training job. The data must be encrypted at rest and in transit, and the training dataset split must be stratified by a protected attribute to avoid introducing bias. How should the engineer configure the data pipeline to meet these requirements?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Store the CSVs in S3 with SSE-S3 encryption, use File input mode in SageMaker training, and rely on built-in random shuffle for splits.",
      "B": "Provision an encrypted FSx for Lustre file system, copy preprocessed files there, and configure the training job with File mode and a JSON split manifest.",
      "C": "Mount an encrypted Amazon EFS volume in the training container, copy data via a Processing job that shuffles and splits, then train using File mode.",
      "D": "Store preprocessed data in S3 with SSE-KMS encryption, use Pipe input mode for the training job, and implement a stratified shuffle-and-split routine inside the training container to ensure splits respect the protected attribute."
    },
    "explanation": "Option D ensures end-to-end encryption with SSE-KMS, leverages Pipe mode to stream data efficiently, and uses a custom stratified shuffle-and-split routine in the container to maintain fairness across protected groups."
  },
  {
    "taskStatement": "2.1",
    "stem": "A fintech company is building a credit-scoring model to classify loan applicants as high or low risk. The dataset consists of 1 million rows of structured tabular data with a mix of numerical and categorical features of moderate cardinality. Regulatory requirements mandate that the model be highly interpretable for audit, and the application demands low-latency inference. Data scientists want to minimize manual feature engineering. Which SageMaker built-in algorithm should the ML engineer choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use the XGBoost built-in algorithm with default hyperparameters.",
      "B": "Use the Factorization Machines built-in algorithm to capture feature interactions.",
      "C": "Use the Linear Learner built-in algorithm configured for logistic regression.",
      "D": "Use the K-Nearest Neighbors built-in algorithm for classification."
    },
    "explanation": "Linear Learner in logistic regression mode provides a fully interpretable model with low inference latency and minimal feature engineering. XGBoost offers higher accuracy but less interpretability. Factorization Machines capture interactions but are harder to audit. KNN has high latency and is not suited for large datasets."
  },
  {
    "taskStatement": "2.1",
    "stem": "A legal analytics startup needs to automate abstractive summarization of large contract documents. The team lacks expertise in training sequence-to-sequence models and requires a fully managed, high-quality solution that supports few-shot prompting. Which modeling approach should the ML engineer select?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon Comprehend\u2019s extractive summarization API on the contracts.",
      "B": "Fine-tune a T5 sequence-to-sequence model on SageMaker using the Hugging Face framework from JumpStart.",
      "C": "Invoke a foundation model via Amazon Bedrock (for example, Titan) with a prompt template for abstractive summarization.",
      "D": "Build a custom TensorFlow sequence-to-sequence model from scratch on SageMaker training instances."
    },
    "explanation": "Amazon Bedrock provides managed foundation models that excel at few-shot abstractive summarization with minimal ML expertise. Comprehend only supports extractive summarization. Fine-tuning on SageMaker or building from scratch incurs significant development and infrastructure overhead."
  },
  {
    "taskStatement": "2.1",
    "stem": "An industrial IoT provider collects streaming time-series sensor data from thousands of devices and needs to detect anomalies in near real time. The provider wants a fully managed service requiring minimal ML development and configuration. Which AWS service or approach should the ML engineer choose?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Train and deploy a Random Cut Forest (RCF) model on SageMaker for anomaly detection.",
      "B": "Develop and train a custom LSTM-based anomaly detector on SageMaker using TensorFlow.",
      "C": "Use Amazon Lookout for Metrics to automatically detect anomalies in the time-series data.",
      "D": "Use the anomaly-detection recipe in SageMaker Studio JumpStart and deploy the generated pipeline."
    },
    "explanation": "Amazon Lookout for Metrics is a fully managed anomaly-detection service that automatically tracks time-series metrics with minimal configuration. Training an RCF or LSTM on SageMaker requires substantial ML development and ongoing maintenance. JumpStart recipes simplify development but still require managing model training and pipelines."
  },
  {
    "taskStatement": "2.2",
    "stem": "An ML engineer previously ran a hyperparameter tuning job on a deep learning model and wants to optimize a new tuning job by reusing the results of that earlier job. Additionally, to reduce overall training cost and time, the engineer needs to automatically stop underperforming training jobs during tuning. Which configuration will meet these requirements?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run a new hyperparameter tuning job using BayesianOptimization search, with MaxNumberOfTrainingJobs set to 100, and leave EarlyStoppingType unset.",
      "B": "Run a warm start tuning job with type TRANSFER_LEARNING and disable early stopping to leverage prior results only.",
      "C": "Run a warm start hyperparameter tuning job with type IDENTICAL_DATA_AND_ALGORITHM and set EarlyStoppingType to Auto.",
      "D": "Run a random search tuning job with MaxNumberOfTrainingJobs set to 50 and enable EarlyStoppingType to OfflineStopping."
    },
    "explanation": "A warm start of type IDENTICAL_DATA_AND_ALGORITHM reuses prior tuning results for the same algorithm and search space, and setting EarlyStoppingType to Auto applies SageMaker\u2019s median stopping rule to halt underperforming jobs early."
  },
  {
    "taskStatement": "2.2",
    "stem": "A data scientist has fine-tuned a 2 GB PyTorch BERT model for text classification on a GPU instance. The model must now be deployed to a CPU-based inference endpoint with a memory footprint below 500 MB while maintaining at least 95% of its original accuracy. Which approach best meets these requirements with the least development effort?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Export the model to ONNX, write custom quantization code to convert weights to int8, and then deploy the ONNX model on the CPU endpoint.",
      "B": "Compile and optimize the trained PyTorch model using Amazon SageMaker Neo to perform graph optimizations and int8 quantization, then deploy the Neo-compiled model to the CPU endpoint.",
      "C": "Rewrite the model architecture to reduce hidden layer sizes by 50%, retrain from scratch on CPU, and deploy the smaller model.",
      "D": "Use AWS Lambda with a custom container to dynamically load and prune model weights at inference time to reduce memory usage."
    },
    "explanation": "SageMaker Neo automates model graph optimization and precision quantization (e.g., int8), reducing size and improving CPU performance with minimal code changes while preserving accuracy."
  },
  {
    "taskStatement": "2.2",
    "stem": "A research team has trained a custom XGBoost model locally and now wants to integrate it into Amazon SageMaker for managed hosting, hyperparameter tuning, and CI/CD pipelines without rewriting the training code. What is the most appropriate way to import and serve this externally trained model in SageMaker?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Package the model artifact into a model.tar.gz, build a custom inference Docker container (BYOC) that loads the artifact, push it to Amazon ECR, and create a SageMaker Model referencing that container and S3 artifact.",
      "B": "Translate the local training code into a SageMaker Script Mode training script, run it in a built-in XGBoost container, and register the new model.",
      "C": "Use the SageMaker SDK to call CreateTrainingJob with LocalMode enabled to import and train the model artifact directly.",
      "D": "Upload the model artifact to Amazon SageMaker Model Registry without a container and deploy it to a serverless endpoint."
    },
    "explanation": "Bringing an externally trained model into SageMaker requires a Bring-Your-Own-Container (BYOC) that contains the inference logic; SageMaker Model Registry alone cannot host artifacts without a container."
  },
  {
    "taskStatement": "2.3",
    "stem": "A financial services company is building a fraud detection model. The base fraud rate in production is 1%. On a validation set of 10,000 transactions (100 frauds, 9,900 non-frauds), two candidate models yield the following metrics:\n\nModel A: precision = 0.667, recall = 0.8\nModel B: precision = 0.8, recall = 0.6\n\nThe business cost of a false positive (FP) is $1 and the cost of a false negative (FN) is $10. Which model has the lower expected cost per 10,000 transactions, and which should the company choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Choose Model A: expected cost = (FP=40\u00d7$1)+(FN=20\u00d7$10) = $240",
      "B": "Choose Model B: expected cost = (FP=15\u00d7$1)+(FN=40\u00d7$10) = $415",
      "C": "Both models have the same cost",
      "D": "Cannot decide without additional metrics"
    },
    "explanation": "Compute TP, FP, FN for each: Model A: TP=0.8\u00d7100=80, FP=80\u00d7(1/0.667\u22121)=40, FN=20 \u2192 cost=40+200=240. Model B: TP=60, FP=60\u00d7(1/0.8\u22121)=15, FN=40 \u2192 cost=15+400=415. Model A yields lower cost."
  },
  {
    "taskStatement": "2.3",
    "stem": "An ML engineer needs to detect if a production classification model begins to rely on different features over time due to data drift. Which SageMaker Clarify monitor class should the engineer configure to track changes in feature contributions (for example, SHAP values) between a baseline and production data?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ModelBiasMonitor",
      "B": "ModelExplainabilityMonitor",
      "C": "ModelQualityMonitor",
      "D": "DataQualityMonitor"
    },
    "explanation": "ModelExplainabilityMonitor tracks explainability metrics such as SHAP feature attributions over time. The other monitors handle bias metrics (ModelBiasMonitor), overall prediction quality (ModelQualityMonitor), or input data distributions (DataQualityMonitor)."
  },
  {
    "taskStatement": "2.3",
    "stem": "A deep neural network trained on SageMaker displays stagnating training loss and high validation loss. The engineer enabled SageMaker Debugger\u2019s default rules and then enabled the vanishing gradient rule, which flagged anomalies in the earliest layers. Which action best addresses the vanishing gradient problem in these layers?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase the global learning rate to accelerate gradient propagation",
      "B": "Replace sigmoid/tanh activations with ReLU (and use He initialization)",
      "C": "Add L2 weight regularization to penalize large weights",
      "D": "Reduce batch size to increase gradient noise and variability"
    },
    "explanation": "Vanishing gradients in early layers are mitigated by using activation functions with constant gradients (ReLU) and appropriate weight initializations (He). Other options (higher learning rate, L2 penalty, smaller batches) do not directly solve vanishing gradient issues."
  },
  {
    "taskStatement": "3.1",
    "stem": "A fintech firm has developed a credit-scoring ML model (<50 MB) that must serve real-time API requests with no more than 20 ms 95th-percentile latency. Traffic patterns are unpredictable, ranging from 5 to 500 requests per minute. The firm wants to minimize infrastructure management overhead and pay only for the compute capacity it uses, while ensuring consistent low-latency performance. Which SageMaker deployment infrastructure should the ML engineer select?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy the model to a SageMaker serverless endpoint with default concurrency limits.",
      "B": "Deploy the model to a multi-model real-time SageMaker endpoint on a single ml.m5.large instance with auto scaling.",
      "C": "Deploy the model to a SageMaker real-time endpoint on ml.c5.large instances with a provisioned concurrency configuration and target-tracking auto scaling.",
      "D": "Deploy the model to a SageMaker asynchronous inference endpoint with default VCPU provisioning."
    },
    "explanation": "A serverless endpoint can cold-start and breach latency requirements. A single-instance multi-model endpoint cannot guarantee sub-20 ms under unpredictable load. Asynchronous inference is batch-oriented. A real-time endpoint with provisioned concurrency keeps containers warm and uses target-tracking scaling to elastically adjust capacity while maintaining low latency."
  },
  {
    "taskStatement": "3.1",
    "stem": "An industrial IoT company needs to deploy a vision ML model to 1 000 ARM-based edge cameras with intermittent connectivity. The model must run local inference with latency <50 ms and accept periodic updates without manual intervention. Which deployment infrastructure should the ML engineer choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Containerize the model and deploy it to AWS Lambda functions on AWS Greengrass Core.",
      "B": "Use SageMaker Edge Manager to package and deploy the model directly to AWS IoT devices.",
      "C": "Compile and optimize the model with SageMaker Neo for the target ARM architecture and deploy it to devices using AWS IoT Greengrass.",
      "D": "Deploy the model to a SageMaker real-time endpoint and configure the devices to call the endpoint when connected."
    },
    "explanation": "Lambda on Greengrass incurs container startup and may not meet <50 ms. Edge Manager provides monitoring but still requires a runtime-optimized model. A real-time endpoint requires connectivity. SageMaker Neo compiles and optimizes the model for ARM, and AWS IoT Greengrass handles offline deployment and periodic updates with minimal ops overhead."
  },
  {
    "taskStatement": "3.1",
    "stem": "A retail company requires an automated, end-to-end CI/CD pipeline for their ML workflow: data preprocessing, training with hyperparameter tuning, model registration and approval, and blue/green deployment to production endpoints. The pipeline must integrate with a Git repository for version control, provide step-level observability, and support easy rollback. Which orchestrator should the ML engineer select to meet these requirements with minimal custom infrastructure?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS CodePipeline with custom AWS Lambda functions for each ML workflow step.",
      "B": "Amazon Managed Workflows for Apache Airflow (MWAA) with DAGs defining each stage.",
      "C": "AWS Step Functions orchestrating AWS Batch jobs for training and deploying Step Functions tasks.",
      "D": "Amazon SageMaker Pipelines with integrated steps for data processing, training, model registry, approval, and deployment."
    },
    "explanation": "CodePipeline requires significant custom Lambda code for ML steps. MWAA is general-purpose and needs custom operators. Step Functions plus Batch require building and managing additional compute and monitoring. SageMaker Pipelines natively integrates preprocessing, training, tuning, registry, approval, and deployment, supports Git integration, lineage, monitoring, and rollback with minimal custom infrastructure."
  },
  {
    "taskStatement": "3.2",
    "stem": "A machine learning team must deploy a SageMaker real-time multi-model endpoint in private subnets with no internet access. The models are packaged as custom Docker images stored in Amazon ECR. The team wants to automate provisioning of all networking, ECR, and SageMaker resources using infrastructure as code (IaC) with minimal operational overhead, and to enforce coding best practices and unit tests. Which approach best meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Write an AWS CloudFormation template that defines the VPC, private subnets, ECR repository, SageMaker model, endpoint configuration, and endpoint resources.",
      "B": "Develop an AWS CDK application (in a supported language) that defines the VPC with interface endpoints, ECR repository, SageMaker model, endpoint configuration, and auto scaling policies, and run unit tests against the CDK constructs.",
      "C": "Use an AWS SAM template to define a Lambda function that creates the VPC and provisions the SageMaker endpoint when invoked.",
      "D": "Author a Terraform module to provision the VPC, ECR repository, and SageMaker resources, and manage state in an S3 backend."
    },
    "explanation": "AWS CDK provides a programmable IaC framework with built-in support for unit testing of constructs, reduces YAML/JSON boilerplate compared to raw CloudFormation, and automates resource provisioning including VPC interface endpoints and SageMaker auto scaling policies with minimal operational overhead."
  },
  {
    "taskStatement": "3.2",
    "stem": "A company\u2019s SageMaker real-time endpoint experiences sudden traffic spikes, causing increased latency. The current target-tracking auto scaling policy uses CPUUtilization. The operations team wants to trigger scale-out more quickly on incoming inference requests while avoiding over-provisioning during lulls. Which metric should they use in the target-tracking policy?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CPUUtilization",
      "B": "ModelLatency",
      "C": "InvocationsPerInstance",
      "D": "MemoryUtilization"
    },
    "explanation": "InvocationsPerInstance measures the number of inference requests handled per instance, providing a direct signal of request load and enabling faster scaling on bursts, whereas CPUUtilization and MemoryUtilization lag and ModelLatency may not correlate directly with load volume."
  },
  {
    "taskStatement": "3.2",
    "stem": "An ML engineer needs an automated, end-to-end pipeline that builds custom inference container images on code changes, pushes them to ECR, and updates a SageMaker endpoint to use the new image\u2014all defined as code. Which design has the least operational overhead?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use EventBridge to trigger a CodeBuild project on Git push; CodeBuild builds and pushes the image to ECR; a CloudFormation stack update is triggered manually to point the endpoint to the new image.",
      "B": "Define a CodePipeline pipeline (in AWS CDK) that uses CodeCommit, a CodeBuild action to build/push the image, and a CloudFormation action to update the SageMaker endpoint; deploy the pipeline via the CDK app.",
      "C": "Install AWS CLI scripts on an EC2 instance to poll CodeCommit for changes, build and push the container, then run AWS CLI to update the endpoint.",
      "D": "Use CodePipeline with CodeCommit and a Lambda step to build the container image, then invoke an AWS SAM deployment to update the endpoint."
    },
    "explanation": "A CDK-defined CodePipeline with CodeCommit, CodeBuild, and CloudFormation actions provides a fully managed, declarative CI/CD pipeline with minimal custom code, built-in integration, and automated endpoint updates, reducing operational overhead compared to scripting or Lambda-based workarounds."
  },
  {
    "taskStatement": "3.3",
    "stem": "An ML engineer is tasked with building a fully automated CI/CD pipeline for an Amazon SageMaker\u2013based model. The pipeline must: 1) pull code and configuration from a Git repository, 2) run unit tests and data validation, 3) execute data transformation in a SageMaker Processing job, 4) train the model, 5) register the trained model in the SageMaker Model Registry, and 6) perform a canary deployment to production with traffic shifting. The solution must minimize custom Lambda code and leverage native AWS services. Which pipeline architecture best meets these requirements?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS CodePipeline with stages: Source (GitHub), CodeBuild (run tests and data validation), CloudFormation (provision Processing and Training jobs and register model), Manual approval, CloudFormation (update endpoint).",
      "B": "Use AWS CodePipeline with stages: Source (GitHub), CodeBuild (run tests, invoke direct SDK calls to Processing and Training, register model), Manual approval, CodeBuild (use AWS CLI to update endpoint).",
      "C": "Implement a SageMaker Pipeline that includes Processing, Training, Evaluation, and Model Registry steps, and trigger it via EventBridge on git push. Use SageMaker Pipeline\u2019s built-in endpoint update step with canary configuration.",
      "D": "Use AWS CodePipeline with stages: Source (GitHub), CodeBuild (invoke a SageMaker Pipeline via AWS CLI to run Processing, Training, Evaluation, and register the model), Approval, CodeDeploy (configured for SageMaker endpoint canary deployment and traffic shifting)."
    },
    "explanation": "Option D minimizes custom code by delegating ML workflow orchestration to SageMaker Pipelines and uses native CodePipeline stages and CodeDeploy for canary traffic shifting. It cleanly separates CI (CodeBuild) from CD (CodeDeploy) without custom Lambdas."
  },
  {
    "taskStatement": "3.3",
    "stem": "A new CodePipeline contains a CodeBuild action that uses the AWS CLI to start SageMaker Training and Model Deployment jobs. The CodeBuild project\u2019s IAM role has permissions for sagemaker:CreateTrainingJob, sagemaker:CreateModel, and sagemaker:CreateEndpoint, but the build fails with an AccessDenied error stating that the role cannot be passed. What is the LEAST-privilege IAM change required to allow the CodeBuild stage to succeed?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Add iam:PassRole permission for the SageMaker execution role to the CodeBuild project's IAM role policy.",
      "B": "Update the SageMaker execution role trust policy to allow sts:AssumeRole from codepipeline.amazonaws.com.",
      "C": "Add iam:PassRole permission for the CodeBuild project role to the CodePipeline service role policy.",
      "D": "Add sts:AssumeRole permission for the SageMaker execution role in the CodeBuild project's trust policy."
    },
    "explanation": "When CodeBuild calls SageMaker, it must pass the SageMaker service execution role. Granting iam:PassRole on that role in the CodeBuild project's IAM role policy satisfies the least-privilege requirement."
  },
  {
    "taskStatement": "3.3",
    "stem": "A financial services team uses SageMaker Model Monitor to emit CloudWatch metrics when data drift exceeds thresholds. They need to automatically retrain the model end-to-end (processing, training, evaluation, registry) and roll out the new model with a linear traffic shift as soon as drift is detected. Which combination of AWS services and configurations will satisfy these requirements with minimal custom code?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create a CloudWatch alarm on the Model Monitor drift metric and an EventBridge rule that triggers an AWS CodePipeline pipeline. In CodePipeline, invoke a SageMaker Pipeline (processing, training, evaluation, model registration) and add a CodeDeploy stage configured for SageMaker endpoint linear traffic shifting.",
      "B": "Subscribe an SNS topic to the Model Monitor violation notification and use an AWS Lambda function to start a SageMaker Pipeline and update the endpoint via SDK with gradual traffic shifting.",
      "C": "Configure Model Monitor to directly invoke a SageMaker Pipeline on drift and include a built-in traffic shifting step in the SageMaker Pipeline definition.",
      "D": "Schedule a daily CodeBuild job to query Model Monitor metrics, and if drift is detected, run a training job and invoke a CloudFormation change set to update the endpoint."
    },
    "explanation": "Option A uses native integration: CloudWatch\u2192EventBridge to trigger CodePipeline, SageMaker Pipelines for retraining, and CodeDeploy for linear traffic shifting. This minimizes custom code and leverages managed services end-to-end."
  },
  {
    "taskStatement": "4.1",
    "stem": "An e-commerce company deployed a recommendation model on SageMaker and needs to monitor both input data drift and model bias in production. They have a baseline training dataset for drift detection and fairness metrics for bias detection. To minimize operational overhead and leverage fully managed monitoring, which solution meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a single SageMaker Model Monitor job with default settings to detect both drift and bias; schedule hourly.",
      "B": "Configure SageMaker Model Monitor (DefaultModelMonitor) to run a data quality monitoring job with baseline statistics and constraints for input features, and SageMaker Clarify ModelBiasMonitor to run a bias monitoring job with baseline fairness metrics; configure both on a daily schedule.",
      "C": "Use SageMaker Clarify's ModelExplainabilityMonitor to detect both data drift and bias by specifying a SHAP baseline, and schedule it hourly.",
      "D": "Use AWS Lambda triggered every hour to run custom Python scripts that compute drift metrics and bias metrics against baselines; send alerts via Amazon SNS."
    },
    "explanation": "SageMaker Model Monitor (DefaultModelMonitor) is optimized for data drift and quality monitoring using baseline statistics and constraints. SageMaker Clarify\u2019s ModelBiasMonitor is designed to monitor bias against fairness baselines. Combining these two fully managed monitors meets both drift and bias requirements with minimal custom code and operational overhead. Options A and C misuse or over-simplify the services, and D introduces unnecessary custom code."
  },
  {
    "taskStatement": "4.1",
    "stem": "A SaaS provider hosts a real-time image classification endpoint on Amazon SageMaker. They have observed intermittent spikes in 5XX invocation errors and increased inference latency impacting user experience. The operations team needs a solution with the least operational overhead to automatically detect and alert on these issues in near real time. Which approach should the team implement?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Enable Amazon CloudWatch Logs for the endpoint; configure a metric filter to count '5XX' errors; create CloudWatch alarms for '5XX' error count and for 'ModelLatency' metric.",
      "B": "Configure SageMaker Model Monitor to capture inference requests and responses; schedule a data quality monitoring job every 5 minutes with a custom script to check for errors and latency; publish custom metrics to CloudWatch.",
      "C": "Configure DataCaptureConfig on the endpoint to capture all invocations to S3; create an AWS Lambda function triggered by S3 events that calculates error rates and latency; publish custom CloudWatch metrics and alarms.",
      "D": "Use SageMaker Clarify ModelExplainabilityMonitor to detect anomalies in output embeddings, which will indirectly detect errors and latency issues; schedule it on an hourly basis."
    },
    "explanation": "SageMaker endpoints emit built-in CloudWatch metrics such as Invocation5XXErrors and ModelLatency. Creating CloudWatch alarms on these metrics provides near real-time monitoring with zero custom code and minimal overhead. Model Monitor (B) and Lambda/S3 solutions (C) introduce unnecessary complexity, and Clarify (D) is not designed for error or latency monitoring."
  },
  {
    "taskStatement": "4.1",
    "stem": "A financial analytics team deployed a credit-scoring model on SageMaker real-time endpoints. They want to detect univariate feature drift in critical numeric input features (such as \u2018annual_income\u2019 and \u2018debt_ratio\u2019) and alert if the distribution has shifted beyond acceptable thresholds. They already have training data to serve as a baseline. Which SageMaker feature and configuration should the team use to meet this requirement with minimal custom coding?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure SageMaker Clarify ModelBiasMonitor with the training dataset as a baseline and set the 'drift_detection' parameter for numeric features; schedule hourly.",
      "B": "Use SageMaker Model Monitor's DefaultModelMonitor to create a MonitorSchedule with a DataQualityJobDefinition that specifies baseline_statistics and baseline_constraints created from the training dataset; set the monitoring schedule to run hourly.",
      "C": "Deploy a Python-based Lambda function that loads the training dataset baseline, fetches the latest batch of inference requests, computes Kolmogorov\u2013Smirnov tests for each numeric feature, and sends alerts via Amazon SNS.",
      "D": "Use Amazon CloudWatch Metrics Insight to query the 'InputDataMean' metric for each feature and configure alarms when mean deviates by more than a threshold from the training data mean."
    },
    "explanation": "SageMaker Model Monitor\u2019s DefaultModelMonitor supports univariate feature drift detection by automatically generating baseline_statistics and baseline_constraints from training data, and scheduling the monitoring job. This fully managed solution minimizes custom coding. Clarify\u2019s ModelBiasMonitor (A) is for fairness, not general drift, and options C and D require significant custom development or rely on unsupported metric calculations."
  },
  {
    "taskStatement": "4.2",
    "stem": "A company hosts a SageMaker real-time inference endpoint that experiences predictable daily traffic spikes at 9 AM and 6 PM, plus unpredictable fluctuations throughout the day. The ML engineer needs to minimize costs during off-peak hours while ensuring the endpoint maintains a p95 latency below 100 ms. The solution must use built-in AWS services and require minimal custom code. Which configuration should the engineer implement?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure a step scaling policy in Amazon CloudWatch that scales out when CPU Utilization > 70% and scales in when CPU Utilization < 30%; set MinCapacity=1 and MaxCapacity=10.",
      "B": "Deploy an AWS Lambda function triggered by Amazon EventBridge schedule rules at 8:50 AM and 5:50 PM to call UpdateEndpointWeightsAndCapacities, and use a target tracking policy for unpredictable loads.",
      "C": "Define an Application Auto Scaling target tracking policy on the SageMaker endpoint based on p95 latency with MinCapacity=1, MaxCapacity=10, and add a scheduled scaling action to increase capacity to 8 at 8:50 AM and 5:50 PM.",
      "D": "Use Spot Instances by converting the endpoint to asynchronous inference, configure Spot for inference to de-provision overnight, and rely on target tracking for bursts."
    },
    "explanation": "Option C leverages built-in Application Auto Scaling for SageMaker to handle both scheduled and dynamic scaling in a single service with minimal custom code. Step and Lambda solutions are more complex and less precise; asynchronous/Spot inference cannot guarantee real-time p95 latency."
  },
  {
    "taskStatement": "4.2",
    "stem": "An ML engineer needs to optimize cost and performance of multiple SageMaker real-time inference endpoints. Each endpoint has different traffic patterns and latency requirements. The engineer wants to identify the most cost-effective instance type for each endpoint, based on sample inference payloads, throughput, and latency SLOs. Which tool or workflow should the engineer use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Compute Optimizer to generate EC2 instance recommendations and manually map those to SageMaker endpoints.",
      "B": "Use SageMaker Inference Recommender to run profiling jobs with sample payloads and choose the instance types with the lowest cost per inference that meet the SLOs.",
      "C": "Use AWS Trusted Advisor to forecast inference endpoint costs and recommend Reserved Instances.",
      "D": "Use AWS Cost Explorer\u2019s RI purchase recommendations to apply savings to SageMaker endpoint instance types."
    },
    "explanation": "SageMaker Inference Recommender is designed to profile real-world inference workloads and recommend the optimal instance types for cost and performance. Compute Optimizer and Trusted Advisor do not profile SageMaker workloads at the application level."
  },
  {
    "taskStatement": "4.2",
    "stem": "A global team uses SageMaker across 10 AWS accounts under consolidated billing. They must enforce that every SageMaker training job, endpoint, and pipeline carries the tags Project and CostCenter, automatically deny creations without them, and generate a centralized monthly cost dashboard per project. Which solution meets these requirements with the least operational overhead?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy AWS Config rules in each account to audit SageMaker resource tags, use AWS Service Catalog with TagOptions to solicit the tags, and build QuickSight dashboards from Cost Explorer filtered by tags.",
      "B": "Implement AWS Organizations Tag Policies to require Project and CostCenter tags on SageMaker resources, enable AWS Config for compliance evaluation, and use Cost Explorer/AWS Budgets with tag filters for centralized dashboards.",
      "C": "Create Service Control Policies to forbid creation of untagged SageMaker resources, use CloudTrail + Lambda to remediate missing tags, and aggregate costs via Cost Explorer.",
      "D": "Enforce all SageMaker deployments through AWS CloudFormation StackSets with mandatory tags in the templates, and view costs in the AWS Billing console."
    },
    "explanation": "AWS Organizations Tag Policies provide native, centralized enforcement of required tags across accounts with minimal operational effort. Combined with AWS Config compliance checks and Cost Explorer/AWS Budgets, this delivers automated enforcement and centralized cost reporting. Other options require custom code or more operational overhead."
  },
  {
    "taskStatement": "4.3",
    "stem": "An enterprise mandates that all SageMaker training and inference workloads must run within a secured VPC without any public internet access. However, these workloads need to read training data from Amazon S3, pull container images from Amazon ECR, write logs to Amazon CloudWatch Logs, and use a customer-managed AWS KMS key. The security team requires the solution to minimize ongoing maintenance overhead while enforcing least-privilege network connectivity. Which architectural configuration meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable SageMaker network isolation and launch jobs outside any VPC; rely on network isolation to prevent internet access.",
      "B": "Launch SageMaker jobs inside a VPC with no internet gateway, configure interface VPC endpoints for S3, ECR (both API and Docker), CloudWatch Logs, and KMS, and remove any NAT gateways.",
      "C": "Launch SageMaker jobs in the default VPC and attach a security group that blocks 0.0.0.0/0; use public endpoints for AWS services.",
      "D": "Use a network ACL on the private subnet to deny all outbound traffic; allow S3 and ECR access by whitelisting their public IP ranges."
    },
    "explanation": "Enabling interface VPC endpoints for S3, ECR, CloudWatch Logs, and KMS in a VPC without an Internet Gateway ensures workloads have private, least-privilege access to required AWS services and keys, while preventing any public internet access. Network isolation alone (A) doesn\u2019t provide VPC-private access to AWS services. Security groups cannot filter based on AWS service endpoints (C). NACLs cannot reliably allow AWS service traffic by IP (D)."
  },
  {
    "taskStatement": "4.3",
    "stem": "A company\u2019s sensitive training data is stored in an S3 bucket encrypted with a customer-managed KMS key (DataKey). They must run SageMaker training jobs that decrypt the data and write model artifacts to a separate S3 bucket encrypted with another KMS key (OutputKey). The SageMaker execution role must follow the principle of least privilege. Which combination of IAM role policy statements and KMS key policy statements satisfies this requirement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "IAM role: Grant s3:GetObject and s3:PutObject on both buckets and kms:Decrypt, kms:Encrypt on both keys. KMS key policies: Trust the entire account principal for all operations.",
      "B": "IAM role: Grant s3:GetObject on the data bucket; s3:PutObject on the artifact bucket; kms:Decrypt and kms:GenerateDataKey on DataKey; kms:Encrypt and kms:GenerateDataKey on OutputKey. KMS key policies: Specify the SageMaker execution role as the only principal allowed to use each key.",
      "C": "IAM role: Grant s3:* on both buckets and kms:* on both keys. KMS key policies: No changes needed (account admins manage keys).",
      "D": "Use S3 bucket policies to grant SageMaker service principal full access, and KMS grants to allow any IAM principal in account to decrypt keys."
    },
    "explanation": "Least-privilege requires the IAM role only have s3:GetObject on the data bucket, s3:PutObject on the output bucket, and only the specific KMS operations. The key policies must explicitly allow the SageMaker execution role to use each key. Over-permissive (A, C) or relying on bucket policies alone (D) violates least-privilege."
  },
  {
    "taskStatement": "4.3",
    "stem": "A security team manages multiple AWS accounts using AWS Organizations. They want to ensure that data scientists in member accounts can only create, update, or delete SageMaker endpoints within their own accounts, and prevent any SageMaker endpoint operations in accounts outside their OU. Which mechanism should be implemented at the organization level to enforce this restriction?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use an AWS Organizations service control policy (SCP) at the OU level to Deny all sagemaker:CreateEndpoint, UpdateEndpoint, and DeleteEndpoint actions when the resource\u2019s account ID does not match the requesting account.",
      "B": "Apply IAM permission boundaries to the data scientists\u2019 roles in each member account that restrict sagemaker:* to specific endpoint ARNs.",
      "C": "Attach a resource-based policy to each SageMaker endpoint specifying allowed IAM principals from the same account.",
      "D": "Use the SageMaker service-linked role policy to restrict endpoint operations to certain accounts."
    },
    "explanation": "An AWS Organizations SCP can centrally deny SageMaker endpoint management actions across accounts outside the OU, enforcing the requirement. IAM permission boundaries (B) must be set per role and cannot prevent actions outside the OU. SageMaker endpoints do not support resource-based policies for management operations (C). Service-linked roles cannot enforce cross-account restrictions (D)."
  },
  {
    "taskStatement": "1.1",
    "stem": "A fintech company needs to ingest 1 million financial transaction events per second into Amazon SageMaker Feature Store for real-time fraud detection. The solution must support low latency, horizontal scalability, and minimal data loss. Which ingestion architecture meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon Kinesis Data Firehose to deliver events to S3 and batch import into SageMaker Feature Store Offline Store.",
      "B": "Use Amazon Kinesis Data Streams with enhanced fan-out consumers and AWS Lambda functions invoking the PutRecord API to ingest into the SageMaker Feature Store Online Store.",
      "C": "Use Amazon MSK with brokers backed by EBS volumes and use a Kafka consumer to push records into the Feature Store offline bucket.",
      "D": "Use AWS IoT Core to route transaction events to SageMaker Feature Store via a custom AWS Lambda integration."
    },
    "explanation": "Real-time, low-latency ingest into the Feature Store Online Store requires Kinesis Data Streams (for horizontal scaling and enhanced fan-out) combined with Lambda invoking the PutRecord API. Firehose and batch imports do not meet real-time SLAs; MSK and IoT Core approaches add unnecessary complexity and latency."
  },
  {
    "taskStatement": "1.1",
    "stem": "An analytics team has 200 GiB of nested JSON log files in Amazon S3. They require interactive profiling and transformation in SageMaker Data Wrangler with minimal cost and latency. Which file format should they convert the data into before ingestion?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CSV files (flattening nested structures into columns).",
      "B": "Apache Parquet with nested column support and predicate pushdown.",
      "C": "Avro files with JSON serialization.",
      "D": "RecordIO binary format."
    },
    "explanation": "Parquet is a columnar storage format with efficient compression, nested column support, and predicate pushdown, which significantly reduces I/O and speeds interactive profiling in Data Wrangler. CSV lacks nested support, Avro is row-oriented, and RecordIO is optimized for deep learning, not interactive ETL."
  },
  {
    "taskStatement": "1.1",
    "stem": "An ML engineer must merge hourly transaction updates from Amazon RDS and real-time clickstream events from DynamoDB Streams into a single dataset for training in SageMaker Data Wrangler, while automatically handling schema evolution in both sources. Which ingestion solution should the engineer choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Schedule an AWS Glue ETL job to export RDS to S3 and join with Lambda-pushed DynamoDB data once per hour.",
      "B": "Use SageMaker Data Wrangler\u2019s built-in connectors for Amazon RDS and for DynamoDB streams to import and join data in a single flow.",
      "C": "Provision an EMR Spark cluster to read from both sources nightly and write merged Parquet to S3.",
      "D": "Use AWS DMS to replicate both RDS and DynamoDB to Redshift and join via Redshift Spectrum in Data Wrangler."
    },
    "explanation": "Data Wrangler connectors natively support incremental read and schema inference for RDS and DynamoDB streams, simplifying merges and automatically handling schema changes. Glue jobs or EMR require more operational overhead, and DMS replication to Redshift adds extra cost and latency."
  },
  {
    "taskStatement": "1.1",
    "stem": "A genomics research team requires sub-millisecond file I/O and high throughput for a 500 GiB NFS-based dataset during preprocessing before model training. Which AWS storage service should they provision to meet performance and POSIX compatibility requirements?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "Amazon EFS One Zone SSD backed by NFS.",
      "B": "Amazon FSx for Lustre file system with link to S3.",
      "C": "Amazon FSx for NetApp ONTAP with SSD volumes.",
      "D": "Amazon S3 bucket mounted via VPC endpoint."
    },
    "explanation": "FSx for Lustre delivers the lowest POSIX read-latencies (sub-millisecond) and highest throughput for large NFS-based datasets. EFS has higher latencies; FSx ONTAP adds data management features but lower throughput; S3 is object storage, not POSIX."
  },
  {
    "taskStatement": "1.1",
    "stem": "An e-commerce company experiences high startup latency for SageMaker training jobs reading hundreds of small CSV files (~5\u201310 MiB each) from S3. Without modifying the training code, which solution will most reduce startup time?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable Amazon S3 Transfer Acceleration on the bucket.",
      "B": "Use AWS Glue to consolidate small CSV files into larger Parquet files and update the S3 prefix.",
      "C": "Upgrade to a SageMaker instance with higher network throughput.",
      "D": "Change the training input mode to Pipe mode."
    },
    "explanation": "Consolidating small files into fewer, larger Parquet files minimizes S3 list and open calls, dramatically reducing startup latency without code changes. Transfer Acceleration and network upgrades do not address metadata overhead; Pipe mode still requires metadata enumeration."
  },
  {
    "taskStatement": "1.1",
    "stem": "A media streaming platform requires exactly-once processing of real-time events into its ML ingestion pipeline for feature computation. Which combination of services and configurations provides this guarantee?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Kinesis Data Firehose with a retry buffer.",
      "B": "Amazon Kinesis Data Streams with Enhanced Fan-Out (guarantees at-least-once).",
      "C": "Amazon MSK (Kafka) configured with idempotent producers and transactional writes.",
      "D": "DynamoDB Streams with AWS Lambda and conditional writes to downstream store."
    },
    "explanation": "Kafka with idempotent producers and transactional writes supports exactly-once semantics end-to-end. Kinesis Data Streams and Firehose only guarantee at-least-once; DynamoDB Streams plus Lambda may still result in duplicates without complex idempotency logic."
  },
  {
    "taskStatement": "1.1",
    "stem": "An ML pipeline ingests unvalidated CSV files into Amazon S3, but inconsistent schemas and missing columns cause downstream failures. Which AWS service can enforce schema validation during ingestion and alert on violations?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Glue Schema Registry with schema-validation-enabled data producers.",
      "B": "AWS Config rules for S3 object schemas.",
      "C": "Amazon Macie classification jobs.",
      "D": "AWS Lake Formation schema enforcement."
    },
    "explanation": "AWS Glue Schema Registry can define, register and enforce schemas at ingestion for streaming and batch, and can reject or alert on malformed records. Lake Formation handles data access control, not row-level schema validation."
  },
  {
    "taskStatement": "1.1",
    "stem": "A large oil company must migrate 100 TiB of POSIX file-system data to Amazon S3 within 48 hours, with incremental file changes tracked and minimal management overhead. Which service meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Snowball Edge device shipment.",
      "B": "AWS DataSync configured for incremental sync to S3.",
      "C": "Amazon S3 Transfer Acceleration over the internet.",
      "D": "AWS Storage Gateway file gateway."
    },
    "explanation": "AWS DataSync optimizes incremental transfer of POSIX files to S3 with built-in scheduling and encryption, suitable for large, continuously changing data. Snowball has shipment delays; Transfer Acceleration and Storage Gateway don\u2019t handle incremental POSIX sync as efficiently."
  },
  {
    "taskStatement": "1.1",
    "stem": "An ML engineer wants to profile, clean, and transform streaming IoT telemetry data directly in SageMaker Data Wrangler without intermediate storage. Which ingestion method should the engineer use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use the Amazon Kinesis Data Streams connector in Data Wrangler to ingest data in real time.",
      "B": "Upload data to S3 and use the S3 connector for batch profiling.",
      "C": "Route data through AWS IoT Analytics to an S3 channel and ingest.",
      "D": "Crawl raw data with AWS Glue and import the Glue table."
    },
    "explanation": "The Kinesis Data Streams connector in Data Wrangler enables real-time profiling and transformation of streaming data. Other options introduce batch windows or extra services, increasing latency and operational overhead."
  },
  {
    "taskStatement": "1.1",
    "stem": "When mounting an Amazon EFS file system to a SageMaker Studio notebook in a VPC, which configuration step ensures secure, low-latency access?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Mount the EFS file system over the public internet using TLS.",
      "B": "Create EFS mount targets in the same VPC subnets as the SageMaker notebooks and attach security groups allowing NFS traffic (TCP/2049).",
      "C": "Use AWS Direct Connect to mount the EFS file system from on-premises.",
      "D": "Peer the notebook VPC to another VPC containing EFS and mount through a transit gateway."
    },
    "explanation": "To achieve secure, low-latency NFS access from Studio in the same VPC, you must create EFS mount targets in the same subnets and open NFS ports in security groups. Public mounts or cross-VPC peering add latency and complexity."
  },
  {
    "taskStatement": "1.1",
    "stem": "A cross-account pipeline must ingest data from an S3 bucket in Account A into SageMaker Studio in Account B. To minimize privileges and management, which approach should be used?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a bucket ACL granting Account B the s3:GetObject permission.",
      "B": "Create an IAM role in Account B with a trust policy for Account A\u2019s principal, and update the bucket policy to allow s3:GetObject for that role.",
      "C": "Use an AWS Organizations SCP to allow cross-account access.",
      "D": "Set up VPC peering between the two accounts and access the bucket over the interface endpoint."
    },
    "explanation": "Creating a cross-account IAM role in Account B with trust from Account A and granting that role permission in the bucket policy provides least-privilege, auditable access. Bucket ACLs are legacy, and Organizations SCP/VPC peering do not directly enable S3 access control."
  },
  {
    "taskStatement": "1.1",
    "stem": "A data ingestion job reading from S3 into SageMaker notebooks is intermittently throttled with HTTP 503 SlowDown errors. Which approach will most effectively reduce these errors while maintaining high throughput?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Implement exponential backoff with jitter in the client\u2019s S3 request retry logic.",
      "B": "Enable S3 Transfer Acceleration to optimize network path.",
      "C": "Configure the bucket as requester-pays.",
      "D": "Increase the IAM request quota for the S3 service."
    },
    "explanation": "503 SlowDown errors indicate S3 throttling; the recommended mitigation is implementing exponential backoff with jitter to retry requests gracefully. Transfer Acceleration and requester-pays do not address throttling, and IAM quotas are unrelated."
  },
  {
    "taskStatement": "1.1",
    "stem": "An Amazon SageMaker training job mounts an Amazon FSx for Lustre file system backed by an 8 TiB S3 data repository. After heavy parallel reads, the engineer observes metadata lookup latencies >100 ms for POSIX operations. How can the engineer optimize metadata performance?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase the Lustre file system\u2019s throughput capacity to provision more metadata IOPS.",
      "B": "Switch the workload to Amazon EFS for metadata caching.",
      "C": "Directly mount the underlying S3 bucket instead of Lustre.",
      "D": "Implement a local cache on the training instance\u2019s EBS volume."
    },
    "explanation": "FSx for Lustre provides scalable metadata throughput proportionate to the configured throughput capacity. Increasing capacity raises available metadata IOPS and reduces latency. EFS and direct S3 mounts do not meet the sub-millisecond metadata requirements."
  },
  {
    "taskStatement": "1.1",
    "stem": "A data science team must join millions of small S3 objects with a 50 GiB Redshift table for feature engineering before training. The join must complete in the lowest end-to-end latency. Which ingestion and compute architecture should they choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run an AWS Glue ETL job to merge the data into a Parquet file and reload into SageMaker.",
      "B": "Use Redshift Spectrum to query the S3 objects in place and join directly with the local Redshift table.",
      "C": "Provision an EMR Spark cluster to read both sources and write joined output to S3.",
      "D": "Use Athena CTAS to precompute the join and then read from S3 in SageMaker."
    },
    "explanation": "Redshift Spectrum can directly query external S3 data and join with internal tables, minimizing data movement and achieving the lowest latency. Glue, EMR, and Athena introduce additional data shuffles or write phases, adding latency."
  },
  {
    "taskStatement": "1.1",
    "stem": "When performing offline ingestion into SageMaker Feature Store, which file format and partitioning scheme will maximize batch import throughput and query efficiency?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A single unpartitioned CSV file containing all records.",
      "B": "Many small gzipped JSON files partitioned by feature group name.",
      "C": "Parquet files partitioned by ingestion date (yyyy/mm/dd) and hour.",
      "D": "Avro files with no partitions."
    },
    "explanation": "Partitioning Parquet files by date and hour enables parallel reads by the batch import process and efficient predicate pushdown, maximizing throughput and query efficiency. Single large CSVs or unpartitioned formats become bottlenecks; Avro lacks columnar benefits."
  },
  {
    "taskStatement": "1.2",
    "stem": "You have a numeric feature in your dataset that is strongly right-skewed. You plan to preprocess the data in SageMaker Data Wrangler and then train a linear regression model. Which sequence of transformations in Data Wrangler will best reduce skewness and satisfy the assumptions of linear regression?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Standardize the feature first, then apply a log transform",
      "B": "Apply a log transform first, then standardize the resulting values",
      "C": "Min\u2013max scale the feature first, then apply a log transform",
      "D": "One-hot encode the feature, then normalize the encoded columns"
    },
    "explanation": "A log transform first reduces skew, and subsequent standardization yields zero mean and unit variance, aligning with linear regression assumptions."
  },
  {
    "taskStatement": "1.2",
    "stem": "Your streaming sensor pipeline ingests data via Kinesis Data Streams. You need to filter out invalid readings and impute missing values in real time with sub-second latency. Which solution provides the lowest operational overhead and meets performance requirements?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Kinesis Data Analytics SQL to filter and invoke Lambda for imputation",
      "B": "Use a Lambda function to filter and then call SageMaker Data Wrangler batch job for imputation",
      "C": "Use Kinesis Data Analytics for Apache Flink to filter and impute within the streaming application",
      "D": "Use AWS Glue streaming ETL job to filter and impute before writing to S3"
    },
    "explanation": "Kinesis Data Analytics for Apache Flink supports stateful, low-latency streaming transforms including filter and impute within the service, minimizing custom infrastructure."
  },
  {
    "taskStatement": "1.2",
    "stem": "A categorical feature has 5,000 unique values. You intend to use a tree-based algorithm and want to avoid excessive dimensionality. Which encoding technique should you apply using SageMaker Data Wrangler?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "One-hot encoding of all categories",
      "B": "Label encoding (assigning integer codes)",
      "C": "Frequency encoding (replace category with its occurrence frequency)",
      "D": "Hashing trick with a large hash space"
    },
    "explanation": "Frequency encoding reduces cardinality to a single numeric value per category, retaining information without high-dimensional expansion."
  },
  {
    "taskStatement": "1.2",
    "stem": "You need to remove duplicate records from two large S3 datasets in preparation for feature engineering. Which AWS Glue Spark ETL approach ensures schema enforcement and minimal data loss?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Glue DynamicFrame.drop_duplicates on the combined DynamicFrame",
      "B": "Convert to Spark DataFrame and call DataFrame.distinct()",
      "C": "Use a SageMaker Processing job with custom dedupe code",
      "D": "Use a Glue DataBrew recipe step to drop duplicates"
    },
    "explanation": "Glue DynamicFrame.drop_duplicates preserves schema and metadata, integrates with the Glue catalog, and scales automatically."
  },
  {
    "taskStatement": "1.2",
    "stem": "You need to detect and remove outliers beyond 3 standard deviations for a numeric column using AWS Glue DataBrew. Which recipe step accomplishes this with minimal effort?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Add a Filter rows step with condition > mean + 3*stddev",
      "B": "Use the built-in Remove outliers step and set the z-score threshold to 3",
      "C": "Add a Cluster rows step and drop the smallest cluster",
      "D": "Use Impute missing step to replace values beyond 3 standard deviations"
    },
    "explanation": "The Remove outliers step in DataBrew natively handles z-score thresholds, detecting and dropping extreme values automatically."
  },
  {
    "taskStatement": "1.2",
    "stem": "Your team needs to implement a custom feature transformation in a visual pipeline that includes Python code (e.g., complex binning logic). Which SageMaker tool supports this requirement and collaborative workflows?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Glue DataBrew",
      "B": "SageMaker Data Wrangler",
      "C": "AWS Glue Python shell job",
      "D": "Spark on Amazon EMR notebook"
    },
    "explanation": "Data Wrangler provides a visual flow with built-in support for custom Python steps, versioning, and collaboration in Studio."
  },
  {
    "taskStatement": "1.2",
    "stem": "You need to compute a 7-day rolling mean on a time-series feature for millions of records. Which approach in an AWS managed service offers the highest throughput with minimal code?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run a SageMaker Processing job with PySpark window functions",
      "B": "Write a Glue Spark ETL job using Spark SQL window functions",
      "C": "Use SageMaker Data Wrangler\u2019s rolling statistics transform",
      "D": "Use Lambda functions with batched DynamoDB lookups"
    },
    "explanation": "Data Wrangler\u2019s built-in rolling statistic transform executes efficiently in a managed environment without custom code or cluster management."
  },
  {
    "taskStatement": "1.2",
    "stem": "You have a free-text column that needs tokenization into word tokens and conversion into integer IDs for embedding. Which Data Wrangler transformation sequence should you apply?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use the Tokenize step (word level) and then Encode step (label encoding)",
      "B": "Use TextVectorization step with TF-IDF output",
      "C": "One-hot encode the text column directly",
      "D": "Write a custom PySpark transform in SageMaker Processing"
    },
    "explanation": "The Tokenize step splits text into words, and Label Encoding maps each token to a unique integer ID, preparing for embeddings."
  },
  {
    "taskStatement": "1.2",
    "stem": "Before training, you must enforce schema constraints (data types, value ranges) programmatically. Which AWS feature will you use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Glue Data Quality rules leveraging Deequ",
      "B": "SageMaker Data Wrangler profile job",
      "C": "DataBrew column profiling summary",
      "D": "Glue Data Catalog crawler validation"
    },
    "explanation": "Glue Data Quality uses Deequ to define and evaluate table constraints and thresholds in code, enforcing schema at scale."
  },
  {
    "taskStatement": "1.2",
    "stem": "You plan to store engineered features for real-time inference in SageMaker Feature Store. Which Data Wrangler export option supports batching data into a Feature Group?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a SageMaker Processing job to call PutRecord API",
      "B": "Use a Lambda function to batch and call PutRecordBatch",
      "C": "Export directly from Data Wrangler to SageMaker Feature Store",
      "D": "Upload CSV to S3 and configure offline store only"
    },
    "explanation": "Data Wrangler has a direct export feature to batch-write transformed data into a Feature Store Feature Group."
  },
  {
    "taskStatement": "1.2",
    "stem": "You need to reduce a numeric feature\u2019s cardinality to 100 bins based on equal-frequency intervals using a no-code solution. Which DataBrew step do you choose?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Group by step with custom quantile aggregation",
      "B": "Bin numeric values step with equal-frequency option",
      "C": "Cluster rows step with K-means clustering",
      "D": "Custom recipe with Python UDF"
    },
    "explanation": "The Bin numeric values step with equal-frequency creates quantile-based bins automatically, reducing cardinality."
  },
  {
    "taskStatement": "1.2",
    "stem": "To impute missing age values by country group using DataBrew, which step sequence is correct?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use Partition by country, then Impute missing step with median strategy",
      "B": "Aggregate to compute medians, then perform a join back",
      "C": "Apply global median imputation for the age column",
      "D": "Export data and impute in SageMaker Processing"
    },
    "explanation": "Partitioning by country followed by median imputation uses group-wise statistics directly in DataBrew with minimal extra steps."
  },
  {
    "taskStatement": "1.2",
    "stem": "Two 1 TB CSV files in S3 must be inner-joined on a composite key before feature engineering. You want a serverless, managed service that handles scaling and scripting. Which approach is best?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Run an AWS Glue ETL job using DynamicFrame.join",
      "B": "Launch an EMR cluster with custom PySpark code",
      "C": "Use a SageMaker Processing PySpark job",
      "D": "Perform a CTAS join in Amazon Athena"
    },
    "explanation": "AWS Glue ETL with DynamicFrame.join provides serverless scaling, schema enforcement, and integration with the Glue Data Catalog."
  },
  {
    "taskStatement": "1.2",
    "stem": "You need to anonymize email addresses in real time within a Kinesis Data Stream, ensuring <200 ms processing per record. Which architecture meets this requirement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Glue streaming ETL job with built-in masking",
      "B": "Kinesis Data Analytics for Apache Flink with a UDF to hash emails",
      "C": "AWS Lambda subscribed to the stream invoking a hashing library",
      "D": "SageMaker Data Wrangler in streaming mode"
    },
    "explanation": "Kinesis Data Analytics for Flink executes in-stream UDFs with low, consistent latency, meeting sub-200 ms requirements without cold starts."
  },
  {
    "taskStatement": "1.2",
    "stem": "Duplicate events may occur within a 2-minute window in your Kinesis stream. You need to drop duplicates in real time before storing to S3. Which solution is most appropriate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Kinesis Data Analytics for Flink, keyBy event ID and apply a 2-minute deduplication window",
      "B": "Configure an AWS Glue streaming job with dedupe enabled",
      "C": "Trigger a Lambda function per record and check DynamoDB for prior IDs",
      "D": "Run an EMR streaming job with custom dedupe code"
    },
    "explanation": "Kinesis Data Analytics for Flink supports stateful windowed deduplication on event keys with minimal infrastructure overhead."
  },
  {
    "taskStatement": "1.3",
    "stem": "An ML engineer needs to set up automated, scheduled data quality checks on an Amazon S3 dataset to validate completeness, uniqueness, and detect numeric outliers over time. Which solution meets these requirements with the LEAST operational overhead?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Glue DataBrew to author a recipe with transformations and schedule it as a DataBrew job.",
      "B": "Use AWS Glue Data Quality to define data quality rules and configure a schedule to run them.",
      "C": "Use SageMaker Clarify DataQualityCheckConfig in a SageMaker Processing job triggered by EventBridge.",
      "D": "Use Amazon Athena SQL queries inside a Lambda function scheduled by EventBridge."
    },
    "explanation": "AWS Glue Data Quality is designed for automated, rule-based validation and scheduling of data quality checks with minimal operational overhead."
  },
  {
    "taskStatement": "1.3",
    "stem": "A data scientist must detect pre-training label disparity between male and female groups in a binary classification dataset. Which SageMaker Clarify metric should be used?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "pre_training_bias:CI (class imbalance)",
      "B": "pre_training_bias:DPL (difference in proportions of labels)",
      "C": "post_training_bias:CI",
      "D": "post_training_bias:DPL"
    },
    "explanation": "Difference in proportions of labels (DPL) is the appropriate pre-training metric to measure label distribution disparity between protected groups."
  },
  {
    "taskStatement": "1.3",
    "stem": "A dataset for fraud detection has severe class imbalance (0.5% positives). The engineer wants synthetic minority oversampling before training. Which AWS service/plugin should be used to implement SMOTE?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Specify class_weight in the training script and rely on model loss adjustment.",
      "B": "Use AWS Glue DataBrew sampling transform to oversample the minority class.",
      "C": "Use a SageMaker Processing job that implements imbalanced-learn\u2019s SMOTE algorithm.",
      "D": "Use SageMaker Clarify to automatically generate synthetic samples."
    },
    "explanation": "A SageMaker Processing job allows you to run custom code (e.g., imbalanced-learn) to generate SMOTE synthetic samples before training."
  },
  {
    "taskStatement": "1.3",
    "stem": "A tabular dataset containing PII must be anonymized before model training. The engineer wants to mask names, emails, and SSNs in CSV files with minimal coding. Which solution should be implemented?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use AWS Glue DataBrew recipe steps with the built-in PII Mask transform on the relevant columns.",
      "B": "Write a Lambda function to scan each file in S3, mask PII, and write back to S3.",
      "C": "Use SageMaker Clarify\u2019s bias analysis and misapply it for PII detection.",
      "D": "Use AWS KMS custom encryption to encrypt only the PII columns at rest."
    },
    "explanation": "AWS Glue DataBrew provides built-in PII Mask transforms that can mask specified columns with minimal code."
  },
  {
    "taskStatement": "1.3",
    "stem": "An ML engineer uses an Amazon EFS file system to store training data. To meet compliance, data must be encrypted at rest and in transit between the SageMaker training instance and EFS. Which configuration meets these requirements?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable only SSE-KMS encryption on the EFS file system.",
      "B": "Configure the EFS mount with NFSv4 and no encryption.",
      "C": "Use Amazon FSx for Lustre with default settings.",
      "D": "Enable EFS encryption at rest (SSE-KMS) and mount using EFS mount options with encryption in transit (TLS)."
    },
    "explanation": "To meet both requirements, enable SSE-KMS on EFS and use the EFS mount option to enforce TLS encryption in transit."
  },
  {
    "taskStatement": "1.3",
    "stem": "A dataset contains multiple records per user_id. To prevent data leakage, how should an ML engineer split the dataset into train and test subsets?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Randomly split 80/20 at the record level.",
      "B": "Use stratified split to preserve label proportions.",
      "C": "Use a group-based split (e.g., scikit-learn\u2019s GroupShuffleSplit) with user_id as the group key.",
      "D": "Split based on time, taking the latest 20% of records as test."
    },
    "explanation": "A group-based split ensures that all records for a given user_id are either in train or test, preventing leakage."
  },
  {
    "taskStatement": "1.3",
    "stem": "A text dataset with PII must be anonymized before training. The engineer wants automatic detection of names and SSNs. Which AWS service should be used in a SageMaker Processing job?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Glue DataBrew PII Masking",
      "B": "Amazon Comprehend PII detection API",
      "C": "SageMaker Clarify bias detection",
      "D": "AWS Lake Formation data labeling"
    },
    "explanation": "Amazon Comprehend\u2019s PII detection API can be called from a Processing job to automatically identify and anonymize PII in text."
  },
  {
    "taskStatement": "1.3",
    "stem": "To detect statistical parity in a numeric target variable across demographic groups before training, which SageMaker Clarify component should be used?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "DataQualityCheckConfig in SageMaker Clarify",
      "B": "PreTrainingBiasCheckConfig in a SageMaker Clarify Processing job",
      "C": "ModelBiasMonitor",
      "D": "BatchTransform with a custom script"
    },
    "explanation": "PreTrainingBiasCheckConfig in a Clarify Processing job enables computation of pre-training bias metrics on the dataset."
  },
  {
    "taskStatement": "1.3",
    "stem": "Before ingesting S3 data into SageMaker, an engineer needs to automatically discover and classify PII fields at scale. Which AWS service should be used?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Config",
      "B": "Amazon Athena",
      "C": "Amazon Macie",
      "D": "AWS CloudTrail"
    },
    "explanation": "Amazon Macie automatically discovers and classifies PII in S3 objects at scale."
  },
  {
    "taskStatement": "1.3",
    "stem": "A SageMaker training job must encrypt all EBS volumes with a customer-managed KMS key. How should the engineer configure the training job?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Specify the KmsKeyId parameter in the CreateTrainingJob API.",
      "B": "Configure the InputDataConfig to include an EncryptionKeyId.",
      "C": "Enable SSE-KMS on the S3 bucket only.",
      "D": "Use a SageMaker Pipeline with a default KMS key."
    },
    "explanation": "The CreateTrainingJob API\u2019s KmsKeyId parameter applies the specified customer-managed KMS key to encrypt all training EBS volumes."
  },
  {
    "taskStatement": "1.3",
    "stem": "A company must enforce fine-grained access control on PII columns in its Data Catalog tables. Which AWS capability enables this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "S3 bucket policies on the underlying data files.",
      "B": "AWS Lake Formation LF-Tags attached to Data Catalog columns.",
      "C": "IAM identity-based policies only.",
      "D": "AWS KMS key policies on the Data Catalog."
    },
    "explanation": "Lake Formation LF-Tags on Data Catalog columns provide column-level access control for sensitive data."
  },
  {
    "taskStatement": "1.3",
    "stem": "An engineer needs to ensure that a string column contains only unique values before training. Which AWS service and rule type should be used?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Glue DataBrew with a completeness rule.",
      "B": "AWS Glue Data Quality with a passing ratio rule.",
      "C": "SageMaker Clarify DataQualityCheckConfig uniqueness check.",
      "D": "AWS Glue Data Quality with a uniqueness rule."
    },
    "explanation": "AWS Glue Data Quality supports a rule to check column uniqueness to enforce that values are unique."
  },
  {
    "taskStatement": "1.3",
    "stem": "To prevent order-based bias in large S3 datasets during training, how can the engineer shuffle data in a SageMaker training job?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use File input mode (default shuffling).",
      "B": "Use RecordIO input mode (shuffles automatically).",
      "C": "Use Pipe input mode with ShuffleConfig set to an appropriate buffer size.",
      "D": "Download and shuffle in the training script only."
    },
    "explanation": "Pipe input mode with ShuffleConfig allows SageMaker to shuffle streaming records before training to avoid order bias."
  },
  {
    "taskStatement": "1.3",
    "stem": "A dataset contains missing numeric values that must be imputed with the median before feature engineering. Which tool provides a built-in transform for this?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Glue DataBrew recipe step \u201cFill missing values\u201d with median.",
      "B": "SageMaker Debugger preprocessing hook.",
      "C": "SageMaker Feature Store ingestion transform.",
      "D": "AWS Glue Data Quality imputation rule."
    },
    "explanation": "AWS Glue DataBrew includes a \u201cFill missing values\u201d transform that can impute missing entries with statistics like median."
  },
  {
    "taskStatement": "1.3",
    "stem": "An engineer wants to use a consistent, repeatable set of cleaned and validated features for both offline training and real-time inference. Which AWS capability should be used?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon S3 with versioned CSV files.",
      "B": "Amazon DynamoDB table with precomputed features.",
      "C": "SageMaker Feature Store to store and retrieve features.",
      "D": "Amazon Redshift external table."
    },
    "explanation": "SageMaker Feature Store provides a centralized store for features that ensures consistency between offline training and online inference."
  },
  {
    "taskStatement": "2.1",
    "stem": "A credit risk team needs a highly interpretable, low-latency regression model for predicting applicant default probability using 50 numerical and categorical features. They require clear feature coefficients for regulatory reporting. Which SageMaker built-in algorithm should they choose?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "XGBoost built-in algorithm with SHAP explanations",
      "B": "Linear Learner built-in algorithm in regression mode",
      "C": "Factorization Machines built-in algorithm",
      "D": "DeepAR forecasting algorithm"
    },
    "explanation": "Linear Learner in regression mode provides direct feature weights for interpretability and low latency. XGBoost can require SHAP post hoc analysis, and other algorithms aren\u2019t designed for regression interpretability."
  },
  {
    "taskStatement": "2.1",
    "stem": "An online advertising platform must predict click-through rates with hundreds of categorical features of high cardinality. The team wants to capture pairwise interactions between sparse features efficiently. Which built-in SageMaker algorithm is most appropriate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "XGBoost built-in algorithm",
      "B": "Linear Learner built-in algorithm",
      "C": "Factorization Machines built-in algorithm",
      "D": "K-Means built-in clustering algorithm"
    },
    "explanation": "Factorization Machines are designed to model interactions among high-cardinality categorical features more efficiently than tree-based or linear models."
  },
  {
    "taskStatement": "2.1",
    "stem": "A retail company needs to forecast hourly product demand for 10,000 SKUs with seasonal patterns and occasional promotions. They require a probabilistic forecast of future demand. Which built-in SageMaker algorithm should they select?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "DeepAR forecasting algorithm",
      "B": "XGBoost regression algorithm",
      "C": "Linear Learner regression algorithm",
      "D": "K-Nearest Neighbors built-in algorithm"
    },
    "explanation": "DeepAR provides probabilistic time series forecasts at scale and handles seasonality and promotions. Other algorithms do not directly support probabilistic forecasting."
  },
  {
    "taskStatement": "2.1",
    "stem": "An operations team needs to detect anomalies in streaming server CPU metrics. They require a one-class unsupervised method that adapts to drift. Which SageMaker built-in algorithm meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Isolation Forest built-in algorithm",
      "B": "Random Cut Forest built-in algorithm",
      "C": "One-Class SVM via script mode",
      "D": "K-Means built-in clustering algorithm"
    },
    "explanation": "Random Cut Forest is an unsupervised, streaming-friendly anomaly detection algorithm that adapts to drift; it\u2019s a SageMaker built-in algorithm."
  },
  {
    "taskStatement": "2.1",
    "stem": "A global publisher must classify news articles in 10 languages with minimal training data per language. They need an AWS managed NLP service with built-in support. Which service should they choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Custom BERT model on SageMaker",
      "B": "Custom TF-based text classifier in script mode",
      "C": "Amazon Comprehend DetectDominantLanguage and Custom Classification APIs",
      "D": "Amazon Translate to English plus custom classifier"
    },
    "explanation": "Amazon Comprehend offers multi-language classification with minimal training data and managed infrastructure. Custom models require more data and maintenance."
  },
  {
    "taskStatement": "2.1",
    "stem": "A biotech startup wants to identify cell types in microscopy images with minimal ML expertise. They prefer a no-code AWS solution that adapts to new classes. Which service should they use?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Custom CNN built in TensorFlow on SageMaker script mode",
      "B": "Deploy a PyTorch model on SageMaker endpoint",
      "C": "Use SageMaker AutoML via built-in algorithms",
      "D": "SageMaker Canvas image classification with Amazon Rekognition Custom Labels integration"
    },
    "explanation": "SageMaker Canvas provides a no-code interface, and integration with Rekognition Custom Labels supports labeling and incremental class adaptation without deep ML expertise."
  },
  {
    "taskStatement": "2.1",
    "stem": "A call-center analytics team needs real-time transcription and sentiment analysis of customer calls. They require a managed service with low operations overhead. Which combination should they use?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Custom DeepSpeech model on SageMaker",
      "B": "Amazon Transcribe for transcription and Amazon Comprehend for sentiment",
      "C": "Deploy open-source Kaldi on SageMaker",
      "D": "Amazon Lex chatbot for transcription and sentiment"
    },
    "explanation": "Amazon Transcribe and Comprehend are managed services for ASR and sentiment analysis, respectively, minimizing operational overhead compared to custom models."
  },
  {
    "taskStatement": "2.1",
    "stem": "A financial modeling team needs to generate synthetic financial reports using a foundation model. They require a generative text service with minimal training. Which AWS service best fits?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Bedrock with a foundation LLM",
      "B": "Custom GPT-2 on SageMaker",
      "C": "Use TensorFlow Sequence-to-Sequence on SageMaker",
      "D": "Amazon Translate with custom glossary"
    },
    "explanation": "Amazon Bedrock provides foundation LLMs for text generation, reducing heavy training and tuning efforts versus custom Seq2Seq models."
  },
  {
    "taskStatement": "2.1",
    "stem": "An edge computing use case requires running an image classifier on IoT cameras with limited compute. They have a trained ResNet model. Which SageMaker feature should they use to optimize the model?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker built-in Image Classification algorithm",
      "B": "SageMaker Script Mode with TensorFlow",
      "C": "SageMaker Neo compilation to target edge architecture",
      "D": "Containerize model for SageMaker endpoint"
    },
    "explanation": "SageMaker Neo compiles and optimizes trained models for edge devices, reducing latency and resource usage on IoT cameras."
  },
  {
    "taskStatement": "2.1",
    "stem": "A team needs to quickly prototype a multi-label text classification for internal documents with zero code. They want to experiment with pre-trained models and fine-tune them. Which SageMaker capability should they use?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "SageMaker Clarify for data bias analysis",
      "B": "SageMaker Model Monitor for drift alerts",
      "C": "Notebook instance with pre-built scripts",
      "D": "SageMaker JumpStart solution templates for multi-label text classification"
    },
    "explanation": "SageMaker JumpStart provides pre-trained model solutions and templates for multi-label classification that can be fine-tuned with minimal code."
  },
  {
    "taskStatement": "2.1",
    "stem": "A startup has a small tabular dataset (5,000 rows) and needs a quick binary classifier with built-in regularization and automated hyperparameter tuning. Which SageMaker built-in algorithm and mode is most appropriate?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Linear Learner in binary classification mode",
      "B": "XGBoost regression mode",
      "C": "K-Means clustering mode",
      "D": "DeepAR forecasting mode"
    },
    "explanation": "Linear Learner supports binary classification with built-in regularization and integrates with SageMaker Automatic Model Tuning for small datasets efficiently."
  },
  {
    "taskStatement": "2.1",
    "stem": "A fraud detection team requires an algorithm that handles class imbalance and produces probabilistic scores for each transaction. They prefer a tree-based approach. Which built-in algorithm should they select?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Linear Learner with L1 regularization",
      "B": "XGBoost built-in algorithm",
      "C": "Random Cut Forest",
      "D": "K-Nearest Neighbors"
    },
    "explanation": "XGBoost produces probabilistic outputs, handles class imbalance via objective weighting, and is a high-performance tree-based algorithm."
  },
  {
    "taskStatement": "2.1",
    "stem": "A medical imaging project needs semantic segmentation on X-ray images. The team has no pre-built algorithm in SageMaker. They want minimal development overhead. Which approach should they take?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Use SageMaker built-in Object Detection algorithm",
      "B": "Use Rekognition Custom Labels",
      "C": "Use JumpStart semantic segmentation pre-trained model in script mode",
      "D": "Develop a U-Net from scratch in a notebook"
    },
    "explanation": "JumpStart offers pre-trained semantic segmentation models that can be fine-tuned in script mode, reducing overhead compared to building models from scratch."
  },
  {
    "taskStatement": "2.1",
    "stem": "A business needs to extract key phrases from customer reviews in Spanish. They want a managed solution that supports custom phrase detection. Which service should they choose?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Custom spaCy model on SageMaker",
      "B": "Custom PyTorch NLP in script mode",
      "C": "Amazon Translate to English plus Comprehend key phrases",
      "D": "Amazon Comprehend with custom entity recognizer for Spanish"
    },
    "explanation": "Amazon Comprehend supports custom entity detection in multiple languages including Spanish, avoiding translation pipelines or heavy custom models."
  },
  {
    "taskStatement": "2.1",
    "stem": "A team must choose between deploying a SageMaker built-in algorithm versus a pre-packaged deep learning framework in script mode. They prioritize faster training on CPU-only instances for a moderate-sized tabular dataset. Which should they choose?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "SageMaker built-in XGBoost algorithm",
      "B": "TensorFlow in script mode",
      "C": "PyTorch in script mode",
      "D": "Bring-Your-Own container with Scikit-learn"
    },
    "explanation": "XGBoost built-in algorithm is optimized for CPU training on tabular data and will train faster with minimal configuration compared to deep learning frameworks."
  },
  {
    "taskStatement": "2.2",
    "stem": "A data scientist has an existing SageMaker automatic model tuning (AMT) job that produced optimal hyperparameter values for a regression model. New data arrives and the scientist wants to reuse the previous tuning results as a starting point for a new hyperparameter tuning job to save compute time. Which warm start configuration type should the scientist choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a warm start tuning job with type IDENTICAL_DATA_AND_ALGORITHM.",
      "B": "Use a warm start tuning job with type TRANSFER_LEARNING.",
      "C": "Use a warm start tuning job with type CURRENT_BEST.",
      "D": "Use a regular hyperparameter tuning job without warm start."
    },
    "explanation": "TRANSFER_LEARNING reuses previous tuning job results to inform new search on similar data or algorithm. IDENTICAL_DATA_AND_ALGORITHM reruns the same search space exactly. CURRENT_BEST and non\u2013warm-start jobs don\u2019t utilize prior results to reduce compute."
  },
  {
    "taskStatement": "2.2",
    "stem": "An ML engineer wants to reduce wasted compute and stop unpromising training jobs early during an AMT hyperparameter tuning job. Which configuration change will achieve this with minimal code changes?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set EarlyStoppingType to AUTO in the hyperparameter tuning job configuration.",
      "B": "Set MaxRuntimePerTrainingJob to a lower value.",
      "C": "Add a CloudWatch alarm to terminate long-running training jobs.",
      "D": "Wrap each training job in a custom script that kills the process if validation loss stops improving."
    },
    "explanation": "Setting EarlyStoppingType='Auto' enables SageMaker to automatically terminate unpromising training jobs. Lowering MaxRuntime limits total time but doesn\u2019t target unpromising jobs. CloudWatch alarms and custom scripts add operational overhead."
  },
  {
    "taskStatement": "2.2",
    "stem": "A team trains a large PyTorch model on 8 GPU instances, but training is slow and network communication is a bottleneck. Which SageMaker feature will provide an efficient out-of-the-box distributed training solution to accelerate this job?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure Horovod for distributed training.",
      "B": "Enable smdistributed.dataparallel in Script Mode.",
      "C": "Use the built-in PyTorch Multi-GPU Estimator.",
      "D": "Use a parameter server architecture implemented in the training script."
    },
    "explanation": "smdistributed DataParallel integrates with Script Mode and optimizes gradient synchronization. Horovod requires extra setup, custom parameter server needs code changes, and there is no separate built-in PyTorch multi-GPU Estimator."
  },
  {
    "taskStatement": "2.2",
    "stem": "During training of a convolutional neural network, an ML engineer observes overfitting: training accuracy far exceeds validation accuracy. Which single change to the training job will increase generalization with the least complexity?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase the dropout rate from 0.25 to 0.75.",
      "B": "Reduce the initial learning rate by a factor of ten.",
      "C": "Add L2 weight-decay regularization to the optimizer.",
      "D": "Switch to early stopping and set patience to 3 epochs."
    },
    "explanation": "L2 weight decay penalizes large weights and reduces overfitting without requiring new callbacks. Excessive dropout may underfit; reducing learning rate doesn\u2019t directly address overfitting; early stopping adds extra tuning and monitoring."
  },
  {
    "taskStatement": "2.2",
    "stem": "An ML engineer has trained both a random forest and an XGBoost model for the same classification task and wants to combine them to improve accuracy. Which ensemble approach should the engineer implement to learn an optimal combination of predictions?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a meta-learner on the model outputs (stacking).",
      "B": "Average the predictions from both models (bagging).",
      "C": "Train the XGBoost model to correct errors of the random forest (boosting).",
      "D": "Select the model with the higher validation accuracy for each input dynamically (voting)."
    },
    "explanation": "Stacking uses a meta\u00ad-model trained on base-model outputs to learn optimal weights. Bagging pools multiple instances of one algorithm. Boosting uses sequential training, not two distinct models. Voting is a simple unweighted ensemble."
  },
  {
    "taskStatement": "2.2",
    "stem": "A financial services firm needs to track, promote, and audit multiple versions of their ML models in SageMaker. Which feature should they use to centrally manage model versions and stages (e.g., Approved, Pending)?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon S3 object versioning on the model artifacts bucket.",
      "B": "SageMaker Model Registry.",
      "C": "SageMaker Experiments.",
      "D": "AWS CodeCommit repository."
    },
    "explanation": "SageMaker Model Registry is designed to store, version, and annotate model artifacts and their approval lifecycle. S3 versioning tracks raw objects but lacks metadata. Experiments track experiments, not production stages. CodeCommit is source control."
  },
  {
    "taskStatement": "2.2",
    "stem": "When using smdistributed.dataparallel for PyTorch training, the network bandwidth is still limiting training throughput. Which additional configuration can reduce communication overhead?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch to Horovod with NCCL backend.",
      "B": "Enable gradient compression (fp16) in smdistributed.dataparallel.",
      "C": "Use MPI and ring-allreduce instead of reduce-scatter.",
      "D": "Increase batch size to amortize communication."
    },
    "explanation": "smdistributed DataParallel supports fp16 gradient compression to reduce bandwidth. Horovod setup is heavier, using MPI doesn\u2019t enable compression, and larger batches may help but don\u2019t directly reduce communication volume."
  },
  {
    "taskStatement": "2.2",
    "stem": "An ML engineer is constrained by budget and can run at most 50 training jobs for hyperparameter tuning. Which search strategy in SageMaker AMT will likely find the best solution within the least number of jobs?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Grid search.",
      "B": "Random search.",
      "C": "Bayesian optimization.",
      "D": "Genetic algorithm search."
    },
    "explanation": "Bayesian optimization balances exploration and exploitation, converging faster. Grid search is exhaustive, random search is less efficient, and genetic algorithms aren\u2019t natively supported in SageMaker AMT."
  },
  {
    "taskStatement": "2.2",
    "stem": "A company must deploy a deep learning model to edge devices that have strict memory limits. Which approach using SageMaker will produce the smallest model artifact with minimal manual optimization?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Apply manual weight pruning in the training script.",
      "B": "Use SageMaker Neo to compile and quantize the model for the target device.",
      "C": "Enable mixed-precision training and save fp16 weights.",
      "D": "Use model distillation to train a smaller student network."
    },
    "explanation": "SageMaker Neo automates compilation and quantization for the target hardware, reducing model size. Manual pruning requires custom code; mixed precision lowers memory at runtime but not artifact size; distillation requires additional training."
  },
  {
    "taskStatement": "2.2",
    "stem": "An ML engineer wants to fine-tune a large pre-trained Hugging Face transformer using SageMaker. Which method requires the least boilerplate code?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Bring your own container with transformers installed.",
      "B": "Use the SageMaker Hugging Face estimator in Script Mode.",
      "C": "Translate the model code to MXNet and use the built-in MXNet Estimator.",
      "D": "Use Amazon SageMaker built-in text classification algorithm."
    },
    "explanation": "The SageMaker Hugging Face estimator simplifies fine-tuning with minimal code. BYOC adds container management; translating to MXNet is error-prone; no built-in algorithm for large transformer fine-tuning exists."
  },
  {
    "taskStatement": "2.2",
    "stem": "A data scientist configures an AMT tuning job but forgot to configure early stopping. Which change should they make to the tuning configuration to enable automated early stopping of poor performing jobs?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set EarlyStoppingType='Auto' and choose an appropriate WaitInterval.",
      "B": "Lower the MaxJobs parameter to force quicker completion.",
      "C": "Define a CloudWatch rule to terminate jobs with low metrics.",
      "D": "Wrap the training script in a debugger rule to kill unresponsive jobs."
    },
    "explanation": "EarlyStoppingType='Auto' with WaitInterval allows SageMaker to stop jobs whose objective does not improve. Lowering MaxJobs only limits total count. CloudWatch rules and debugger scripts are more operationally complex."
  },
  {
    "taskStatement": "2.2",
    "stem": "A neural network training job shows high training accuracy but validation accuracy plateaus early. The engineer wants an automated mechanism to detect this during training. Which SageMaker Debugger rule should they enable?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "VanishingGradientDetector.",
      "B": "LossNotDecreasing.",
      "C": "OverfitDetector.",
      "D": "WeightUpdateVerifier."
    },
    "explanation": "OverfitDetector monitors training vs. validation metrics to detect early signs of overfitting. VanishingGradientDetector and LossNotDecreasing focus on gradients and loss. WeightUpdateVerifier checks parameter updates."
  },
  {
    "taskStatement": "2.2",
    "stem": "A team wants to track parameters, metrics, and artifacts across multiple training jobs and compare runs in SageMaker. Which feature should they use?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "SageMaker Model Monitor.",
      "B": "SageMaker Experiments.",
      "C": "Amazon CloudWatch metrics.",
      "D": "AWS X-Ray."
    },
    "explanation": "SageMaker Experiments is built to organize, track, and compare training runs. Model Monitor observes deployed models. CloudWatch and X-Ray are for logs and traces, not training experiment management."
  },
  {
    "taskStatement": "2.2",
    "stem": "A neural network trained on tabular data shows many small nonzero weights and generalizes poorly. Which regularization change will encourage sparsity and reduce overfitting?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase the L2 (weight\u2010decay) coefficient.",
      "B": "Increase dropout to 0.8.",
      "C": "Switch to L1 regularization on weights.",
      "D": "Use batch normalization after every layer."
    },
    "explanation": "L1 regularization (Lasso) encourages many weights to become exactly zero, producing sparse models. L2 shrinks but doesn\u2019t enforce sparsity. Dropout and batch normalization address generalization but not weight sparsity."
  },
  {
    "taskStatement": "2.3",
    "stem": "You have trained a binary classifier on a dataset where the positive class constitutes only 1% of the data. On a held-out test set, the model achieves an AUC-ROC of 0.90, but its precision at low recall is poor. Which evaluation metric should you use to better assess the model\u2019s ability to identify the minority positive class?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Accuracy",
      "B": "AUC-ROC",
      "C": "Area Under the Precision-Recall Curve (AUC-PR)",
      "D": "Log Loss"
    },
    "explanation": "With extreme class imbalance, the precision-recall curve (AUC-PR) focuses on performance for the positive class and is more informative than AUC-ROC or accuracy."
  },
  {
    "taskStatement": "2.3",
    "stem": "A regression model predicts house prices. Stakeholders are more concerned about a few very large prediction errors than many small ones. Which metric should you minimize?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Mean Absolute Error (MAE)",
      "B": "Root Mean Square Error (RMSE)",
      "C": "R\u00b2 (Coefficient of Determination)",
      "D": "Mean Absolute Percentage Error (MAPE)"
    },
    "explanation": "RMSE penalizes large errors more heavily than MAE, making it appropriate when large deviations are particularly undesirable."
  },
  {
    "taskStatement": "2.3",
    "stem": "An ML engineer wants to run reproducible training experiments that track hyperparameters, metrics, and artifacts across multiple training jobs in Amazon SageMaker. Which AWS service should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon CloudWatch Logs",
      "B": "AWS CodePipeline",
      "C": "Amazon SageMaker Experiments",
      "D": "AWS Config"
    },
    "explanation": "SageMaker Experiments provides managed tracking of experiments, trials, hyperparameters, metrics, and artifacts to ensure reproducibility."
  },
  {
    "taskStatement": "2.3",
    "stem": "During training, you notice that your model\u2019s loss on the training set stops decreasing early and plateaus. Which built-in SageMaker Debugger rule should you enable to detect this convergence issue?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "OverfitDetector",
      "B": "LossNotDecreasing",
      "C": "LearningRateFinder",
      "D": "WeightNormMonitor"
    },
    "explanation": "The LossNotDecreasing rule monitors training loss and raises an alert if it does not decrease sufficiently, indicating convergence problems."
  },
  {
    "taskStatement": "2.3",
    "stem": "You want to generate local feature\u2010level explanations (SHAP values) for your production model using SageMaker Clarify. Which configuration should you use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "DataBiasConfig in ClarifyProcessingJob",
      "B": "ModelBiasConfig in ClarifyProcessingJob",
      "C": "SHAPConfig in ClarifyProcessingJob",
      "D": "DriftConfig in ClarifyProcessingJob"
    },
    "explanation": "SHAPConfig enables Clarify to compute SHAP feature attributions for local explainability of model predictions."
  },
  {
    "taskStatement": "2.3",
    "stem": "To select an optimal classification threshold that maximizes F1 score, what procedure should you perform on your validation data?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Evaluate F1 at various probability thresholds and choose the threshold yielding the highest F1",
      "B": "Select the threshold where ROC curve slope equals 1",
      "C": "Use the threshold where precision equals recall",
      "D": "Choose the threshold that balances true positive and false positive rates"
    },
    "explanation": "Maximizing F1 requires evaluating F1 at multiple thresholds on validation data and selecting the threshold with the highest F1."
  },
  {
    "taskStatement": "2.3",
    "stem": "Your deep neural network exhibits vanishing gradient issues. Which SageMaker Debugger configuration helps you inspect gradient distributions during training?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable the base and gradient_histogram collections",
      "B": "Use the TorchLrFinder rule",
      "C": "Use the OverfitDetector rule",
      "D": "Configure only the losses collection"
    },
    "explanation": "The gradient_histogram collection captures gradient distributions so you can detect vanishing or exploding gradients."
  },
  {
    "taskStatement": "2.3",
    "stem": "You need to perform A/B testing between two model variants in production with minimal overhead. Which SageMaker feature supports traffic splitting between variants?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Deploy two separate endpoints and use Route 53 weighted routing",
      "B": "Configure two production variants in a single endpoint and set variant weights",
      "C": "Use an AWS Lambda function to proxy and split traffic",
      "D": "Use AWS CodePipeline for traffic routing"
    },
    "explanation": "SageMaker endpoints support multiple production variants with configurable weights for built-in traffic splitting (A/B testing)."
  },
  {
    "taskStatement": "2.3",
    "stem": "For a multi-class classification problem with imbalanced classes, which evaluation metric gives equal importance to each class regardless of its frequency?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Micro-averaged F1 score",
      "B": "Macro-averaged F1 score",
      "C": "Overall accuracy",
      "D": "Weighted precision"
    },
    "explanation": "Macro-averaged F1 computes the F1 score for each class and averages them equally, treating all classes with equal importance."
  },
  {
    "taskStatement": "2.3",
    "stem": "You compare two regression models: Model A (RMSE = 5.0), Model B (RMSE = 4.8). How can you determine whether the observed difference is statistically significant?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Compare the RMSE values directly",
      "B": "Compute bootstrapped confidence intervals for the RMSE difference",
      "C": "Use one-way ANOVA on prediction residuals",
      "D": "Compare R\u00b2 values"
    },
    "explanation": "Bootstrapping provides confidence intervals on the RMSE difference to determine if the improvement is statistically significant."
  },
  {
    "taskStatement": "2.3",
    "stem": "Which SageMaker Clarify metric measures the difference in the proportion of positive outcomes between two demographic groups?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Class imbalance (CI)",
      "B": "Difference in Proportions of Labels (DPL)",
      "C": "Kolmogorov\u2013Smirnov (KS) statistic",
      "D": "Confusion matrix parity"
    },
    "explanation": "DPL quantifies the difference in label rates (positive outcome proportions) between a sensitive group and the baseline group."
  },
  {
    "taskStatement": "2.3",
    "stem": "Your model\u2019s training loss decreases steadily while validation loss increases after a point. Which SageMaker Debugger rule can detect this behavior automatically?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "OverfitDetector",
      "B": "LossNotDecreasing",
      "C": "LearningRateFinder",
      "D": "GradientHistogram"
    },
    "explanation": "OverfitDetector monitors divergence between training and validation losses to detect overfitting during training."
  },
  {
    "taskStatement": "2.3",
    "stem": "You want to identify a suitable initial learning rate for faster convergence. Which SageMaker Debugger rule should you run?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "LearningRateFinder",
      "B": "LossNotDecreasing",
      "C": "OverfitDetector",
      "D": "EarlyStoppingRule"
    },
    "explanation": "LearningRateFinder systematically varies the learning rate to help identify an optimal learning rate for training convergence."
  },
  {
    "taskStatement": "2.3",
    "stem": "You need to compute evaluation metrics at multiple classification thresholds using SageMaker Clarify. Which configuration option enables this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Specify multiple thresholds in the ClarifyProcessor\u2019s ModelPredictionsConfig",
      "B": "Use SageMaker Model Monitor\u2019s default job",
      "C": "Run a SageMaker batch transform and manually compute metrics",
      "D": "Use SageMaker Automatic Model Tuning"
    },
    "explanation": "You can configure multiple thresholds in ModelPredictionsConfig for Clarify processing jobs to compute metrics (precision, recall, F1) at those thresholds."
  },
  {
    "taskStatement": "2.3",
    "stem": "During training you observe that both training and validation losses are high and nearly identical, and the model fails to improve. What issue does this indicate?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Overfitting",
      "B": "Underfitting",
      "C": "Data leakage",
      "D": "Gradient explosion"
    },
    "explanation": "High and similar training/validation losses indicate underfitting, meaning the model is too simplistic to capture the underlying data patterns."
  },
  {
    "taskStatement": "3.1",
    "stem": "A startup has developed an ML model for an internal analytics tool that is invoked fewer than 100 times per day. Each inference must return within 200 ms, and total monthly cost must be minimized. Which SageMaker deployment option best meets these requirements?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Provision a real-time SageMaker endpoint with minimal instance capacity and auto scaling.",
      "B": "Deploy the model as a SageMaker serverless endpoint.",
      "C": "Use a SageMaker asynchronous endpoint with low concurrency settings.",
      "D": "Schedule a SageMaker batch transform job once per day."
    },
    "explanation": "A serverless endpoint incurs no idle instance cost, supports <200 ms latency for light workloads, and is optimal for very low invocation volume."
  },
  {
    "taskStatement": "3.1",
    "stem": "A machine learning team must host 500 small models (<50 MB each) behind a single API. All models share identical inference logic but have different weights. Which deployment infrastructure should be used to minimize cost and management overhead?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A multi-model SageMaker endpoint.",
      "B": "500 separate real-time SageMaker endpoints.",
      "C": "A SageMaker batch transform job per model.",
      "D": "Amazon ECS on Fargate with mounted S3 weights."
    },
    "explanation": "Multi-model endpoints load models on demand into a shared container, reducing cost and simplifying management compared to separate endpoints."
  },
  {
    "taskStatement": "3.1",
    "stem": "An automotive OEM wants to run an object-detection model on in-vehicle devices with limited compute and no internet connectivity. The model must start in <50 ms after request. Which deployment approach meets these requirements?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Host the model on a SageMaker real-time endpoint and stream data from the vehicle.",
      "B": "Use a SageMaker asynchronous endpoint with cached results on the vehicle gateway.",
      "C": "Compile the model with SageMaker Neo and deploy it to the edge container on the device.",
      "D": "Package the model in a Lambda@Edge function and invoke it from the vehicle."
    },
    "explanation": "SageMaker Neo compiles and optimizes models for specific hardware and enables sub-50 ms local inference without connectivity."
  },
  {
    "taskStatement": "3.1",
    "stem": "A document-processing model accepts 8 MB JSON payloads and may run up to 10 minutes per request. Customers must receive a response per request. Which SageMaker endpoint type is the most cost-effective?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Asynchronous SageMaker endpoint.",
      "B": "Real-time SageMaker endpoint.",
      "C": "Serverless SageMaker endpoint.",
      "D": "SageMaker batch transform job."
    },
    "explanation": "Asynchronous endpoints support large payloads, long processing durations, and return one response per request, avoiding persistent instance costs of real-time endpoints."
  },
  {
    "taskStatement": "3.1",
    "stem": "An e-commerce site needs <100 ms inference latency for its image-classification model at peak traffic. Daily traffic patterns vary widely. Which compute environment should the ML engineer choose?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CPU-based serverless SageMaker endpoint.",
      "B": "GPU-based serverless SageMaker endpoint.",
      "C": "CPU-based real-time SageMaker endpoint with fixed instance count.",
      "D": "GPU-based real-time SageMaker endpoint with auto scaling."
    },
    "explanation": "GPU real-time endpoints deliver required low latency, and auto scaling adjusts capacity to traffic variation."
  },
  {
    "taskStatement": "3.1",
    "stem": "A custom TensorFlow model requires specific OS libraries and drivers that are not available in SageMaker built-in containers. What is the least operationally intensive way to deploy the model?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Translate the model code to one compatible with a built-in container.",
      "B": "Run a batch transform job instead of hosting a real-time endpoint.",
      "C": "Build a custom Docker container, push to ECR, and use it in a SageMaker endpoint.",
      "D": "Use a Lambda function with layers containing the required libraries."
    },
    "explanation": "Custom containers in SageMaker allow full control over dependencies and integrate seamlessly with endpoint deployment."
  },
  {
    "taskStatement": "3.1",
    "stem": "A regulated enterprise requires all inference traffic to traverse a secured, private network with no public internet access. Which configuration satisfies this requirement with minimal changes?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy a public SageMaker endpoint and block internet via security groups.",
      "B": "Deploy a SageMaker real-time endpoint in the customer VPC with private subnets and no NAT gateway.",
      "C": "Use a serverless SageMaker endpoint and disable outbound access.",
      "D": "Host the model in EKS and restrict Internet at cluster level."
    },
    "explanation": "Launching the endpoint in private subnets of the customer VPC ensures no public internet access without additional NAT configuration."
  },
  {
    "taskStatement": "3.1",
    "stem": "An ML engineer must deploy a new model version and gradually shift 20% of production traffic to it for canary testing, with automatic rollback on errors. Which SageMaker feature accomplishes this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy a new real-time endpoint and use a Route 53 weighted record.",
      "B": "Create a second endpoint and manually switch after validation.",
      "C": "Use a SageMaker batch transform job for the canary.",
      "D": "Use SageMaker endpoint\u2010update with traffic shifting via endpoint configurations."
    },
    "explanation": "SageMaker\u2019s endpoint configuration supports traffic weights for A/B and canary deployments and automatic rollback on alarms."
  },
  {
    "taskStatement": "3.1",
    "stem": "Your team uses Amazon Managed Workflows for Apache Airflow (MWAA) but wants tighter integration with SageMaker model deployments and lineage tracking. Which orchestrator should you choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Continue with MWAA and use custom operators.",
      "B": "SageMaker Pipelines.",
      "C": "AWS Step Functions directly.",
      "D": "AWS CodePipeline."
    },
    "explanation": "SageMaker Pipelines provides built-in integration for training, model registry, and deployment with lineage tracking."
  },
  {
    "taskStatement": "3.1",
    "stem": "A model requires a preprocessing container and a separate inference container on each request. How should you deploy this in SageMaker?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Multi-container real-time endpoint with both containers in the same inference pipeline.",
      "B": "Two separate endpoints chained via Lambda.",
      "C": "Batch transform job with a custom script calling both containers.",
      "D": "ECS service with multiple containers per task."
    },
    "explanation": "SageMaker multi-container endpoints allow a preprocessor and model container to run sequentially in the same endpoint."
  },
  {
    "taskStatement": "3.1",
    "stem": "A lightweight NLP model (<10 MB) must be invoked by other AWS services via API and handle occasional spikes of up to 50 requests per second. You want to avoid managing servers. Which deployment target is best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Serverless SageMaker endpoint.",
      "B": "Real-time SageMaker endpoint with a single instance.",
      "C": "Amazon Lambda container image with the model packaged and hosted behind an API Gateway.",
      "D": "ECS Fargate service."
    },
    "explanation": "A Lambda container image offers serverless scaling to handle spikes and exposes a simple API gateway endpoint without EC2 management."
  },
  {
    "taskStatement": "3.1",
    "stem": "Your team needs to optimize inference performance on ARM-based instances in AWS. The model currently runs on x86 CPU. Which deployment approach will yield the greatest performance gain?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch to a serverless endpoint on ARM.",
      "B": "Compile the model with SageMaker Neo for ARM and host on ARM EC2 instances.",
      "C": "Use a GPU-based real-time endpoint on x86.",
      "D": "Deploy in ECS with ARM container images."
    },
    "explanation": "Neo compiles and optimizes for ARM hardware, delivering significant inference speedups compared to generic CPU execution."
  },
  {
    "taskStatement": "3.1",
    "stem": "A hybrid environment uses on-premises Kubernetes and AWS. You need to deploy an ML model so that it's available to both environments with unified CI/CD. Which deployment infrastructure is most appropriate?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker serverless endpoint with VPC peering.",
      "B": "SageMaker real-time endpoint in a public subnet.",
      "C": "ECS with Fargate in a public VPC.",
      "D": "Amazon EKS with GPU node group and a shared Terraform pipeline."
    },
    "explanation": "EKS provides Kubernetes compatibility on-prem and in AWS and integrates with existing CI/CD pipelines."
  },
  {
    "taskStatement": "3.1",
    "stem": "A data-science team must process nightly batches of 10 million records through an ML model. Sub-second latency is not required, but overall runtime must complete within 2 hours. Which deployment strategy is optimal?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High-capacity real-time SageMaker endpoint with multiple GPU instances.",
      "B": "Serverless SageMaker endpoint with high concurrency.",
      "C": "SageMaker batch transform job with GPU instances.",
      "D": "SageMaker asynchronous endpoint with high timeouts."
    },
    "explanation": "Batch transform jobs are optimized for high-throughput offline inference and can leverage large GPU fleets to meet deadlines without request-based endpoint costs."
  },
  {
    "taskStatement": "3.1",
    "stem": "Your organization uses Kubernetes microservices on Amazon EKS. You must deploy an ML model as a microservice in this architecture with GPU acceleration. Which approach do you choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a SageMaker real-time endpoint and call it from EKS pods.",
      "B": "Deploy the model in a container on EKS with a GPU instance profile.",
      "C": "Package the model in a Lambda function behind an Application Load Balancer.",
      "D": "Use SageMaker multi-model endpoint within the VPC and proxy through EKS."
    },
    "explanation": "Hosting directly on EKS with GPU nodes integrates with existing microservices, avoids network hops, and provides GPU acceleration under Kubernetes control."
  },
  {
    "taskStatement": "3.2",
    "stem": "You have two CloudFormation stacks: one provisioning a VPC with subnets and security groups, and another provisioning a SageMaker real-time endpoint. The endpoint stack needs to reference the VPC subnets and security group ARNs created by the VPC stack. What is the most maintainable way to share these values?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "In the VPC stack, export the subnet IDs and security group ARNs using Outputs with Export names. In the endpoint stack, import them with Fn::ImportValue.",
      "B": "Store the subnet IDs and security group ARNs in an S3 object in the VPC stack and download them in CloudFormation custom resources in the endpoint stack.",
      "C": "Pass the subnet IDs and security group ARNs as parameters manually each time you deploy the endpoint stack.",
      "D": "Use a Lambda-backed custom resource in the endpoint stack to call DescribeStacks on the VPC stack and parse the JSON output."
    },
    "explanation": "Cross-stack references via Outputs and Fn::ImportValue is the recommended, maintainable approach for sharing values between CloudFormation stacks."
  },
  {
    "taskStatement": "3.2",
    "stem": "You need to configure application autoscaling for a SageMaker real-time endpoint variant using CloudFormation. Which CloudFormation resource type must you declare to register the endpoint variant with AWS Application Auto Scaling?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS::SageMaker::Endpoint",
      "B": "AWS::ApplicationAutoScaling::ScalableTarget",
      "C": "AWS::SageMaker::EndpointConfiguration",
      "D": "AWS::ApplicationAutoScaling::ScheduledAction"
    },
    "explanation": "AWS::ApplicationAutoScaling::ScalableTarget is required to register the SageMaker endpoint variant with Application Auto Scaling before you attach scaling policies."
  },
  {
    "taskStatement": "3.2",
    "stem": "Your SageMaker endpoint experiences unpredictable spikes in traffic throughout the day. You want to configure auto scaling to trigger when each instance receives more than 100 requests per minute. Which scaling metric should you choose in your scaling policy?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CPUUtilization",
      "B": "ModelLatency",
      "C": "Invocation4xxErrors",
      "D": "InvocationsPerInstance"
    },
    "explanation": "InvocationsPerInstance is the supported Application Auto Scaling metric for SageMaker endpoints to scale based on request volume per instance."
  },
  {
    "taskStatement": "3.2",
    "stem": "You deploy a SageMaker endpoint inside a VPC without a NAT gateway to save cost. The endpoint fails to pull the container from ECR and the model artifacts from S3. Which minimum set of VPC endpoints must you script to restore functionality?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "An interface endpoint for com.amazonaws.<region>.s3 only",
      "B": "Interface endpoints for com.amazonaws.<region>.ecr.api and com.amazonaws.<region>.ecr.dkr only",
      "C": "Interface endpoints for com.amazonaws.<region>.ecr.api, com.amazonaws.<region>.ecr.dkr, and a gateway endpoint for com.amazonaws.<region>.s3",
      "D": "Interface endpoints for com.amazonaws.<region>.sagemaker.api and com.amazonaws.<region>.sagemaker.runtime"
    },
    "explanation": "SageMaker endpoints in a private VPC need ECR API and DKR interface endpoints plus the S3 gateway endpoint to pull container images and model artifacts."
  },
  {
    "taskStatement": "3.2",
    "stem": "You want to deploy a multi-model endpoint using CloudFormation to host multiple model artifacts in a single container. Which property must you use when defining AWS::SageMaker::Model to support this use case?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "PrimaryContainer",
      "B": "Containers",
      "C": "InferenceExecutionConfig",
      "D": "ModelPackageName"
    },
    "explanation": "The Containers property (a list) is required to define a multi-model endpoint in CloudFormation, whereas PrimaryContainer supports only a single container."
  },
  {
    "taskStatement": "3.2",
    "stem": "Your inference container is stored in Amazon ECR and your SageMaker model execution role must pull the image. When scripting the IAM role in CloudFormation, which managed policy should you attach to grant the minimum required permissions?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "AmazonSageMakerFullAccess",
      "B": "AmazonEC2ContainerServiceforEC2Role",
      "C": "AmazonEC2ContainerRegistryReadOnly",
      "D": "AmazonS3ReadOnlyAccess"
    },
    "explanation": "AmazonEC2ContainerRegistryReadOnly grants the least-privilege permissions necessary for SageMaker to pull container images from ECR."
  },
  {
    "taskStatement": "3.2",
    "stem": "Your organization hit the export limit for CloudFormation cross-stack references. You still need to share subnet IDs and security group IDs across stacks. What alternative approach should you use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use nested stacks instead of separate stacks.",
      "B": "Store the values in SSM Parameter Store and reference them with dynamic references.",
      "C": "Batch all values into a single export by concatenating comma-delimited strings.",
      "D": "Use AWS Organizations to share parameters between accounts."
    },
    "explanation": "Storing shared configuration in SSM Parameter Store avoids export limits and allows multiple stacks to reference values via dynamic references."
  },
  {
    "taskStatement": "3.2",
    "stem": "You maintain several CloudFormation templates that define similar network resources for different teams. You want to reuse and standardize these resource definitions. Which CloudFormation feature is best suited for this purpose?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Custom resources",
      "B": "StackSets",
      "C": "Macros",
      "D": "Nested stacks"
    },
    "explanation": "Nested stacks allow you to factor out common resource definitions into reusable templates and include them in multiple parent stacks."
  },
  {
    "taskStatement": "3.2",
    "stem": "When writing AWS CDK code for your SageMaker endpoint, you need to import an existing VPC by its tags so your endpoint can deploy into it. Which CDK method is most appropriate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ec2.Vpc.fromVpcAttributes()",
      "B": "ec2.Vpc.import()",
      "C": "ec2.Vpc.fromLookup()",
      "D": "ec2.Vpc.fromEnv()"
    },
    "explanation": "ec2.Vpc.fromLookup() performs a context lookup by tags or name at synthesis time to import existing VPCs into CDK apps."
  },
  {
    "taskStatement": "3.2",
    "stem": "You are using AWS CDK to build and push a custom Docker container for SageMaker inference. Which CDK construct should you use to define and publish the image asset?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "ecr.Repository",
      "B": "DockerImageAsset",
      "C": "ContainerImage.fromAsset",
      "D": "EcrDockerImage"
    },
    "explanation": "DockerImageAsset (from aws-cdk-lib/aws-ecr-assets) builds a Docker image from a local directory and publishes it to ECR automatically."
  },
  {
    "taskStatement": "3.2",
    "stem": "You want to reduce training costs by using managed Spot Instances for a SageMaker training job defined in CloudFormation. Which property must you add to the AWS::SageMaker::TrainingJob resource?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "EnableManagedSpotTraining: true",
      "B": "UseSpotInstances: true",
      "C": "RuntimeSpotMode: Managed",
      "D": "SpotConfiguration: ManagedSpot"
    },
    "explanation": "EnableManagedSpotTraining set to true enables SageMaker managed Spot training for cost savings."
  },
  {
    "taskStatement": "3.2",
    "stem": "You need to deploy a Lambda function in the same VPC as your SageMaker endpoint so it can invoke the endpoint privately. Which CloudFormation property must you include in the AWS::Lambda::Function resource?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "NetworkConfiguration",
      "B": "SecurityGroupIds",
      "C": "SubnetIds",
      "D": "VpcConfig"
    },
    "explanation": "VpcConfig ({ SubnetIds, SecurityGroupIds }) on AWS::Lambda::Function places the Lambda inside the specified VPC."
  },
  {
    "taskStatement": "3.2",
    "stem": "You configured Application Auto Scaling for your SageMaker endpoint to use CPUUtilization, but you observe that scaling never occurs. Which explanation is correct?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "You must enable detailed monitoring on the endpoint to expose CPU metrics.",
      "B": "SageMaker endpoints only support InvocationsPerInstance as a built-in scaling metric.",
      "C": "CPUUtilization is supported only for asynchronous endpoints.",
      "D": "You must configure a CloudWatch alarm for CPUUtilization even when using Application Auto Scaling."
    },
    "explanation": "Application Auto Scaling for SageMaker real-time endpoints supports only the InvocationsPerInstance metric; CPUUtilization is not supported directly."
  },
  {
    "taskStatement": "3.2",
    "stem": "When you register your SageMaker endpoint variant with AWS Application Auto Scaling in CDK, what is the correct format of the resourceId property?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "endpoint/YourEndpointName/variant/AllTraffic",
      "B": "endpoint/YourEndpointName/variantName/AllTraffic",
      "C": "sagemaker:endpoint:YourEndpointName:variant:AllTraffic",
      "D": "YourEndpointName/AllTraffic"
    },
    "explanation": "The resourceId for a real-time SageMaker endpoint variant must be specified as \"endpoint/<endpointName>/variant/<variantName>\"."
  },
  {
    "taskStatement": "3.2",
    "stem": "You are deploying a new version of your VpcConfig for a SageMaker endpoint in CloudFormation. You modify the Subnets list in the template and redeploy, but the change is not applied. Why?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CloudFormation cannot update VpcConfig on an existing EndpointConfiguration; you must modify another property.",
      "B": "You must delete the EndpointConfiguration resource manually before CloudFormation can apply changes.",
      "C": "CloudFormation only creates a new AWS::SageMaker::EndpointConfiguration when the Endpoint resource\u2019s EndpointConfigName property is updated; you haven\u2019t changed it.",
      "D": "VpcConfig changes require an update to AWS::SageMaker::Model, not EndpointConfiguration."
    },
    "explanation": "Modifying VpcConfig in the EndpointConfiguration has no effect until you update the Endpoint resource\u2019s EndpointConfigName to reference the new configuration."
  },
  {
    "taskStatement": "3.2",
    "stem": "You have an existing CloudFormation template defining your SageMaker model and endpoint. You decide to adopt AWS CDK but want to reuse the existing template without rewriting it. Which CDK construct allows you to embed and extend the existing template?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CfnModel",
      "B": "TemplatePart",
      "C": "IncludeTemplate",
      "D": "CfnInclude"
    },
    "explanation": "CfnInclude (from aws-cdk-lib/cloudformation-include) lets you import an existing CloudFormation template into a CDK app for extension."
  },
  {
    "taskStatement": "3.3",
    "stem": "A data science team needs to implement a CI/CD pipeline that automates model training, testing, and deployment for a SageMaker ML model. The pipeline should enforce a manual approval before deploying to production and use AWS CodePipeline with minimal custom code. Which pipeline configuration meets these requirements?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a manual approval action between the CodeBuild action that trains the model and the CloudFormation Deploy action that updates the production SageMaker endpoint.",
      "B": "Use a Lambda function as a CodePipeline action to pause for approval between the training and deployment stages.",
      "C": "Configure the SageMaker Model Training action to require manual confirmation before executing the SageMaker Model Deploy action.",
      "D": "Use a CloudWatch Events rule to trigger a manual deployment after the training stage finishes."
    },
    "explanation": "CodePipeline supports manual approval actions natively; inserting an Approval action between the training (CodeBuild) stage and the deployment (CloudFormation Deploy endpoint) stage provides a built-in manual gate with minimal custom code."
  },
  {
    "taskStatement": "3.3",
    "stem": "An ML engineer must trigger a CI/CD pipeline whenever new training data arrives in an S3 bucket. The pipeline uses CodePipeline and CodeBuild to preprocess data and train a SageMaker model. Which configuration best accomplishes this requirement with event-driven automation?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure the S3 bucket as the Source stage in CodePipeline and enable change detection polling.",
      "B": "Create a CloudWatch Events rule for s3:ObjectCreated:* that targets the CodePipeline API to start a pipeline execution.",
      "C": "Use an S3 Event Notification to invoke a Lambda function that calls StartPipelineExecution for the CodePipeline.",
      "D": "Configure EventBridge to listen for S3 notifications and invoke the CodeBuild project directly."
    },
    "explanation": "Using an S3 event notification to invoke a Lambda function that calls StartPipelineExecution provides event-driven triggering with fine-grained control and low latency."
  },
  {
    "taskStatement": "3.3",
    "stem": "A team wants to deploy updated SageMaker endpoints in a blue/green deployment fashion as part of their CI/CD pipeline. They need to shift traffic gradually to the new model and enable easy rollback if issues occur. Which approach using AWS native services meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a SageMaker CreateModel action in CodePipeline with the 'Blue/Green' deployment type and specify the traffic shifting percentage.",
      "B": "Use CloudFormation Deploy action in CodePipeline configured with CodeDeploy SafeMode for CloudFormation change sets to perform traffic shifting between endpoints.",
      "C": "Invoke the SageMakerUpdateEndpoint API in CodeBuild and script traffic weights in the buildspec.",
      "D": "Configure a Lambda function in a pipeline action that calls UpdateEndpointWeightsAndCapacities for traffic shifting."
    },
    "explanation": "Using a CloudFormation Deploy action with CodeDeploy-managed change sets enables native blue/green deployments and traffic shifting with rollback capabilities without custom scripts."
  },
  {
    "taskStatement": "3.3",
    "stem": "A CodeBuild project in a CI/CD pipeline needs to access resources inside a VPC (private RDS and SageMaker endpoint). Builds are failing because the project cannot reach VPC-only endpoints. How should you modify the CodeBuild configuration to allow access while maintaining least privilege?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add the CodeBuild project to the same security group as the RDS instance and SageMaker endpoint.",
      "B": "Configure the CodeBuild project with VPC configuration specifying the subnets and security groups that allow access to the private endpoints.",
      "C": "Create a NAT gateway in the VPC and enable public access for the CodeBuild project.",
      "D": "Enable CodeBuild network isolation and whitelist the VPC CIDR block."
    },
    "explanation": "Configuring the CodeBuild project with the correct VPC, subnets, and security groups allows it to access private resources securely and maintain least privilege."
  },
  {
    "taskStatement": "3.3",
    "stem": "You need to add automated testing stages to your ML CI/CD pipeline: unit tests for preprocessing code, integration tests against a development SageMaker endpoint, and performance validation of the new model. Which pipeline actions should you include to satisfy these requirements?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Three CodeBuild actions: one running pytest for code, one invoking the dev endpoint via AWS CLI, and one running a custom performance test script.",
      "B": "One CodeBuild action with sequential buildspec phases for unit, integration, and performance tests.",
      "C": "Unit tests as a CodeBuild action, integration tests as a Lambda invoke action, and performance tests as a SageMaker Batch Transform action.",
      "D": "Integration tests first, then unit tests, then performance validation, all in a single CodeBuild action."
    },
    "explanation": "Separating testing into distinct CodeBuild actions for unit, integration, and performance ensures isolation and clear visibility of failures, and uses the native build environment to invoke tests."
  },
  {
    "taskStatement": "3.3",
    "stem": "An ML engineer has a SageMaker Pipeline defined for data preprocessing, model training, and evaluation. To include this SageMaker Pipeline in a larger CodePipeline CI/CD workflow, which native CodePipeline action type should they use to start and monitor the SageMaker Pipeline execution?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS CloudFormation action with a custom resource to invoke the SageMaker Pipeline.",
      "B": "AWS Lambda invoke action that calls StartPipelineExecution.",
      "C": "SageMakerPipeline action type provided by AWS CodePipeline.",
      "D": "CodeBuild action using AWS CLI to start the SageMaker Pipeline."
    },
    "explanation": "CodePipeline provides the SageMakerPipeline action to natively integrate SageMaker Pipelines as a stage, handling execution and status monitoring without custom scripts."
  },
  {
    "taskStatement": "3.3",
    "stem": "Your ML CI/CD pipeline uses CodeBuild to package and push Docker images to a private ECR repository encrypted with a customer-managed KMS key. Builds are failing when CodeBuild attempts to push the image. What minimum IAM policy change should you apply to the CodeBuild service role to resolve this issue?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add kms:Decrypt permission on the KMS key used to encrypt the ECR repository.",
      "B": "Add kms:GenerateDataKey permission on the KMS key.",
      "C": "Add kms:Encrypt and kms:Decrypt permissions on the KMS key.",
      "D": "Add kms:DescribeKey permission on the KMS key."
    },
    "explanation": "CodeBuild needs kms:GenerateDataKey to encrypt the image layers before pushing to ECR. Decrypt is not required for push operations."
  },
  {
    "taskStatement": "3.3",
    "stem": "A team adopts GitFlow branching for ML code and infrastructure definitions. How should branches map to CodePipeline stages to implement dev, test, and prod environments with automated promotion?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Map the 'develop' branch as the Source for the dev pipeline, 'release' for test, and 'master' for prod, each with its own CodePipeline using source triggers.",
      "B": "Use 'feature' branches for dev, 'develop' for test, and 'release' for prod, all in a single pipeline with multiple source actions.",
      "C": "Use 'master' branch for all environments and control deployments with manual approval actions.",
      "D": "Use 'hotfix' branches to promote code directly to production pipeline."
    },
    "explanation": "GitFlow maps develop \u2192 dev pipeline, release \u2192 test pipeline, and master \u2192 production pipeline, enabling automated promotions based on branch."
  },
  {
    "taskStatement": "3.3",
    "stem": "You configured SageMaker Model Monitor to detect data drift for a production endpoint. You want your CI/CD pipeline to automatically retrain the model when drift is detected. Which event pattern and target configuration should you use in EventBridge to integrate drift detection with your pipeline?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Event pattern: SageMaker Model Monitor DataQualityCheckNotification; Target: CodePipeline StartPipelineExecution action.",
      "B": "Event pattern: CloudWatch Alarm for drift metric; Target: Lambda that updates the SageMaker endpoint.",
      "C": "Event pattern: SageMaker TrainingJobStateChange; Target: EventBridge rule that triggers retraining.",
      "D": "Event pattern: S3:ObjectCreated for captured data; Target: CodeBuild to start training."
    },
    "explanation": "Model Monitor emits DataQualityCheckNotification events; capturing these in an EventBridge rule targeting StartPipelineExecution automates retraining when drift is detected."
  },
  {
    "taskStatement": "3.3",
    "stem": "You want to define your entire CI/CD pipeline in AWS CDK, including stages for building, testing, and deploying a SageMaker model. Which CDK construct and patterns should you use to best represent stages and actions?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use pipelines.CodePipeline construct with pipelines.CodeBuildStep and pipelines.ShellStep.",
      "B": "Use aws_codepipeline.Pipeline with aws_codepipeline_actions.CodeBuildAction and aws_codepipeline_actions.CloudFormationCreateUpdateStackAction.",
      "C": "Use aws_sagemaker.CfnPipeline and embed CodePipeline definitions as metadata.",
      "D": "Use a single CodeBuild project in CDK and run all steps in buildspec."
    },
    "explanation": "Using aws_codepipeline.Pipeline with native CodeBuild and CloudFormation actions allows explicit definition of CI/CD stages and is the recommended pattern in CDK for pipelines."
  },
  {
    "taskStatement": "3.3",
    "stem": "During a CodePipeline execution, the CodeBuild step that builds the Docker container for a custom SageMaker algorithm fails due to insufficient privileges for Docker. Which setting in the CodeBuild project should you enable to allow Docker builds?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set privileged mode to true in the CodeBuild project settings.",
      "B": "Assign the CodeBuild role to the DockerUsers group in IAM.",
      "C": "Enable inbound and outbound network access in CodeBuild.",
      "D": "Grant CodeBuild service role permissions for sagemaker:CreateAlgorithm."
    },
    "explanation": "Privileged mode allows CodeBuild to run Docker commands needed for building and pushing container images."
  },
  {
    "taskStatement": "3.3",
    "stem": "You need to validate model performance metrics generated during training before deploying the model in the CI/CD pipeline. Which CodePipeline action can you use to automatically compare the new metrics against a baseline and fail the pipeline if the model does not meet thresholds?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a Lambda invoke action that reads metrics from S3 and throws an error if thresholds are not met.",
      "B": "Use a CloudWatch Alarm action in CodePipeline to evaluate metrics.",
      "C": "Use the built-in SageMaker Model Quality check action in CodePipeline.",
      "D": "Use a CodeBuild action with a buildspec that runs a custom validation script."
    },
    "explanation": "CodePipeline does not have a native metric gating action; invoking a Lambda to assert metric thresholds provides a serverless, automated gate."
  },
  {
    "taskStatement": "3.3",
    "stem": "In your pipeline, the output of the model training stage is a model artifact location in S3. You need to pass this dynamic S3 path to the SageMaker CreateModel deployment stage in CodePipeline. How should you configure the deployment action to consume this artifact location?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use CodePipeline artifact variables and parameter overrides in the SageMaker CreateModel action configuration.",
      "B": "Hardcode the S3 path in the CreateModel action ARN.",
      "C": "Write the path to Systems Manager Parameter Store and reference it in the deployment stage.",
      "D": "Use a Lambda function to retrieve the location and call CreateModel."
    },
    "explanation": "Artifact variables allow CodePipeline stages to access outputs from previous stages and dynamically pass them to action parameters without custom code."
  },
  {
    "taskStatement": "3.3",
    "stem": "Your team is evaluating whether to use SageMaker Pipelines or AWS CodePipeline for end-to-end ML workflows. They need automated data preprocessing, hyperparameter tuning, model evaluation, and deployment gates. Which is the most appropriate orchestration tool?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Pipelines for ML steps and embed it as a stage in AWS CodePipeline for approvals and multi-account deployments.",
      "B": "Use AWS CodePipeline exclusively for all ML and deployment tasks.",
      "C": "Use AWS Step Functions to orchestrate both ML and deployment steps.",
      "D": "Use Amazon Managed Workflows for Apache Airflow (MWAA) to orchestrate SageMaker training and deploy."
    },
    "explanation": "SageMaker Pipelines provides first-class support for ML workflows; embedding it in CodePipeline adds enterprise-grade CI/CD features like approvals and cross-account deployments."
  },
  {
    "taskStatement": "3.3",
    "stem": "To implement automated rollback on a SageMaker endpoint if post-deployment smoke tests fail, which deployment pattern should you configure in CodePipeline and CodeDeploy?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "All-at-once deployment with a Lambda test hook in CodeDeploy that triggers rollback on failure.",
      "B": "Canary deployment type in CloudFormation Deploy action with pre-traffic and post-traffic validation Lambda hooks.",
      "C": "Linear deployment with time-based rollout and manual approval for rollback.",
      "D": "In-place deployment with CodeBuild test stage preceding deployment."
    },
    "explanation": "Using a canary deployment with pre- and post-traffic validation hooks in CodeDeploy allows automated testing and rollback if smoke tests fail."
  },
  {
    "taskStatement": "4.1",
    "stem": "An ML engineer has deployed a real-time SageMaker inference endpoint and needs to detect both input feature distribution drift and prediction quality degradation in production with minimal custom code. Which combination of SageMaker services and configurations should the engineer use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure DataCaptureConfig on the endpoint, deploy a DefaultModelMonitor job to detect data drift, and schedule a ModelQualityMonitor job with ground truth to detect prediction drift.",
      "B": "Configure DataCaptureConfig on the endpoint and schedule a Clarify DataBiasMonitor job to detect both data and prediction drift.",
      "C": "Stream inference events to Kinesis Data Firehose and use AWS Glue to analyze drift for both features and predictions.",
      "D": "Use SageMaker Clarify ModelExplainabilityMonitor to detect distribution shifts and model accuracy degradation."
    },
    "explanation": "Use DataCaptureConfig + DefaultModelMonitor for input drift and ModelQualityMonitor with ground truth for prediction/concept drift; other options do not cover both aspects or require more custom work."
  },
  {
    "taskStatement": "4.1",
    "stem": "Before enabling continuous data drift detection on a production SageMaker endpoint, an ML engineer must establish baseline statistics and constraints. Which approach should the engineer use to generate these baselines automatically?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run a SageMaker Processing job using the DefaultModelMonitor container on a representative historical dataset in S3.",
      "B": "Use SageMaker Clarify DataBiasMonitor to create baseline drift constraints on the training data.",
      "C": "Write custom code to compute statistics and manually author the constraint JSON file.",
      "D": "Use AWS Glue DataBrew to profile the data and export a constraint file to S3."
    },
    "explanation": "The DefaultModelMonitor Processing container provides built-in functionality to compute baseline statistics and constraints; other options either don\u2019t generate constraints in the required format or don\u2019t support SageMaker\u2019s drift detectors."
  },
  {
    "taskStatement": "4.1",
    "stem": "A financial services company must monitor inference data while excluding PII fields from retention. Which configuration in SageMaker Model Monitor allows the engineer to capture inference payloads but filter out PII attributes?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set DataCaptureConfig sample_percentage to 0 and use encryption to prevent PII capture.",
      "B": "Use Clarify ModelBiasMonitor with an exclude_columns parameter for PII fields.",
      "C": "Configure DataCaptureConfig with a CaptureFilter to exclude PII JSON paths before writing to S3.",
      "D": "Enable a processing container script to mask PII after the monitor job runs."
    },
    "explanation": "DataCaptureConfig\u2019s CaptureFilter lets you specify JSONPath or CSV column filters so only non-PII fields are captured; other methods either capture PII or require post-processing."
  },
  {
    "taskStatement": "4.1",
    "stem": "A Model Monitor violation report shows a numeric feature\u2019s distribution exceeding baseline constraints for variance only. What is the most appropriate next step?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Adjust the variance threshold in the baseline constraint to suppress false positives.",
      "B": "Investigate the production data distribution change, update training data or retrain the model if the shift reflects new valid patterns.",
      "C": "Disable monitoring for that feature to reduce alert noise.",
      "D": "Regenerate the baseline constraints on the current production data without retraining."
    },
    "explanation": "A constraint violation indicates real drift; you should investigate and retrain or augment training data if needed. Changing thresholds or disabling the monitor risks missing true drift."
  },
  {
    "taskStatement": "4.1",
    "stem": "An operations team requires near real-time alerts whenever model input data drift is detected. Which integration provides automated notifications upon Model Monitor constraint violations?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Have a Lambda function poll the S3 violation reports folder every minute.",
      "B": "Create an Amazon EventBridge rule for SageMaker MonitoringExecutionStatus change events and target an SNS topic.",
      "C": "Subscribe an SNS notification directly to the Model Monitor S3 bucket.",
      "D": "Use CloudWatch Logs Insights to search for violations and trigger alarms."
    },
    "explanation": "EventBridge can capture SageMaker monitoring execution status change events and forward them to SNS; polling or log-based solutions introduce delay or complexity."
  },
  {
    "taskStatement": "4.1",
    "stem": "A team wants to detect anomalies in inference latency, error rates, and invoke rates on a SageMaker real-time endpoint. Which approach will meet these requirements with the least operational overhead?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use Amazon CloudWatch built-in metrics (Latency, Invocations, 4xx/5xx error counts) and configure CloudWatch Alarms.",
      "B": "Use SageMaker Model Monitor to detect infrastructure anomalies.",
      "C": "Use SageMaker Clarify to monitor model performance metrics.",
      "D": "Enable AWS CloudTrail on the endpoint and analyze logs in S3."
    },
    "explanation": "CloudWatch automatically captures endpoint performance and error metrics; Model Monitor focuses on data quality, and Clarify focuses on bias/explainability."
  },
  {
    "taskStatement": "4.1",
    "stem": "Which statement best describes concept drift in a deployed ML model?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A change in the input feature distribution compared to baseline data.",
      "B": "A change in the statistical relationship between input features and target labels over time.",
      "C": "An imbalance in class label frequencies in production data.",
      "D": "A bias introduced during model training that only affects edge cases."
    },
    "explanation": "Concept drift refers to changes in P(Y|X), i.e., the relationship between features and labels; data drift is about P(X) changes."
  },
  {
    "taskStatement": "4.1",
    "stem": "An ML engineer needs to gradually shift traffic to a canary variant of a SageMaker endpoint and automatically roll back if error rates exceed a threshold. Which deployment mechanism should they use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker multi\u2010variant endpoints with AWS CodeDeploy canary traffic\u2010shifting and configure a CloudWatch Alarm on 5xx error rate.",
      "B": "Deploy two separate endpoints and manually switch DNS when errors are low.",
      "C": "Use SageMaker asynchronous endpoints with weighted routing.",
      "D": "Use SageMaker batch transform jobs scheduled hourly and compare error rates."
    },
    "explanation": "SageMaker multi\u2010variant endpoints integrated with CodeDeploy support canary traffic shifts and automatic rollback via CloudWatch Alarms."
  },
  {
    "taskStatement": "4.1",
    "stem": "To detect shifts in feature attributions of a production model over time, which SageMaker monitoring job should an ML engineer schedule?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A Clarify ModelExplainabilityMonitor job with SHAP baseline and monitoring configuration.",
      "B": "A Clarify DataBiasMonitor job on the inference data.",
      "C": "A DefaultModelMonitor job focusing on data quality constraints.",
      "D": "A ModelQualityMonitor job comparing accuracy against ground truth."
    },
    "explanation": "ModelExplainabilityMonitor (SHAP) jobs track how feature attributions change; data bias or quality monitors don\u2019t measure attribution shifts."
  },
  {
    "taskStatement": "4.1",
    "stem": "What is the minimum supported frequency for scheduling a SageMaker MonitoringSchedule to detect production data drift?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Every 5 minutes",
      "B": "Every 1 minute",
      "C": "Every hour",
      "D": "Every 24 hours"
    },
    "explanation": "SageMaker MonitoringSchedule supports a minimum interval of 5 minutes; shorter intervals are not allowed."
  },
  {
    "taskStatement": "4.1",
    "stem": "An engineering team uses batch transform jobs for asynchronous inference and needs to monitor prediction quality drift against offline ground truth. Which solution requires the least operational overhead?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable DataCaptureConfig on the batch transform endpoint and run DefaultModelMonitor.",
      "B": "Implement a custom Lambda to compare S3 outputs to ground truth and publish metrics.",
      "C": "Schedule a ModelQualityMonitor ProcessingJob using the batch transform output folder and S3 ground truth labels.",
      "D": "Use Clarify DataBiasMonitor on the batch transform results."
    },
    "explanation": "Scheduling a ModelQualityMonitor ProcessingJob directly on transform outputs and labels uses built-in monitoring; other options require custom polling or inappropriate monitors."
  },
  {
    "taskStatement": "4.1",
    "stem": "A production endpoint\u2019s data quality monitor runs but model accuracy has degraded without any data drift reported. What additional monitoring configuration is required?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a Clarify DataBiasMonitor to detect label shifts.",
      "B": "Increase the sampling percentage in DataCaptureConfig.",
      "C": "Enable multivariant monitoring on the same monitor.",
      "D": "Configure and schedule a ModelQualityMonitor job with ground truth labels to detect accuracy degradation."
    },
    "explanation": "DataQuality monitors P(X) shifts, not P(Y|X); ModelQualityMonitor with ground truth is required to detect concept or accuracy drift."
  },
  {
    "taskStatement": "4.1",
    "stem": "A company must monitor production inference outputs for fairness drift (e.g., change in demographic parity) over time. Which SageMaker Clarify monitor should they schedule?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Clarify DataBiasMonitor",
      "B": "Clarify ModelBiasMonitor",
      "C": "ModelExplainabilityMonitor",
      "D": "DefaultModelMonitor"
    },
    "explanation": "ModelBiasMonitor detects fairness metrics (demographic parity, equalized odds) on inference data; DataBiasMonitor analyzes training set biases."
  },
  {
    "taskStatement": "4.1",
    "stem": "An ML engineer must filter out sensitive fields from inference data before it reaches the Model Monitor processing container. Which component should they customize?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provide a custom preprocessing script in the MonitoringSchedule\u2019s ProcessingJobConfig.",
      "B": "Write a post-monitoring ETL job to remove sensitive columns from violation reports.",
      "C": "Modify the DefaultModelMonitor container image to drop PII.",
      "D": "Configure DataCaptureConfig to drop features via sample_percentage."
    },
    "explanation": "A custom preprocessing script in MonitoringSchedule\u2019s ProcessingJobConfig lets you transform or filter data before constraint evaluation; other methods occur after capture or require image modification."
  },
  {
    "taskStatement": "4.1",
    "stem": "An ML engineer wants to integrate drift detection into a SageMaker Pipelines workflow and automatically trigger retraining when drift is detected. Which pipeline step should they include to evaluate drift violations before invoking the retraining branch?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a MonitoringStep with fail_on_violation=True to stop the pipeline on any drift.",
      "B": "Use a ConditionStep to check if the MonitoringStep status equals 'Failed'.",
      "C": "Use a RegisterModel step to register the model only if no drift is detected.",
      "D": "Use a ConditionStep to inspect the MonitoringStep output property 'BaselineViolations' > 0 and branch accordingly."
    },
    "explanation": "A ConditionStep can examine the MonitoringStep output (e.g., violated constraint count) and route to retraining; this avoids hard failures and enables branching logic."
  },
  {
    "taskStatement": "4.2",
    "stem": "An ML team needs to right-size SageMaker inference endpoints to minimize cost while maintaining performance. They want to benchmark actual model inference performance (latency and under various loads) to choose optimal instance types. Which AWS service should they use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Compute Optimizer",
      "B": "AWS SageMaker Inference Recommender",
      "C": "AWS Cost Explorer",
      "D": "AWS Trusted Advisor"
    },
    "explanation": "SageMaker Inference Recommender runs performance benchmarks on your model to recommend optimal instance families and sizes. Compute Optimizer provides general EC2/EBS recommendations, not ML-specific inference benchmarks."
  },
  {
    "taskStatement": "4.2",
    "stem": "A financial organization observes unexpected monthly spikes in SageMaker inference costs due to seasonal usage. They need to receive near real-time alerts when cost anomalies occur in their AWS account. Which solution meets this requirement with minimal operational overhead?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create a CloudWatch alarm on the AWS/Billing namespace EstimatedCharges metric",
      "B": "Configure an AWS Budget with email notifications when forecasted spend exceeds a threshold",
      "C": "Enable AWS Cost Anomaly Detection and configure alerts for anomaly events",
      "D": "Schedule daily Cost Explorer reports and parse them with Lambda for thresholds"
    },
    "explanation": "AWS Cost Anomaly Detection uses machine learning to detect unusual cost spikes and can send near real-time alerts without manual report parsing or budget forecasting."
  },
  {
    "taskStatement": "4.2",
    "stem": "A deployed SageMaker endpoint is experiencing intermittent increases in tail latency. The ML engineer needs to trace requests through the network stack and service mesh to pinpoint network bottlenecks. Which AWS service or feature should they enable?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run CloudWatch Logs Insights on the endpoint logs",
      "B": "Monitor EC2 CPUUtilization metrics in CloudWatch",
      "C": "Enable AWS X-Ray integration with SageMaker",
      "D": "Enable VPC Flow Logs on the endpoint\u2019s ENIs"
    },
    "explanation": "AWS X-Ray provides distributed tracing across network and service calls, enabling end-to-end latency analysis. VPC Flow Logs show packet metadata but not distributed service traces."
  },
  {
    "taskStatement": "4.2",
    "stem": "For compliance, the security team wants to audit all Amazon SageMaker CreateEndpoint API calls over the past 30 days and retain logs for 90 days. Which configuration achieves this with the LEAST administrative overhead and ensures immutability?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Enable CloudTrail management events for SageMaker and send to an S3 bucket with Object Lock enabled",
      "B": "Create a CloudWatch Events rule for CreateEndpoint and log events to CloudWatch Logs",
      "C": "Enable CloudTrail Data events on CreateEndpoint and stream to Kinesis Data Firehose",
      "D": "Enable CloudTrail Insights to capture anomalous CreateEndpoint activity"
    },
    "explanation": "CloudTrail management events capture all CreateEndpoint API calls; delivering them to an S3 bucket with Object Lock provides immutability. Data events and Insights are unnecessary for standard API logging."
  },
  {
    "taskStatement": "4.2",
    "stem": "A director wants a single dashboard that shows per-endpoint invocation count, average 95th percentile latency, and cost per hour for each SageMaker endpoint in the account. Which solution meets this requirement with minimal development effort?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Build a custom web app that calls CloudWatch metrics API and Cost Explorer API",
      "B": "Use Amazon QuickSight to visualize a dataset combining AWS Cost and Usage Reports and CloudWatch metrics",
      "C": "Use CloudWatch Dashboards with metric math combining performance and billing metrics",
      "D": "Use the AWS Pricing API to fetch rates and combine with CloudWatch metrics in dashboards"
    },
    "explanation": "QuickSight can natively ingest both Cost and Usage Report data (cost per resource) and CloudWatch metrics to build a unified dashboard with minimal custom code. CloudWatch Dashboards cannot display cost per resource granularity."
  },
  {
    "taskStatement": "4.2",
    "stem": "To reduce off-peak costs, an ML engineer wants to automatically update SageMaker endpoint instance types to smaller ones every night and revert to original sizes each morning. Which solution requires the LEAST operational overhead?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Use SageMaker Pipelines with a scheduled pipeline to call UpdateEndpointConfig",
      "B": "Create EventBridge cron rules that trigger Lambda functions invoking UpdateEndpointConfig",
      "C": "Configure AWS Step Functions with Wait states and Lambda tasks to update endpoints",
      "D": "Use Systems Manager Automation documents scheduled via State Manager"
    },
    "explanation": "Using EventBridge with cron schedules triggering lightweight Lambda functions to call UpdateEndpointConfig is the simplest and lowest-overhead scheduling solution."
  },
  {
    "taskStatement": "4.2",
    "stem": "A SageMaker real-time endpoint is exhibiting memory pressure, but memory utilization is not visible in CloudWatch by default. To monitor container-level memory for this endpoint, what should the ML engineer do?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable SageMaker Debugger profiling to collect memory metrics",
      "B": "Deploy the endpoint on EC2 instances where memory metrics are published by default",
      "C": "Install and configure the CloudWatch agent via a container lifecycle configuration",
      "D": "Use CloudWatch metric filters on container logs to estimate memory usage"
    },
    "explanation": "The CloudWatch agent must be installed inside the container via a lifecycle configuration script to collect OS-level metrics such as memory usage. Debugger profiles model internals, not OS memory."
  },
  {
    "taskStatement": "4.2",
    "stem": "An ML application experiences periodic bursts of inference requests that exceed the 70% CPU utilization threshold. The ML engineer wants to add step scaling to the existing auto scaling policy to provision two additional instances when InvocationsPerInstance exceeds 100 for 2 minutes, and remove one instance when it drops below 50 for 5 minutes. Which configuration meets these requirements?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Define two CloudWatch alarms on the SageMakerVariantInvocationsPerInstance metric with thresholds 100 (2-minute evaluation) and 50 (5-minute evaluation), and attach a step scaling policy with +2 and -1 adjustments",
      "B": "Use CPUUtilization alarms instead of InvocationsPerInstance and a target-tracking policy",
      "C": "Use a single target-tracking policy on InvocationsPerInstance with a target value of 75",
      "D": "Configure two AWS Budgets for usage and link them to auto scaling actions"
    },
    "explanation": "Step scaling requires separate CloudWatch alarms on the SageMakerVariantInvocationsPerInstance metric with specified evaluation periods and corresponding step adjustments (+2, -1). Using CPUUtilization or budgets would not meet the specified invocation-based requirements."
  },
  {
    "taskStatement": "4.2",
    "stem": "A financial ML endpoint must maintain 99th percentile latency under 200 ms. To automate scaling to achieve this SLO, which Application Auto Scaling policy should be configured?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Target tracking policy on CPUUtilization at 70%",
      "B": "Target tracking policy on SageMakerVariantInvocationLatency at the p99 200 ms target",
      "C": "Step scaling policy on InvocationsPerInstance thresholds",
      "D": "Step scaling policy on ModelLatencyAvg metric"
    },
    "explanation": "A target-tracking policy on the p99 lifecycle metric of SageMakerVariantInvocationLatency targeting a 200 ms threshold will automatically adjust capacity to maintain the latency SLO. CPUUtilization or average latency would be less precise for the p99 requirement."
  },
  {
    "taskStatement": "4.2",
    "stem": "After deploying a model, the team wants to trigger a retraining SageMaker pipeline whenever a new training dataset file is uploaded to S3. They also want to log all such retraining triggers for audit. Which combination of AWS services should they use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure an S3 EventBridge notification on object upload to start the pipeline, and rely on CloudTrail to log the pipeline StartPipelineExecution API call",
      "B": "Use a CloudWatch scheduled rule to scan S3 daily and start the pipeline if new files exist, logging via CloudWatch Logs",
      "C": "Deploy a Lambda function to poll S3 every hour, invoke the pipeline, and log to DynamoDB",
      "D": "Send S3 events to SNS and have the pipeline poll SNS, with audit in CloudWatch Metrics"
    },
    "explanation": "Using an S3 EventBridge notification provides immediate trigger of the pipeline on new data. CloudTrail automatically logs the StartPipelineExecution API call for auditing."
  },
  {
    "taskStatement": "4.2",
    "stem": "Which AWS service provides generalized compute resource recommendations, including SageMaker endpoints and EC2 instances, based on historical utilization metrics and can be applied across accounts with minimal configuration?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Inference Recommender",
      "B": "AWS Compute Optimizer",
      "C": "AWS Cost Explorer rightsizing recommendations",
      "D": "AWS Trusted Advisor"
    },
    "explanation": "AWS Compute Optimizer analyzes historical utilization across EC2, SageMaker endpoints, and other resources to recommend optimal instance types, whereas Inference Recommender focuses specifically on ML inference benchmarking."
  },
  {
    "taskStatement": "4.2",
    "stem": "An engineering team needs to break down monthly SageMaker spend by project. They require cost allocation by tagging endpoints and jobs, and reporting at tag granularity. Which steps should they take?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Tag SageMaker endpoints and training jobs with project identifiers, activate those tags in AWS Billing Cost Allocation Tags, and use Cost Explorer with tag filters",
      "B": "Create separate AWS accounts per project and use consolidated billing",
      "C": "Use CloudWatch metric dimensions to filter cost metrics",
      "D": "Group SageMaker resources into separate CloudFormation stacks and view stack costs"
    },
    "explanation": "Activating resource tags for cost allocation and filtering in Cost Explorer is the standard way to break down costs by project without needing separate accounts."
  },
  {
    "taskStatement": "4.2",
    "stem": "To optimize inference costs for stable production workloads with predictable traffic, which purchasing option should an ML engineer choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "On-Demand SageMaker Instances",
      "B": "SageMaker Savings Plans",
      "C": "EC2 Reserved Instances attached to SageMaker endpoints",
      "D": "SageMaker Spot Instances"
    },
    "explanation": "SageMaker Savings Plans offer a commitment discount on SageMaker compute usage for stable workloads. Spot Instances are not supported for real-time endpoints and Reserved Instances apply only to EC2, not SageMaker directly."
  },
  {
    "taskStatement": "4.2",
    "stem": "A model training workflow uses Amazon FSx for Lustre as input storage. The ML engineer notices frequent I/O throttling. To monitor FSx performance and identify bottlenecks, which CloudWatch metrics should they add to their dashboard?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "BurstCreditBalance and DataReadIOBytes",
      "B": "FreeStorageCapacity and NumberOfConnections",
      "C": "DataWriteIOPS and MetadataOperations",
      "D": "PercentIOPSUtilization and NetworkThroughput"
    },
    "explanation": "BurstCreditBalance shows available throughput credits and DataReadIOBytes shows actual read throughput, key metrics for diagnosing FSx for Lustre I/O throttling issues."
  },
  {
    "taskStatement": "4.2",
    "stem": "An ML engineer is troubleshooting intermittent 5XX errors from inference endpoints. The engineer has enabled data capture to S3. To quickly identify patterns in the errors, which approach provides the fastest operational insight?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure a CloudWatch Logs subscription filter to Lambda to parse S3 logs",
      "B": "Use CloudWatch Logs Insights with a query on the /aws/sagemaker/Endpoints log group to count HTTPStatus5XX occurrences",
      "C": "Download the S3 logs locally and run custom scripts",
      "D": "Use AWS X-Ray traces to calculate error percentages"
    },
    "explanation": "CloudWatch Logs Insights can rapidly query large volumes of log data in the /aws/sagemaker/Endpoints log group to identify and aggregate 5XX errors without moving data."
  },
  {
    "taskStatement": "4.3",
    "stem": "An ML engineer has deployed a SageMaker inference endpoint that writes inference results and debug logs to an S3 bucket encrypted with a customer-managed KMS key. The security team requires that only this endpoint can write to the bucket and decrypt the objects. Which configuration meets these requirements with the least administrative effort?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Attach an IAM policy to the SageMaker execution role allowing s3:PutObject on the bucket and kms:Decrypt on the CMK.",
      "B": "Configure the S3 bucket policy to allow s3:PutObject only from the VPC endpoint used by SageMaker.",
      "C": "Configure the S3 bucket policy with a condition aws:SourceArn equal to the SageMaker endpoint ARN, and update the KMS key policy to grant only the SageMaker execution role decrypt permissions.",
      "D": "Create an S3 ACL that grants the SageMaker service principal write access and rely on the IAM role for decryption."
    },
    "explanation": "A resource-based S3 bucket policy using aws:SourceArn ensures only that SageMaker endpoint can write objects, and the CMK key policy must explicitly grant the SageMaker execution role kms:Decrypt. IAM identity policies alone or ACLs cannot enforce both write and decrypt at the resource level as simply."
  },
  {
    "taskStatement": "4.3",
    "stem": "A security audit finds that any IAM user in the account can create SageMaker Studio user profiles, but the security team wants only members of the IAM group ProdMLUsers to be allowed. Which approach enforces this requirement at the SageMaker domain level?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add an identity-based IAM policy to the ProdMLUsers group allowing sagemaker:CreateUserProfile.",
      "B": "Attach a Service Control Policy in AWS Organizations denying sagemaker:CreateUserProfile unless the user is in ProdMLUsers.",
      "C": "Attach a resource-based policy to the SageMaker Domain that allows CreateUserProfile only when aws:PrincipalIsInGroup equals ProdMLUsers.",
      "D": "Use an S3 bucket policy to block studio creation from principals not in the group."
    },
    "explanation": "A SageMaker Domain resource policy can restrict CreateUserProfile to principals in a specific IAM group via aws:PrincipalIsInGroup. SCPs or identity policies alone cannot enforce at the domain resource level."
  },
  {
    "taskStatement": "4.3",
    "stem": "An organization wants to run SageMaker training jobs in a VPC with no internet egress, but the training data resides in S3. Which network configuration meets these requirements while minimizing cost and operational overhead?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provision a NAT Gateway in the private subnet to access S3.",
      "B": "Attach an Internet Gateway to the VPC and allow routing to S3.",
      "C": "Create a VPC Gateway endpoint for S3 and update the private route table to direct S3 traffic through it.",
      "D": "Create an Interface VPC endpoint for SageMaker in the VPC and rely on AWS private networking for S3 access."
    },
    "explanation": "A Gateway VPC endpoint for S3 allows private, cost-effective access to S3 from a VPC with no internet egress. An interface endpoint for SageMaker does not enable S3 data access, and a NAT Gateway adds cost."
  },
  {
    "taskStatement": "4.3",
    "stem": "A data science team wants to share a SageMaker Model Registry package with a partner AWS account. The model artifacts are encrypted by a customer-managed CMK. The partner must be able to deploy the model from their account. Which combination of steps is required?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "In the CMK console, add the partner account as a key administrator, and update the model package resource policy to grant DescribeModelPackage.",
      "B": "Add the partner AWS account principal to the CMK key policy with decrypt permissions, and attach a resource policy to the model package allowing that account DescribeModelPackage and CreateModel.",
      "C": "Enable cross-account sharing in SageMaker Model Registry settings and share the CMK ARN.",
      "D": "Create a cross-account IAM role in the partner account with sagemaker:CreateModel permission and trust the partner\u2019s account; no changes to CMK."
    },
    "explanation": "Both the CMK and the model package need resource policies. The key policy must allow the partner account to decrypt, and the model package policy must allow DescribeModelPackage and CreateModel so they can register and deploy it."
  },
  {
    "taskStatement": "4.3",
    "stem": "A centralized CodePipeline in Account A builds ML packages and needs to deploy a trained model to a SageMaker endpoint in Account B. The security team requires least privilege. How should the cross-account permissions be configured?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "In Account A, grant the CodePipeline role sagemaker:* across all resources in Account B and trust Account B\u2019s principals.",
      "B": "Use CloudFormation StackSets from Account A to provision SageMaker resources in Account B without IAM role assumption.",
      "C": "In Account B, create an IAM role with sagemaker:CreateEndpoint and related actions, trust policy allowing assumption by the CodePipeline role\u2019s ARN in Account A, and configure the pipeline to assume that role.",
      "D": "In Account B, add the CodePipeline service principal from Account A to an IAM group with full SageMaker privileges."
    },
    "explanation": "The pipeline in Account A should assume a dedicated IAM role in Account B that has only the permissions needed to deploy the SageMaker endpoint. This follows least-privilege and standard cross-account role assumption practices."
  },
  {
    "taskStatement": "4.3",
    "stem": "The security team requires that all SageMaker data-plane API calls (for example, CreateModel, InvokeEndpoint) be logged in CloudTrail. By default, only management events are recorded. What must the ML engineer do to capture these data-plane operations?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable CloudTrail management events in all regions.",
      "B": "In the CloudTrail console, enable data events for Amazon SageMaker to record data-plane API calls.",
      "C": "Enable AWS Config recording for SageMaker resource types.",
      "D": "Create CloudWatch Logs metric filters for SageMaker API calls."
    },
    "explanation": "To log data-plane API operations such as CreateModel and InvokeEndpoint, you must enable CloudTrail data events specifically for the SageMaker service. Management events alone do not capture these."
  },
  {
    "taskStatement": "4.3",
    "stem": "An enterprise wants to expose a private SageMaker inference endpoint to on-premises clients over their VPN without using the internet. Which architecture meets this requirement securely with minimal overhead?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy an Amazon NLB in front of the SageMaker endpoint and configure an Interface VPC endpoint (AWS PrivateLink) for the NLB; route on-prem VPN traffic to it.",
      "B": "Create an Internet-facing Application Load Balancer in front of the endpoint and restrict access via CIDR.",
      "C": "Peer the SageMaker VPC with the on-premises network and access the endpoint via peering.",
      "D": "Use AWS Transit Gateway to route to a public-facing endpoint with strong security groups."
    },
    "explanation": "Using a Network Load Balancer with a PrivateLink interface endpoint allows on-premises VPN clients to connect privately without exposing the endpoint to the internet, and with minimal additional components."
  },
  {
    "taskStatement": "4.3",
    "stem": "A security policy requires that every SageMaker training job include a CostCenter tag and use the execution role arn:aws:iam::123456789012:role/MLExecRole. Which feature can the ML engineer use to enforce both requirements at job creation?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use an IAM permissions boundary on ML users to restrict tag values.",
      "B": "Deploy an AWS Organizations SCP that denies sagemaker:CreateTrainingJob when tags or role do not match.",
      "C": "Enable an AWS Config rule for SageMaker jobs to require tags and roles.",
      "D": "Attach an IAM policy to users with a condition on sagemaker:RequestTag/CostCenter and sagemaker:ResourceTag/CostCenter and a condition requiring the UseServiceRole parameter equals MLExecRole."
    },
    "explanation": "An IAM identity policy with sagemaker:RequestTag and sagemaker:ResourceTag conditions can prevent job creation if the CostCenter tag is missing or if the specified ServiceRole is not MLExecRole. SCPs cannot inspect request tags."
  },
  {
    "taskStatement": "4.3",
    "stem": "A processing job running in SageMaker needs to mount an Amazon EFS file system for intermediate data. The security team requires encryption at rest and in transit for EFS, and only the processing job\u2019s subnets should be able to mount it. Which configuration meets these requirements?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create an unencrypted EFS, rely on encryption in transit only, and restrict mount via security group.",
      "B": "Create an encrypted EFS with SSE-KMS, mount it in the processing job with encryption in transit disabled.",
      "C": "Use FSx for Lustre with default encryption, and mount via a VPN.",
      "D": "Create an EFS file system encrypted at rest with a CMK, enable encryption in transit (TLS) on the mount target, and restrict the EFS security group to only allow mount traffic from the processing job\u2019s security group."
    },
    "explanation": "An Amazon EFS file system can be encrypted at rest using a CMK and can enforce TLS encryption in transit. Security groups can restrict mount access so that only SageMaker processing job instances can connect."
  },
  {
    "taskStatement": "4.3",
    "stem": "The security team wants only members of the SecurityAudit role to read SageMaker logs in CloudWatch Logs. Other users must be denied. How can this be implemented?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Attach a resource-based policy to the specific CloudWatch Log Group that allows only the SecurityAudit role to GetLogEvents and FilterLogEvents.",
      "B": "Encrypt the log group with a CMK and grant decrypt only to the SecurityAudit role.",
      "C": "Use an IAM identity policy denying CloudWatch Logs actions unless aws:PrincipalArn equals the SecurityAudit role.",
      "D": "Move the log group to a different AWS account used by security."
    },
    "explanation": "CloudWatch Log Groups support resource-based policies that can explicitly allow only a principal to perform log-reading actions. This is the most direct way to restrict access at the log-group level."
  },
  {
    "taskStatement": "4.3",
    "stem": "A CI/CD pipeline in CodePipeline uses an ECR repository for custom container images. The security team requires that only this pipeline can pull images and no other IAM principals. Which configuration enforces this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Attach an IAM policy to the pipeline\u2019s role allowing ecr:BatchGetImage on the repo.",
      "B": "Add an ECR repository resource policy that allows ecr:BatchGetImage for the CodePipeline service role ARN and denies all others.",
      "C": "Use SCP to deny ecr:BatchGetImage globally for all principals except the pipeline.",
      "D": "Rely on IAM identity policies on all users to not allow ECR actions."
    },
    "explanation": "An ECR repository policy can directly specify which principals (the pipeline role) are allowed to pull images and deny everyone else, enforcing container image access at the resource level."
  },
  {
    "taskStatement": "4.3",
    "stem": "The security team wants to prevent developers from creating any SageMaker notebook instances or Studio domains that are internet-facing. Which control can achieve this across the organization?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use an IAM permissions boundary for all developers disallowing sagemaker:CreateDomain.",
      "B": "Apply an AWS Organizations Service Control Policy that denies sagemaker:CreateDomain or CreateNotebookInstance when NetworkIsolation is false.",
      "C": "Attach an identity policy to each developer denying notebook creation.",
      "D": "Use AWS Config to detect and remediate non-VPC notebooks."
    },
    "explanation": "A Service Control Policy can centrally deny creation of SageMaker notebooks or domains unless the NetworkIsolation parameter is set to true, preventing internet access organization-wide."
  },
  {
    "taskStatement": "4.3",
    "stem": "An automated retraining workflow triggers a Lambda function via EventBridge. The function needs only permissions to DescribeEndpoint and CreateTrainingJob for a specific endpoint and training job prefix. How should the IAM role be defined to follow least privilege?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Grant sagemaker:* on all resources in the account.",
      "B": "Grant sagemaker:DescribeEndpoint and sagemaker:CreateTrainingJob on all SageMaker ARNs.",
      "C": "Grant sagemaker:DescribeEndpoint on the specific endpoint ARN and sagemaker:CreateTrainingJob with a resource ARN pattern matching the training-job-prefix*, and no other actions.",
      "D": "Attach AWS managed SageMaker full-access policy."
    },
    "explanation": "Defining resource-level permissions for only the specific endpoint and a wildcard training job prefix ensures the Lambda role has the minimum permissions required for its function."
  },
  {
    "taskStatement": "4.3",
    "stem": "Security requirements dictate that SageMaker notebook instances access internal services only on port 443 and cannot initiate outbound traffic to the internet. Which combination of controls enforces this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure the notebook instance\u2019s security group to allow outbound 443 to specific subnets and attach an Internet Gateway to the VPC.",
      "B": "Use a NAT Gateway with a route table restricting to internal CIDR.",
      "C": "Configure the SG to allow outbound TCP 443 to internal subnet CIDR, set the route table for the private subnet with no internet gateway, and use a VPC Endpoint for required AWS service calls.",
      "D": "Deploy a firewall appliance in the VPC to filter outbound traffic."
    },
    "explanation": "Removing an internet gateway and not configuring a NAT Gateway prevents internet egress. A security group can allow only TCP 443 to internal subnets. VPC endpoints enable needed AWS service access without the internet."
  },
  {
    "taskStatement": "4.3",
    "stem": "An ML engineer needs to capture inference requests and responses in a SageMaker endpoint. The security team requires that the data capture archive in S3 be encrypted at rest with a CMK and that all data in transit use TLS. Which configuration achieves this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable S3 default encryption on the bucket and configure DataCaptureConfig without specifying KMS.",
      "B": "Specify a KMS key in the SageMaker ModelRegistry settings and enable DataCaptureConfig.",
      "C": "Use an S3 bucket policy to require server-side encryption and rely on endpoint defaults.",
      "D": "In the endpoint\u2019s DataCaptureConfig, set EnableCapture true and specify the CMK KmsKeyId, and ensure the endpoint uses HTTPS (TLS) invocations."
    },
    "explanation": "DataCaptureConfig allows specifying a customer-managed KMS key for server-side encryption of captured data. SageMaker endpoints always use HTTPS for traffic, ensuring TLS in transit."
  },
  {
    "taskStatement": "1.1",
    "stem": "A data engineering team must transfer 100 TB of historical log files from an on-premises data center to Amazon S3 within 48 hours. The network bandwidth is limited to 1 Gbps and is cost-constrained. Which ingestion method meets the requirement with the lowest operational overhead?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS DataSync over the 1 Gbps link with parallel transfers.",
      "B": "Use S3 Transfer Acceleration over the internet connection.",
      "C": "Ship multiple AWS Snowball Edge devices and import data.",
      "D": "Use a custom multi-threaded upload client over the VPN."
    },
    "explanation": "At 1 Gbps, 100 TB over the network exceeds 48 hours; Snowball Edge avoids bandwidth constraints and minimizes operational complexity."
  },
  {
    "taskStatement": "1.1",
    "stem": "An ML workflow requires high-throughput reading of a 200 GB dataset during training in SageMaker. The data is stored in CSV format in S3. Training jobs are currently I/O-bound. Which change yields the greatest reduction in I/O time?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable S3 Transfer Acceleration on the bucket.",
      "B": "Convert the dataset to Parquet with Snappy compression and use Apache Arrow for reading.",
      "C": "Increase the SageMaker instance EBS volume throughput.",
      "D": "Enable S3 request rate optimization on the bucket."
    },
    "explanation": "Parquet is a columnar, compressed format that reduces data transfer size and read time, outperforming acceleration or EBS tweaks."
  },
  {
    "taskStatement": "1.1",
    "stem": "A streaming ingestion pipeline uses Amazon Kinesis Data Streams with 10 shards. Downstream training jobs require data partitioned by customer ID. The engineer notices uneven data distribution and hot shard errors. Which solution evenly distributes load?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase to 20 shards without changing partition key.",
      "B": "Use a composite primary key combining customer ID and timestamp.",
      "C": "Switch to Amazon MSK and let Kafka partition dynamically.",
      "D": "Hash the customer ID with a random suffix as the partition key."
    },
    "explanation": "Appending a random suffix to the customer ID spreads records across shards evenly, avoiding hot shards while preserving grouping on average."
  },
  {
    "taskStatement": "1.1",
    "stem": "A SageMaker Data Wrangler flow reads multiple small JSON files (~100 MB each) from S3. The job startup and cataloging latency dominates time. How can the engineer minimize this overhead?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Consolidate the small files into larger partitioned Parquet files.",
      "B": "Increase the SageMaker Data Wrangler instance type.",
      "C": "Use S3 Select on each JSON file.",
      "D": "Enable S3 Requester Pays on the bucket."
    },
    "explanation": "Consolidating files into fewer larger Parquet partitions reduces per-object overhead and speeds up scanning and schema inference."
  },
  {
    "taskStatement": "1.1",
    "stem": "An application uses Amazon Kinesis Data Firehose to deliver JSON records to S3. The delivery latency spikes under peak load. Which Firehose configuration change will smooth delivery latency while controlling costs?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set buffer size to minimum and buffer interval to default.",
      "B": "Set buffer size to maximum and buffer interval to minimum.",
      "C": "Increase buffer size to max and buffer interval to 300 seconds.",
      "D": "Decrease buffer size to minimum and buffer interval to 300 seconds."
    },
    "explanation": "Larger buffer size with a longer interval results in fewer deliveries, smoothing spikes and reducing egress costs."
  },
  {
    "taskStatement": "1.1",
    "stem": "A team must ingest near-real-time IoT telemetry (5 KB messages, 5,000 msg/sec) into S3 for batch training. Which AWS service combination best meets throughput and minimal operational overhead?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS IoT Core + Lambda writing to S3.",
      "B": "Kinesis Data Streams + Kinesis Data Firehose to S3.",
      "C": "MSK cluster + custom consumer that writes to S3.",
      "D": "Directly upload from devices to S3 using the AWS SDK."
    },
    "explanation": "Kinesis Streams scales to handle throughput and Firehose auto-batches and delivers to S3 without infra management."
  },
  {
    "taskStatement": "1.1",
    "stem": "A data scientist merges customer profile data from Amazon RDS and clickstream logs stored in S3. They need an automated, serverless solution with minimal code. Which approach is optimal?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Deploy an EMR cluster with Spark to join RDS and S3 datasets.",
      "B": "Use AWS Glue ETL jobs on a scheduled basis.",
      "C": "Use SageMaker Processing with custom Pandas scripts.",
      "D": "Use AWS Glue DataBrew recipe to connect to RDS and S3 and join datasets."
    },
    "explanation": "DataBrew provides serverless, no-code recipes to join RDS and S3 datasets, minimizing code and infra."
  },
  {
    "taskStatement": "1.1",
    "stem": "A company requires secure ingestion of PII data into Amazon S3. Data must be encrypted in transit and at rest, and decryption only occurs in SageMaker. Which configuration satisfies these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable SSE-S3 on the bucket and HTTPS for uploads.",
      "B": "Configure HTTPS for uploads, enable SSE-KMS with a CMK granting only SageMaker decryption.",
      "C": "Use client-side encryption and store keys in AWS Secrets Manager.",
      "D": "Enable SSE-KMS and grant all users read access to the CMK."
    },
    "explanation": "SSE-KMS with CMK access restricted to SageMaker ensures encrypted transit and at-rest encryption, with decryption only in SageMaker."
  },
  {
    "taskStatement": "1.1",
    "stem": "An ML pipeline uses Amazon MSK to stream data into S3 via Kafka Connect. During peak, Kafka Connect tasks lag behind. Which change will improve ingestion throughput?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase Kafka Connect workers and partitions for the connector topic.",
      "B": "Switch connector batch.size to 1.",
      "C": "Use a single large Kafka Connect worker.",
      "D": "Decrease connector retries to zero."
    },
    "explanation": "More workers and partitions allow parallelism in Kafka Connect, increasing ingestion throughput and reducing lag."
  },
  {
    "taskStatement": "1.1",
    "stem": "A training job on SageMaker needs to read a dataset stored on Amazon FSx for Lustre for low-latency I/O. The dataset originates in S3. Which configuration provides the fastest initial data access?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Mount FSx for Lustre and copy data manually from S3.",
      "B": "Use AWS DataSync to mirror S3 to FSx for Lustre.",
      "C": "Use FSx\u2019s native S3 integration by specifying the S3 bucket as the data repository.",
      "D": "Use a SageMaker Processing job to prefetch data into EBS."
    },
    "explanation": "FSx for Lustre native S3 integration transparently caches and streams S3 data into the file system without manual copy."
  },
  {
    "taskStatement": "1.1",
    "stem": "A dataset stored in S3 is consumed by multiple SageMaker training instances concurrently. To minimize read latency and S3 request costs, what ingestion pattern should be used?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Each instance downloads the full dataset to its EBS volume.",
      "B": "Use S3 Select API within training scripts.",
      "C": "Use a shared EFS mount backed by S3.",
      "D": "Use S3 Replication with Cross-Region Replication disabled and parallel GETs with HTTP Keep-Alive."
    },
    "explanation": "Parallel GETs with HTTP Keep-Alive reuse connections, reducing latency and request overhead compared to duplicated EBS or EFS."
  },
  {
    "taskStatement": "1.1",
    "stem": "A company needs to ingest 10 TB of data daily from S3 into SageMaker Feature Store with minimal lag. Which approach scales ingestion while controlling costs?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use service API PutRecord sequentially for each feature.",
      "B": "Use the batch ingest offline feature with RecordIO protobuf files.",
      "C": "Use real-time endpoint calls for each record.",
      "D": "Use Glue Streaming to write directly to Feature Store."
    },
    "explanation": "Batch ingest with RecordIO is optimized for large-scale ingestion into Feature Store and is cost-effective compared to per-record API calls."
  },
  {
    "taskStatement": "1.1",
    "stem": "An engineer must ingest data from multiple AWS accounts into a central S3 data lake. Data producers may intermittently lose network connectivity. Which ingestion pattern ensures reliability and scalability?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use S3 Cross-Account replication with retry policies.",
      "B": "Use a central Firehose delivery stream with producers as PUT clients.",
      "C": "Use AWS DataSync agents in each account.",
      "D": "Use AWS Transfer Family SFTP endpoints in the central account."
    },
    "explanation": "Cross-Account replication handles intermittent producers with built-in retry and scales to multi-account sources without agents."
  },
  {
    "taskStatement": "1.1",
    "stem": "A SageMaker processing job fails intermittently reading from an S3 bucket due to 503 throttling errors. Which change reduces the throttling while optimizing cost?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable S3 Transfer Acceleration.",
      "B": "Increase the processing instance count.",
      "C": "Enable S3 Requester Pays and use signed requests with exponential backoff in the SDK.",
      "D": "Switch the bucket to use SSE-C encryption."
    },
    "explanation": "Signed requests with retry/backoff reduce throttling; Requester Pays can shift cost but throttling reduction via SDK backoff is key."
  },
  {
    "taskStatement": "1.1",
    "stem": "A compliance team requires that logs ingested into S3 by Kinesis Firehose are immutable and versioned. Which S3 bucket configuration achieves this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable MFA Delete and SSE-S3.",
      "B": "Enable Object Lock in Governance mode with versioning.",
      "C": "Enable default encryption with SSE-KMS.",
      "D": "Enable Lifecycle rules to transition logs to Glacier."
    },
    "explanation": "Object Lock with versioning set to Governance mode enforces immutability on ingested S3 objects."
  },
  {
    "taskStatement": "1.1",
    "stem": "During a large dataset migration, objects are written to an S3 bucket in parallel. The team observes increased 503 errors. Which bucket-level configuration resolves this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable S3 Transfer Acceleration.",
      "B": "Enable default encryption with SSE-KMS.",
      "C": "Enable MFA Delete on the bucket.",
      "D": "Enable S3 request rate optimization on the bucket."
    },
    "explanation": "Request rate optimization (prefix deprecation) reduces 503 errors under high parallel write rates by distributing keys."
  },
  {
    "taskStatement": "1.1",
    "stem": "A pipeline uses AWS Glue to extract data from an Amazon Aurora MySQL database to S3. The Glue job times out during snapshot extraction. Which approach resolves the timeout while minimizing developer effort?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase the connection timeout in the Glue job script.",
      "B": "Use AWS Database Migration Service with full load to S3.",
      "C": "Split the Glue job into multiple smaller jobs.",
      "D": "Export Aurora snapshot to S3 using native export."
    },
    "explanation": "Using DMS for full load to S3 handles large data extracts reliably with minimal custom code versus Glue timeouts."
  },
  {
    "taskStatement": "1.1",
    "stem": "An ML engineer must ingest encrypted CSV files from S3 to SageMaker Processing. The files are encrypted with SSE-KMS using a custom CMK. The processing role lacks decrypted access. What adjustment grants the least privilege?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Attach an IAM policy to the processing role allowing kms:Decrypt on the CMK.",
      "B": "Re-encrypt the files with SSE-S3.",
      "C": "Use client-side encryption and share the key.",
      "D": "Grant the role AmazonS3FullAccess."
    },
    "explanation": "Granting kms:Decrypt on the CMK to the processing role grants only needed permissions rather than broad S3 access."
  },
  {
    "taskStatement": "1.1",
    "stem": "A data lake uses S3 with AWS Lake Formation. Data ingestion from streaming sources must respect table-level LF permissions. Which integration ensures permissions enforcement?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Kinesis Data Firehose direct to S3 outputs.",
      "B": "Use Lambda to write to S3 then catalog with Glue.",
      "C": "Use Lake Formation transactions with AWS Glue streaming jobs.",
      "D": "Use Athena to INSERT INTO Lake Formation tables."
    },
    "explanation": "Lake Formation transactions in Glue streaming enforce table-level LF permissions during ingestion."
  },
  {
    "taskStatement": "1.1",
    "stem": "An ML pipeline ingests files via S3 Event Notifications to Lambda. Under high throughput, some events are lost. Which change ensures all records are ingested?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase the Lambda concurrency limit.",
      "B": "Use SQS as the Event Notification target before invoking Lambda.",
      "C": "Switch to SNS Event Notifications.",
      "D": "Enable retry behavior in S3."
    },
    "explanation": "Using SQS between S3 and Lambda buffers events, ensuring delivery and retry rather than direct event loss."
  },
  {
    "taskStatement": "1.1",
    "stem": "A company plans to centralize logs from multiple regions into a single S3 bucket. Logs must remain in native partition folders. How should a cross-region replication rule be configured?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a replication rule with IncludePuts and prefix replication per region.",
      "B": "Use replication time control to replicate objects hourly.",
      "C": "Use batch Athena CTAS to copy logs across regions.",
      "D": "Use DataSync to mirror entire buckets."
    },
    "explanation": "A replication rule with region-specific prefixes replicates objects into same partition paths in central bucket."
  },
  {
    "taskStatement": "1.1",
    "stem": "A secure pipeline must ingest data from Amazon DynamoDB to S3 for ML training. The transfer must be encrypted, serverless, and cost-efficient. Which service accomplishes this?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Use EMR Spark job to export DynamoDB to S3.",
      "B": "Use AWS DataSync to copy table exports.",
      "C": "Use AWS Glue with DynamoDB connector.",
      "D": "Use DynamoDB Export to S3 with SSE-KMS encryption."
    },
    "explanation": "DynamoDB native Export to S3 is serverless, encrypted, and cost-efficient compared to EMR or Glue."
  },
  {
    "taskStatement": "1.1",
    "stem": "An ML pipeline needs to ingest image files from multiple SFTP servers into S3. The process must be fully AWS managed and scalable. Which solution meets these requirements?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Deploy EC2 instances running custom SFTP-to-S3 scripts.",
      "B": "Use AWS DataSync agents on EC2.",
      "C": "Use AWS Transfer Family SFTP endpoints with custom identity providers and S3 backing.",
      "D": "Use Glue FTP connector."
    },
    "explanation": "AWS Transfer Family provides managed SFTP endpoints that store files directly in S3, scaling without custom servers."
  },
  {
    "taskStatement": "1.1",
    "stem": "A pipeline needs to ingest sensor data at 100 MB/s into S3 for real-time analytics. Which combination of services and configurations meets throughput while controlling costs?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Kinesis Data Streams with 100 shards directly writing to S3.",
      "B": "MSK with 5 broker nodes, MirrorMaker to Firehose with gzip compression.",
      "C": "Direct Firehose delivery with default shard count.",
      "D": "Lambda triggered by S3 upload events."
    },
    "explanation": "MSK scales to high throughput; MirrorMaker populates Firehose which batches and compresses data to S3 cost-effectively."
  },
  {
    "taskStatement": "1.1",
    "stem": "During ingest of nested JSON event data into S3, the team needs to infer schema and partition by event date. Which AWS Glue configuration fulfills this with minimal code?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Use a custom PySpark Glue job to parse JSON and write Parquet.",
      "B": "Use Athena CTAS to convert and partition data.",
      "C": "Use SageMaker Processing to flatten and write to S3.",
      "D": "Use AWS Glue JSON classifier with crawler partitioning on event_date field."
    },
    "explanation": "A Glue crawler with JSON classifier can infer schema and apply partitioning on a specified JSON field automatically."
  },
  {
    "taskStatement": "1.1",
    "stem": "A dataset must be ingested daily from an external partner to S3 via the internet. The partner cannot use AWS SDK. How should the company securely and reliably ingest files?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provision an AWS Transfer Family SFTP endpoint backed by the S3 bucket.",
      "B": "Provide S3 PUT URLs with public write ACL.",
      "C": "Use HTTP POST to S3 REST API with credentials.",
      "D": "Ask partner to email files and upload manually."
    },
    "explanation": "Transfer Family SFTP endpoint allows partners to upload over SFTP without SDK, delivering files directly and securely to S3."
  },
  {
    "taskStatement": "1.1",
    "stem": "The team needs to ingest 500 GB of data daily from an on-prem Hadoop cluster to S3 under network constraints. Which approach provides incremental ingestion and optimizes bandwidth?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Snowball Edge weekly ingestion.",
      "B": "Deploy AWS DataSync agent with incremental sync enabled.",
      "C": "Use a Glue job with JDBC connector.",
      "D": "Use AWS Glue DataBrew to pull data."
    },
    "explanation": "DataSync incremental sync only transfers changed data daily, optimizing bandwidth and automating ingestion."
  },
  {
    "taskStatement": "1.1",
    "stem": "An ML pipeline reads an S3 directory of Avro files during training. The engineer wants to reduce startup latency when listing thousands of files. Which S3 configuration reduces LIST cost?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable SSE-KMS on the bucket.",
      "B": "Enable bucket versioning.",
      "C": "Use S3 Inventory to generate daily manifest and use the manifest for file listing.",
      "D": "Enable S3 Transfer Acceleration."
    },
    "explanation": "S3 Inventory provides a manifest file listing objects, eliminating costly API ListObjects calls during job startup."
  },
  {
    "taskStatement": "1.1",
    "stem": "A team needs to ingest HTTP logs into S3 for historical analysis. Logs are generated at 10 GB/hour. The solution must auto-scale and require no server management. Which ingestion architecture is best?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "EMR cluster with Flink reading logs.",
      "B": "Kinesis Data Firehose with Lambda data transformation to S3.",
      "C": "Custom ECS service tailing logs and writing to S3.",
      "D": "S3 multipart upload from application servers."
    },
    "explanation": "Firehose auto-scales ingestion and can deliver data to S3 with optional Lambda transformation, requiring no servers."
  },
  {
    "taskStatement": "1.1",
    "stem": "A multi-region application produces data that must land in a centralized S3 bucket in the same AWS Region as processing. Replication must preserve object metadata and ACLs. Which configuration meets this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use CloudTrail replication to central bucket.",
      "B": "Configure Firehose across regions.",
      "C": "Use DataSync cross-region replication.",
      "D": "Use cross-region replication with replicate object metadata and ACL."
    },
    "explanation": "S3 cross-region replication configured to replicate metadata and ACLs preserves all object attributes in central bucket."
  },
  {
    "taskStatement": "1.1",
    "stem": "A SageMaker training job must read millions of small JSON files from S3. The engineer notices high GET request costs. Which solution reduces GETs and maintains low latency?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Combine JSON files into larger S3 objects, e.g., Parquet, using a Glue job.",
      "B": "Enable S3 Intelligent-Tiering.",
      "C": "Use S3 Select for each JSON file.",
      "D": "Use Lifecycle rules to consolidate files."
    },
    "explanation": "Combining small files into larger Parquet objects reduces individual GET calls and lowers request costs."
  },
  {
    "taskStatement": "1.1",
    "stem": "To ingest clickstream events into S3 with exactly-once semantics, an engineer uses Kinesis Data Streams and Lambda to write to S3. At times, duplicates occur. Which pattern ensures exactly-once writes?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use PutObject with versioning enabled.",
      "B": "Use S3 Multipart upload with unique upload IDs.",
      "C": "Use Kinesis Data Firehose with S3 delivery and deduplication via record ID.",
      "D": "Use DynamoDB streams and batch writes to S3."
    },
    "explanation": "Firehose supports exactly-once delivery semantics to S3 when configured with record IDs for deduplication."
  },
  {
    "taskStatement": "1.1",
    "stem": "A data lake's S3 bucket is configured with CloudWatch Events to start Glue crawlers upon new object creation. Under heavy load, some crawlers start late and miss events. How ensure timely ingestion?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase crawler concurrency in Glue.",
      "B": "Introduce a Step Functions state machine triggered by EventBridge to queue and throttle crawler invocations.",
      "C": "Use Lambda to start crawlers directly.",
      "D": "Enable S3 Intelligent-Tiering."
    },
    "explanation": "Using Step Functions to queue and throttle ensures orderly crawler invocations without overload compared to direct triggers."
  },
  {
    "taskStatement": "2.2",
    "stem": "An ML engineer is training a PyTorch model in SageMaker script mode on a single GPU instance. The training job often runs out of GPU memory when using a batch size of 256. To continue training with minimal code changes and without incurring additional infrastructure cost, which action should the engineer take?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable mixed precision training (automatic mixed precision) in the training script.",
      "B": "Switch to SageMaker built-in algorithm XGBoost with the same batch size.",
      "C": "Decrease the number of training epochs to reduce memory footprint.",
      "D": "Use a larger EC2 instance type with more GPU memory."
    },
    "explanation": "Mixed precision reduces memory usage with minimal code changes. Switching algorithms or decreasing epochs doesn\u2019t reduce per-batch memory; upgrading instance increases cost."
  },
  {
    "taskStatement": "2.2",
    "stem": "A data scientist uses SageMaker Automatic Model Tuning (AMT) with Bayesian optimization to tune a deep neural network. The search is converging too slowly. The search space contains 10 continuous and 5 categorical hyperparameters. What adjustment will most speed up convergence?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch from Bayesian optimization to random search.",
      "B": "Increase the maximum number of training jobs in the tuning job.",
      "C": "Reduce the hyperparameter search space by fixing low-impact parameters to default values.",
      "D": "Use an IDENTICAL_DATA_AND_ALGORITHM warm start tuning job type."
    },
    "explanation": "Reducing search space focuses Bayesian optimization on impactful parameters. Random search may be less efficient; warm start of type IDENTICAL_DATA_AND_ALGORITHM duplicates prior runs, not solving slow convergence."
  },
  {
    "taskStatement": "2.2",
    "stem": "An ML engineer trains a large transformer model using SageMaker script mode with TensorFlow. Training takes 48 hours. The engineer wants to halve training time with minimal hyperparameter changes. Which approach meets this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Double the batch size and halve the number of epochs.",
      "B": "Enable distributed data parallel training across two GPU instances.",
      "C": "Use L2 regularization with higher weight to speed up convergence.",
      "D": "Decrease learning rate and increase number of epochs."
    },
    "explanation": "Distributed training splits workload across GPUs, reducing wall-clock time. Changing regularization or learning rate affects training dynamics, not time directly; doubling batch/halving epochs may hurt convergence."
  },
  {
    "taskStatement": "2.2",
    "stem": "A tabular dataset shows early overfitting: validation loss increases after 5 epochs. Which combination of techniques best reduces overfitting while preserving model capacity?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase batch size and remove dropout layers.",
      "B": "Decrease learning rate and increase number of layers.",
      "C": "Add L2 regularization with high weight and increase epochs.",
      "D": "Implement dropout, add L2 weight decay, and enable early stopping after no validation improvement for 3 epochs."
    },
    "explanation": "Dropout+L2 regularization reduce overfitting; early stopping avoids unnecessary epochs. Increasing layers or removing dropout worsens overfitting."
  },
  {
    "taskStatement": "2.2",
    "stem": "A company uses a SageMaker built-in XGBoost algorithm to classify credit risk. The model size on disk is 1.5 GB, exceeding edge device storage limit of 500 MB. Which strategy best reduces model size without retraining from scratch?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Decrease max_depth and num_round in a new training job.",
      "B": "Use post-training model pruning and quantization using SageMaker Neo compilation.",
      "C": "Enable L1 regularization in the current model registry version.",
      "D": "Decrease the learning rate and increase the number of trees."
    },
    "explanation": "SageMaker Neo can prune and quantize a compiled model without retraining. Changing training hyperparameters requires retraining."
  },
  {
    "taskStatement": "2.2",
    "stem": "An ML engineer fine-tunes a pre-trained BERT model on SageMaker JumpStart. To prevent catastrophic forgetting of pre-trained weights, which configuration should they apply?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use lower learning rate for pre-trained layers and higher rate for newly added classification head.",
      "B": "Initialize all layers randomly before fine-tuning.",
      "C": "Freeze the classification head and only train transformer layers.",
      "D": "Increase batch size by fourfold to stabilize gradients."
    },
    "explanation": "Lower LR on pretrained layers preserves learned features; random init or freezing head loses benefits."
  },
  {
    "taskStatement": "2.2",
    "stem": "During hyperparameter tuning with AMT, an engineer observes that many training jobs fail due to out-of-memory errors for large batch sizes. To optimize tuning efficiency, which AMT configuration change is best?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase the maximum parallel training jobs.",
      "B": "Switch from Bayesian to grid search.",
      "C": "Decrease the minimum number of training jobs.",
      "D": "Add a conditional statement in the training script to skip batch sizes causing OOM and return low objective metric."
    },
    "explanation": "Skipping invalid hyperparameter combinations via script invalidation avoids wasted jobs; changing search strategy or job counts doesn\u2019t prevent failures."
  },
  {
    "taskStatement": "2.2",
    "stem": "A deep CNN trained in SageMaker exhibits slow convergence and high variance among epoch losses. Which combination of techniques addresses both issues?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Remove data augmentation and add more layers.",
      "B": "Increase learning rate and decrease batch size.",
      "C": "Apply batch normalization, use learning rate scheduler, and add dropout layers.",
      "D": "Switch optimizer from Adam to stochastic gradient descent with fixed LR."
    },
    "explanation": "Batch norm and LR scheduling stabilize training; dropout reduces variance. Removing augmentation or switching optimizer without scheduler may worsen convergence."
  },
  {
    "taskStatement": "2.2",
    "stem": "An ML engineer needs to integrate a scikit-learn RandomForest model trained locally into SageMaker for batch transform inference. Which step is required?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Wrap the model in a SageMaker built-in algorithm container.",
      "B": "Create a custom inference script and container using SageMaker script mode to load the pickle file.",
      "C": "Use SageMaker Clarify to register the model directly from local filesystem.",
      "D": "Upload the model artifact to SageMaker Model Registry via console."
    },
    "explanation": "Local models require a custom container/script in script mode. Built-in algorithms can\u2019t import arbitrary pickles."
  },
  {
    "taskStatement": "2.2",
    "stem": "A large NLP model fine-tuned via Hugging Face in SageMaker shows slow throughput per epoch. Which SageMaker feature reduces training time without code changes?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable early stopping in AMT.",
      "B": "Use SageMaker Model Monitor to detect drift.",
      "C": "Enable managed spot training with distributed training.",
      "D": "Switch to a built-in algorithm for NLP tasks."
    },
    "explanation": "Managed distributed spot training uses multiple instances to speed training; early stopping and monitoring don\u2019t affect raw throughput."
  },
  {
    "taskStatement": "2.2",
    "stem": "An engineer uses SageMaker script mode to train a PyTorch model. The training time per epoch increases over time due to GPU memory fragmentation. How to mitigate this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable gradient checkpointing to reduce peak memory usage.",
      "B": "Increase batch size to better pack memory.",
      "C": "Disable data parallelism to reduce fragmentation.",
      "D": "Switch from PyTorch to TensorFlow."
    },
    "explanation": "Gradient checkpointing trades compute for memory, alleviating fragmentation. Increasing batch size worsens it; switching frameworks is heavy-handed."
  },
  {
    "taskStatement": "2.2",
    "stem": "A SageMaker AMT job tunes hyperparameters including learning_rate and optimizer choice. The team needs to incorporate prior tuning results to guide the new job. Which warm start type is appropriate?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "IDENTICAL_DATA_AND_ALGORITHM",
      "B": "TRANSFER_LEARNING",
      "C": "STANDARD",
      "D": "TRANSFER_LEARNING with previous job as parent"
    },
    "explanation": "TRANSFER_LEARNING warm start uses prior results from parent tuning jobs; IDENTICAL_DATA_AND_ALGORITHM reuses only matching configs."
  },
  {
    "taskStatement": "2.2",
    "stem": "An ML researcher wants repeated reproducible experiments in SageMaker. Which practice ensures identical results across training runs?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use random search in AMT.",
      "B": "Use spot instances to save costs.",
      "C": "Enable early stopping after fixed epochs.",
      "D": "Set and log explicit random seeds, fix training hyperparameters, use the Model Registry for versioning."
    },
    "explanation": "Explicit seed and consistent config plus registry ensure reproducibility; early stopping and spot instances introduce variability."
  },
  {
    "taskStatement": "2.2",
    "stem": "A team notices high variance between ensemble members when combining models. To improve ensemble performance, which strategy is best?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use identical architecture and data splits for each member.",
      "B": "Use diverse base learners and bootstrap sampling to increase diversity.",
      "C": "Increase depth of all tree-based models.",
      "D": "Use only the top-performing single model."
    },
    "explanation": "Ensemble benefits from diverse learners; identical architectures reduce benefit."
  },
  {
    "taskStatement": "2.2",
    "stem": "An engineer needs to fine-tune a large foundation model from Amazon Bedrock in SageMaker for a specialized downstream task. Which approach integrates seamlessly?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Export the model weights and import into a SageMaker built-in algorithm.",
      "B": "Download the model artifacts and retrain from scratch.",
      "C": "Use SageMaker JumpStart to fine-tune the model with custom dataset.",
      "D": "Convert Bedrock model to TensorFlow SavedModel and deploy."
    },
    "explanation": "JumpStart provides ready fine-tuning pipelines for Bedrock foundation models."
  },
  {
    "taskStatement": "2.2",
    "stem": "A deep learning model exhibits underfitting on training and validation sets. To improve fit, which hyperparameter adjustment is most effective?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase model capacity by adding layers or units.",
      "B": "Increase dropout rate.",
      "C": "Increase L2 regularization weight.",
      "D": "Decrease number of training epochs."
    },
    "explanation": "Underfitting requires more capacity; dropout and L2 worsen underfitting."
  },
  {
    "taskStatement": "2.2",
    "stem": "An automatic model tuning job uses AMT with a goal metric of validation error. The team now needs to optimize two metrics: inference latency and accuracy. Which solution supports this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run two separate AMT jobs and manually choose trade-off.",
      "B": "Use Bayesian optimization with a composite metric manually computed in script.",
      "C": "Switch search strategy to grid search for multi-objective coverage.",
      "D": "Use SageMaker multi-objective hyperparameter tuning-enabled job."
    },
    "explanation": "Multi-objective tuning in SageMaker supports simultaneous optimization of metrics."
  },
  {
    "taskStatement": "2.2",
    "stem": "A training job using Hugging Face Transformers in SageMaker uses a JSON Lines dataset on S3. The engineer observes slow data loading. Which training configuration change accelerates data throughput?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch input mode to Pipe mode.",
      "B": "Use File mode with Amazon FSx for Lustre linked to the S3 bucket.",
      "C": "Decrease number of worker processes in DataLoader.",
      "D": "Disable shuffling of the dataset."
    },
    "explanation": "FSx for Lustre caches data for high throughput; pipe mode streams raw S3, slower for many small records."
  },
  {
    "taskStatement": "2.2",
    "stem": "An ML engineer wants to prune a TensorFlow model during SageMaker training to reduce inference latency while preserving >98% of accuracy. Which training callback should be used?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "EarlyStopping on validation loss.",
      "B": "LearningRateScheduler.",
      "C": "TensorFlow Model Optimization pruning callback.",
      "D": "ReduceLROnPlateau."
    },
    "explanation": "TF Model Optimization pruning callback prunes weights during training; others adjust learning rate or stop early."
  },
  {
    "taskStatement": "2.2",
    "stem": "A model trained on SageMaker exhibits high training throughput but low GPU utilization. Which configuration change most directly improves GPU utilization?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase per-instance batch size.",
      "B": "Decrease the number of GPUs.",
      "C": "Enable early stopping.",
      "D": "Use a smaller instance type."
    },
    "explanation": "Batch size affects GPU utilization; larger batches fill GPU compute better."
  },
  {
    "taskStatement": "2.2",
    "stem": "A team uses AMT with max_jobs=50 and max_parallel_jobs=5. They need to reduce total run time but keep exploring same search space. Which change achieves this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase search space cardinality.",
      "B": "Increase max_parallel_jobs to 10.",
      "C": "Decrease max_jobs to 25.",
      "D": "Switch to random search strategy."
    },
    "explanation": "Increasing parallel jobs reduces wall-clock time; random search doesn\u2019t guarantee faster convergence."
  },
  {
    "taskStatement": "2.2",
    "stem": "An existing model version in SageMaker Model Registry is retrained with new hyperparameters. To automate promoting the version when validation accuracy improves by >1%, which approach is best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Manually compare metrics and click Promote in console.",
      "B": "Use SageMaker Experiments to tag and promote.",
      "C": "Define a SageMaker Pipeline with conditional ModelRegistry promotion step.",
      "D": "Write a Lambda triggered by CloudWatch alarm on metrics."
    },
    "explanation": "Pipelines support conditional promotion; Lambda approach requires custom code and integration."
  },
  {
    "taskStatement": "2.2",
    "stem": "During distributed training on multiple GPU instances, training noise leads to divergence. What change reduces variance across replicas?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase initial learning rate.",
      "B": "Disable gradient clipping.",
      "C": "Use smaller batch size per replica.",
      "D": "Enable synchronized BatchNorm across replicas."
    },
    "explanation": "Synchronized BatchNorm ensures consistent statistics; other changes may worsen instability."
  },
  {
    "taskStatement": "2.2",
    "stem": "A SageMaker TensorFlow training job time per epoch is dominated by data preprocessing in training script. How to offload preprocessing to speed training?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Processing or Glue to preprocess and save TFRecords before training.",
      "B": "Enable profiler for training jobs.",
      "C": "Switch to script mode with Python SDK.",
      "D": "Use hyperparameter tuning to find optimal preprocessing parameters."
    },
    "explanation": "Preprocessing offline reduces per-epoch overhead; tuning and profiling don\u2019t offload work."
  },
  {
    "taskStatement": "2.2",
    "stem": "An engineer needs to train a model with imbalanced classes. To ensure balanced gradient updates per batch, which DataLoader or training config change is best?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Shuffle dataset each epoch.",
      "B": "Use a weighted random sampler to form balanced batches.",
      "C": "Increase number of epochs.",
      "D": "Add L1 regularization to loss."
    },
    "explanation": "Weighted sampler balances classes per batch; shuffling alone doesn\u2019t ensure balance."
  },
  {
    "taskStatement": "2.2",
    "stem": "A team wants to combine three diverse models into a final prediction in SageMaker. They need an ensemble pipeline that retrains all members and the meta-learner automatically. Which SageMaker feature should they use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Batch Transform.",
      "B": "SageMaker Clarify.",
      "C": "SageMaker Neo.",
      "D": "SageMaker Pipelines with Ensemble step implementations."
    },
    "explanation": "Pipelines can orchestrate multi-step ensemble training; other services unrelated."
  },
  {
    "taskStatement": "1.2",
    "stem": "You have a dataset with highly skewed numeric features and missing values partitioned across multiple S3 prefixes. You need a pipeline that imputes missing values with the median per feature, applies a log transform to reduce skew, and writes the cleaned data to Amazon SageMaker Feature Store. Which approach meets these requirements with the least custom code?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use an AWS Glue Spark ETL job to compute medians via AWS Glue Data Quality, apply a PySpark log transform, and write to Feature Store using the SageMaker SDK.",
      "B": "Use a SageMaker Data Wrangler flow: add a fill-missing recipe with median, add a built-in log transform step, then export directly to Feature Store.",
      "C": "Use AWS Glue DataBrew: create a project, add fill-missing and log transform recipes, and publish to S3 for later ingestion into Feature Store.",
      "D": "Run a SageMaker Processing job with a custom Scikit-Learn script that imputes medians, applies log transforms, and writes to Feature Store."
    },
    "explanation": "Data Wrangler provides built-in median imputation and log transform steps and can export directly to Feature Store, minimizing custom code."
  },
  {
    "taskStatement": "1.2",
    "stem": "A streaming application ingests JSON records into Amazon Kinesis Data Streams. Each record contains nested user attributes and a categorical field with dozens of categories. You must extract the nested field, one-hot encode the categorical feature, and persist the output in S3 in Parquet format. Which solution is most operationally efficient?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Kinesis Data Analytics for Apache Flink with SQL to flatten JSON, then implement a user-defined function for one-hot encoding, and sink to S3.",
      "B": "Trigger an AWS Lambda function on Kinesis shards to parse JSON, use pandas.get_dummies for encoding, and write Parquet to S3.",
      "C": "Use SageMaker Data Wrangler with a streaming data source, apply transforms, and export to S3.",
      "D": "Use AWS Glue Streaming ETL with Apache Spark: apply JSON extract, use Spark ML OneHotEncoderEstimator, and write Parquet to S3."
    },
    "explanation": "AWS Glue Streaming ETL natively supports Spark transformations on streaming data, including JSON flattening and OneHotEncoder, with minimal custom code and built-in S3 sink."
  },
  {
    "taskStatement": "1.2",
    "stem": "You have a multi-class categorical feature with 1000 unique values, most of which appear in fewer than 5% of the records. You need to encode it for a tree-based model without creating high dimensionality. Which feature-engineering technique should you apply in SageMaker Data Wrangler?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use one-hot encoding with drop-last to reduce one dimension.",
      "B": "Apply label encoding directly to assign integer codes.",
      "C": "Group infrequent categories into an \"Other\" bucket, then apply one-hot encoding on the top categories.",
      "D": "Apply ordinal encoding based on average target probability per category."
    },
    "explanation": "Combining infrequent categories into \"Other\" reduces cardinality before one-hot encoding, balancing dimensionality and model interpretability."
  },
  {
    "taskStatement": "1.2",
    "stem": "A text column contains customer reviews that must be tokenized and vectorized for an NLP model. You need to integrate this into a SageMaker feature-engineering pipeline with minimal custom code. Which service and sequence accomplishes this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Glue DataBrew: add tokenization recipe, export to S3, and then train with a custom tokenizer.",
      "B": "Use a SageMaker Processing job with the Hugging Face tokenizer, then push token IDs to Feature Store.",
      "C": "Use SageMaker Data Wrangler: add a built-in text tokenizer transform, then export token counts to Feature Store.",
      "D": "Stream reviews through Lambda to call Amazon Comprehend for tokenization, then store results in S3."
    },
    "explanation": "A SageMaker Processing job with Hugging Face tokenizer integrates easily into pipelines and can write token IDs directly to Feature Store."
  },
  {
    "taskStatement": "1.2",
    "stem": "You need to discretize a continuous feature into bins of equal frequency and standardize another feature to zero mean and unit variance, using managed AWS services. Which combination is most appropriate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Data Wrangler recipe: add quantile binning for equal-frequency bins and Z-score normalization for standardization.",
      "B": "Use AWS Glue DataBrew: add equal-width binning recipe and MinMax scale recipe.",
      "C": "Run a SageMaker Processing job with a Scikit-Learn pipeline that uses KBinsDiscretizer and StandardScaler.",
      "D": "Use AWS Glue ETL: write PySpark code to bucketBy frequency and use VectorAssembler with StandardScaler."
    },
    "explanation": "Data Wrangler supports quantile binning (equal frequency) and Z-score normalization out of the box, minimizing custom code."
  },
  {
    "taskStatement": "1.2",
    "stem": "Your dataset contains duplicate records due to multiple ingestion systems. You need a low-code AWS solution to deduplicate by a composite key and write the result back to S3. Which approach is best?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Use SageMaker Data Wrangler: add a drop duplicates recipe on the composite key, then export to S3.",
      "B": "Run a Glue Spark ETL job with custom PySpark dropDuplicates call on the composite key.",
      "C": "Use AWS Glue DataBrew: create a project, add a dedup recipe on the composite key, and publish the output to S3.",
      "D": "Implement a SageMaker Processing job with Pandas drop_duplicates and write to S3."
    },
    "explanation": "DataBrew provides a built-in deduplication recipe that can drop duplicates on any key and export to S3 without custom code."
  },
  {
    "taskStatement": "1.2",
    "stem": "You must engineer features that capture hour-of-day and day-of-week from a timestamp column for downstream modeling. The solution must integrate with SageMaker Pipelines and Feature Store. Which components do you use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A SageMaker ProcessingStep using a custom Pandas script to extract features and call PutRecord to Feature Store.",
      "B": "A SageMaker Data Wrangler step in the pipeline: use built-in ExtractTimestamp transforms for hour and weekday, and export to Feature Store.",
      "C": "An AWS Glue job to run Python code, write to S3, then a ProcessingStep to ingest into Feature Store.",
      "D": "Lambda functions behind EventBridge that trigger on S3 updates to compute features and write to Feature Store."
    },
    "explanation": "Data Wrangler can be used as a ProcessingStep in SageMaker Pipelines, with built-in timestamp extract transforms and direct export to Feature Store."
  },
  {
    "taskStatement": "1.2",
    "stem": "A dataset contains outliers in multiple numeric columns. You need to detect and cap them at the 1st and 99th percentiles before modeling. Which AWS Workflow is most efficient?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a SageMaker Processing job with custom code to compute percentiles and cap outliers.",
      "B": "Use SageMaker Data Wrangler: add an outlier detection transform to compute percentiles and use a value mapping recipe to cap values.",
      "C": "Write a Glue Spark ETL job using approxQuantile to find percentiles and withColumn to cap values.",
      "D": "Use AWS Glue DataBrew: add a percentile filter recipe then a replace recipe to cap extremes."
    },
    "explanation": "Data Wrangler outlier detection can compute percentiles and supports value mapping recipes to cap outliers without custom code."
  },
  {
    "taskStatement": "1.2",
    "stem": "You have a categorical feature with high cardinality and need to generate target-guided encoding (mean target value per category) without leaking test data information. Which pipeline design prevents leakage?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a single SageMaker Processing job to compute global means on full dataset, apply to all splits.",
      "B": "Use Data Wrangler to compute target means on full dataset and join back to features.",
      "C": "Use Glue ETL to compute means on training set and apply same mapping to test set in the same job.",
      "D": "Define two ProcessingSteps in a SageMaker Pipeline: first compute category-target means on the training split only, store mapping; second step applies mapping to train and test splits separately."
    },
    "explanation": "Separating computation on training split from application to test split in different steps prevents target leakage."
  },
  {
    "taskStatement": "1.2",
    "stem": "A feature column contains text with inconsistent casing and punctuation. You need to normalize the text (lowercase, remove punctuation), tokenize, and compute TF-IDF vectors, using managed AWS services. Which sequence is correct?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker ProcessingStep: run a Scikit-Learn script for lowercasing, regex to remove punctuation, CountVectorizer with TF-IDF transformer.",
      "B": "Use Data Wrangler: add lowercase and remove-pattern transforms, then export tokens to S3 for external TF-IDF calculation.",
      "C": "Use Glue DataBrew recipes: add text lowercase and remove punctuation recipes, then a custom recipe for TF-IDF.",
      "D": "Use AWS Lambda triggered on S3: call Amazon Comprehend to normalize and tokenize, then compute TF-IDF in Lambda."
    },
    "explanation": "A ProcessingStep gives full control to use Scikit-Learn pipeline for normalization and TF-IDF vectorization directly in SageMaker Pipelines."
  },
  {
    "taskStatement": "1.2",
    "stem": "You need to enrich your dataset with rolling window features (7-day sum and average) on time-series data stored in S3. Which AWS service and pattern is most appropriate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Glue Streaming ETL with sliding window in Spark Structed Streaming to compute rolling features.",
      "B": "Use SageMaker Data Wrangler to define window aggregation transforms directly.",
      "C": "Schedule a SageMaker Processing job with PySpark code that reads S3 parquet, applies window functions, and writes enriched data back to S3.",
      "D": "Use AWS Lambda triggered by new data to update rolling aggregates in Amazon DynamoDB."
    },
    "explanation": "SageMaker Processing with PySpark supports Spark SQL window functions for rolling aggregates in a batch fashion, suitable for scheduled enrichment."
  },
  {
    "taskStatement": "1.2",
    "stem": "Your dataset has a highly imbalanced binary label. You need to generate synthetic minority samples using SMOTE within a managed AWS pipeline. Which service and pattern achieves this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Data Wrangler\u2019s built-in synthetic data generation transforms.",
      "B": "Use a SageMaker Processing job with Imbalanced-Learn\u2019s SMOTE, then export to S3.",
      "C": "Use AWS Glue ETL with custom PySpark code to perform SMOTE.",
      "D": "Use AWS Glue DataBrew with a custom recipe for synthetic minority sampling."
    },
    "explanation": "Only a Processing job supports custom Python libraries like Imbalanced-Learn to run SMOTE within a managed pipeline."
  },
  {
    "taskStatement": "1.2",
    "stem": "You have feature interactions that must be generated (pairwise products) for a logistic regression model. You want to do this at scale on a large dataset in S3. What is the most serverless approach?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run a SageMaker Processing job on large ML instances with a custom script to generate interactions.",
      "B": "Use AWS Glue ETL on Spark with PySpark to compute pairwise products and write back to S3.",
      "C": "Use SageMaker Batch Transform with a script that outputs interactions.",
      "D": "Use AWS Glue DataBrew: create a recipe to generate new columns via formula for each interaction and publish to S3."
    },
    "explanation": "DataBrew scales serverlessly with partitions and can generate new columns via formulas for interactions without managing infrastructure."
  },
  {
    "taskStatement": "1.2",
    "stem": "You need to normalize a set of numeric features using robust scaling (subtract median and divide by IQR) within SageMaker Pipelines. Which component do you include?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A ProcessingStep that calls AWS Glue Data Quality for robust scaling.",
      "B": "A ProcessingStep that uses a DataBrew project for IQR scaling.",
      "C": "A ProcessingStep with a custom Scikit-Learn RobustScaler script.",
      "D": "A TransformStep using built-in SageMaker XGBoost with robust scaling parameters."
    },
    "explanation": "A ProcessingStep with a custom script is required for RobustScaler since no built-in recipe exists in Data Wrangler or DataBrew."
  },
  {
    "taskStatement": "1.2",
    "stem": "You must anonymize PII in free-text user comments by replacing names and emails, then vectorize text. Which AWS-managed solution minimizes custom code?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Processing with a custom regex script for PII removal and TF-IDF vectorization.",
      "B": "Use a SageMaker Processing job combining Amazon Comprehend PII entity detection API for redaction, then Scikit-Learn vectorizer.",
      "C": "Use AWS Glue DataBrew with PII detection recipes and export tokens to S3 for vectorization.",
      "D": "Use AWS Lambda to call Comprehend for redaction, store in S3, then run DataWrangler for vectorization."
    },
    "explanation": "A single Processing job can orchestrate Comprehend API calls and vectorization in one managed step."
  },
  {
    "taskStatement": "1.2",
    "stem": "You need to transform multi-value categorical fields (lists of tags per item) into indicator features and store them for low-latency inference. Which pipeline configuration should you choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "In SageMaker Data Wrangler, use the split multi-value transform to explode tags into rows, one-hot encode them, pivot back to wide format, and export to online Feature Store.",
      "B": "In Glue ETL, use PySpark explode then OneHotEncoderEstimator and write to S3 for offline Feature Store.",
      "C": "Use Lambda functions to parse tags into booleans and write JSON to DynamoDB.",
      "D": "Use SageMaker Processing with pandas to create indicator columns and write to S3."
    },
    "explanation": "Data Wrangler supports split multi-value transforms and direct export to online Feature Store for low-latency inference."
  },
  {
    "taskStatement": "1.2",
    "stem": "A dataset contains decimal representations of categorical codes. You need to convert these codes to binary bit-vector features (one bit per significant bit) for modeling. Which approach is most straightforward using Data Wrangler?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a ProcessingStep with custom Python bit-operation code.",
      "B": "Use Glue ETL with PySpark bit operations in withColumn.",
      "C": "Use SageMaker Data Wrangler\u2019s custom transform node: write a small Pandas UDF to convert codes to bit vectors.",
      "D": "Use DataBrew with multiple formula recipes to extract bits manually."
    },
    "explanation": "A custom transform node in Data Wrangler allows concise Pandas UDF bit extraction without full ETL management."
  },
  {
    "taskStatement": "1.2",
    "stem": "You want to join two large datasets in S3 by a composite key, perform feature computations (ratio of two columns), and write the result in parquet. Which solution scales most cost-effectively?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Data Wrangler to join and compute ratio then export.",
      "B": "Use AWS Glue ETL on Apache Spark with pushdown predicates and write to S3.",
      "C": "Use a SageMaker Processing job with PySpark on large instances.",
      "D": "Use AWS Glue DataBrew: import both datasets, join in project, and publish result."
    },
    "explanation": "AWS Glue ETL offers serverless Spark with pushdown and partition pruning, minimizing cost for large joins."
  },
  {
    "taskStatement": "1.2",
    "stem": "Your feature store offline table has outdated records. You need to batch ingest new features while preserving historical records for audit. Which pattern do you follow?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Overwrite the offline store using Feature Group import mode FULL Refresh.",
      "B": "Use a Glue job to append to the offline table in the Feature Store\u2019s backing S3 bucket.",
      "C": "Use a ProcessingJob to write new features to the same S3 prefix without Feature Store APIs.",
      "D": "Use SageMaker Feature Store Batch PutRecord API to append new records, which preserves history if record identifier and event time increase."
    },
    "explanation": "Batch PutRecord appends new feature values and keeps older versions for audit based on eventTime."
  },
  {
    "taskStatement": "1.2",
    "stem": "You need to generate polynomial features (up to degree 3) on a subset of continuous variables for a regression model, using SageMaker Pipelines. Which step do you include?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A ProcessingStep that runs a Scikit-Learn PolynomialFeatures transformer on the selected columns.",
      "B": "A TransformStep using XGBoost\u2019s built-in polynomial feature parameters.",
      "C": "A Data Wrangler step with custom Python recipe for polynomial features.",
      "D": "A Glue ETL job scheduled separately and results stored in S3."
    },
    "explanation": "A ProcessingStep allows direct use of Scikit-Learn PolynomialFeatures within the pipeline."
  },
  {
    "taskStatement": "1.2",
    "stem": "Your data contains timestamp strings in multiple formats. You must standardize to ISO8601 before downstream feature extraction. Which AWS service and method handle this with minimal code?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Processing with a custom regex parser to normalize formats.",
      "B": "Use Glue ETL with custom Python UDF in Spark to parse and format timestamps.",
      "C": "Use SageMaker Data Wrangler: add multiple parse-date recipes with input patterns, unify output to ISO8601, then export.",
      "D": "Use DataBrew: define multiple date parsing recipes and publish standardized data."
    },
    "explanation": "Data Wrangler supports multiple parse-date recipes and an output format selection, reducing custom code."
  },
  {
    "taskStatement": "1.2",
    "stem": "You must encode a hierarchical categorical feature (e.g., Country > State > City) to capture both levels for a tree model. Which sequence in Data Wrangler best preserves hierarchy?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "One-hot encode City only.",
      "B": "Create combined category codes (Country_State_City) then one-hot encode top N combined categories and bucket rest as Other.",
      "C": "Label encode each level separately.",
      "D": "Use Hash encoding on City and ignore higher levels."
    },
    "explanation": "Creating combined codes preserves hierarchy and one-hot encoding top combinations captures joint distribution with controlled dimensionality."
  },
  {
    "taskStatement": "1.2",
    "stem": "A dataset includes text descriptions with HTML markup. You must remove tags, lowercase, and compute the length of each description. Which managed flow meets this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Data Wrangler: add a custom transform node that runs a small Python function to strip HTML, lowercase, and compute length.",
      "B": "Use a Glue ETL job with BeautifulSoup and Python to process, then write to S3.",
      "C": "Use AWS Lambda triggered on S3 to clean text and store lengths in DynamoDB.",
      "D": "Use Glue DataBrew recipes: remove HTML pattern, lowercase, compute new column via formula."
    },
    "explanation": "DataBrew\u2019s formula for length may be limited; Data Wrangler custom node allows running Python code for HTML stripping and length calculation."
  },
  {
    "taskStatement": "1.2",
    "stem": "Your model benefits from interaction terms between a date and a numeric feature. You need to encode weekday \u00d7 metric interactions. Which pipeline component choice is correct?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use DataBrew: create numeric \u00d7 weekday formula recipes.",
      "B": "Use Glue ETL: write PySpark to compute interaction terms.",
      "C": "Use a SageMaker ProcessingStep with a Pandas script to extract weekday and multiply by the metric, then export.",
      "D": "Use Data Wrangler: extract weekday and use the \u201cGenerate feature combinations\u201d transform."
    },
    "explanation": "Data Wrangler does not auto-generate cross features; a ProcessingStep with Pandas gives precise control for interaction terms."
  },
  {
    "taskStatement": "1.2",
    "stem": "You have high-cardinality ordinal data where label encoding might mislead the model. You need ordinal encoding that respects order but maps to a uniform distribution. Which transform do you apply?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use one-hot encoding after sorting ordinal values.",
      "B": "Use Quantile ordinal encoding: map ordinal rank to quantile value between 0 and 1 via Data Wrangler\u2019s custom transform.",
      "C": "Use label encoding directly.",
      "D": "Use binary encoding to reduce dimensionality."
    },
    "explanation": "Quantile ordinal encoding maps each ordinal category to its quantile position, preserving order without magnitude assumptions."
  },
  {
    "taskStatement": "1.2",
    "stem": "You need to integrate SageMaker Ground Truth labeled data into your feature-engineering flow in Data Wrangler, then merge those labels with features. Which pattern works?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Import the Ground Truth output manifest into Data Wrangler via import manifest, join on record IDs, and proceed with transforms.",
      "B": "Use Glue ETL to read manifest JSON and your features, join, then export to S3.",
      "C": "Use a ProcessingJob to read S3 manifest and feature data and merge in code.",
      "D": "Configure DataBrew to read manifest and features and merge via project join recipes."
    },
    "explanation": "Data Wrangler can directly import a Ground Truth manifest and join on record identifiers within the flow."
  },
  {
    "taskStatement": "1.2",
    "stem": "During feature engineering, you need to normalize numeric fields per group (e.g., by customer segment) to zero mean within each group. Which service supports this grouping and transform without custom code?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Processing job with Pandas groupby and transform.",
      "B": "AWS Glue Streaming ETL or Glue batch ETL? Actually Data Wrangler? But grouping per segment: Data Wrangler supports groupby transforms in custom? It doesn't. So correct: ProcessingStep.",
      "C": "SageMaker Data Wrangler: add an \u201cAggregate\u201d transform with groupby and join back.",
      "D": "AWS Glue DataBrew: add \u201cGroup\u201d recipe with normalization."
    },
    "explanation": "Data Wrangler does not natively support group-based normalization; you need a custom script in a ProcessingStep to group and normalize."
  },
  {
    "taskStatement": "1.2",
    "stem": "You want to cleanse geographical coordinate outliers using interquartile range and then project lat/long into distance features from a fixed point. Which pipeline step do you use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A SageMaker ProcessingStep running a Python script that computes IQR, filters outliers, then applies haversine distance formula.",
      "B": "Data Wrangler recipes: outlier removal and geospatial transform.",
      "C": "Glue DataBrew: percentile filter and custom geospatial formula.",
      "D": "Lambda functions for geospatial cleaning and a Data Wrangler step for distance."
    },
    "explanation": "No managed transform for geospatial distance; ProcessingStep with Python gives full control for outlier filtering and distance computation."
  },
  {
    "taskStatement": "1.2",
    "stem": "A categorical feature contains hierarchical JSON structures per row. You need to flatten to multiple indicator features at any depth. Which approach scales best with minimal code?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Processing with a recursive JSON flattener in Python.",
      "B": "Use AWS Glue ETL on Spark with from_json and explode operations to flatten, then OneHotEncoderEstimator.",
      "C": "Use DataBrew JSON flatten recipe then one-hot encode.",
      "D": "Use Data Wrangler custom transform to flatten JSON manually."
    },
    "explanation": "Glue ETL\u2019s built-in from_json and explode support scalable JSON flattening; Spark\u2019s OneHotEncoder completes encoding."
  },
  {
    "taskStatement": "1.2",
    "stem": "You must generate time-lagged features (t-1, t-2) for a time series dataset with irregular intervals. Which solution handles irregular sampling properly?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Use Glue ETL with window functions assuming regular partitions.",
      "B": "Use Data Wrangler\u2019s lag transform with fixed intervals.",
      "C": "Use a ProcessingStep with pandas shift (but assumes uniform index).",
      "D": "Use a SageMaker Processing job with custom script that resamples by timestamp and computes lag based on nearest previous record."
    },
    "explanation": "Irregular intervals require custom resampling and lag logic in code, best done in a ProcessingStep."
  },
  {
    "taskStatement": "1.2",
    "stem": "Your model requires polynomial and interaction features, missing-value flags, and one-hot encodings in one cohesive flow. Which AWS-managed solution is most integrated?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Glue ETL job with custom PySpark combining all transformations.",
      "B": "SageMaker Processing job orchestrating multiple scripts.",
      "C": "SageMaker Data Wrangler flow: use built-in polynomial, fill missing flag, encoding, and export to Feature Store.",
      "D": "DataBrew project with custom recipes for each transformation."
    },
    "explanation": "Data Wrangler supports polynomial features, missing-value flagging, and categorical encodings in a single flow with direct exports."
  },
  {
    "taskStatement": "1.3",
    "stem": "A bank is building a credit approval model. The training dataset contains a binary \u201capproved\u201d label and a protected \u201cgender\u201d feature. Before training, the ML engineer needs to quantify label imbalance (difference in approval rates) between male and female applicants. Which SageMaker Clarify configuration correctly measures this metric?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run a Clarify \u201cdataset_analysis\u201d job and specify the \u201cmean_difference\u201d metric for the \u201cgender\u201d feature.",
      "B": "Run a Clarify pre_training_bias job, set facet_name to \u201cgender\u201d, label_values_or_threshold to ['approved'], and include the \u201cdifference_in_proportions\u201d metric.",
      "C": "Run a Clarify post_training_bias job after model training and view SHAP importances for \u201cgender.\u201d",
      "D": "Use SageMaker Model Monitor baseline for data quality to detect label skew."
    },
    "explanation": "Pre-training bias jobs with facet_name and difference_in_proportions compute DPL for the protected feature before training."
  },
  {
    "taskStatement": "1.3",
    "stem": "A healthcare provider must de-identify PHI in free-text clinical notes stored on S3 before any ML processing. Which AWS service and approach automatically finds and redacts PHI entities in text files?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon Comprehend Medical de-identification API to detect and redact PHI, then store de-identified text back in S3.",
      "B": "Use AWS Glue DataBrew PII masking transform to remove PHI patterns.",
      "C": "Use Amazon Macie to classify S3 objects and replace PHI entities.",
      "D": "Run a SageMaker Clarify data quality job to identify and filter PHI."
    },
    "explanation": "Comprehend Medical de-identification API is purpose-built for PII/PHI redaction in clinical text."
  },
  {
    "taskStatement": "1.3",
    "stem": "An ML engineer uses an Amazon EFS file system to store HIPAA-regulated data for SageMaker training. To meet compliance, they must encrypt data at rest and in transit. Which configuration satisfies both requirements?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable EFS encryption at rest with a customer-managed CMK, create EFS mount targets with TLS, and reference the EFS file system in the training job.",
      "B": "Enable S3 SSE-KMS on the EFS bucket and configure SageMaker network isolation.",
      "C": "Switch to FSx for Lustre with in-transit encryption enabled and mount in training job.",
      "D": "Enable encryption on the training instance root volume and use HTTPS to access EFS."
    },
    "explanation": "EFS supports encryption at rest via CMKs and in-transit TLS mount targets; training jobs can mount encrypted EFS directly."
  },
  {
    "taskStatement": "1.3",
    "stem": "A regression dataset has 20% missing values in the numeric \u201cincome\u201d column. The engineer must impute missing values by median without leaking test information. Using AWS DataBrew, which sequence of steps is correct?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Profile full dataset to compute global median, then impute \u201cincome\u201d on combined data and split into train/test.",
      "B": "First split dataset into train/test in DataBrew, then apply a recipe step on both sets using the median calculated from the training set.",
      "C": "Run a SageMaker Processing job with scikit-learn SimpleImputer on combined data, then split.",
      "D": "Use AWS Glue ETL to aggregate median and fill missing values, then split datasets."
    },
    "explanation": "To avoid leakage you must split first, compute median on training data, and apply same imputation to test."
  },
  {
    "taskStatement": "1.3",
    "stem": "Before training, an engineer must verify that the \u201cuser_id\u201d column in CSV files is non-null and unique. Which AWS Glue Data Quality configuration accomplishes this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Define a Data Quality ruleset with NOT_NULL and UNIQUENESS rules on \u201cuser_id\u201d, run a Data Quality job and inspect violations.",
      "B": "Use SageMaker Data Wrangler profile and manually inspect unique count for \u201cuser_id.\u201d",
      "C": "Write a SageMaker Processing pandas script to drop nulls and duplicates.",
      "D": "Use AWS Glue DataBrew with a data quality job and assertions on \u201cuser_id.\u201d"
    },
    "explanation": "AWS Glue Data Quality jobs with built-in rules handle NOT_NULL and UNIQUENESS assertions at scale."
  },
  {
    "taskStatement": "1.3",
    "stem": "A large S3 dataset may contain anomalous rows that degrade model accuracy. Which AWS tool combination efficiently detects and removes outliers across multiple features during preprocessing?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Data Wrangler to profile the dataset, apply the Outlier Detection transform, and add recipe steps to filter anomalies.",
      "B": "Write custom Spark in AWS Glue to compute z-scores and drop outliers then load cleaned data to S3.",
      "C": "Run a SageMaker Clarify data bias job to detect feature outliers and filter.",
      "D": "Use AWS Glue Data Quality to generate anomaly reports and manually remove rows."
    },
    "explanation": "Data Wrangler\u2019s built-in outlier detection recipe automates detection and removal within the ETL workflow."
  },
  {
    "taskStatement": "1.3",
    "stem": "The S3 training data files are partitioned by date, causing temporal ordering bias. To randomize samples across partitions in SageMaker training with minimal overhead, which option is best?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Specify ShuffleConfig with \u201cFullyReplicated\u201d in the S3 input channel of the training job.",
      "B": "Use AWS Glue DataBrew to shuffle rows within each file and write to new S3 files.",
      "C": "Run a SageMaker Processing job using AWS Wrangler to shuffle and rewrite the dataset.",
      "D": "Use Lambda to concatenate and shuffle data before training."
    },
    "explanation": "SageMaker\u2019s native ShuffleConfig on S3 inputs randomizes across objects without extra ETL."
  },
  {
    "taskStatement": "1.3",
    "stem": "A dataset contains an \u201cssn\u201d column that must be irreversibly hashed before training. Which AWS service and transform securely performs this operation with minimal code?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use AWS Glue DataBrew \u201cMaskValue\u201d transform on \u201cssn\u201d column with SHA-256 salt and export the recipe.",
      "B": "Write a SageMaker Processing job in Python to hash values and save to S3.",
      "C": "Use AWS Glue Spark ETL with hash functions to overwrite values.",
      "D": "Use SageMaker Clarify anonymization to drop the \u201cssn\u201d field."
    },
    "explanation": "DataBrew\u2019s MaskValue step supports cryptographic hashing with salt in a low-code recipe."
  },
  {
    "taskStatement": "1.3",
    "stem": "In a fraud dataset, the minority class is only 1% of records. The engineer wants to quantify this imbalance via SageMaker Clarify and then mitigate it. Which metric and mitigation approach should be used?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use the ClassImbalance metric to measure ratio and apply SMOTE to oversample the minority class.",
      "B": "Use DifferenceInProportions metric and apply random undersampling of the majority class.",
      "C": "Use KLDivergence metric to detect drift and add class weights during training.",
      "D": "Use MutualInformation to rank features and drop low-importance ones."
    },
    "explanation": "ClassImbalance gives the minority ratio; SMOTE is a standard synthetic oversampling method to correct it."
  },
  {
    "taskStatement": "1.3",
    "stem": "An organization mandates that all S3 input data for SageMaker training be encrypted with a customer-managed CMK. How should the training job be configured to honor SSE-KMS on input channels?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "In InputDataConfig for each S3 channel, set S3DataDistributionType to \u201cFullyReplicated\u201d and specify KmsKeyId with the CMK ARN.",
      "B": "Enable EncryptVolume on the training instance and rely on default S3 key.",
      "C": "Set EnableNetworkIsolation to true to force KMS encryption.",
      "D": "Configure OutputDataConfig KmsKeyId \u2013 input will inherit the same key."
    },
    "explanation": "Specifying KmsKeyId on InputDataConfig ensures SageMaker uses SSE-KMS to decrypt input data with the CMK."
  },
  {
    "taskStatement": "1.3",
    "stem": "A deployed ML model processes IoT telemetry. To detect production input drift and alert when numeric features shift beyond a threshold, which AWS service and metric should be used?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Model Monitor with a data quality JobDefinition that uses the KLDivergence metric.",
      "B": "Use SageMaker Clarify explainability to monitor SHAP drift in features.",
      "C": "Use AWS Config to evaluate drift rules on input buckets.",
      "D": "Use AWS CloudWatch anomaly detection on CPU usage of endpoint."
    },
    "explanation": "Model Monitor\u2019s data quality jobs with KLDivergence compare production vs. baseline distributions to detect drift."
  },
  {
    "taskStatement": "1.3",
    "stem": "A dataset contains an ordinal \u201crating\u201d feature encoded as text (\u201cpoor\u201d, \u201caverage\u201d, \u201cgood\u201d, \u201cexcellent\u201d). To prepare for numeric modeling and avoid bias, which AWS DataBrew transform sequence is correct?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "One-hot encode \u201crating\u201d directly in DataBrew recipe.",
      "B": "Apply label encoding mapping \u201cpoor\u201d\u21920,\u2026\u201cexcellent\u201d\u21923 then one-hot encode in training.",
      "C": "Define custom map transform in DataBrew to assign ordinal values (0\u20133) preserving order, and leave as numeric.",
      "D": "Use SageMaker Clarify to auto-encode text categories during training."
    },
    "explanation": "Ordinal features should be mapped to numeric preserving order; one-hot would lose ordinality."
  },
  {
    "taskStatement": "1.3",
    "stem": "A regulated dataset includes random noise in a \u201csalary\u201d column to preserve privacy. Before training, the engineer needs to detect abnormal noise levels that might distort the model. Which tool and check accomplish this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Glue DataBrew to profile mean and std deviation of \u201csalary\u201d across batches.",
      "B": "Use SageMaker Clarify data quality job with \u201cstatistics\u201d check to compare batch distributions to baseline.",
      "C": "Use SageMaker Model Monitor feature drift job with PSI metric on \u201csalary.\u201d",
      "D": "Use AWS Glue Data Quality anomaly detection rule on \u201csalary.\u201d"
    },
    "explanation": "Clarify data quality jobs support statistics checks to compare current vs. baseline distributions for numeric features."
  },
  {
    "taskStatement": "1.3",
    "stem": "A ML pipeline ingests user-submitted text that may contain offensive words. The engineer must remove any rows containing profanity before training. Which approach is most automated?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Glue DataBrew FilterRows with a profanity regex.",
      "B": "Run a custom SageMaker Processing job calling an external profanity API.",
      "C": "Use AWS Glue DataBrew with built-in \u201cReplaceText\u201d transform referencing a profanity wordlist to flag and drop rows.",
      "D": "Use SageMaker Clarify to identify toxic text and filter."
    },
    "explanation": "DataBrew\u2019s ReplaceText transform and conditional filter can flag profanity and drop offending rows in a recipe."
  },
  {
    "taskStatement": "1.3",
    "stem": "A sensitive column \u201cemail\u201d must be pseudonymized by replacing with reversible tokens, ensuring only the training job can reverse it. Which design below meets this requirement securely?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use DataBrew MaskValue transform with a static salt stored in S3.",
      "B": "Use SageMaker Processing with Python to hash emails and store hash key on the notebook.",
      "C": "Use AWS Glue ETL to encrypt email using default KMS key without rotation.",
      "D": "Use a SageMaker Processing job that calls AWS KMS GenerateDataKey to encrypt each email, storing ciphertext and encrypted data key columns."
    },
    "explanation": "GenerateDataKey provides a data key for encrypt/decrypt; processing job can encrypt email with the data key and store encrypted data key per row."
  },
  {
    "taskStatement": "2.1",
    "stem": "A health insurance company needs to predict claim costs using tabular data with dozens of numeric and categorical features. The model must be explainable for compliance and deployed quickly without custom code. Which modeling approach should the ML engineer choose?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Train a deep neural network in SageMaker with a custom PyTorch script.",
      "B": "Use the SageMaker built-in XGBoost linear learner in linear-learner mode.",
      "C": "Deploy an Amazon Bedrock foundation model for tabular regression.",
      "D": "Use SageMaker JumpStart to fine-tune a random forest model."
    },
    "explanation": "The SageMaker linear learner in linear mode offers both speed of built-in algorithms and coefficient-based explainability, meeting compliance without custom code."
  },
  {
    "taskStatement": "2.1",
    "stem": "A startup has millions of short text support tickets. They need to automatically categorize them into about ten categories. They require near real-time inference and minimal infrastructure management. Which approach is most appropriate?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Use SageMaker built-in Latent Dirichlet Allocation (LDA) to extract topics.",
      "B": "Train a K-means clustering model on bag-of-words features.",
      "C": "Use Amazon Comprehend custom classification endpoints.",
      "D": "Fine-tune a SageMaker JumpStart GPT model for classification."
    },
    "explanation": "Amazon Comprehend custom classification provides a managed, low-latency service optimized for text classification without heavy infrastructure or custom training."
  },
  {
    "taskStatement": "2.1",
    "stem": "A financial firm must model customer churn using a dataset of 100,000 rows and 200 features, many of which are sparse. They need both high accuracy and moderate interpretability. Which algorithm should they select?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Regularized gradient-boosted trees with SHAP explainability in SageMaker.",
      "B": "Deep neural network with attention layers in SageMaker Training.",
      "C": "K-nearest neighbors classifier on the full feature set.",
      "D": "SageMaker linear learner without feature interactions."
    },
    "explanation": "Gradient-boosted trees provide strong accuracy on structured data and SHAP integration yields moderate interpretability, balancing both requirements."
  },
  {
    "taskStatement": "2.1",
    "stem": "A retailer wants to forecast hourly product demand across 500 stores. They need a model that handles seasonality and multiple time series simultaneously with minimal custom development. Which AWS capability should they use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Build a Seq2Seq model in PyTorch with SageMaker script mode.",
      "B": "Use SageMaker built-in DeepAR forecasting.",
      "C": "Fine-tune a Bedrock LLM for time series forecasting.",
      "D": "Use Amazon Forecast with a custom recipe for seasonal demand."
    },
    "explanation": "Amazon Forecast is a managed service specialized in multi-dimensional time series forecasting with built-in seasonality handling and minimal development."
  },
  {
    "taskStatement": "2.1",
    "stem": "An e-commerce company needs to detect anomalous orders in real time. The data has 50 continuous and categorical features. They want full control over algorithm selection and hyperparameters. Which modeling approach is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train an Isolation Forest using SageMaker built-in with batch inference enabled.",
      "B": "Deploy Amazon Lookout for Metrics for anomaly detection.",
      "C": "Use SageMaker KNN anomaly detection with default parameters.",
      "D": "Fine-tune a JumpStart anomaly detection model on Amazon Bedrock."
    },
    "explanation": "SageMaker built-in Isolation Forest gives full control over hyperparameters and supports custom batch inference while being managed."
  },
  {
    "taskStatement": "2.1",
    "stem": "A logistics company wants to segment customers into behavioral clusters using both shipment frequency and cost. They expect clear cluster centroids for interpretability. Which algorithm should they choose?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "DBSCAN clustering in SageMaker Processing.",
      "B": "Hierarchical clustering in SageMaker Spark.",
      "C": "Gaussian Mixture Model with EM in SageMaker.",
      "D": "K-means clustering with the SageMaker built-in algorithm."
    },
    "explanation": "K-means provides clear centroids and is available as a built-in SageMaker algorithm, balancing interpretability and managed operation."
  },
  {
    "taskStatement": "2.1",
    "stem": "A media company needs to recommend movies using collaborative filtering on user-item ratings. They require a scalable solution integrated with SageMaker. Which modeling approach fits?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a matrix factorization model in Scikit-Learn on SageMaker Training.",
      "B": "Use Amazon Personalize batch recommendations.",
      "C": "Use SageMaker built-in factorization machines algorithm.",
      "D": "Fine-tune a JumpStart recommender system foundation model."
    },
    "explanation": "SageMaker factorization machines are optimized for collaborative filtering, scalable, and integrated into SageMaker workflows."
  },
  {
    "taskStatement": "2.1",
    "stem": "A biotech startup must classify protein sequences. Labeled data is scarce, and interpretability is not critical. They want to leverage pretrained models. Which approach is most appropriate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a CNN from scratch in SageMaker script mode.",
      "B": "Fine-tune a JumpStart protein foundation model.",
      "C": "Use SageMaker built-in XGBoost with k-mer features.",
      "D": "Develop a random forest on physicochemical features."
    },
    "explanation": "JumpStart protein foundation models leverage pretrained knowledge, requiring limited data and engineering, ideal for scarce labels without interpretability needs."
  },
  {
    "taskStatement": "2.1",
    "stem": "A compliance-driven enterprise must predict default risk on personal loans. They need to justify each decision to auditors. Which modeling approach balances accuracy and full explainability?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker built-in linear learner and present feature coefficients.",
      "B": "Train an XGBoost model and approximate SHAP explanations.",
      "C": "Deploy an Amazon Bedrock LLM to explain historical decisions.",
      "D": "Use DeepAR to forecast default probabilities."
    },
    "explanation": "Linear learner provides transparent coefficients for direct auditing, ensuring full explainability though with modest accuracy trade-off."
  },
  {
    "taskStatement": "2.1",
    "stem": "A gaming company needs to classify images of player avatars into categories. They want to minimize GPU cost and only require coarse-grained accuracy. Which SageMaker option should they choose?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Fine-tune a JumpStart ResNet model.",
      "B": "Train a custom CNN in PyTorch on GPU.",
      "C": "Use SageMaker built-in image classification algorithm on CPU.",
      "D": "Deploy Amazon Rekognition with custom labels."
    },
    "explanation": "The built-in image classifier can run on CPU for coarse tasks with lower cost, avoiding GPU and heavy customization."
  },
  {
    "taskStatement": "2.1",
    "stem": "An IoT company collects multivariate sensor time series and needs to detect anomalies streaming. They prefer managed services and minimal model maintenance. Which approach is best?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Train an LSTM autoencoder in SageMaker script mode.",
      "B": "Use the SageMaker built-in random cut forest algorithm in batch.",
      "C": "Fine-tune an LLM for time series anomaly detection.",
      "D": "Use Amazon Lookout for Equipment for streaming anomalies."
    },
    "explanation": "Amazon Lookout for Equipment is managed, supports streaming anomaly detection on sensor data with minimal maintenance."
  },
  {
    "taskStatement": "2.1",
    "stem": "A marketing team wants to segment emails by topic automatically. They need a fully managed NLP solution that adapts over time. Which AWS service should they use?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Train LDA in SageMaker Processing.",
      "B": "Use Amazon Comprehend topic modeling APIs.",
      "C": "Fine-tune a JumpStart GPT-3 model for topic extraction.",
      "D": "Use Amazon SageMaker BlazingText for unsupervised topic clusters."
    },
    "explanation": "Amazon Comprehend provides managed topic modeling that adapts to new data without custom training."
  },
  {
    "taskStatement": "2.1",
    "stem": "A telecom wants to predict customer churn with an imbalanced dataset and needs to prioritize recall over precision. They plan to tune thresholds. Which algorithm and metric combination should they choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train XGBoost in SageMaker and optimize for recall using hyperparameter tuning with custom F-beta metric.",
      "B": "Use logistic regression and evaluate accuracy.",
      "C": "Deploy random forest and adjust class weights only at inference.",
      "D": "Use built-in factorization machines with default evaluation."
    },
    "explanation": "XGBoost with custom metric optimization allows tuning hyperparameters for recall, matching business priority on false negatives."
  },
  {
    "taskStatement": "2.1",
    "stem": "A media analytics team needs to extract entities from video transcripts. They require deep customization of entity categories and managed model hosting. What should they choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a custom spaCy NER model on SageMaker Processing.",
      "B": "Fine-tune a JumpStart language model and deploy on SageMaker.",
      "C": "Use Amazon Comprehend custom entity recognition endpoints.",
      "D": "Use Amazon Textract to extract text and regex on transcripts."
    },
    "explanation": "Comprehend custom entity recognition offers managed training and hosting with customizable entities, minimizing infra effort."
  },
  {
    "taskStatement": "2.1",
    "stem": "A fraud detection use case has a growing number of streaming events per second and requires sub-millisecond inference. They need high throughput and low latency with minimal operational overhead. Which approach fits?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy XGBoost model on SageMaker real-time endpoint on CPU.",
      "B": "Use SageMaker serverless inference with ProvisionedConcurrency.",
      "C": "Deploy model on ECS Fargate behind API Gateway.",
      "D": "Use Lambda to host model via Docker image."
    },
    "explanation": "Serverless inference with provisioned concurrency handles variable volumes and low latency without managing servers."
  },
  {
    "taskStatement": "2.1",
    "stem": "A retailer wants price elasticity estimates per product category, requiring interpretable coefficients. They have limited compute budget. Which modeling approach should they select?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Train a tree-based ensemble and use SHAP.",
      "B": "Fine-tune a Bedrock regression foundation model.",
      "C": "Use SageMaker XGBoost regressor default.",
      "D": "Use SageMaker linear learner with L1 regularization for sparse features."
    },
    "explanation": "Linear learner yields direct coefficients for price elasticity, with L1 handling sparsity and minimal compute overhead."
  },
  {
    "taskStatement": "2.1",
    "stem": "An ML engineer must automatically classify customer reviews into positive or negative sentiment. They need near zero code and managed endpoints. Which AWS service should they choose?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Build a logistic regression in SageMaker script mode.",
      "B": "Use Amazon Comprehend sentiment analysis endpoints.",
      "C": "Fine-tune a JumpStart BERT model.",
      "D": "Train SageMaker built-in BlazingText skip-gram model."
    },
    "explanation": "Comprehend provides out-of-the-box sentiment analysis API with no code and managed hosting."
  },
  {
    "taskStatement": "2.1",
    "stem": "A company has sparse high-dimensional clickstream data and wants a scalable algorithm for binary classification. They prioritize L1 regularization to perform feature selection. Which SageMaker built-in algorithm should they choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker built-in linear learner with L1 mode.",
      "B": "Built-in XGBoost with default settings.",
      "C": "Built-in KNN.",
      "D": "Built-in K-means."
    },
    "explanation": "Linear learner supports L1 regularization for feature selection and scales to high dimensions."
  },
  {
    "taskStatement": "2.1",
    "stem": "A healthcare provider needs to detect disease from X-ray images with regulatory constraints on data privacy. They want to use a managed service and avoid moving data off-premises. Which solution is best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker built-in image classification in public endpoints.",
      "B": "Deploy a JumpStart Vision transformer in SageMaker Studio.",
      "C": "Configure SageMaker Inference on AWS Outposts with the built-in image classification algorithm.",
      "D": "Use Amazon Rekognition for custom labels."
    },
    "explanation": "SageMaker on AWS Outposts runs locally, preserving data privacy without sending images off-premises."
  },
  {
    "taskStatement": "2.1",
    "stem": "A startup needs to detect default probability from loan applications with limited GPU budget. They require class probability outputs and fast training. Which algorithm should they pick?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deep AR forecasting model.",
      "B": "Built-in XGBoost classifier.",
      "C": "Built-in K-means clustering.",
      "D": "Built-in PCA."
    },
    "explanation": "XGBoost classifier offers fast training on CPU/GPU, outputs probabilities, and is cost-efficient."
  },
  {
    "taskStatement": "2.1",
    "stem": "A legal firm wants to identify key contract clauses in documents. They need a managed NLP solution they can tailor without hosting infrastructure. Which should they use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Comprehend custom classification for clause categories.",
      "B": "Fine-tune JumpStart GPT-3 and host on SageMaker.",
      "C": "Use SageMaker built-in BlazingText.",
      "D": "Train a custom BERT model in SageMaker script mode."
    },
    "explanation": "Comprehend custom classification lets them define clause categories and uses managed endpoints without infra overhead."
  },
  {
    "taskStatement": "2.1",
    "stem": "A search engine needs to rank results by relevance using user click logs. They require pairwise ranking and integration with SageMaker. Which algorithm is suitable?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Built-in random forest classifier.",
      "B": "Built-in KNN algorithm.",
      "C": "DeepAR forecasting.",
      "D": "SageMaker Factorization Machines with ranking objective."
    },
    "explanation": "Factorization Machines support pairwise ranking objectives and integrate seamlessly in SageMaker pipelines."
  },
  {
    "taskStatement": "2.1",
    "stem": "An agricultural startup wants to detect crop diseases from leaf images. They have limited labeled data and need transfer learning with minimal fine-tuning. Which approach should they choose?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Train a CNN from scratch in PyTorch on SageMaker.",
      "B": "Use SageMaker built-in XGBoost on flattened image features.",
      "C": "Fine-tune a JumpStart transfer learning image classification model.",
      "D": "Deploy Amazon Rekognition Custom Labels."
    },
    "explanation": "JumpStart transfer learning models require minimal labeled data and fine-tuning for image tasks, reducing effort."
  },
  {
    "taskStatement": "2.1",
    "stem": "A robotics company collects multi-modal sensor data (images and lidar). They need a model that natively handles heterogeneous inputs. Which modeling approach is best?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "Ensemble separate built-in algorithms for each modality.",
      "B": "Train a multimodal PyTorch model in SageMaker script mode.",
      "C": "Use Amazon SageMaker built-in Linear Learner with concatenated features.",
      "D": "Fine-tune a JumpStart text foundation model."
    },
    "explanation": "A custom PyTorch script enables combining different data types in a single model, which built-ins cannot natively handle."
  },
  {
    "taskStatement": "2.1",
    "stem": "A voice assistant service must detect five spoken commands in streaming audio. They need low latency and a managed solution. Which AWS offering should they use?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Train a CNN in SageMaker to classify spectrograms.",
      "B": "Use SageMaker built-in KNN on MFCC vectors.",
      "C": "Fine-tune a JumpStart Transformer.",
      "D": "Use Amazon Transcribe to convert speech to text and Comprehend custom classification."
    },
    "explanation": "Transcribe plus Comprehend custom classification offers managed low-latency command detection without building audio models."
  },
  {
    "taskStatement": "2.1",
    "stem": "A rideshare company needs to price surge multipliers based on real-time location and demand. They require sub-second inference at scale. Which modeling approach and deployment should they choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a decision tree in SageMaker and deploy on CPU endpoint.",
      "B": "Use SageMaker real-time multi-model endpoints with XGBoost model.",
      "C": "Host a neural network on ECS Fargate.",
      "D": "Use Amazon Forecast predictor endpoint."
    },
    "explanation": "XGBoost on multi-model endpoints provides low-latency at scale with minimal endpoint overhead for multiple regions."
  },
  {
    "taskStatement": "2.3",
    "stem": "A financial services company observes that their fraud-detection model has high overall accuracy but low recall on high-value transactions. They suspect the model underfits rare, high-value cases. Which of the following evaluation strategies best identifies this issue?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Plot a single ROC curve on the full dataset and inspect AUC",
      "B": "Compute macro-averaged precision across all transaction values",
      "C": "Segment the test set by transaction value and compute per-segment recall",
      "D": "Evaluate overall F1 score without stratification"
    },
    "explanation": "Segmenting by transaction value and computing recall highlights underperformance on high-value cases. Overall AUC or aggregated metrics mask segment-specific issues."
  },
  {
    "taskStatement": "2.3",
    "stem": "An ML engineer deploys two XGBoost models: a control and a new variant. After 1M predictions, the variant shows a slightly higher latency but 2% higher recall. How should they evaluate if this trade-off is acceptable?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Compare single-sample inference logs for outliers",
      "B": "Plot precision-recall curves and latency distributions for both variants",
      "C": "Compute overall accuracy for both and pick model with higher accuracy",
      "D": "Use confusion matrix of the slower model only"
    },
    "explanation": "Precision-recall curves reveal how recall gain trades off against precision, while latency distributions quantify performance cost. Overall accuracy and single-sample latency don\u2019t capture distribution details."
  },
  {
    "taskStatement": "2.3",
    "stem": "During hyperparameter tuning of a neural network, SageMaker Debugger reports gradient vanishing in early layers. Which metric or visualization should you use to pinpoint the affected layers?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Plot TensorBoard histograms of weight and gradient distributions per layer",
      "B": "Inspect model\u2019s overall loss curve only",
      "C": "Compute validation accuracy after each epoch",
      "D": "View average prediction confidence on test data"
    },
    "explanation": "Layer-wise weight and gradient histograms directly show vanishing or exploding gradients. Loss curves and accuracy trends are too coarse to locate the problem."
  },
  {
    "taskStatement": "2.3",
    "stem": "A classification model shows 95% accuracy but customers complain of frequent false positives. Which metric will best capture the operational false-positive rate?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ROC AUC",
      "B": "Accuracy",
      "C": "Macro F1 score",
      "D": "False positive rate (FP/(FP+TN))"
    },
    "explanation": "False positive rate directly measures proportion of non-events incorrectly flagged. Aggregate metrics and AUC do not quantify FP in production terms."
  },
  {
    "taskStatement": "2.3",
    "stem": "An ML engineer wants to compare a shadow variant\u2019s performance to production. They have stored logs with predictions and ground truth over a week. What\u2019s the most reproducible approach?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Re-run inference on production endpoint and compute metrics",
      "B": "Compute metrics in real time on live data",
      "C": "Use the stored logs to batch calculate identical evaluation metrics for both models",
      "D": "Use shadow variant accuracy reported in CloudWatch metrics"
    },
    "explanation": "Batch calculating evaluation metrics on the same logged data ensures reproducibility. CloudWatch reports may differ in metric definitions, and re-running inference can produce drift."
  },
  {
    "taskStatement": "2.3",
    "stem": "After deploying a regression model, business users report that error increases when feature \"loan_amount\" exceeds $100k. Which diagnostic step is most appropriate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Check overall RMSE",
      "B": "Plot residuals against loan_amount",
      "C": "Compute R\u00b2 on training data",
      "D": "Re-train model without loan_amount"
    },
    "explanation": "Residual-vs-feature plots reveal heteroscedasticity or feature-specific error patterns. Overall RMSE or training R\u00b2 mask inequalities across feature ranges."
  },
  {
    "taskStatement": "2.3",
    "stem": "SageMaker Clarify indicates a high feature importance contribution from \"zipcode\" in loan approval model. Finance team worries about proxy bias. What evaluation should be done next?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Compute disparity metrics for protected groups defined by zipcode demographics",
      "B": "Remove zipcode and retrain immediately",
      "C": "Ignore since importance doesn\u2019t imply bias",
      "D": "Compute overall feature correlation matrix"
    },
    "explanation": "High importance for zipcode may proxy demographic bias; computing disparity metrics for protected groups reveals actual bias. Simply removing the feature may degrade performance without confirming bias."
  },
  {
    "taskStatement": "2.3",
    "stem": "A model\u2019s AUC on validation is 0.85 but drops to 0.75 in production. Which step best isolates the cause?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase regularization to reduce overfitting",
      "B": "Retrain with more epochs",
      "C": "Change the threshold to match validation TPR",
      "D": "Compare feature distribution between validation and production using Clarify data drift reports"
    },
    "explanation": "Distribution drift analysis identifies covariate changes causing performance drop. Regularization and threshold tuning don\u2019t address data mismatch."
  },
  {
    "taskStatement": "2.3",
    "stem": "During batch experiments, an engineer notices that increasing tree depth raises precision but lowers recall. Which plot best illustrates this trade-off?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Precision-recall curve parametrized by depth",
      "B": "ROC curve at each depth",
      "C": "Confusion matrix for the deepest tree only",
      "D": "Histogram of prediction confidences"
    },
    "explanation": "Precision-recall curves for each depth show the precision/recall trade-off directly. ROC curves and single-depth confusion matrices do not illuminate that trade dynamically."
  },
  {
    "taskStatement": "2.3",
    "stem": "After hyperparameter tuning, two models have identical validation loss but different generalization gaps. Which metric indicates stronger overfitting?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Higher training loss",
      "B": "Larger difference between training and validation accuracy",
      "C": "Higher validation AUC",
      "D": "Lower inference latency"
    },
    "explanation": "A large gap between training and validation accuracy indicates overfitting. Validation AUC and latency don\u2019t measure overfitting directly."
  },
  {
    "taskStatement": "2.3",
    "stem": "An NLP classification model shows significantly higher F1 for short texts versus long texts. How do you quantify performance disparity?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Overall F1 score",
      "B": "Word count distribution histogram",
      "C": "Group test samples by text length bins and compute per-bin F1",
      "D": "Pearson correlation between length and predicted label"
    },
    "explanation": "Bin by text length and compute F1 per bin to quantify disparity. Overall metrics and correlation don\u2019t isolate the problem."
  },
  {
    "taskStatement": "2.3",
    "stem": "You need to determine if a drop in model accuracy after a code change is due to random seed variance or genuine performance loss. Which practice ensures reproducibility?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Fix random seed across training, log model parameters, and compare artifacts",
      "B": "Re-run training with different seeds and average",
      "C": "Use early stopping to stabilize performance",
      "D": "Only compare inference results on live data"
    },
    "explanation": "Fixing seeds and logging artifacts ensures reproducible runs. Averaging across seeds helps smooth noise but doesn\u2019t isolate change impact."
  },
  {
    "taskStatement": "2.3",
    "stem": "An engineer suspects label leakage because a high validation AUC is too good to be true. Which diagnostic will reveal leakage?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase regularization",
      "B": "Shuffle labels and retrain to observe AUC",
      "C": "Add more layers",
      "D": "Compute feature correlations only"
    },
    "explanation": "Shuffling labels destroys true signal; if AUC remains high, leakage exists. Feature correlations alone may miss leakage patterns."
  },
  {
    "taskStatement": "2.3",
    "stem": "A deployed model exhibits high memory consumption during inference. Which SageMaker Debugger profiler metric helps pinpoint bottlenecks?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Overall endpoint invocation count",
      "B": "CloudWatch memory usage graphs",
      "C": "Inference latencies per request",
      "D": "Model container\u2019s PeakHostMemoryUsage metric"
    },
    "explanation": "PeakHostMemoryUsage from Debugger profiling identifies model container memory peaks. Invocation counts and latency don\u2019t measure memory usage.\n"
  },
  {
    "taskStatement": "2.3",
    "stem": "When comparing two regression models, you find one has lower MAE but higher RMSE. What does this indicate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "The second model has larger outlier errors",
      "B": "The second model performs better on average",
      "C": "The first model is overfitting",
      "D": "The first model has higher variance"
    },
    "explanation": "Higher RMSE than MAE implies presence of larger outlier errors, since RMSE penalizes them more. Average performance may be similar.\n"
  },
  {
    "taskStatement": "2.3",
    "stem": "You deploy an image classifier and monitor per-class recall. Class A recall drops below SLA overnight. Logs show no data drift. What is your next diagnostic?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase batch size",
      "B": "Retrain with more epochs",
      "C": "Analyze inference input distribution for class A by sampling inputs",
      "D": "Reduce learning rate"
    },
    "explanation": "Sampling inputs checks if unexpected inputs are arriving for Class A. No drift in overall features doesn\u2019t guarantee class-specific input change.\n"
  },
  {
    "taskStatement": "2.3",
    "stem": "Your model\u2019s production log shows many low-confidence predictions. Which threshold-based method helps maintain production SLA?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use maximum probability per sample as a calibration correction",
      "B": "Implement a confidence threshold to send low-confidence cases to manual review",
      "C": "Retrain until all predictions exceed a fixed threshold",
      "D": "Always return top-2 classes to clients"
    },
    "explanation": "A confidence threshold with fallback to manual review maintains SLA. Calibration alone won\u2019t meet SLA guarantees.\n"
  },
  {
    "taskStatement": "2.3",
    "stem": "An ML pipeline reruns hyperparameter tuning daily. You want to compare results over time. Which practice aids trend analysis?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Log only best tuning job\u2019s metrics",
      "B": "Store only final model artifacts",
      "C": "Compare only today's and yesterday's tuning graphs",
      "D": "Archive tuning job metrics and hyperparameter configurations in SageMaker Experiments"
    },
    "explanation": "SageMaker Experiments archives metrics and configs over time, enabling trend and retrospective analysis. Comparing partial data isn\u2019t sufficient.\n"
  },
  {
    "taskStatement": "2.3",
    "stem": "After retraining a model with more data, confusion matrix shows increased FN but decreased FP. Business impact of FNs is higher. How do you adjust the model?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Shift decision threshold to favor recall over precision",
      "B": "Increase regularization",
      "C": "Reduce training data size",
      "D": "Switch to a different algorithm"
    },
    "explanation": "Threshold adjustment trades precision and recall bias to reduce FNs, aligning with higher cost of FNs. Algorithm shift or data size changes are heavier-handed.\n"
  },
  {
    "taskStatement": "2.3",
    "stem": "SageMaker Clarify reports a DPL (difference in proportions of labels) of 0.12 for a protected group. Which conclusion is correct?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model is fair, since DPL < 0.2",
      "B": "There is moderate demographic bias requiring mitigation",
      "C": "DPL only applies to continuous features",
      "D": "Bias can be ignored if overall accuracy is high"
    },
    "explanation": "DPL of 0.12 indicates non-trivial bias that should be addressed. Fairness thresholds vary, but non-zero DPL warrants mitigation. Accuracy doesn\u2019t nullify bias.\n"
  },
  {
    "taskStatement": "2.3",
    "stem": "You need to diagnose why training loss plateaus early. Which combined SageMaker Debugger rule is most helpful?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "LossNotDecreasing and LargeGradient",
      "B": "VanishingGradient and OverTraining",
      "C": "LossNotDecreasing and VanishingGradient",
      "D": "OverTraining and HighThroughput"
    },
    "explanation": "LossNotDecreasing flags plateau, VanishingGradient identifies small gradients. Together they pinpoint stalled training due to gradient issues.\n"
  },
  {
    "taskStatement": "2.3",
    "stem": "After enabling endpoint data capture, you observe data skew for feature X. Which remediation reduces skew immediately?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Retrain online with captured data",
      "B": "Increase endpoint instance count",
      "C": "Switch to a different instance type",
      "D": "Implement input feature transformation to normalize X at inference"
    },
    "explanation": "Applying transformation at inference normalizes skewed feature values immediately. Retraining is longer-term and won\u2019t reduce skew on streaming data.\n"
  },
  {
    "taskStatement": "2.3",
    "stem": "Your binary classifier has high variance between folds in cross-validation. What change will most reduce variance?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use ensembles like random forests",
      "B": "Increase learning rate",
      "C": "Use fewer features",
      "D": "Reduce training set size"
    },
    "explanation": "Ensembling reduces model variance by averaging multiple learners. Lower LR or fewer features may increase bias. Reducing data increases variance.\n"
  },
  {
    "taskStatement": "2.3",
    "stem": "A class-imbalanced dataset yields high recall but poor precision. Which metric-guided tuning step helps restore balance?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase tree depth",
      "B": "Use mean squared error loss",
      "C": "Optimize model using precision-recall AUC as objective",
      "D": "Switch to accuracy objective"
    },
    "explanation": "Using PR AUC aligns hyperparameter tuning to both precision and recall on imbalanced data. MSE or accuracy objectives aren\u2019t appropriate for classification imbalance.\n"
  },
  {
    "taskStatement": "2.3",
    "stem": "An LSTM model overfits after 10 epochs. You add dropout but performance still degrades. Which debugging step helps find root cause?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase L2 regularization",
      "B": "Use Debugger\u2019s tensor shape rule to confirm no unintended dimension mismatch causing overfitting",
      "C": "Increase learning rate",
      "D": "Reduce batch size"
    },
    "explanation": "Tensor shape mismatches can cause incorrect weight updates that appear as overfitting. Debugger shape rules detect such bugs. Regularization alone may mask deeper issues.\n"
  },
  {
    "taskStatement": "2.3",
    "stem": "Your model produces inconsistent outputs when served on GPU versus CPU instances. How do you isolate source?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Retrain separately on GPU and CPU",
      "B": "Enable inference latency metrics",
      "C": "Use SageMaker Clarify to detect drift",
      "D": "Run inference deterministically on both environments with same input batch and compare element-wise outputs"
    },
    "explanation": "Deterministic inference comparison on CPU vs GPU pinpoints numerical instability or library inconsistency. Drift detection and latency metrics don\u2019t isolate environment differences.\n"
  },
  {
    "taskStatement": "2.3",
    "stem": "A regression model\u2019s residuals exhibit non-constant variance. Which transformation corrects this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Standardize all features",
      "B": "Apply log transform to the target variable",
      "C": "Increase polynomial degree of predictors",
      "D": "Use one-hot encoding for categorical features"
    },
    "explanation": "Log-transforming the dependent variable often stabilizes variance (heteroscedasticity). Feature standardization doesn\u2019t address residual variance issues.\n"
  },
  {
    "taskStatement": "2.3",
    "stem": "After changing preprocessing, model performance unexpectedly improved in test but dropped in validation. Which practice uncovers this data leakage?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Inspect preprocessing pipelines for operations applied before train-test split",
      "B": "Increase test set size",
      "C": "Add more regularization",
      "D": "Use k-fold CV on original split"
    },
    "explanation": "Applying preprocessing (e.g., normalization) before splitting leaks test information. Reviewing pipeline order reveals leakage.\n"
  },
  {
    "taskStatement": "2.3",
    "stem": "Your model\u2019s precision increases when you add a new feature but recall drops. You need to quantify the net business impact. What analysis should you perform?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Compute new overall F1 score",
      "B": "Plot ROC curve",
      "C": "Estimate expected cost change using per-error cost matrix",
      "D": "Compare AUC-PR before/after"
    },
    "explanation": "Cost matrix analysis translates precision/recall changes into business impact. F1 and curves are abstract performance measures without cost context."
  },
  {
    "taskStatement": "3.1",
    "stem": "A fintech startup has an image-based document verification model that requires sub-100ms inference latency at the edge. They must deploy the optimized model to ARM-based devices with intermittent connectivity. Which deployment infrastructure best meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Host the model on a SageMaker real-time endpoint in a VPC and use IoT Greengrass to pull predictions.",
      "B": "Compile the model with SageMaker Neo for ARM, deploy to IoT Greengrass devices for local real-time inference.",
      "C": "Package the model in a custom Docker container, deploy to AWS Lambda on ARM architecture.",
      "D": "Use a SageMaker serverless endpoint and cache responses on edge devices."
    },
    "explanation": "SageMaker Neo produces highly optimized ARM binaries for local (<100ms) inference. IoT Greengrass ensures offline capability."
  },
  {
    "taskStatement": "3.1",
    "stem": "An e-commerce application has a recommendation model that sees large traffic spikes during flash sales. It runs in a private VPC, uses GPU instances, and needs multi-model hosting with auto-scaling. Which architecture is optimal?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy separate single-model SageMaker real-time endpoints per model with target-tracking scaling.",
      "B": "Use a SageMaker serverless endpoint with model registry versioning.",
      "C": "Containerize models and host on ECS Fargate behind an ALB with custom scaling scripts.",
      "D": "Use a multi-model SageMaker real-time endpoint in the VPC with provisioned concurrency and auto-scaling policies."
    },
    "explanation": "Multi-model endpoint reduces overhead, shares GPU, and auto-scaling in VPC meets spikes."
  },
  {
    "taskStatement": "3.1",
    "stem": "A research team needs to batch-process 10 TB of genomic data weekly. They want to minimize cost, use the same container as real-time inference, and avoid idle pods. Which infra is best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create a SageMaker asynchronous endpoint with batch transform inside it.",
      "B": "Deploy models on EKS pods with KNative autoscaling.",
      "C": "Use SageMaker batch transform jobs with the real-time container and spot instances.",
      "D": "Host on a long-running ECS Fargate service and schedule container tasks."
    },
    "explanation": "Batch transform jobs spin up only for jobs, reuse container, support spot to reduce cost."
  },
  {
    "taskStatement": "3.1",
    "stem": "A global SaaS company requires GDPR compliance. Their NLP model must serve EU-only traffic in dedicated subnets and scale on demand. Which deployment solution is most appropriate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy regional SageMaker real-time endpoints in EU-based subnets with endpoint auto scaling.",
      "B": "Deploy a single SageMaker serverless endpoint and enforce subnet restrictions.",
      "C": "Host on ECS Fargate in one EU cluster and route all traffic through it.",
      "D": "Use Lambda functions behind API Gateway with VPC peering to subnets."
    },
    "explanation": "Regional real-time endpoints in EU subnets ensure data residency and scalable performance."
  },
  {
    "taskStatement": "3.1",
    "stem": "A startup needs to test a new fraud-detection model in production without risking customer impact. They want to route 5% of traffic to the new model, track performance, and easily roll back. Which deployment strategy and infra should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy two SageMaker asynchronous endpoints and split payloads manually.",
      "B": "Use a SageMaker serverless endpoint for the new model and shift DNS entries.",
      "C": "Deploy the new model to a SageMaker real-time endpoint variant in the same endpoint, configure a canary traffic shift.",
      "D": "Host the new model in ECS Fargate and use Application Load Balancer weighted routing."
    },
    "explanation": "Real-time endpoint variants support canary deployments and easy rollback within the same endpoint."
  },
  {
    "taskStatement": "3.1",
    "stem": "An advertising platform has multiple small models (<100MB) that share similar preprocessing code. They need to deploy 50 models, optimize resource utilization, and minimize cost. What is the best approach?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Deploy 50 separate real-time SageMaker endpoints.",
      "B": "Use ECS Fargate and spin up a container per model.",
      "C": "Bundle all models into one Docker image and host on a single serverless endpoint.",
      "D": "Use a SageMaker multi-model endpoint with shared container in a single VPC endpoint."
    },
    "explanation": "Multi-model endpoints share the container and infra, reducing cost and overhead for many small models."
  },
  {
    "taskStatement": "3.1",
    "stem": "A biotech firm must run long-running inference jobs (up to 30 minutes each) that process large data packages. They want predictable scaling and pay-per-use cost. Which SageMaker deployment fits best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure a SageMaker asynchronous inference endpoint with appropriate max payload size.",
      "B": "Use a SageMaker real-time endpoint and extend Lambda timeouts.",
      "C": "Submit each job as a batch transform.",
      "D": "Host it on ECS Fargate with long-running tasks."
    },
    "explanation": "Asynchronous endpoints handle long payloads, non-blocking, scale predictably and charge per call."
  },
  {
    "taskStatement": "3.1",
    "stem": "A media company needs GPU acceleration for video-related inference but wants to minimize idle GPU costs during off-hours. They can tolerate 1\u20132 second cold start. Which infra is optimal?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Keep a provisioned GPU SageMaker real-time endpoint running with auto-scaling.",
      "B": "Use SageMaker serverless endpoints configured with GPU support.",
      "C": "Deploy on ECS Fargate GPU instances with manual scaling schedules.",
      "D": "Host on Lambda GPU functions behind an ALB."
    },
    "explanation": "Serverless endpoints with GPU avoid idle costs and manage cold starts within tolerance."
  },
  {
    "taskStatement": "3.1",
    "stem": "An auto manufacturer needs to deploy multiple model versions concurrently for A/B tests, with equal traffic share and easy rollback. They must run in the same VPC. Which infra supports this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy separate ECS services per version behind an ALB.",
      "B": "Use Lambda versions and aliases in VPC.",
      "C": "Deploy separate SageMaker serverless endpoints per version.",
      "D": "Use a SageMaker real-time endpoint with two production variants and traffic weights."
    },
    "explanation": "Production variants in one real-time endpoint allow weighted traffic splitting and quick rollback."
  },
  {
    "taskStatement": "3.1",
    "stem": "A retailer needs to deploy a recommendation model to handle unpredictable holiday season traffic. They need <50ms latency, GPU inference, and minimal ops overhead. Which solution is best?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use SageMaker serverless endpoints with GPU acceleration and auto-scaling.",
      "B": "Deploy a spot-based ECS GPU cluster with custom autoscaler.",
      "C": "Maintain a provisioned GPU real-time endpoint with scheduled scaling.",
      "D": "Run inference in Lambda with EFS-mounted model files."
    },
    "explanation": "Serverless GPU endpoints auto-scale without manual capacity management and meet latency."
  },
  {
    "taskStatement": "3.1",
    "stem": "A healthcare application requires batch scoring of PHI data with encryption at rest and in transit. They want to reuse existing VPC resources and avoid cross-account data movement. Best infra?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker serverless endpoint in VPC with data capture.",
      "B": "Real-time endpoint in VPC with KMS encryption.",
      "C": "Batch transform jobs configured in the same VPC and encrypted S3.",
      "D": "ECS Fargate with EFS encryption."
    },
    "explanation": "Batch transform in VPC uses S3 encrypted data, no constant endpoint and reuse VPC security."
  },
  {
    "taskStatement": "3.1",
    "stem": "A gaming company needs to deploy a scoring model to hundreds of edge kiosks with no always-on internet and limited compute. They need occasional connectivity to sync metrics. Which infra fits?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a SageMaker real-time endpoint through AWS IoT Core.",
      "B": "Compile via SageMaker Neo, deploy to Lambda@Edge on Greengrass devices.",
      "C": "Host containerized model on small EC2 instances at each kiosk.",
      "D": "Deploy inference code as AWS Lambda functions invoked by MQTT."
    },
    "explanation": "Neo-compiled model on Greengrass devices runs locally, syncs periodically over IoT Core."
  },
  {
    "taskStatement": "3.1",
    "stem": "A logistics company has a micro-batch inference requirement: group deliveries into sets of 100 and process every 5 minutes. They want minimal latency overhead and no idle endpoints. What infrastructure should they choose?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy a real-time endpoint and buffer requests into batches.",
      "B": "Use serverless endpoint with batched invocation.",
      "C": "Submit jobs to ECS Fargate tasks scheduled every 5 minutes.",
      "D": "Configure a SageMaker asynchronous inference endpoint with batching config."
    },
    "explanation": "Asynchronous endpoint supports batching and scheduled triggers without idle infra."
  },
  {
    "taskStatement": "3.1",
    "stem": "A startup wants to experiment with GPU- and CPU-based inference to compare cost-performance trade-offs. They need a common orchestration tool and seamless model promotion. Which approach is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Pipelines to deploy the same model to GPU and CPU real-time endpoints.",
      "B": "Script deployments in Terraform to provision ECS GPU and CPU clusters.",
      "C": "Use AWS Batch for GPU and Lambda for CPU inference.",
      "D": "Manually launch EC2 GPU and CPU instances and copy containers."
    },
    "explanation": "SageMaker Pipelines automates deployment steps, tracks versions, and supports multiple compute targets."
  },
  {
    "taskStatement": "3.1",
    "stem": "A financial modeling team needs to deploy Python-based models that require custom system libraries. They want minimal management and autoscaling. Which deployment target is most appropriate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy as a SageMaker serverless endpoint with layers.",
      "B": "Use Lambda with custom runtime layer.",
      "C": "Build a custom Docker container and host on SageMaker real-time endpoint in VPC.",
      "D": "Use ECS Fargate with code-build pipelines."
    },
    "explanation": "Custom container on SageMaker real-time endpoint supports all dependencies, auto-scales, VPC support."
  },
  {
    "taskStatement": "3.3",
    "stem": "A data science team wants to automate their ML workflow so that any code commit on the \u2018develop\u2019 branch runs unit tests, data validation, and triggers a SageMaker training pipeline. After successful training, the model should be registered and automatically deployed to a staging endpoint for integration tests, before manual approval pushes to production. Which CI/CD design best meets these requirements?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use CodePipeline with a Source action on GitHub \u2018develop\u2019, CodeBuild to run unit tests and data validation scripts, a CodeBuild action to invoke a SageMaker Pipeline for training, a stage to register the model in the Model Registry, a CloudFormation action to deploy to staging, an integration-test CodeBuild action, followed by a manual approval and a Lambda action to swap the staging endpoint to production.",
      "B": "Use SageMaker Pipelines with Git integration triggering on \u2018develop\u2019 for training and registration, then use a separate CodePipeline triggered by new Model Registry entries to deploy to staging and run integration tests, with manual approval for production.",
      "C": "Use CodePipeline with a GitHub source on \u2018develop\u2019, a CodeBuild stage to run unit tests and data quality checks, a SageMaker Pipeline action to train and register the model, a CloudFormation stage to deploy to a staging endpoint, a CodeBuild integration-test stage, then a manual approval stage and a CloudFormation canary-deployment stage for production.",
      "D": "Use CodeDeploy directly to push code changes to SageMaker endpoints, adding a CodeDeploy pre-deployment hook to run tests and trigger training before deployment."
    },
    "explanation": "Option C uses CodePipeline native actions for each stage, integrates SageMaker Pipelines, staging, tests, manual approval, and a canary production deployment with minimal glue code."
  },
  {
    "taskStatement": "3.3",
    "stem": "A compliance policy requires monthly retraining of an ML model using new data uploaded to s3://company/data/monthly/ at 1 AM on the first of each month. The workflow must run data quality checks, trigger SageMaker model training, register the model, and send an email if any stage fails. How should you configure this with minimal overhead?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create an EventBridge scheduled rule that invokes a Lambda function which runs data quality checks, starts a SageMaker training job, registers the model, and publishes to SNS on failure.",
      "B": "Create an EventBridge scheduled rule to start a CodePipeline at 1 AM monthly. In CodePipeline, add: a Source stage (S3 polling), a CodeBuild stage for data quality checks, a SageMaker Pipeline invoke stage for training and registration, and a CloudWatch Events action to notify via SNS on failure.",
      "C": "Use SageMaker Pipelines with a Schedule trigger for monthly runs, include DataQualityCheck, Train, Register steps, and configure an SNS Alarm for failed steps.",
      "D": "Use AWS Batch scheduled on EventBridge to run a container that executes the entire workflow and sends SNS notifications."
    },
    "explanation": "Option B leverages EventBridge\u2192CodePipeline schedule, built-in actions (S3 source, CodeBuild, SageMaker Pipeline, SNS) with minimal custom code."
  },
  {
    "taskStatement": "3.3",
    "stem": "An organization maintains separate dev and prod AWS accounts. They want a centralized CI/CD in dev that, upon merging to master in CodeCommit, triggers a deployment pipeline in prod account to update the SageMaker endpoint. How should they configure this cross-account pipeline?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "In dev account create a CodePipeline with a CloudFormation action targeting prod account using StackSets.",
      "B": "Push artifacts from dev S3 bucket to prod S3 bucket, then have a prod pipeline polling that bucket to start deployment.",
      "C": "Use AWS CodeDeploy cross-account deployments from dev to prod.",
      "D": "In dev CodePipeline, add an IAM role action with a role ARN in prod account. Grant the dev pipeline role permission in prod to assume that role. In prod, that role\u2019s trust policy allows the dev pipeline service role to assume it and run deployment actions."
    },
    "explanation": "Option D correctly uses IAM role assumption to allow a pipeline in one account to execute actions in another account without custom polling."
  },
  {
    "taskStatement": "3.3",
    "stem": "A team follows Gitflow: feature/* branches, develop, release/*, and master. They want two pipelines: one for develop that runs tests and staging deployments on merges to develop, and one for production on merges to master. How should they filter source triggers in CodePipeline?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use two CodePipelines with GitHub source actions configured: one with branch filter pattern '^refs/heads/develop$' and webhooks enabled, the other with '^refs/heads/master$'.",
      "B": "Use a single pipeline triggered on all branches; in the first stage inspect the branch name and abort if not develop or master.",
      "C": "Use CloudWatch Events on push events with wildcard branch patterns to start pipelines via Lambda.",
      "D": "Configure CodePipeline to poll S3 artifacts tagged with branch names to distinguish branches."
    },
    "explanation": "Option A uses CodePipeline\u2019s built-in branch filters in source actions to isolate triggers per pipeline."
  },
  {
    "taskStatement": "3.3",
    "stem": "A data engineering pipeline produces a delta file in S3 upon completion. You must ensure that your ML CI/CD pipeline ingests this file, validates schema against a baseline, and fails if mismatched, before triggering training. How do you integrate this into CodePipeline?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Have CodePipeline poll the S3 prefix for the delta file and pass it to a Lambda that does schema validation and returns success/fail.",
      "B": "Trigger CodePipeline with an S3 event notification directly to the pipeline\u2019s StartPipelineExecution API.",
      "C": "Use an EventBridge rule matching the S3 PutObject event to call StartPipelineExecution, then in CodePipeline add a CodeBuild stage that runs a schema-validation script against the new file and fails on mismatch.",
      "D": "Use Lambda to copy the file to a Kinesis stream, use a CodePipeline source action on the stream, and a CodeBuild validation stage."
    },
    "explanation": "Option C uses EventBridge\u2192CodePipeline to start on S3 writes and uses CodeBuild to validate schema, failing if mismatched."
  },
  {
    "taskStatement": "3.3",
    "stem": "Your ML pipeline requires a secret API key available to training jobs at build time. You must not expose the secret in plain text or logs. Which CodePipeline stage configuration meets this requirement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Store the API key in S3 encrypted with KMS. In CodeBuild, download and decrypt it inside the build script.",
      "B": "Store the API key in AWS Secrets Manager. In the CodeBuild project definition, set an environment variable that references the Secrets Manager ARN and enable 'Reveal secrets in build logs' to false.",
      "C": "Hard-code the secret in an encrypted parameter in Systems Manager Parameter Store and pull it during the build.",
      "D": "Pass the secret as a plaintext environment variable in the CodePipeline action configuration, relying on IAM to protect it."
    },
    "explanation": "Option B uses Secrets Manager with environment variables in CodeBuild, ensuring secrets aren\u2019t logged."
  },
  {
    "taskStatement": "3.3",
    "stem": "A reloadable data pipeline updates feature data daily. You want to trigger model retraining automatically whenever a new featureset lands in the SageMaker Feature Store. What\u2019s the least\u2010maintenance way to integrate this into your CI/CD pipeline?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Modify your daily ETL job to call the CodePipeline StartPipelineExecution API at its end.",
      "B": "Use S3 event notifications on the Feature Store underlying S3 bucket to trigger a Lambda that starts the pipeline.",
      "C": "Add a CloudWatch alarm on Feature Store PutRecord metrics and have it trigger pipeline via SNS.",
      "D": "Create an EventBridge rule matching the SageMaker Feature Store PutRecord API via CloudTrail, target StartPipelineExecution for your training pipeline."
    },
    "explanation": "Option D leverages CloudTrail events in EventBridge to detect new records and start the pipeline with zero custom code in the ETL job."
  },
  {
    "taskStatement": "3.3",
    "stem": "You need to implement a canary deployment for a real-time SageMaker endpoint in your pipeline. It must shift 5% traffic to the new model initially, then 50%, then 100%, with automatic rollback on errors. Which combination of actions do you use in CodePipeline?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use CloudFormation deploy action with TYPE=BLUE_GREEN and trafficRoutingConfig CanaryInterval (5%) and CanaryInterval (50%), and a CloudWatch alarm action to auto-rollback.",
      "B": "Use CodeDeploy with application type 'AWS Lambda' against the SageMaker endpoint and set canaryDeploymentConfig.",
      "C": "Invoke a Lambda function in each stage to call UpdateEndpointWeightsAndCapacities on the endpoint.",
      "D": "Use a custom CodeBuild action to call the SageMaker API to shift traffic and monitor latency; if latency rises, call rollback API."
    },
    "explanation": "Option A uses CloudFormation blue/green with canary traffic routing and automatic rollback via alarms, requiring no custom code."
  },
  {
    "taskStatement": "3.3",
    "stem": "A governance requirement mandates that any pipeline change must be peer-reviewed before execution. You want to enforce code reviews for the buildspec.yaml and pipeline definition in CDK. What CI/CD pattern accomplishes this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Allow developers to commit directly to main; have a pre-build hook that runs only if PR label 'reviewed' exists.",
      "B": "Store CDK pipeline code and buildspec in a feature branch, require PR approval in CodeCommit/GitHub, merge only after two approvers, then trigger pipeline on merge to main.",
      "C": "Use CodeBuild PreBuild commands to validate a 'reviewed-by' tag in Git metadata before proceeding.",
      "D": "Configure a manual approval action at the start of every pipeline run to verify that PRs were reviewed externally."
    },
    "explanation": "Option B leverages standard Git-based code-review gating before code merges and pipeline triggers."
  },
  {
    "taskStatement": "3.3",
    "stem": "Your pipeline runs SageMaker training jobs in parallel for hyperparameter tuning. Sometimes runs collide and exceed resource quotas, causing failures. How do you prevent concurrent training executions?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Limit the maximum concurrent CodeBuild builds in the project settings to 1.",
      "B": "Use a Lambda in your pipeline to list current SageMaker jobs and fail fast if any RUNNING jobs exist.",
      "C": "Configure the CodePipeline stage for training to use an ActionConfiguration including a custom concurrency token in your state machine so that only one execution at a time proceeds.",
      "D": "Set the SageMaker training action\u2019s maximum concurrency to 1 in the pipeline definition."
    },
    "explanation": "Option C uses a concurrency token to serialize that action\u2019s invocations; CodePipeline supports concurrencyToken to prevent parallel runs."
  },
  {
    "taskStatement": "3.3",
    "stem": "You want to include a data-skew check using SageMaker Clarify in your CI/CD before training. Which pipeline stage sequence and actions accomplish this without custom containers?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "After unit tests, add a CodeBuild stage that invokes SageMaker Clarify ProcessingStep via AWS CLI, passing baseline and new dataset; fail build on detected skew.",
      "B": "Add a custom Docker build to run a Python script that calls Clarify SDK inside CodeBuild.",
      "C": "Trigger SageMaker Pipelines containing a ClarifyBaselineStep; ingest results via SNS into CodePipeline.",
      "D": "Use Lambda action in CodePipeline to call Clarify\u2019s API and post results to S3, then have pipeline poll for results."
    },
    "explanation": "Option A uses CodeBuild with the AWS CLI to call SageMaker Clarify ProcessingJob, avoiding custom Docker images."
  },
  {
    "taskStatement": "3.3",
    "stem": "Your pipeline must run unit tests, then integration tests against a temporary SageMaker endpoint after deployment, and delete the endpoint automatically when tests complete or fail. What do you add?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A CodeDeploy preTraffic hook to run tests and a postRollback hook to delete the endpoint.",
      "B": "A Lambda in CodePipeline to run tests and then call DeleteEndpoint.",
      "C": "A CodeBuild test stage that spins up the endpoint, tests, then calls DeleteEndpoint in the same build.",
      "D": "Two CodeBuild actions: one for integration tests against the staging endpoint, then one that deletes the endpoint by calling AWS CLI delete-endpoint; add them sequentially and configure failureAction=Abort."
    },
    "explanation": "Option D cleanly separates testing and teardown in CodeBuild stages, ensuring teardown always runs after tests if preceding stages succeed."
  },
  {
    "taskStatement": "3.3",
    "stem": "A developer accidentally pushed sensitive data to a feature branch. You need to prevent that branch from ever triggering your CI/CD pipeline. How do you modify your pipeline\u2019s source action?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a whitelist of allowed branch patterns in the buildspec to abort on unallowed patterns.",
      "B": "Configure the GitHub source action to include an exclude filter 'refs/heads/feature/sensitive*'.",
      "C": "Use a Lambda in the source stage to inspect commits and abort if the file pattern matches.",
      "D": "Add a pre-build CodeBuild action that checks commit history for sensitive files."
    },
    "explanation": "Option B uses the source action\u2019s exclude filter to prevent specific branch patterns from triggering the pipeline."
  },
  {
    "taskStatement": "3.3",
    "stem": "Your ML pipeline needs to use Spot Instances for cost-saving on training. How do you enable this in your CodePipeline/SageMaker training integration?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "In the SageMaker Pipeline definition YAML, set ResourceConfig.AuxiliarySpotCapacity to the desired percentage and invoke via CodePipeline\u2019s SageMakerPipelineAction.",
      "B": "Add a CodeBuild stage that runs 'aws sagemaker create-training-job' CLI with --use-spot-instances flag.",
      "C": "Wrap training in a Lambda function configured with capacityType=SPOT and call it from CodePipeline.",
      "D": "Switch the CodePipeline action type to AWS Batch and configure Spot in the ComputeEnvironment."
    },
    "explanation": "Option A leverages SageMaker Pipelines spot training support via ResourceConfig and CodePipeline\u2019s SageMakerPipelineAction without custom code."
  },
  {
    "taskStatement": "3.3",
    "stem": "You need to trigger your CI/CD pipeline whenever a pull request is opened or updated, and only for PRs targeting develop. How is this configured?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure CodePipeline Source with GitHub webhook on all PR events and in the build stage inspect the target branch.",
      "B": "Use EventBridge rule for GitHub PullRequest events filtering on repository name.",
      "C": "Enable GitHub Pull Request webhooks in the source action and set the targetReference filter to 'refs/heads/develop'.",
      "D": "Use a Lambda subscriber to GitHub webhooks to call StartPipelineExecution when PR target is develop."
    },
    "explanation": "Option C uses the source action\u2019s webhook pull request event support with targetReference filters to limit triggers to PRs against develop."
  },
  {
    "taskStatement": "3.3",
    "stem": "Your staging endpoint runs expensive benchmarks as part of integration tests. To avoid excessive cost, you only want to run tests when changes affect inference code. How do you optimize your pipeline?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Cache build artifacts between runs to speed up deployment.",
      "B": "Use a manual approval before integration tests.",
      "C": "Use a Lambda pre-check in CodePipeline to compare diff of inference directory and skip tests stage if no changes.",
      "D": "Split your repository: place inference code in its own folder and configure two pipelines\u2014one that triggers full workflow on inference folder changes (using source filters) and one for other changes."
    },
    "explanation": "Option D uses separate pipelines with source filtering to avoid running expensive tests when unrelated code changes."
  },
  {
    "taskStatement": "3.3",
    "stem": "Your pipeline fails intermittently at the SageMaker Pipeline invocation stage due to transient throttling errors. How do you make the pipeline more resilient?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Wrap the SageMakerPipelineAction in a CodeBuild action with a retry loop using exponential backoff on throttling errors.",
      "B": "Increase the service quota for StartPipelineExecution calls.",
      "C": "Add a manual retry approval stage after the failure.",
      "D": "Use a Lambda-based custom action that catches throttles and auto-retries once."
    },
    "explanation": "Option A uses a CodeBuild wrapper to implement retries with backoff, improving resilience without manual intervention."
  },
  {
    "taskStatement": "3.3",
    "stem": "You want your CI/CD pipeline to automatically rollback to the last known good model if performance metrics degrade after deployment. How can you implement this in CodePipeline?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Include a manual approval stage that runs a performance test, and if it fails, manually invoke rollback.",
      "B": "Use a Lambda in the post-deploy stage to run a test; if metrics below threshold, call UpdateEndpoint to the previous variant.",
      "C": "Use a CloudWatch alarm on performance metrics to trigger a CodePipeline retrigger of the deploy stage with the previous model via CodeDeploy automatic rollback.",
      "D": "Publish the previous model as an alias in Model Registry and switch alias in a Lambda action on failure."
    },
    "explanation": "Option C ties CloudWatch alarms to CodePipeline\u2019s deploy stage with a configured rollback in CodeDeploy, automating fallback."
  },
  {
    "taskStatement": "3.3",
    "stem": "Your team uses GitHub Flow, pushing directly to main and using feature branches only for experiments. They want every merge to main to trigger a full pipeline. But they also use semantic version tags (e.g., v1.2.0) to denote releases. How do you ensure only tag pushes trigger production deployments?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure CodePipeline source to only trigger on main branch updates.",
      "B": "Use two pipelines with GitHub source actions: one listening to refs/tags/v* (for production) and another to refs/heads/main (for tests), and in the production pipeline restrict to tag pattern '^refs/tags/v[0-9]+.*$'.",
      "C": "Add logic in the build stage to parse GIT_REF and only continue if it\u2019s a tag.",
      "D": "Use EventBridge pattern matching on GitHub Tag events to start the pipeline."
    },
    "explanation": "Option B cleanly separates pipelines by source filters: one triggers on tag pushes for production deployments."
  },
  {
    "taskStatement": "3.3",
    "stem": "A pipeline source stage uses AWS CodeCommit. Developers occasionally force-push and rewrite history, causing source metadata mismatches. How do you make the pipeline robust against force pushes?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable polling instead of webhooks in the CodeCommit source action.",
      "B": "Add a pre-build stage to Git fetch --prune and reset --hard origin/branch.",
      "C": "Configure the CodePipeline source action to use 'FullClone' fetch method to always get the current tip regardless of history rewrite.",
      "D": "Disable code commit history rewrite in the repository settings."
    },
    "explanation": "Option C uses the FullClone fetch mode so the pipeline always pulls the latest commit independent of history changes."
  },
  {
    "taskStatement": "3.3",
    "stem": "You need to integrate model explainability tests into your pipeline: after training, fail the pipeline if any feature attribution for protected classes exceeds a threshold. Which stage should you add?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a CodeBuild stage that runs a SageMaker Clarify ModelExplainabilityProcessingJob via AWS CLI with the trained model, parses the results JSON, and exits non-zero if attribution drift exceeds the threshold.",
      "B": "Invoke a Lambda function in a pipeline action that calls Clarify SDK and uses CloudWatch metrics to decide pass/fail.",
      "C": "Embed the Clarify step inside the SageMaker Pipeline and then have CodePipeline poll for model registry tags to decide.",
      "D": "Use a CloudFormation custom resource to run the Clarify job and roll back the stack on failure."
    },
    "explanation": "Option A uses CodeBuild to orchestrate Clarify explainability checks, parse results, and enforce pipeline success/failure cleanly."
  },
  {
    "taskStatement": "3.3",
    "stem": "A pipeline stage builds and pushes a Docker image to ECR, then a SageMaker training action uses that image. Occasionally, the training job starts before the image is available, causing errors. How do you prevent this race condition?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a fixed sleep delay in the training action\u2019s custom script.",
      "B": "Use an EventBridge rule on ECR image-push to trigger the pipeline second stage.",
      "C": "Configure CodeBuild to push to a different tag and have training wait on tag propagation.",
      "D": "Split the build-and-push into one CodeBuild stage that outputs the image URI as an artifact, and configure the next SageMakerPipelineAction to consume that artifact, which enforces sequencing."
    },
    "explanation": "Option D uses CodePipeline artifact dependency to guarantee that the training stage does not start until the image has been successfully pushed and the URI passed along."
  },
  {
    "taskStatement": "3.3",
    "stem": "Your pipeline\u2019s CodeBuild unit-test stage frequently downloads the entire repository, slowing builds. How do you optimize clone behavior?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Configure the GitHub source action to use 'ShallowClone' with a depth of 1, and enable CodeBuild\u2019s source cache for the repo.",
      "B": "Use Git LFS to store large binaries outside the repo.",
      "C": "Split the repo into micro-repos each with its own pipeline.",
      "D": "Switch the source action to S3 and upload only changed files manually."
    },
    "explanation": "Option A combines shallow cloning at the source action with CodeBuild caching to speed up fetch time without repo restructuring."
  },
  {
    "taskStatement": "3.3",
    "stem": "A CI/CD pipeline should only deploy models when both unit tests and integration tests pass. However, you need integration tests to run against a live endpoint, which itself requires deployment. How do you model this in CodePipeline?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run both unit and integration tests in the same CodeBuild action after deployment.",
      "B": "Chain two separate pipelines: one for tests, one for deployment, and only call the deploy pipeline if tests pass.",
      "C": "In a single pipeline: Stage1 run unit tests via CodeBuild, Stage2 deploy to staging via CloudFormation, Stage3 run integration tests via CodeBuild, Stage4 manual approval and production deploy.",
      "D": "Use a SageMaker Pipeline that supports validation steps natively and call it from CodePipeline."
    },
    "explanation": "Option C lays out a linear pipeline with distinct stages for unit tests, deployment, integration tests, and production approval."
  },
  {
    "taskStatement": "3.3",
    "stem": "You want to notify your team in Slack whenever any pipeline stage fails, including the stage name and error message. How do you configure this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Attach an SNS topic to pipeline notifications and configure an AWS Chatbot integration for Slack.",
      "B": "Create a CloudWatch EventBridge rule matching CodePipeline 'Stage Execution Failed' events, target a Lambda that formats the message and posts to Slack via webhook.",
      "C": "In each CodePipeline stage\u2019s onFailure hook, invoke a Lambda to post to Slack.",
      "D": "Enable CloudTrail logs for CodePipeline and stream failures to Slack using Log Insights."
    },
    "explanation": "Option B uses a single EventBridge rule and a Lambda to capture failure events and send formatted Slack messages, minimizing per-stage configuration."
  },
  {
    "taskStatement": "3.3",
    "stem": "An executive insists that production deployments require two separate approvers. How can you enforce this in CodePipeline?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add two successive ManualApproval actions in the production deployment stage, each assigned to a different user group.",
      "B": "Configure the ManualApproval action with 'approvalThreshold' set to 2.",
      "C": "Use IAM policies to require MFA for invoking the production stage.",
      "D": "Chain two identical manual approval actions but restrict both to the same group."
    },
    "explanation": "Option A ensures two distinct approvals by having two manual approval actions in series, each can be assigned to different user sets."
  },
  {
    "taskStatement": "3.3",
    "stem": "Your CodePipeline deploys a SageMaker model via CloudFormation. When the deployment fails, the pipeline remains stuck in a FAILED state and you must manually clean up. How do you configure automatic rollback of the CloudFormation stack?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a CloudFormation DeleteStack action after the deploy action conditioned on failure.",
      "B": "Use a Lambda in the onFailure hook to delete the stack.",
      "C": "Set the CloudFormation action\u2019s 'ActionMode' to CHANGE_SET_REPLACE and 'RollbackOnFailure' parameter to true.",
      "D": "Wrap the CloudFormation deploy in an AWS Step Functions state machine with a Catch to delete the stack."
    },
    "explanation": "Option C leverages CloudFormation action parameters to rollback automatically, avoiding manual cleanup."
  },
  {
    "taskStatement": "3.3",
    "stem": "You need to run parallel hyperparameter tuning and data quality steps in your pipeline to reduce total runtime. How do you implement parallel actions in CodePipeline?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create multiple pipelines and start them simultaneously from a Lambda.",
      "B": "Use SageMaker Pipelines which natively parallelizes steps.",
      "C": "Run one CodeBuild action that multiplexes both tasks in background processes.",
      "D": "Define two actions within the same pipeline stage; CodePipeline will execute them in parallel, then proceed only when both succeed."
    },
    "explanation": "Option D uses CodePipeline\u2019s support for parallel actions in a single stage to run tasks concurrently."
  },
  {
    "taskStatement": "4.1",
    "stem": "An online retailer deploys a real-time inference endpoint for product recommendations. Shortly after launch, they notice that the distribution of a key categorical feature \"user_region\" has shifted, although model accuracy remains within SLA. Which automated monitoring solution should they implement to detect and alert on this feature distribution change with minimal custom coding?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure a SageMaker Model Quality Monitor job to check for data drift on \"user_region\" using ground truth labels.",
      "B": "Use SageMaker Model Monitor DataQuality baselining to establish constraints on \"user_region\" and schedule a drift detection job.",
      "C": "Deploy SageMaker Clarify to compute SHAP values for \"user_region\" on each inference and alert when feature importance changes.",
      "D": "Write an AWS Lambda that polls CloudWatch logs, computes a KS test on \"user_region\", and sends SNS alerts."
    },
    "explanation": "Model Monitor DataQuality jobs can baseline feature distributions and automatically detect drift with minimal custom code. ModelQualityMonitor requires labels; Clarify focuses on bias/importance rather than raw distribution; a Lambda solution requires more custom work."
  },
  {
    "taskStatement": "4.1",
    "stem": "A financial institution uses a SageMaker real-time endpoint for credit scoring. They need to monitor inference latency degradation and data schema violations. Which combination of SageMaker features meets both requirements natively?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure SageMaker Model Debugger to capture latency and data quality metrics.",
      "B": "Use CloudWatch Contributor Insights for the endpoint to detect schema violations and latency issues.",
      "C": "Enable Data Capture on the endpoint and run SageMaker Model Monitor with both DataQuality and ModelQuality checks.",
      "D": "Deploy AWS X-Ray for latency tracing and AWS Glue Data Quality for schema validation."
    },
    "explanation": "Data Capture plus Model Monitor can track payload schema violations via DataQuality checks and latency via custom metrics. Model Debugger focuses on training/debug; Contributor Insights and Glue don\u2019t natively integrate inference monitoring as well."
  },
  {
    "taskStatement": "4.1",
    "stem": "A company wants to detect concept drift in predicted probabilities of a binary classifier in production in near real-time. They also want to minimize cost. Which monitoring configuration should they choose?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run a SageMaker Clarify ModelExplainabilityMonitor on each batch of incoming requests.",
      "B": "Configure an asynchronous SageMaker endpoint with built-in data drift detection.",
      "C": "Use SageMaker Model Quality Monitor with ground truth labels and a 5-minute schedule.",
      "D": "Enable Data Capture on the endpoint and schedule a SageMaker Model Monitor DataQuality drift check on predicted score distribution."
    },
    "explanation": "Without ground truth, ModelQualityMonitor isn\u2019t applicable. Clarify ExplainabilityMonitor focuses on SHAP. Asynchronous endpoints have no built-in drift. DataQuality drift checks on predictions detect concept drift cost effectively."
  },
  {
    "taskStatement": "4.1",
    "stem": "An ML engineer deployed two production variants (A and B) for A/B testing. They need to monitor emerging data skew between the variants for a continuous categorical feature \"device_type\". Which approach best surfaces variant-specific data skew?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure two separate Model Monitor DataQuality jobs, each capturing and analyzing \"device_type\" for its variant.",
      "B": "Use a single Model Monitor DataQuality job that collects data from both variants and uses filters on \"variant_name\".",
      "C": "Use SageMaker Clarify to detect bias between variants on \"device_type\".",
      "D": "Aggregate CloudWatch logs across variants and run a post-processing job to compare distributions."
    },
    "explanation": "Separate DataQuality jobs allow per-variant baselines and alerts. A single job with filters isn't supported; Clarify is for bias interpretation, not raw skew; log aggregation is more custom work."
  },
  {
    "taskStatement": "4.1",
    "stem": "During inference, a regression model returns NaN for target predictions intermittently. Which Model Monitor configuration will automatically detect this anomaly and notify the team?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Enable SageMaker Model Debugger to capture NaNs during inference.",
      "B": "Use Model Monitor DataQuality baseline constraints to flag NaN values in the prediction column.",
      "C": "Set up Amazon CloudWatch anomaly detection on inference response logs to catch NaNs.",
      "D": "Configure SageMaker Clarify to test for invalid prediction values."
    },
    "explanation": "DataQuality monitors can enforce no-NaN constraints on any column. Debugger focuses on training tensors; CloudWatch anomaly detection requires custom log parsing; Clarify is for bias/explainability."
  },
  {
    "taskStatement": "4.1",
    "stem": "A language translation model hosted on SageMaker Edge Manager runs on devices with intermittent connectivity. The team needs to detect and report when local input data statistics drift significantly. Which architecture is most appropriate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Model Monitor DataQuality jobs in the AWS cloud to poll edge logs hourly.",
      "B": "Install SageMaker Debugger on devices to log input metrics to CloudWatch.",
      "C": "Embed the SageMaker Edge Manager monitoring SDK on device to emit metrics to AWS IoT, then trigger a Model Monitor drift check in the cloud.",
      "D": "Deploy AWS Greengrass Lambda functions to run Clarify drift checks locally."
    },
    "explanation": "Edge Manager SDK plus IoT ingestion allows capture of local stats; cloud Model Monitor then runs drift. Debugger isn\u2019t for inference; Greengrass + Clarify unsupported; cloud polling logs is inefficient."
  },
  {
    "taskStatement": "4.1",
    "stem": "An ML engineer must monitor prediction skew between two ensembles serving the same traffic for anomaly detection. They require automatic, per-minute alerts if the divergence of prediction distributions exceeds a threshold. How should they implement this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable Data Capture for both ensemble endpoints, schedule two Model Monitor drift checks on predictions with a 1-minute interval, and configure CloudWatch alarms on the job results.",
      "B": "Run SageMaker Clarify on both ensembles every minute to compute PSI on prediction distributions.",
      "C": "Write a custom Lambda that pulls logs from both endpoints, computes KS test, and pushes CloudWatch metrics.",
      "D": "Use CloudWatch Contributor Insights to track prediction distribution per minute."
    },
    "explanation": "Two Data Capture + Model Monitor drift checks with CloudWatch alarms automates per-minute divergence alerts. Clarify isn\u2019t for raw distribution drift; custom Lambda is higher overhead; Contributor Insights not suited."
  },
  {
    "taskStatement": "4.1",
    "stem": "After deploying a multi-model SageMaker endpoint, a sudden spike in malformed JSON requests is observed. The engineer wants to monitor schema violations per model. Which solution provides the most granular alerts?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure a single Model Monitor DataQuality job on the endpoint to catch all schema violations.",
      "B": "Enable Data Capture and configure a separate DataQuality job per model container with JSON schema constraints.",
      "C": "Use AWS WAF JSON body inspection rules to log violations to CloudWatch.",
      "D": "Parse inference logs with CloudWatch Logs Insights and trigger SNS alerts."
    },
    "explanation": "Separate DataQuality jobs per model in the multi-model endpoint allow per-container schema checks. A single job cannot differentiate models; WAF and custom log parsing add external complexity."
  },
  {
    "taskStatement": "4.1",
    "stem": "A streaming inference pipeline uses an asynchronous endpoint. They want to detect if more than 1% of requests take longer than 10 seconds. Which AWS-native feature combination best achieves this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Model Monitor latency metrics with custom Python script to parse logs.",
      "B": "Configure AWS X-Ray on the endpoint and set up CloudWatch alarms for trace duration.",
      "C": "Enable Data Capture and schedule Model Quality Monitor to evaluate latency distribution.",
      "D": "Enable Data Capture on the endpoint, configure CloudWatch metric filters for invocation duration, and set alarms for >1% above 10s."
    },
    "explanation": "Data Capture plus CloudWatch metric filters allow latency distribution monitoring and alarms without custom training jobs. Model Quality Monitor isn\u2019t for latency; X-Ray traces require code hooks."
  },
  {
    "taskStatement": "4.1",
    "stem": "A healthcare inference service must monitor PII leakage: requests and responses must contain no PHI tokens. The model returns free-form text. Which monitoring approach will detect policy violations?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Model Monitor DataQuality to enforce a regex constraint rejecting PHI patterns in responses.",
      "B": "Configure AWS WAF with a custom rule to block PHI regex in JSON payload.",
      "C": "Enable Data Capture, run a scheduled SageMaker Clarify bias check with custom detectors for PHI tokens.",
      "D": "Write a Lambda for each inference to scan for PHI and generate CloudWatch events."
    },
    "explanation": "Clarify allows custom text analyzers via pre-built detectors to detect sensitive tokens. DataQuality regex can catch patterns but is limited; WAF doesn\u2019t inspect model responses; Lambda is higher overhead."
  },
  {
    "taskStatement": "4.1",
    "stem": "A model predicts customer churn and uses 50 numeric features. They need to detect multivariate drift (covariate shift) across those features. Which monitoring setup is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Model Monitor DataQuality with a multilook at a multivariate drift check (using dataset_format=\u201cJSON\u201d with drift_check_categories).",
      "B": "Configure SageMaker Clarify to compute pairwise SHAP drift metrics for all feature pairs.",
      "C": "Schedule a batch transform job daily and compare covariance matrices via AWS Glue.",
      "D": "Write custom Spark on EMR to compute Mahalanobis distance across features."
    },
    "explanation": "Model Monitor supports multivariate drift checks during DataQuality monitoring. Clarify focuses on feature importance; batch transform and custom Spark are inefficient and higher overhead."
  },
  {
    "taskStatement": "4.1",
    "stem": "A retailer serves an image-classification model via SageMaker. They need to detect when the distribution of image size (in bytes) drifts and when inference latency spikes. Which out-of-the-box features should they enable?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Clarify DataBias monitors on image_size and SageMaker Model Debugger for latency.",
      "B": "Enable Data Capture and schedule Model Monitor DataQuality jobs to track image_size distribution and custom CloudWatch latency metrics.",
      "C": "Configure a Lambda pre-processor to log image_size and usage of SageMaker Profiler for latency.",
      "D": "Deploy a Step Functions workflow around the endpoint to log metrics and analyze separately."
    },
    "explanation": "Data Capture plus Model Monitor DataQuality tracks input size drift; CloudWatch latency metrics catch spikes. Clarify and Debugger don\u2019t cover both needs; lambda and Step Functions add complexity."
  },
  {
    "taskStatement": "4.1",
    "stem": "An enterprise requires that no inference job runs outside their VPC and that monitoring data remains within their private subnet. How can they deploy Model Monitor under these constraints?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable network isolation on Model Monitor jobs.",
      "B": "Run Model Monitor on a public subnet with NAT gateway to pull data.",
      "C": "Use VPC endpoints for S3 and CloudWatch and run jobs on public instances.",
      "D": "Configure Model Monitor Processing Job with VPC configuration in private subnets and use VPC S3 endpoints."
    },
    "explanation": "Model Monitor jobs run as SageMaker Processing Jobs and support VPC config. Private subnets plus S3 VPC endpoints ensure data stays in VPC. Network isolation is for training/inference only."
  },
  {
    "taskStatement": "4.1",
    "stem": "A financial model returns probability scores. The team needs to detect if the median score changes by more than 5% compared to baseline daily. Which feature and configuration accomplish this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Model Monitor DataQuality drift check on the prediction field with a median tolerance constraint of \u00b15%.",
      "B": "Schedule a Clarify ModelBiasMonitor job to compute median changes.",
      "C": "Extract daily predictions to Athena and run SQL analytics to compare medians.",
      "D": "Enable CloudWatch percentile-based alarms on the endpoint\u2019s inference metric."
    },
    "explanation": "DataQuality drift constraints support median checks on numeric fields. Clarify bias monitors aren\u2019t for central tendency; Athena SQL is custom; CloudWatch latency alarms only handle infrastructure metrics."
  },
  {
    "taskStatement": "4.1",
    "stem": "To comply with regulations, a bank must audit all inference inputs and outputs and ensure they cannot be modified post-hoc. Which combination of services and features meets this requirement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable Data Capture to write logs to an encrypted S3 bucket and use ACLs for write-once.",
      "B": "Use Data Capture to store inputs/outputs in an S3 bucket with S3 Object Lock in Compliance mode and encryption by KMS.",
      "C": "Configure CloudTrail to log inference API calls and point logs to S3 with versioning.",
      "D": "Use Athena for logging inputs/outputs and store results in a write-only DynamoDB table."
    },
    "explanation": "Data Capture plus S3 Object Lock (Compliance) and KMS encryption provides immutable audit trail. CloudTrail logs API calls but not payloads; ACLs alone don\u2019t enforce write-once."
  },
  {
    "taskStatement": "4.1",
    "stem": "A data scientist wants to spot-time batch inference data quality issues such as missing values and invalid formats before consuming results. Which approach should they apply?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Clarify to compute bias metrics on batch responses.",
      "B": "Schedule a CloudWatch Logs Insights query on batch transform logs.",
      "C": "Run a SageMaker Processing job with Model Monitor DataQuality checks on batch output.",
      "D": "Validate outputs manually in Jupyter notebooks after batch jobs complete."
    },
    "explanation": "Model Monitor Processing jobs can validate batch outputs via DataQuality checks. Clarify is for bias; Logs Insights doesn\u2019t parse output payload; manual validation lacks automation."
  },
  {
    "taskStatement": "4.1",
    "stem": "An autonomous driving model processes high-frequence sensor data. They need to detect when any input channel has degraded signal quality (e.g., constant zeros) in real time. Which solution best addresses this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Model Debugger to capture input tensor anomalies.",
      "B": "Set up CloudWatch metric filters on inference logs to detect zero-value readings.",
      "C": "Implement an AWS IoT rule to check sensor message contents for zeros.",
      "D": "Enable Data Capture on the endpoint, schedule a Model Monitor DataQuality job with custom tolerances for each channel."
    },
    "explanation": "Data Capture and DataQuality allow custom numeric constraints per feature. Debugger isn\u2019t for inference; CloudWatch requires custom parsing; IoT rules don\u2019t integrate inference model context."
  },
  {
    "taskStatement": "4.1",
    "stem": "A model returns a confidence score as a float between 0 and 1. The team must ensure no inference returns a score outside this range. Which monitoring configuration should they use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Write a custom CloudWatch metric filter on response logs to detect invalid scores.",
      "B": "Use Model Monitor DataQuality constraints to flag scores <0 or >1.",
      "C": "Apply SageMaker Clarify to validate output distributions within range.",
      "D": "Include assertion logic in the inference container to throw errors on invalid values."
    },
    "explanation": "DataQuality constraints can enforce numeric ranges on columns. Clarify doesn\u2019t enforce hard constraints; custom filter or container logic increases maintenance."
  },
  {
    "taskStatement": "4.1",
    "stem": "A marketing model predicts click-through probability and requires monitoring of high skew in the \"campaign_id\" feature usage. Which Service Lens should they use in Model Monitor to inspect this categorical feature?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use the Categorical Lens in SageMaker Model Monitor DataQuality to monitor \"campaign_id\".",
      "B": "Use the Label Lens in SageMaker Clarify to monitor campaign labeling.",
      "C": "Use the Numeric Lens in Model Monitor to inspect category counts.",
      "D": "Use the Bias Lens in Clarify to track category fairness."
    },
    "explanation": "The Categorical Lens in Model Monitor DataQuality jobs specializes in tracking categorical feature distribution. Numeric Lens monitors only numeric features; Clarify lenses focus on bias/explainability."
  },
  {
    "taskStatement": "4.1",
    "stem": "An engineering team notices that model inference failures often coincide with sudden changes in request payload size. They want to correlate error rate with payload size anomalies. Which approach is simplest?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Upload logs to CloudWatch Logs and manually correlate with payload sizes stored in S3.",
      "B": "Instrument the container to push custom CloudWatch metrics for payload size and errors.",
      "C": "Enable Data Capture with custom JSONPath for payload_size and error_code, then run Model Monitor DataQuality job with both fields.",
      "D": "Use AWS X-Ray annotations to trace payload size and exceptions."
    },
    "explanation": "Data Capture with custom JSONPath can extract both payload_size and error_code; DataQuality jobs can detect anomalies and correlations. Container instrumentation or X-Ray require more custom code."
  },
  {
    "taskStatement": "4.1",
    "stem": "A translation service uses a multi-model endpoint. They need to monitor latency at the container level for each model separately. What\u2019s the most direct AWS-native way?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure Model Monitor on the endpoint to track per-model latency.",
      "B": "Enable per-model invocation logging with Data Capture and use CloudWatch metric filters to calculate latency.",
      "C": "Deploy a CloudWatch Agent on the endpoint hosts to capture container metrics.",
      "D": "Use SageMaker Debugger profiling on inference containers."
    },
    "explanation": "Data Capture logs invocation metadata per model; CloudWatch logs filters can compute latency per container. Model Monitor and Debugger don\u2019t provide per-container latency; CloudWatch Agent isn\u2019t supported on managed endpoints."
  },
  {
    "taskStatement": "4.1",
    "stem": "A company must detect malicious adversarial inputs that cause anomalous model outputs. They define an acceptable range for each output. Which monitoring solution enforces this at scale?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model Monitor DataQuality constraints on the output columns with defined min/max thresholds.",
      "B": "Deploy a WAF rule inspecting response bodies against known attack signatures.",
      "C": "Use Clarify Explainability to detect adversarial perturbations.",
      "D": "Stream inference logs to Lambda for custom anomaly detection."
    },
    "explanation": "DataQuality constraint checks on output enforce numeric ranges and can scale. WAF and Clarify aren\u2019t designed for content-based validation; Lambda adds overhead."
  },
  {
    "taskStatement": "4.1",
    "stem": "An endpoint serves an NLP model that occasionally returns excessively long generated text (>1000 tokens). They want real-time alerts when this happens. Which setup is most efficient?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Model Monitor to check response length length via DataQuality.",
      "B": "Run Clarify to measure token counts and alert.",
      "C": "Enable CloudWatch RUM to capture response sizes.",
      "D": "Configure Data Capture to extract response_text, set a DataQuality constraint for max token count, and schedule frequent jobs."
    },
    "explanation": "Data Capture plus DataQuality constraint handles response length checks. Clarify and RUM unsuited; CloudWatch RUM is for front-end metrics."
  },
  {
    "taskStatement": "4.1",
    "stem": "A pharmaceutical company must catch silent inference failures where the model returns a default value of 0. They want automated detection. Which AWS feature configuration accomplishes this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure SageMaker Debugger to flag constant zero outputs.",
      "B": "Use Model Monitor DataQuality constraints to detect when output equals 0 more than a threshold.",
      "C": "Enable CloudWatch metric filters on logs for zero values and alert.",
      "D": "Instrument the model container to throw errors on zeros."
    },
    "explanation": "DataQuality constraints detect when a column value matches an anomaly pattern. Debugger is for training; logs filtering is custom; container instrumentation has higher overhead."
  },
  {
    "taskStatement": "4.1",
    "stem": "To monitor inference drift for streaming predictions without ground truth, which monitor type and category should they choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ModelQualityMonitor with concept drift category.",
      "B": "Clarify ModelBiasMonitor on predictions.",
      "C": "Model Monitor DataQuality with drift_check_categories set to \"data\" on the prediction field.",
      "D": "Clarify ModelExplainabilityMonitor for prediction distribution."
    },
    "explanation": "DataQuality monitors support drift_check_categories on arbitrary fields. ModelQualityMonitor requires labels. Clarify focuses on bias/explainability."
  },
  {
    "taskStatement": "4.1",
    "stem": "A company needs to ensure that feature engineering code doesn\u2019t introduce new features unseen during training. They want to monitor for any new column names in inference payloads. What\u2019s the best strategy?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use Model Monitor DataQuality with input schema constraints listing allowed column names.",
      "B": "Implement a Lambda authorizer on API Gateway to validate JSON keys.",
      "C": "Enable SageMaker Clarify DatasetDriftMonitor on input keys.",
      "D": "Use AWS Config rules on the SageMaker endpoint inference setting."
    },
    "explanation": "DataQuality constraints can enforce allowed input schema. Lambda authorizer works but is outside SageMaker; Clarify and Config aren\u2019t for schema enforcement."
  },
  {
    "taskStatement": "4.1",
    "stem": "An ML engineer observes that the distribution of a numerical feature is bimodal in training but has become unimodal in production. They must detect this change automatically. Which Model Monitor configuration handles multimodal distribution drift?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Clarify ModelBiasMonitor to detect distribution shape changes.",
      "B": "Schedule a batch transform and post-process histograms in Athena.",
      "C": "Use CloudWatch anomaly detection on the feature metric.",
      "D": "Configure Model Monitor DataQuality with DistributionLens on the feature and enable drift checks."
    },
    "explanation": "DistributionLens in DataQuality monitors supports multiple bins for drift detection. Clarify bias monitors don\u2019t assess raw distribution shape; Athena/CloudWatch require custom coding."
  },
  {
    "taskStatement": "4.1",
    "stem": "A subscription service uses asynchronous batch endpoints. They need to monitor end-to-end inference job failures and durations. How can they instrument this without modifying model code?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Use Model Monitor DataQuality to track batch job status and duration.",
      "B": "Configure Clarify to track job failures.",
      "C": "Configure CloudWatch Events on SageMaker BatchTransform job state change and subscribe to SNS.",
      "D": "Deploy GuardDuty to detect anomalies in batch failures."
    },
    "explanation": "CloudWatch Events for job state and duration require no code changes. Model Monitor doesn\u2019t monitor batch job lifecycle; Clarify doesn\u2019t cover jobs; GuardDuty is for security threats."
  },
  {
    "taskStatement": "4.1",
    "stem": "An experiment uses shadow deployments: incoming traffic is mirrored to a new model. They want to compare output distributions in near real time and alert if divergence >2%. Which design is simplest?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Deploy two DataQuality jobs separately and post-process alerts via Lambda.",
      "B": "Enable Data Capture on both endpoints, schedule a single DriftsCheck Job that references both datasets as baseline and target.",
      "C": "Use Clarify to compute PSI between baseline and candidate in near real time.",
      "D": "Implement a Step Functions workflow to fetch logs and compare distributions."
    },
    "explanation": "A single DriftCheck job can accept two datasets as baseline and target. DataQuality jobs per endpoint or Step Functions add complexity; Clarify mismatched purpose."
  },
  {
    "taskStatement": "4.1",
    "stem": "A biotech startup requires monitoring of model feature importance drift in production. They want to detect if SHAP importance of any feature changes by >10%. Which AWS feature combination achieves this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Model Monitor DataQuality drift_check on SHAP values.",
      "B": "Schedule a Model Quality Monitor job with SHAP-based metrics.",
      "C": "Configure Clarify ModelBiasMonitor with feature_importance thresholds.",
      "D": "Enable SageMaker Clarify ModelExplainabilityMonitor on the endpoint with max_absolute_shap_change constraint."
    },
    "explanation": "Clarify\u2019s ModelExplainabilityMonitor captures SHAP importance drift and supports constraints. DataQuality and ModelQualityMonitoring don\u2019t handle SHAP; ModelBiasMonitor focuses on bias not drift."
  },
  {
    "taskStatement": "4.1",
    "stem": "A team must implement an alert if any inference job experiences input payload with more than 50 unique categorical feature values. Which approach scales best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Config custom rules to inspect payloads.",
      "B": "Deploy WAF rate-limit rules on JSON body parser.",
      "C": "Enable Data Capture and configure a Model Monitor DataQuality custom constraint on cardinality >50.",
      "D": "Run Athena queries on S3 logs and send alerts via Lambda."
    },
    "explanation": "DataQuality custom cardinality constraints enforce unique value limits. AWS Config and WAF don\u2019t inspect inference payloads; Athena+Lambda is higher latency."
  },
  {
    "taskStatement": "4.1",
    "stem": "A model is deprecated but still receives stale requests. The team wants to detect any inference calls to the old endpoint and retire it promptly. Which CloudWatch configuration helps?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Set up a CloudWatch metric filter on SageMakerAPI logs for InvokeEndpointOldModel API and alert.",
      "B": "Use Model Monitor to detect zero-inference values.",
      "C": "Configure a Custom Resource in CloudFormation to log invocations.",
      "D": "Enable SageMaker Clarify to flag calls to old endpoint."
    },
    "explanation": "CloudWatch metric filters on API logs for specific endpoint names detect calls. Model Monitor and Clarify aren\u2019t for endpoint invocation detection; custom resource is unnecessary."
  },
  {
    "taskStatement": "4.1",
    "stem": "A gaming app uses an ML endpoint that must maintain 95th percentile latency under 200ms. They need to alert if the SLA is broken. Which AWS native setup is recommended?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Run Model Monitor with a latency constraint on the endpoint logs.",
      "B": "Use CloudWatch percentile-based alarms on the AWS/SageMaker metric InferenceLatency.",
      "C": "Deploy SageMaker Debugger to capture latency tensors.",
      "D": "Implement a Lambda to sample requests and measure SLA."
    },
    "explanation": "CloudWatch percentile-based alarms directly monitor InferenceLatency p95. Model Monitor isn\u2019t designed for latency; Debugger and Lambda require extra work."
  }
]