[
  {
    "taskStatement": "1.1",
    "stem": "Which data format is typically most efficient for analytic queries on large datasets in AWS?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CSV",
      "B": "Apache Parquet",
      "C": "JSON",
      "D": "Plain text"
    },
    "explanation": "Apache Parquet is a columnar storage format optimized for analytical queries, generally providing faster query performance and reduced storage cost compared to row-based formats like CSV."
  },
  {
    "taskStatement": "1.1",
    "stem": "What AWS service would you primarily use to ingest real-time streaming data for machine learning workflows?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon S3",
      "B": "Amazon RDS",
      "C": "Amazon Kinesis",
      "D": "Amazon Redshift"
    },
    "explanation": "Amazon Kinesis is designed to collect, process, and analyze real-time streaming data, making it ideal for streaming ingestion in machine learning pipelines."
  },
  {
    "taskStatement": "1.1",
    "stem": "Which AWS storage option is best suited for low-latency access to machine learning training data in a shared file system?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon EFS",
      "B": "Amazon S3",
      "C": "Amazon Glacier",
      "D": "Amazon RDS"
    },
    "explanation": "Amazon EFS provides a scalable, low-latency, shared file system accessible by multiple instances, suitable for training ML models requiring concurrent access."
  },
  {
    "taskStatement": "1.1",
    "stem": "When ingesting data into Amazon SageMaker Feature Store, which data source types can be used directly?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon DynamoDB and JSON files only",
      "B": "Only CSV files on Amazon S3",
      "C": "Streaming data sources only",
      "D": "Amazon S3, streaming data, and batch data"
    },
    "explanation": "Amazon SageMaker Feature Store supports ingestion from multiple sources including Amazon S3 (batch), streaming data, and direct API calls, enabling flexible feature ingestion."
  },
  {
    "taskStatement": "1.1",
    "stem": "Which AWS service can merge data from multiple sources such as Amazon S3 and Amazon Redshift for data preparation?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Kinesis",
      "B": "AWS Glue",
      "C": "Amazon SageMaker",
      "D": "AWS Snowball"
    },
    "explanation": "AWS Glue is a fully managed extract, transform, and load (ETL) service that can crawl and transform data from multiple sources including S3 and Redshift."
  },
  {
    "taskStatement": "1.1",
    "stem": "What is the purpose of Amazon S3 Transfer Acceleration?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "To transfer files between S3 buckets",
      "B": "To automatically replicate data in S3",
      "C": "To speed up upload and download of data to and from S3 from worldwide locations",
      "D": "To encrypt data stored in S3"
    },
    "explanation": "Amazon S3 Transfer Acceleration accelerates transfers of data over long distances between clients and S3, improving throughput and reducing latency."
  },
  {
    "taskStatement": "1.1",
    "stem": "Which Apache data format is optimized for row-based data storage and streaming consumption?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Apache Parquet",
      "B": "Apache ORC",
      "C": "Apache Avro",
      "D": "CSV"
    },
    "explanation": "Apache Avro is a row-based data serialization system often used in streaming and messaging systems due to its compact binary format and schema evolution support."
  },
  {
    "taskStatement": "1.1",
    "stem": "When selecting a data format for ML ingestion with frequent column retrieval, which format should be preferred?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Columnar formats (e.g., Parquet, ORC)",
      "B": "Row-based formats (e.g., JSON, CSV)",
      "C": "Compressed text formats",
      "D": "XML"
    },
    "explanation": "Columnar formats like Parquet and ORC allow efficient access to specific columns without scanning entire rows, which is beneficial for many ML workloads."
  },
  {
    "taskStatement": "1.1",
    "stem": "Which AWS storage options can be configured to meet high IOPS and low latency needs for ML training data storage?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon S3 with Transfer Acceleration",
      "B": "Amazon Glacier",
      "C": "Amazon FSx for Lustre",
      "D": "Amazon EBS Provisioned IOPS SSD volumes"
    },
    "explanation": "Amazon EBS Provisioned IOPS SSD volumes provide high throughput, consistent, and low-latency block storage ideal for demanding ML training workloads."
  },
  {
    "taskStatement": "1.1",
    "stem": "In a data pipeline, if you must process streaming data and maintain stateful transformations, which AWS streaming service is recommended?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Kinesis Data Streams only",
      "B": "Apache Flink on Amazon Kinesis Data Analytics",
      "C": "AWS Lambda triggered by Kinesis streams",
      "D": "Amazon S3 event notifications"
    },
    "explanation": "Apache Flink supports stateful stream processing and integrates with Kinesis Data Analytics, enabling complex event processing on streaming data."
  },
  {
    "taskStatement": "1.1",
    "stem": "Which AWS service provides serverless ETL capabilities for data transformation before ML ingestion?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon EMR",
      "B": "AWS Glue",
      "C": "Amazon SageMaker",
      "D": "AWS Lambda"
    },
    "explanation": "AWS Glue is a serverless ETL service designed to prepare and transform data for analytics and machine learning workflows without managing infrastructure."
  },
  {
    "taskStatement": "1.1",
    "stem": "When ingesting large datasets into Amazon SageMaker Data Wrangler, which storage source is natively supported?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon EBS volumes",
      "B": "Amazon RDS directly",
      "C": "Amazon DynamoDB only",
      "D": "Amazon S3"
    },
    "explanation": "Amazon SageMaker Data Wrangler natively supports data ingestion from Amazon S3, allowing users to explore and transform datasets easily."
  },
  {
    "taskStatement": "1.1",
    "stem": "Which of the following best describes Apache ORC compared to Apache Parquet?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ORC is row-based while Parquet is columnar",
      "B": "ORC is optimized for JSON data ingestion",
      "C": "Both ORC and Parquet are columnar formats optimized for big data workloads with slightly different serialization implementations",
      "D": "Parquet supports complex nested data, ORC does not"
    },
    "explanation": "Apache ORC and Apache Parquet are both columnar storage formats designed for big data processing with efficient compression and encoding but differ in details of implementation."
  },
  {
    "taskStatement": "1.1",
    "stem": "If you need to merge a large volume of disparate data sources quickly for ML, which AWS tool provides scalable, serverless ETL capabilities?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Athena",
      "B": "AWS Glue",
      "C": "Amazon SageMaker Pipelines",
      "D": "Amazon EMR clusters managed manually"
    },
    "explanation": "AWS Glue is a managed ETL service that simplifies the process of extracting, transforming, and loading data at scale."
  },
  {
    "taskStatement": "1.1",
    "stem": "Which Amazon storage service provides a hierarchical file system interface with high throughput and low latency, suitable for shared ML datasets?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon FSx for NetApp ONTAP",
      "B": "Amazon S3 Glacier",
      "C": "Amazon Elastic Block Store (EBS)",
      "D": "Amazon DynamoDB"
    },
    "explanation": "Amazon FSx for NetApp ONTAP offers enterprise-grade network file storage with a hierarchical structure and supports standard SMB/NFS protocols, ideal for shared ML workloads."
  },
  {
    "taskStatement": "1.1",
    "stem": "Which data ingestion challenge is most likely to cause capacity issues when storing large volumes of streaming data for ML?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Using data formats too compact for easy retrieval",
      "B": "Using batch ingestion instead of streaming",
      "C": "Selecting appropriate AWS region for geographic proximity",
      "D": "Provisioning insufficient throughput and storage capacity for the storage solution"
    },
    "explanation": "Insufficient storage throughput or capacity can cause bottlenecks and failures during ingestion of high-volume streaming data, affecting ML data pipelines."
  },
  {
    "taskStatement": "1.1",
    "stem": "Which data storage option offers the highest durability and availability for ML datasets in AWS?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon EFS",
      "B": "Amazon S3 Standard",
      "C": "Amazon FSx",
      "D": "Amazon EBS"
    },
    "explanation": "Amazon S3 Standard storage class provides eleven 9's (99.999999999%) of durability and high availability, making it a reliable choice for ML data storage."
  },
  {
    "taskStatement": "1.1",
    "stem": "Which of these ingestion mechanisms provides the lowest latency for real-time ML data pipelines on AWS?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Kinesis Data Streams",
      "B": "Amazon S3 batch upload",
      "C": "AWS Glue ETL jobs",
      "D": "Amazon RDS snapshots"
    },
    "explanation": "Amazon Kinesis Data Streams supports real-time streaming data ingestion with low latency, suitable for immediate ML model inference or processing."
  },
  {
    "taskStatement": "1.1",
    "stem": "What are typical tradeoffs when selecting AWS storage options for ML data ingestion?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Availability versus encryption",
      "B": "User interface simplicity versus compute power",
      "C": "Cost, performance (throughput/latency), and data structure compatibility",
      "D": "File size versus file name length"
    },
    "explanation": "Choosing AWS storage requires balancing cost, performance metrics like throughput and latency, and compatibility with data formats and access patterns."
  },
  {
    "taskStatement": "1.1",
    "stem": "Which AWS service can be used to automate troubleshooting and debugging of data ingestion issues related to capacity and scalability in ML pipelines?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS CloudFormation",
      "B": "Amazon CloudWatch",
      "C": "AWS CodePipeline",
      "D": "Amazon QuickSight"
    },
    "explanation": "Amazon CloudWatch monitors metrics and logs that can help identify bottlenecks, capacity constraints, and failures in data ingestion workflows."
  },
  {
    "taskStatement": "1.1",
    "stem": "Which AWS service is commonly used to ingest and process large-scale IoT sensor data streams for ML training purposes?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Kinesis Data Streams",
      "B": "Amazon S3 Glacier",
      "C": "Amazon EMR",
      "D": "AWS Step Functions"
    },
    "explanation": "Amazon Kinesis Data Streams is designed for ingesting massive, continuous streams of IoT data for real-time processing and ML applications."
  },
  {
    "taskStatement": "1.1",
    "stem": "What is the main advantage of using columnar formats like Parquet and ORC over row-based formats in ML data preparation?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Simpler schema definitions",
      "B": "Better exploitation of nested data",
      "C": "Faster analytics queries by reading only relevant columns",
      "D": "Lower data compression ratios"
    },
    "explanation": "Columnar formats are optimized for analytics by scanning only necessary columns, which reduces disk I/O and speeds up queries."
  },
  {
    "taskStatement": "1.1",
    "stem": "Which method is best to merge datasets from multiple S3 buckets with different data formats for ML ingestion?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Glue ETL jobs to convert and merge data into a common format",
      "B": "Manually download and merge datasets locally",
      "C": "Use Amazon SageMaker directly on multiple sources without transformation",
      "D": "Use Amazon Athena to join data without transformation"
    },
    "explanation": "AWS Glue ETL enables scalable, automated data preparation including merging and format conversion suited for ML ingestion."
  },
  {
    "taskStatement": "1.1",
    "stem": "For training ML models with data stored on Amazon S3, what is a key factor when choosing a data format to optimize training performance?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Simplicity of the format",
      "B": "Ease of human readability",
      "C": "Format popularity",
      "D": "Compatibility with distributed processing frameworks and columnar compression"
    },
    "explanation": "Columnar, compressed data formats like Parquet are optimized for distributed ML training frameworks, reducing IO and improving performance."
  },
  {
    "taskStatement": "1.1",
    "stem": "Which AWS storage service supports storing petabytes of unstructured data accessible via REST APIs?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon EFS",
      "B": "Amazon Elastic Block Store (EBS)",
      "C": "Amazon S3",
      "D": "Amazon FSx for Windows File Server"
    },
    "explanation": "Amazon S3 is an object storage service designed for massive scalability accessible via RESTful APIs."
  },
  {
    "taskStatement": "1.1",
    "stem": "When ingesting data for ML, why might you choose JSON over CSV format?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "JSON files are smaller in size than CSV",
      "B": "JSON supports nested and hierarchical data structures",
      "C": "JSON is easier to read for humans",
      "D": "CSV supports nested structures better"
    },
    "explanation": "JSON supports complex nested and hierarchical data which cannot be represented in flat CSV files, useful for certain ML datasets."
  },
  {
    "taskStatement": "1.1",
    "stem": "Which AWS service provides a scalable, serverless, managed Apache Spark environment for data processing and ingestion?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Glue DataBrew",
      "B": "Amazon EMR with manual provisioning",
      "C": "AWS Glue",
      "D": "Amazon SageMaker Studio"
    },
    "explanation": "AWS Glue offers serverless Spark environments with automatic scaling for ETL workloads, including data ingestion and transformation."
  },
  {
    "taskStatement": "1.1",
    "stem": "Which AWS storage solution provides native encryption at rest and in transit with automatic key management suitable for sensitive ML data?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon EBS without encryption",
      "B": "Amazon S3 with AWS KMS integration",
      "C": "Local instance storage on EC2",
      "D": "Amazon EFS without encryption"
    },
    "explanation": "Amazon S3 integrates with AWS KMS for encryption at rest with automatic key management and supports encryption in transit with HTTPS."
  },
  {
    "taskStatement": "1.1",
    "stem": "Which AWS service integrates with Apache Kafka to ingest streaming data for ML workflows?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon S3",
      "B": "AWS Glue",
      "C": "Amazon EMR",
      "D": "Amazon Managed Streaming for Apache Kafka (MSK)"
    },
    "explanation": "Amazon MSK is a fully managed service for Apache Kafka, facilitating streaming data ingestion into ML pipelines."
  },
  {
    "taskStatement": "1.1",
    "stem": "What is the primary benefit of using Amazon S3 Storage Class Analysis when planning data storage for ML workflows?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Identify data access patterns to optimize cost and performance",
      "B": "Encrypt data automatically",
      "C": "Automatically categorize data by content type",
      "D": "Convert data formats to Parquet"
    },
    "explanation": "S3 Storage Class Analysis helps analyze how frequently data is accessed to recommend cost-effective storage classes without compromising performance."
  },
  {
    "taskStatement": "1.1",
    "stem": "Which of these data ingestion scenarios is best suited for Amazon EBS over Amazon S3?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Storing large volumes of unstructured data for archival",
      "B": "High-throughput block storage with low latency for training datasets attached to EC2 instances",
      "C": "Serving static website content",
      "D": "Long-term immutable backups"
    },
    "explanation": "Amazon EBS offers persistent block storage with low latency suitable for ML training data requiring fast I/O attached directly to EC2."
  },
  {
    "taskStatement": "1.1",
    "stem": "When storing machine learning data that requires compliance with data residency laws, which AWS feature should be considered first?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Using Amazon Glacier for archival",
      "B": "Encrypting data at rest",
      "C": "Selecting AWS Regions to ensure data does not leave jurisdiction",
      "D": "Using public S3 buckets"
    },
    "explanation": "Data residency compliance relates to where data physically resides; choosing AWS regions accordingly ensures legal requirements are met."
  },
  {
    "taskStatement": "1.1",
    "stem": "Which AWS storage type supports hierarchical namespace and is used for big data workloads requiring Hadoop compatibility?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon S3 Standard",
      "B": "Amazon EFS",
      "C": "Amazon DynamoDB",
      "D": "Amazon EMR File System (EMRFS) backed by Amazon S3"
    },
    "explanation": "EMRFS extends Amazon S3 with Hadoop-compatible features like support for hierarchical namespace, making it suitable for big data workloads."
  },
  {
    "taskStatement": "1.1",
    "stem": "Which factor is critical when choosing between Amazon S3 and Amazon EFS for storing machine learning datasets?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Cost differences only",
      "B": "Access patterns and need for shared file system semantics",
      "C": "Supported file formats",
      "D": "Encryption capabilities"
    },
    "explanation": "Amazon EFS provides a POSIX-compliant shared file system for concurrent access by multiple clients, while S3 is object storage optimized for scalability but lacks shared filesystem semantics."
  },
  {
    "taskStatement": "1.1",
    "stem": "Which AWS options help reduce latency when accessing data stored in Amazon S3 from geographically distant locations?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon S3 Transfer Acceleration",
      "B": "Amazon Glacier Deep Archive",
      "C": "Increasing S3 read timeout settings",
      "D": "Using Amazon Macie"
    },
    "explanation": "S3 Transfer Acceleration leverages Amazon CloudFront\u2019s edge locations to provide fast and secure transfers over long distances."
  },
  {
    "taskStatement": "1.1",
    "stem": "When ingesting large datasets into ML pipelines, how does using Apache Parquet help reduce storage costs?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Its row format stores data redundantly to improve speed",
      "B": "Its columnar format enables compression and reduces I/O",
      "C": "It automatically deletes unused data",
      "D": "It moves data to cheaper storage classes"
    },
    "explanation": "Apache Parquet\u2019s columnar format enables better compression and reduces read I/O by storing similar data types together."
  },
  {
    "taskStatement": "1.1",
    "stem": "Which tool would you use to move large datasets from on-premises environments to Amazon S3 for ML training?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Glue DataBrew",
      "B": "Amazon Kinesis Data Streams",
      "C": "AWS DataSync",
      "D": "Amazon SageMaker Studio"
    },
    "explanation": "AWS DataSync securely and efficiently transfers large volumes of data from on-premises storage to AWS storage services like Amazon S3."
  },
  {
    "taskStatement": "1.1",
    "stem": "Which data ingestion method should be used when high throughput and low-latency batch ingestion is required for ML workloads?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Uploading JSON files manually",
      "B": "Using Amazon DynamoDB streams",
      "C": "Streaming via AWS Lambda triggers",
      "D": "Parallel multipart upload to Amazon S3 using Transfer Acceleration"
    },
    "explanation": "Multipart upload with Transfer Acceleration allows efficient, low latency batch data ingestion to Amazon S3."
  },
  {
    "taskStatement": "1.1",
    "stem": "In a multi-tenant ML environment, which AWS service helps securely segregate training data within the same Amazon S3 bucket?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Glue DataBrew",
      "B": "Amazon SageMaker Ground Truth",
      "C": "S3 bucket policies and IAM policies",
      "D": "Amazon EMR security groups"
    },
    "explanation": "S3 bucket policies combined with IAM policies can restrict access to specific prefixes or objects to enforce tenant data isolation."
  },
  {
    "taskStatement": "1.1",
    "stem": "Which of the following is NOT a key consideration when making initial storage decisions for ML raw data in AWS?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Cost of storage",
      "B": "Performance / latency requirements",
      "C": "Data structure and access patterns",
      "D": "Color of the AWS console"
    },
    "explanation": "The color of the AWS console is irrelevant; key considerations include cost, performance, and data format compatibility."
  },
  {
    "taskStatement": "1.1",
    "stem": "Which AWS service can ingest streaming data and apply simple transformations before feeding into ML pipelines?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon S3 Glacier Deep Archive",
      "B": "AWS Lambda triggered by Kinesis streams",
      "C": "Amazon SageMaker Model Monitor",
      "D": "AWS Glue Data Quality"
    },
    "explanation": "AWS Lambda can be triggered by Kinesis streaming data to perform lightweight, real-time transformations before ingestion into ML pipelines."
  },
  {
    "taskStatement": "1.2",
    "stem": "Which AWS service is most suitable for visually exploring, cleaning, and transforming tabular data before training a machine learning model?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Glue Data Quality",
      "B": "Amazon SageMaker Data Wrangler",
      "C": "Amazon Kinesis Data Streams",
      "D": "AWS Lambda"
    },
    "explanation": "Amazon SageMaker Data Wrangler provides an integrated visual interface designed specifically to explore, clean, and transform data interactively, making it ideal for feature engineering before ML training. AWS Glue Data Quality focuses on data validation, Kinesis is for streaming ingestion, and Lambda is for serverless compute rather than data transformation."
  },
  {
    "taskStatement": "1.2",
    "stem": "What is the primary benefit of using Amazon SageMaker Feature Store in the context of feature engineering?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "It automatically selects the best features for model training.",
      "B": "It merges data from multiple unstructured sources.",
      "C": "It provides a centralized repository for storing, retrieving, and sharing engineered features across teams and models.",
      "D": "It performs real-time data encryption for sensitive features."
    },
    "explanation": "SageMaker Feature Store acts as a centralized feature repository that enables consistent use, sharing, and reusability of features across different models and teams, ensuring reliability and reducing duplication in feature engineering workflows."
  },
  {
    "taskStatement": "1.2",
    "stem": "In feature engineering, why might one use binning (discretization) of continuous variables before training a model?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "To reduce the effect of outliers and create categorical groupings that may improve model interpretability.",
      "B": "To increase the dimensionality of the dataset for complex models.",
      "C": "To normalize data distribution to a Gaussian form.",
      "D": "To encode categorical features into numerical format."
    },
    "explanation": "Binning converts continuous features into categorical bins, which helps reduce the impact of outliers and can enhance model interpretability by grouping ranges into categories."
  },
  {
    "taskStatement": "1.2",
    "stem": "Which encoding technique is most appropriate to represent an ordinal categorical variable for a machine learning model?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "One-hot encoding",
      "B": "Tokenization",
      "C": "Binary encoding",
      "D": "Label encoding"
    },
    "explanation": "Label encoding assigns an integer value to each category preserving the order of ordinal variables. One-hot encoding removes order information, which is crucial for ordinal variables."
  },
  {
    "taskStatement": "1.2",
    "stem": "When using SageMaker Data Wrangler to perform feature scaling, which method would be most appropriate to standardize features to have zero mean and unit variance?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Min-max normalization",
      "B": "Standardization (Z-score normalization)",
      "C": "Log transformation",
      "D": "Binning"
    },
    "explanation": "Standardization subtracts the mean and divides by the standard deviation, resulting in features with zero mean and unit variance, which is helpful for many ML algorithms."
  },
  {
    "taskStatement": "1.2",
    "stem": "Which AWS service can be used to generate synthetic labeled datasets by combining annotation with human review to ensure high-quality labels?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon SageMaker Data Wrangler",
      "B": "AWS Glue DataBrew",
      "C": "Amazon SageMaker Ground Truth",
      "D": "AWS Glue"
    },
    "explanation": "Amazon SageMaker Ground Truth is designed to create labeled datasets, often combining machine learning and human annotation workflows to ensure quality."
  },
  {
    "taskStatement": "1.2",
    "stem": "When transforming streaming data for feature engineering, which combination of AWS services is best suited for real-time data transformation in an ML pipeline?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Glue and Amazon Redshift",
      "B": "AWS Lambda and AWS Glue DataBrew",
      "C": "Amazon EMR and Amazon SageMaker Studio",
      "D": "AWS Lambda and Apache Spark on Amazon EMR"
    },
    "explanation": "AWS Lambda enables real-time event-driven processing, and Apache Spark on Amazon EMR can perform scalable stream transformations making this combination effective for streaming data transformation."
  },
  {
    "taskStatement": "1.2",
    "stem": "What kind of data transformation is performed when applying a log transformation on positively skewed continuous data during feature engineering?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "To increase the value range of data",
      "B": "To convert categorical data into numerical",
      "C": "To reduce skewness and make the distribution more symmetric",
      "D": "To one-hot encode categorical variables"
    },
    "explanation": "Log transformation compresses the range of positively skewed data and helps in reducing skewness making the distribution more normal-like."
  },
  {
    "taskStatement": "1.2",
    "stem": "In deduplication, which of the following best describes how AWS Glue or Apache Spark techniques help ensure data quality before model training?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Identify and eliminate duplicate records based on key attributes to prevent bias and overfitting.",
      "B": "Encrypt duplicate records to ensure security compliance.",
      "C": "Automatically merge all duplicate records into one feature vector.",
      "D": "Flag duplicates but retain all for model diversity."
    },
    "explanation": "Removing duplicate records avoids data leakage and bias, reducing overfitting and improving model generalization."
  },
  {
    "taskStatement": "1.2",
    "stem": "Which feature engineering technique is best when you want to reduce the effect of outliers and create robust models by rescaling feature ranges without assuming a Gaussian distribution?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Z-score standardization",
      "B": "Min-max scaling",
      "C": "Logarithmic transformation",
      "D": "Robust scaling using median and interquartile range"
    },
    "explanation": "Robust scaling uses median and interquartile range, making it resistant to outliers as it does not assume normal distribution."
  },
  {
    "taskStatement": "1.2",
    "stem": "When merging multiple datasets for feature engineering, what is a key consideration to avoid data leakage that can compromise model performance?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Ensure all datasets have the same number of rows",
      "B": "Join datasets only on training data features that will be available during inference",
      "C": "Merge datasets without regard to time or sequence",
      "D": "Use all available columns regardless of availability at inference"
    },
    "explanation": "Joining data on features that won\u2019t be known during inference causes data leakage and leads to overly optimistic model performance."
  },
  {
    "taskStatement": "1.2",
    "stem": "What is the effect of tokenization as an encoding technique in the context of text feature engineering for ML models?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Transforms numerical categories to ordinal labels",
      "B": "Converts categorical variables to binary form",
      "C": "Splits text into smaller units such as words or subwords for numeric representation",
      "D": "Creates continuous numerical features from categorical ones"
    },
    "explanation": "Tokenization breaks down text into meaningful segments such as words or subwords, which can then be converted into numeric vectors suitable for modeling."
  },
  {
    "taskStatement": "1.2",
    "stem": "How can using AWS Glue DataBrew assist in the data cleaning step of feature engineering?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "By providing a visual interface to profile data, identify anomalies, and perform transformations without coding",
      "B": "By running large-scale distributed training of machine learning models",
      "C": "By providing GPU-accelerated model deployment",
      "D": "By streaming real-time data ingestion"
    },
    "explanation": "AWS Glue DataBrew offers a no-code experience for data cleaning and transformation by profiling data and detecting anomalies visually, easing the feature engineering process."
  },
  {
    "taskStatement": "1.2",
    "stem": "Which is a drawback of one-hot encoding for high-cardinality categorical features when used in ML models?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "It reduces model interpretability significantly",
      "B": "It introduces unwanted ordinal relationships",
      "C": "It cannot be applied to categorical variables",
      "D": "It significantly increases dimensionality leading to computational inefficiency"
    },
    "explanation": "One-hot encoding transforms categories into sparse binary vectors, which can cause very high dimensionality for high-cardinality features, increasing computational cost."
  },
  {
    "taskStatement": "1.2",
    "stem": "What approach should you take to combine features from multiple sources for training datasets using AWS Glue or Apache Spark?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Randomly join all data without keys to maximize data volume",
      "B": "Use appropriate join keys reflecting relationships and apply transformation logic for schema alignment",
      "C": "Manually concatenate CSV files before ingestion",
      "D": "Merge data only at inference time"
    },
    "explanation": "Using meaningful join keys and aligning schemas ensures data integrity and suitable feature generation necessary for reliable model training."
  },
  {
    "taskStatement": "1.2",
    "stem": "Which feature engineering method helps reduce multicollinearity by combining multiple correlated features into a single or fewer features?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Binning",
      "B": "One-hot encoding",
      "C": "Principal Component Analysis (PCA)",
      "D": "Label encoding"
    },
    "explanation": "PCA transforms correlated features into fewer uncorrelated principal components, reducing multicollinearity and simplifying the feature space."
  },
  {
    "taskStatement": "1.2",
    "stem": "Which AWS tool can you use to identify outliers during the data transformation process?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Model Monitor",
      "B": "SageMaker Data Wrangler",
      "C": "AWS Kinesis Data Streams",
      "D": "Amazon EMR"
    },
    "explanation": "SageMaker Data Wrangler provides built-in visualizations and data profiling features that assist in detecting outliers during transformation."
  },
  {
    "taskStatement": "1.2",
    "stem": "Regarding feature transformation for text data, which AWS service allows semi-automated labeling combined with human review to produce quality labeled datasets efficiently?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon SageMaker Feature Store",
      "B": "AWS Glue DataBrew",
      "C": "Amazon Mechanical Turk alone",
      "D": "Amazon SageMaker Ground Truth"
    },
    "explanation": "SageMaker Ground Truth combines machine learning labeling and human review workflows to create high-quality labeled datasets efficiently."
  },
  {
    "taskStatement": "1.2",
    "stem": "What is a common use case for applying the log transformation technique in feature engineering on numeric data?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Handle skewed distributions to stabilize variance and make data more Gaussian-like.",
      "B": "Convert categorical variables into numeric format.",
      "C": "Normalize data between 0 and 1.",
      "D": "Reduce dimensionality of features."
    },
    "explanation": "Log transformation is used primarily to reduce positive skewness in data, stabilizing variance and meeting assumptions of many modeling algorithms."
  },
  {
    "taskStatement": "1.2",
    "stem": "Which encoding technique preserves the relationship between categories when the values have a natural order?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "One-hot encoding",
      "B": "Binary encoding",
      "C": "Tokenization",
      "D": "Label encoding"
    },
    "explanation": "Label encoding assigns ordered integer labels to categories, preserving ordinal relationships needed for ordinal variables."
  },
  {
    "taskStatement": "1.2",
    "stem": "When preparing features for streaming ML workloads, what is one benefit of using AWS Lambda with streaming data sources like Amazon Kinesis?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Batch processing at fixed intervals only",
      "B": "Real-time, event-driven transformation and filtering of streaming records",
      "C": "Providing GPU-accelerated transformations in real time",
      "D": "Long-running ETL orchestration for large static datasets"
    },
    "explanation": "AWS Lambda can process Kinesis streams with real-time, event-driven functions enabling on-the-fly data transformation and filtering for streaming ML applications."
  },
  {
    "taskStatement": "1.2",
    "stem": "Which of the following is a key skill when using Apache Spark in Amazon EMR for feature engineering?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Developing custom SageMaker algorithms",
      "B": "Visualizing data in SageMaker Studio notebooks",
      "C": "Writing distributed data transformation and aggregation jobs handling large-scale datasets efficiently",
      "D": "Configuring AWS Identity and Access Management (IAM) roles"
    },
    "explanation": "Apache Spark enables distributed processing of large datasets, so writing efficient transformations and aggregations is vital for scalable feature engineering."
  },
  {
    "taskStatement": "1.2",
    "stem": "What is one reason to use AWS Glue Data Quality during feature engineering?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "To validate data quality metrics and detect anomalies before transformation",
      "B": "To handle real-time streaming data processing",
      "C": "To host feature stores for models",
      "D": "To automate hyperparameter tuning"
    },
    "explanation": "AWS Glue Data Quality helps profile and validate datasets by monitoring data quality metrics and detecting anomalies early in the pipeline."
  },
  {
    "taskStatement": "1.2",
    "stem": "Which AWS service lets you orchestrate and automate feature engineering workflows including transformation and labeling?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Athena",
      "B": "AWS Glue DataBrew",
      "C": "Amazon SageMaker Feature Store",
      "D": "Amazon SageMaker Pipelines"
    },
    "explanation": "Amazon SageMaker Pipelines supports automation and orchestration of ML workflows, including data preparation, feature transformation, and labeling."
  },
  {
    "taskStatement": "1.2",
    "stem": "How does one-hot encoding represent categorical variables in a numerical format suitable for ML models?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "By creating binary vectors where each category is represented by a separate feature with values 0 or 1",
      "B": "By assigning integers in range 0 to N to categories",
      "C": "By splitting text into tokens",
      "D": "By calculating logarithmic values of numerical categories"
    },
    "explanation": "One-hot encoding creates a binary vector per category, representing presence or absence, commonly used to encode nominal categorical data."
  },
  {
    "taskStatement": "1.2",
    "stem": "Which is an advantage of using SageMaker Data Wrangler over traditional coding approaches during feature engineering?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "It removes the need for model retraining",
      "B": "Provides an integrated visual and code-based environment for fast, interactive data exploration and transformation",
      "C": "Automatically tunes hyperparameters during feature creation",
      "D": "Provides real-time streaming analytics capabilities"
    },
    "explanation": "Data Wrangler offers both visual UI and code interfaces to speed up data preparation with integrated transformations and quick feature engineering iterations."
  },
  {
    "taskStatement": "1.2",
    "stem": "What type of feature engineering can mitigate missing data issues in your dataset before training ML models?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Imputation techniques like mean/mode substitution or more advanced algorithms",
      "B": "One-hot encoding",
      "C": "Tokenization",
      "D": "Min-max scaling"
    },
    "explanation": "Imputation fills missing values with appropriate estimates to prevent loss of information and improve model robustness."
  },
  {
    "taskStatement": "1.2",
    "stem": "Why is it important to validate quality and consistency of engineered features using tools like SageMaker Data Wrangler before training a model?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "To increase model training speed drastically",
      "B": "To create complex tree-based models automatically",
      "C": "To ensure that incorrect or skewed features do not negatively impact model accuracy and bias",
      "D": "To reduce the number of data points in training"
    },
    "explanation": "Validation ensures feature distributions are correct and consistent, preventing model inaccuracies or bias caused by flawed input features."
  },
  {
    "taskStatement": "1.2",
    "stem": "Which AWS service offers SDKs and APIs to manage and query feature data for ML efficiently across training and inference pipelines?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Glue DataBrew",
      "B": "Amazon Athena",
      "C": "Amazon SageMaker Feature Store",
      "D": "AWS Lambda"
    },
    "explanation": "SageMaker Feature Store provides APIs and SDKs for storing, retrieving, and sharing features, bridging feature engineering between training and inference."
  },
  {
    "taskStatement": "1.2",
    "stem": "You have categorical features with high cardinality. Which encoding technique would balance dimensionality and preserve information better than one-hot encoding?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Label encoding",
      "B": "Binary encoding",
      "C": "Tokenization",
      "D": "Binning"
    },
    "explanation": "Binary encoding converts categories into binary digits, reducing dimensionality compared to one-hot encoding while preserving category information better than label encoding."
  },
  {
    "taskStatement": "1.2",
    "stem": "When performing feature engineering on time series data, which AWS tool is most suitable for scalable batch transformations on large datasets?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Ground Truth",
      "B": "AWS Lambda",
      "C": "Apache Spark on Amazon EMR",
      "D": "SageMaker Feature Store"
    },
    "explanation": "Apache Spark on Amazon EMR provides scalable cluster computing for batch transformations on huge datasets, which is common in time series data preparation."
  },
  {
    "taskStatement": "1.2",
    "stem": "How does feature scaling like normalization impact ML model training and convergence?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "It brings all features to a common scale, improving convergence speed and preventing features with large values from dominating",
      "B": "It encrypts data for security",
      "C": "It increases dimensionality for richer features",
      "D": "It identifies outliers"
    },
    "explanation": "Scaling features to similar ranges helps gradient-based optimizers converge faster and prevents features with large magnitude from biasing the model."
  },
  {
    "taskStatement": "1.2",
    "stem": "If a dataset has missing values that cannot be imputed reasonably, which action should be taken before model training?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Apply one-hot encoding to missing values",
      "B": "Transform missing values into zeros",
      "C": "Remove records with missing values if they are a small percentage or consider feature removal",
      "D": "Use log transformation"
    },
    "explanation": "If imputation is not suitable and missing data points are few, removing those records or features helps maintain data quality."
  },
  {
    "taskStatement": "1.2",
    "stem": "Which AWS feature engineering tool includes support for applying complex user-defined transformations on datasets via custom scripting?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon SageMaker Data Wrangler (supporting Python scripts)",
      "B": "AWS Glue Data Quality",
      "C": "AWS Lambda",
      "D": "Amazon Athena"
    },
    "explanation": "SageMaker Data Wrangler supports applying custom code transformations using Python scripts within its visual dataflow environment."
  },
  {
    "taskStatement": "1.2",
    "stem": "When you perform feature splitting during feature engineering, what is an example of this technique?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Normalizing feature values between 0 and 1",
      "B": "Extracting day, month, year from a timestamp into separate features",
      "C": "Combining features into a single composite feature",
      "D": "One-hot encoding categorical variables"
    },
    "explanation": "Feature splitting breaks complex features into multiple simpler features, such as decomposing a timestamp into date components."
  },
  {
    "taskStatement": "1.2",
    "stem": "What is a recommended approach to handle noisy or erroneous data during feature transformation?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Detect and treat outliers through techniques such as trimming, winsorization, or imputation",
      "B": "Remove all missing values regardless of volume",
      "C": "Use one-hot encoding on noisy features",
      "D": "Ignore noise as models inherently handle it"
    },
    "explanation": "Treating outliers and noisy data through detection and appropriate techniques helps prevent model bias and instability."
  },
  {
    "taskStatement": "1.3",
    "stem": "Which AWS service can be used specifically to detect and mitigate selection and measurement bias in datasets prior to model training?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Glue DataBrew",
      "B": "Amazon SageMaker Model Monitor",
      "C": "Amazon SageMaker Clarify",
      "D": "Amazon Athena"
    },
    "explanation": "Amazon SageMaker Clarify provides functionalities to identify and mitigate bias such as selection and measurement bias in datasets before model training, unlike AWS Glue DataBrew which focuses on cleaning and transforming data."
  },
  {
    "taskStatement": "1.3",
    "stem": "When preparing data to reduce prediction bias in model training, which of the following techniques is LEAST effective?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Shuffling the dataset before splitting",
      "B": "Dataset splitting ensuring balanced representation of classes",
      "C": "Synthetic data augmentation for underrepresented classes",
      "D": "Encrypting the dataset prior to training"
    },
    "explanation": "Encrypting data protects privacy and confidentiality but does not reduce prediction bias directly; techniques like shuffling and augmentation are more effective in addressing bias."
  },
  {
    "taskStatement": "1.3",
    "stem": "Which technique is most appropriate to identify data quality issues that might violate compliance requirements such as PII or PHI in a dataset?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Using Amazon SageMaker Ground Truth to label data",
      "B": "Implementing AWS Glue Data Quality rules",
      "C": "One-hot encoding categorical variables",
      "D": "Applying log transformation on numerical features"
    },
    "explanation": "AWS Glue Data Quality allows defining and running rules that validate data integrity and flag issues related to compliance data, including presence of sensitive information."
  },
  {
    "taskStatement": "1.3",
    "stem": "Which of the following is a recommended practice to address class imbalance (CI) in numeric datasets prior to training?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Perform resampling techniques such as oversampling minority classes",
      "B": "Apply label encoding to class labels",
      "C": "Encrypt sensitive numeric features",
      "D": "Deploy models on serverless endpoints"
    },
    "explanation": "Resampling, like oversampling minority classes, is a common technique to mitigate class imbalance before training models."
  },
  {
    "taskStatement": "1.3",
    "stem": "What is a valid approach to comply with data residency regulations when preparing datasets for ML modeling in AWS?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Store data globally in Amazon S3 buckets in any region",
      "B": "Use Amazon S3 bucket policies to restrict access based on region",
      "C": "Apply one-hot encoding to sensitive attributes",
      "D": "Enable automatic hyperparameter tuning"
    },
    "explanation": "Using S3 bucket policies and replication controls, data residency requirements can be enforced by restricting data storage and access to specific AWS regions."
  },
  {
    "taskStatement": "1.3",
    "stem": "Which encryption technique is best suited to protect sensitive data at rest while maintaining compatibility with SageMaker model training?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Key Management Service (KMS) to encrypt Amazon S3 data",
      "B": "Custom symmetric encryption before uploading to S3",
      "C": "Encrypt data only in transit using SSL/TLS",
      "D": "Obfuscate data by tokenization only"
    },
    "explanation": "AWS KMS integrates seamlessly with S3 and SageMaker enabling encryption at rest without affecting downstream ML workflows."
  },
  {
    "taskStatement": "1.3",
    "stem": "Which AWS service combination helps automate validation of data quality and detect data drift in datasets to ensure data integrity over time?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Lambda and Amazon Athena",
      "B": "Amazon S3 and AWS Glue DataBrew",
      "C": "Amazon SageMaker Ground Truth and Amazon Kinesis",
      "D": "AWS Glue Data Quality and SageMaker Model Monitor"
    },
    "explanation": "AWS Glue Data Quality validates data quality proactively, while SageMaker Model Monitor detects drift during inference, together securing data integrity."
  },
  {
    "taskStatement": "1.3",
    "stem": "In the context of ML data preparation, what does the metric 'Difference in Proportions of Labels (DPL)' signify?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "The fraction of missing values in numerical features",
      "B": "The accuracy difference between training and test sets",
      "C": "A pre-training bias metric quantifying label distribution disparities between groups",
      "D": "The number of epochs needed for convergence"
    },
    "explanation": "DPL measures distributional differences of labels between sensitive groups to quantify bias before model training."
  },
  {
    "taskStatement": "1.3",
    "stem": "Which data preparation step is critical to reduce prediction bias due to dataset imbalance in text classification tasks?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Perform dimensionality reduction using PCA",
      "B": "Use resampling or synthetic data generation for underrepresented classes",
      "C": "Avoid tokenization to preserve original text",
      "D": "Encrypt training data with asymmetric keys"
    },
    "explanation": "In text classification, addressing class imbalance through resampling or synthetic generation reduces prediction bias."
  },
  {
    "taskStatement": "1.3",
    "stem": "Which AWS service can help in anonymizing sensitive fields in datasets to meet compliance requirements before training?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Macie for discovering and protecting sensitive data",
      "B": "Amazon SageMaker Model Monitor",
      "C": "AWS CodePipeline",
      "D": "Amazon Kinesis Data Firehose"
    },
    "explanation": "Amazon Macie automatically discovers, classifies, and protects sensitive data enabling proper anonymization and compliance."
  },
  {
    "taskStatement": "1.3",
    "stem": "Which method is effective to ensure data integrity when merging datasets from multiple sources for ML modeling?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use only JSON format for all source files",
      "B": "Perform one-hot encoding before merging",
      "C": "Validate schema consistency and apply deduplication techniques",
      "D": "Encrypt data in transit only"
    },
    "explanation": "Validating schemas and removing duplicated records ensures data integrity when merging heterogeneous sources."
  },
  {
    "taskStatement": "1.3",
    "stem": "Which of the following is an example of a technique to treat outliers to maintain data integrity before training a machine learning model?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Shuffling dataset records randomly",
      "B": "Capping or removing values that exceed a certain percentile threshold",
      "C": "Encrypting data records for privacy",
      "D": "Label encoding all categorical features"
    },
    "explanation": "Outlier capping limits extreme values to reduce their distortive effects on model training."
  },
  {
    "taskStatement": "1.3",
    "stem": "Which AWS tool is best suited to perform initial storage capacity planning with regards to cost, performance, and data structure for ML datasets?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon SageMaker Clarify",
      "B": "AWS Cost Explorer",
      "C": "Amazon EMR",
      "D": "AWS Glue DataBrew"
    },
    "explanation": "AWS Glue DataBrew helps profile data to understand structure and patterns informing storage decisions with cost-performance tradeoffs."
  },
  {
    "taskStatement": "1.3",
    "stem": "When encrypting data used for training, which AWS capability ensures fine-grained key management and auditing of cryptographic operations?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Key Management Service (KMS)",
      "B": "AWS Secrets Manager",
      "C": "Amazon Macie",
      "D": "AWS CloudTrail"
    },
    "explanation": "AWS KMS provides managed encryption keys with audit capabilities to secure data in AWS ML workflows."
  },
  {
    "taskStatement": "1.3",
    "stem": "What is a primary risk of neglecting to validate data quality and bias prior to training an ML model on sensitive datasets?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Faster training epochs",
      "B": "Deployment of a biased model that causes unfair outcomes",
      "C": "Increased storage costs",
      "D": "Reduced compliance requirements"
    },
    "explanation": "Bias in data can propagate into models causing unethical or illegal biased decisions if not mitigated."
  },
  {
    "taskStatement": "1.3",
    "stem": "Which of the following is NOT an example of a pre-training bias metric applicable for image datasets?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Class imbalance (CI)",
      "B": "Difference in proportions of labels (DPL)",
      "C": "Disparate impact ratio",
      "D": "Root Mean Square Error (RMSE)"
    },
    "explanation": "RMSE measures regression error and is not a bias metric for image dataset distributions."
  },
  {
    "taskStatement": "1.3",
    "stem": "Which AWS service should be used to create high-quality labeled datasets with human-in-the-loop annotation to improve data quality for ML training?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Athena",
      "B": "Amazon Kinesis Data Streams",
      "C": "Amazon SageMaker Ground Truth",
      "D": "AWS Glue Data Quality"
    },
    "explanation": "SageMaker Ground Truth enables human annotation to generate accurate labels and reduce noise in training datasets."
  },
  {
    "taskStatement": "1.3",
    "stem": "What is an appropriate measure to handle missing values in numeric datasets prior to ML modeling to maintain data integrity?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Imputation using mean, median, or model-based approaches",
      "B": "Encrypting missing values with KMS",
      "C": "Discarding all records with missing values regardless of percentage",
      "D": "One-hot encoding missing values"
    },
    "explanation": "Imputation helps fill missing data with reasonable estimates to improve model learning rather than outright data loss."
  },
  {
    "taskStatement": "1.3",
    "stem": "How does SageMaker Clarify assist in maintaining data integrity with respect to compliance in ML workflows?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "By automatically encrypting data with AWS KMS",
      "B": "By providing metrics to detect bias and explainability issues in data and models",
      "C": "By provisioning secured VPC endpoints for SageMaker",
      "D": "By creating anonymized datasets automatically"
    },
    "explanation": "Clarify analyzes data and models for bias and interpretability to ensure fair and compliant ML practices."
  },
  {
    "taskStatement": "1.3",
    "stem": "Which is a cost-effective and scalable storage option for large training datasets that require encryption and compliance with data residency requirements?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Elastic File System (EFS) with no encryption",
      "B": "Amazon S3 Glacier",
      "C": "Amazon S3 with KMS encryption and region scoping",
      "D": "AWS Storage Gateway with local caching"
    },
    "explanation": "Amazon S3 supports data encryption and regional restrictions, fitting cost-effective large dataset storage with compliance."
  },
  {
    "taskStatement": "1.3",
    "stem": "Which method is suitable for validating the quality of streaming data before it is ingested into SageMaker pipelines to prevent biased model predictions?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Using AWS Lambda functions with embedded data validation logic",
      "B": "Deploying models on Amazon EC2 instances",
      "C": "Manually reviewing streaming records",
      "D": "Encrypting streaming data only after ingestion"
    },
    "explanation": "AWS Lambda can automatically validate and transform streaming data in real time before it reaches ML workflows."
  },
  {
    "taskStatement": "1.3",
    "stem": "In the context of model fairness, which practice directly helps reduce measurement bias in datasets?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increasing batch size during model training",
      "B": "Applying one-hot encoding to categorical variables",
      "C": "Limiting training to only numeric features",
      "D": "Ensuring consistent and accurate data collection procedures"
    },
    "explanation": "Measurement bias arises from inconsistencies/errors in data collection; standardizing collection reduces such bias."
  },
  {
    "taskStatement": "1.3",
    "stem": "To meet HIPAA compliance for protected health information (PHI) when preparing data for ML, which action is essential?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Encrypt the training data with symmetric keys only",
      "B": "Classify, anonymize, or mask PHI before training",
      "C": "Store all data in public S3 buckets for accessibility",
      "D": "Use only open-source libraries during training"
    },
    "explanation": "Anonymizing or masking PHI mitigates privacy risks and complies with regulations such as HIPAA."
  },
  {
    "taskStatement": "1.3",
    "stem": "During data preparation, what is the effect of overfitting the training dataset by failing to manage class imbalance properly?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "It improves generalization on new data",
      "B": "It reduces the model's complexity",
      "C": "It causes the model to perform poorly on minority classes",
      "D": "It shortens training duration"
    },
    "explanation": "Class imbalance leads to models biased toward majority classes, resulting in poor minority class performance."
  },
  {
    "taskStatement": "1.3",
    "stem": "Which AWS service assists with maintaining audit trails to ensure continued compliance during ML data processing?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon SageMaker Batch Transform",
      "B": "AWS Shield",
      "C": "Amazon SageMaker Debugger",
      "D": "AWS CloudTrail"
    },
    "explanation": "AWS CloudTrail logs API activity and user actions providing trails necessary for audits and compliance."
  },
  {
    "taskStatement": "1.3",
    "stem": "What type of bias occurs when datasets exclude certain groups leading to underrepresentation in training data?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Selection bias",
      "B": "Measurement bias",
      "C": "Overfitting",
      "D": "Data leakage"
    },
    "explanation": "Selection bias arises when certain demographic or data groups are excluded, undermining model fairness."
  },
  {
    "taskStatement": "1.3",
    "stem": "Which AWS feature allows the secure loading of data into training resources such as Amazon FSx or Amazon EFS ensuring data integrity?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Budgets for cost tracking",
      "B": "AWS Lambda for data transformation",
      "C": "Network configuration with VPC and IAM roles",
      "D": "Amazon CodeBuild for CI/CD"
    },
    "explanation": "Configuring proper IAM roles and VPC networking secures data ingress ensuring integrity and authorized access."
  },
  {
    "taskStatement": "1.3",
    "stem": "To mitigate bias in data labels, which process is preferred before model training?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Rely exclusively on automated labeling",
      "B": "Human-in-the-loop data review and correction",
      "C": "Remove all data points with ambiguous labels",
      "D": "Encrypt label data"
    },
    "explanation": "Human-in-the-loop processes ensure label accuracy and fairness versus fully automated noisy labeling."
  },
  {
    "taskStatement": "1.3",
    "stem": "In ML data preparation, what is the outcome of dataset shuffling combined with stratified splitting?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Both randomized and balanced subsets preserving class proportions",
      "B": "Entirely random splits ignoring class balance",
      "C": "Complete removal of duplicate records",
      "D": "Ensured chronological order of time series data"
    },
    "explanation": "Shuffling randomizes data order, and stratified splitting preserves class distribution improving fairness and evaluation."
  },
  {
    "taskStatement": "1.3",
    "stem": "Which compliance implication requires masking or anonymizing data prior to ML training?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Mitigating overfitting",
      "B": "Improving model interpretability",
      "C": "Protecting personally identifiable information (PII)",
      "D": "Increasing training speed"
    },
    "explanation": "PII protection demands masking or anonymizing sensitive data to comply with privacy regulations."
  },
  {
    "taskStatement": "1.3",
    "stem": "Which approach is most suitable to address measurement bias introduced by inconsistent feature extraction processes?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Discard features prone to encoding",
      "B": "Standardize feature extraction pipelines and logging",
      "C": "Use encryption to mask feature values",
      "D": "Increase the model complexity"
    },
    "explanation": "Standardizing extraction processes reduces inconsistencies causing measurement bias."
  },
  {
    "taskStatement": "1.3",
    "stem": "What is the purpose of using SageMaker Feature Store in the context of data integrity and preparation?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "To encrypt datasets automatically",
      "B": "To annotate data with labels only",
      "C": "To perform unsupervised feature learning",
      "D": "To create, store, and retrieve curated ML features reliably"
    },
    "explanation": "SageMaker Feature Store manages consistent feature repositories ensuring data and feature integrity."
  },
  {
    "taskStatement": "1.3",
    "stem": "Why is it important to detect and correct bias in the training data prior to model deployment in regulated industries?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "To reduce overall training time",
      "B": "To prevent discriminatory outcomes and ensure regulatory compliance",
      "C": "To improve data storage efficiency",
      "D": "To facilitate model compression"
    },
    "explanation": "Bias correction is critical to avoid unfair decisions and legal repercussions in regulated sectors."
  },
  {
    "taskStatement": "1.3",
    "stem": "How does higher class imbalance influence prediction bias in numeric ML datasets without mitigation?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Skewed predictions favoring majority classes",
      "B": "Even distribution of errors across classes",
      "C": "Increased training speed",
      "D": "Improved minority class recall"
    },
    "explanation": "Imbalanced classes cause models to bias predictions towards majority, harming fairness and accuracy."
  },
  {
    "taskStatement": "1.3",
    "stem": "Which AWS service helps identify and mask sensitive data elements in datasets to comply with GDPR or similar data protection laws?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Macie",
      "B": "AWS CloudTrail",
      "C": "Amazon S3 Transfer Acceleration",
      "D": "Amazon EMR"
    },
    "explanation": "Amazon Macie automatically discovers and protects sensitive data such as PII to aid compliance."
  },
  {
    "taskStatement": "1.3",
    "stem": "What data preparation strategy reduces variance in training data to improve model stability without significantly affecting bias?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Random undersampling",
      "B": "Removing entire feature columns",
      "C": "Data normalization and standardization",
      "D": "Encrypting dataset"
    },
    "explanation": "Normalization scales numeric features to reduce variance improving learning while retaining overall data integrity."
  },
  {
    "taskStatement": "1.3",
    "stem": "Which compliance-related consideration must be addressed when migrating ML datasets across geographic AWS regions?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Scaling compute resources automatically",
      "B": "Ensuring data residency and sovereignty requirements are met",
      "C": "Encrypting all model hyperparameters",
      "D": "Applying one-hot encoding globally"
    },
    "explanation": "Data residency laws mandate storage and processing in approved regions, requiring compliance during migration."
  },
  {
    "taskStatement": "1.3",
    "stem": "Which feature of AWS Glue DataBrew is specifically useful to detect anomalies or inconsistencies affecting data integrity before training?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Serverless processing",
      "B": "Integration with SageMaker pipelines",
      "C": "Data lineage tracking",
      "D": "Built-in data profiling and validation rules"
    },
    "explanation": "DataBrew offers data profiling and validation to pinpoint anomalies impacting quality."
  }
]