[
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A company processes IoT sensor data streams at 200 MB/s for ML feature engineering. The data arrives in JSON format and must be ingested into SageMaker Data Wrangler in near-real time. The ingestion solution must minimize operational overhead and storage costs while enabling efficient downstream transformations. Which solution meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy Amazon Kinesis Data Streams to buffer JSON records. Configure an AWS Lambda function to batch and convert records into Parquet, then write to S3. Use Data Wrangler to read from S3.",
      "B": "Use Amazon Kinesis Data Firehose to ingest JSON data directly into an S3 bucket with Parquet conversion and GZIP compression. Configure Data Wrangler to read from that S3 bucket.",
      "C": "Set up an Amazon Managed Streaming for Apache Kafka cluster. Use Kafka Connect to stream data into an Amazon Redshift table. Use Data Wrangler to query Redshift.",
      "D": "Ingest streams into an AWS Glue Streaming ETL job. Write output to Amazon DynamoDB. Use Data Wrangler to fetch items from DynamoDB."
    },
    "explanation": "Kinesis Data Firehose natively converts JSON to Parquet with compression and writes to S3, minimizing operational overhead and cost. The other options add complexity or incur higher cost/management burden."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A dataset consists of structured CSV transaction logs, semi-structured JSON user event logs, and tens of millions of small binary image files. The ML pipeline uses Spark on EMR for preprocessing and SageMaker for training. The dataset has grown to multi-terabyte scale and ingestion performance is suffering. Which storage configuration should the ML engineer choose to optimize throughput, cost, and simplicity?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Store CSV and JSON in S3. Store images in Amazon FSx for Lustre mounted to EMR. Use Spark on EMR to read both sources.",
      "B": "Import all data into Amazon Redshift with Spectrum. Store CSV/JSON in tables, images as BLOBs. UNLOAD data for SageMaker training.",
      "C": "Store all data in Amazon S3 as partitioned, SNAPPY-compressed Parquet files. Configure EMR to read via the S3A connector and SageMaker to use S3 input mode.",
      "D": "Use Amazon EFS to store all raw files. Mount EFS to EMR clusters and SageMaker training jobs as a unified data source."
    },
    "explanation": "Partitioned Parquet on S3 offers cost-effective, high-throughput storage, simple management, and native support in Spark and SageMaker. FSx adds cost/ops, Redshift BLOBs and Spectrum incur complexity, and EFS suffers lower throughput and higher cost for large-scale data."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An ML engineer runs a SageMaker processing job to merge and feature-engineer 50 million small JSON files in Amazon S3. The job fails due to S3 request throttling from excessive LIST and GET operations. Which solution will reduce operational overhead and ensure reliable job completion?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure a VPC endpoint for S3 in the SageMaker processing job to reduce throttling. Increase parallelism to maximize throughput.",
      "B": "Enable S3 Transfer Acceleration on the bucket to speed up GET requests. Use the Transfer Acceleration endpoint in the processing job.",
      "C": "Use AWS Glue to run a job that aggregates the small JSON files into larger SNAPPY-compressed Parquet files in S3. Update the SageMaker processing job to read the Parquet files.",
      "D": "Copy all JSON files to an Amazon EFS file system using AWS DataSync. Configure the SageMaker processing job to read directly from EFS."
    },
    "explanation": "Aggregating small files into larger Parquet files reduces S3 request count and improves throughput with minimal ongoing operations. VPC endpoints and Transfer Acceleration do not change S3 request rate limits; EFS adds complexity and cost."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "An ML engineer needs to encode a high-cardinality categorical feature (10 000 unique values) for a tree-based model. They must minimize memory footprint, avoid one-hot explosion, and preserve ordering where meaningful. Which approach and AWS tool should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use one-hot encoding in SageMaker Data Wrangler to produce sparse binary columns.",
      "B": "Use label encoding in AWS Glue DataBrew to assign integer codes to categories.",
      "C": "Use binary encoding in SageMaker Data Wrangler to compress categories into \u2308log\u2082(10000)\u2309 binary features.",
      "D": "Use frequency encoding in an AWS Glue ETL job to replace each category with its occurrence count."
    },
    "explanation": "Binary encoding (also called bit hashing) reduces dimensionality to \u2308log\u2082(n)\u2309 columns and preserves some ordinal information. SageMaker Data Wrangler supports a built-in binary encoding transform, minimizing memory and avoiding one-hot explosion."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A data scientist is building a cleaning pipeline in SageMaker Data Wrangler for a numerical dataset with extreme outliers and missing values. They need to: (1) cap outliers at the 1st and 99th percentiles, (2) impute remaining missing values with the column median, and (3) standardize features to zero mean and unit variance. Which sequence of Wrangler nodes achieves this with minimal custom code?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Standardize \u2192 percentile clip \u2192 median impute",
      "B": "Median impute \u2192 percentile clip \u2192 standardize",
      "C": "Percentile clip \u2192 median impute \u2192 standardize",
      "D": "Median impute \u2192 standardize \u2192 percentile clip"
    },
    "explanation": "Clipping outliers first avoids imputing capped values. Next, median imputation handles any remaining missing data. Finally, standardization yields zero mean/unit variance on the cleaned data."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A real-time recommendation engine requires streaming user events from Kinesis Data Streams to be transformed (JSON flattening, type conversion) and ingested into SageMaker Feature Store with under-second latency. Which solution minimizes operational overhead and meets latency SLAs?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Implement an AWS Lambda function triggered by Kinesis Data Streams that transforms each record and calls the SageMaker Feature Store PutRecord API.",
      "B": "Run an AWS Glue streaming ETL job on Kinesis Data Streams with PySpark to transform and write directly to Feature Store.",
      "C": "Deploy an Apache Flink application on Amazon EMR to consume Kinesis, perform transformations, write to S3, then batch-load into Feature Store.",
      "D": "Use Amazon Kinesis Data Analytics (Flink SQL) to transform the stream and send to a SageMaker Batch Transform job for periodic ingestion."
    },
    "explanation": "A Lambda function on Kinesis provides sub-second processing with minimal infrastructure to manage. It can call PutRecord directly. Glue streaming and EMR/Flink introduce greater complexity and latency; Batch Transform cannot meet real-time SLAs."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A health-tech startup has a dataset in Amazon S3 with PII fields (names, SSNs) and binary labels. They must prepare the data for SageMaker training by removing or masking PII, validating data quality, computing pre-training bias metrics (class imbalance, difference in proportions), and mitigating identified bias before training. Which sequence of AWS services and actions fulfills these requirements with minimal operational overhead?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon Macie to discover PII \u2192 apply client-side KMS encryption \u2192 run a SageMaker Processing job with a custom script to compute bias metrics.",
      "B": "Use AWS Glue Data Quality to profile dataset \u2192 use AWS Glue ETL to anonymize PII \u2192 run SageMaker Model Monitor to compute pre-training bias metrics.",
      "C": "Use SageMaker Ground Truth to label PII columns \u2192 use SageMaker Data Wrangler to transform data \u2192 use SageMaker Clarify ModelBias to compute post-training bias.",
      "D": "Use SageMaker Data Wrangler to detect and mask PII with built-in transforms \u2192 launch a SageMaker Clarify DataBias job to calculate pre-training CI and DPL metrics \u2192 apply synthetic oversampling for the minority class via Data Wrangler or a SageMaker Processing job."
    },
    "explanation": "Option D leverages Data Wrangler\u2019s built-in PII masking, uses Clarify\u2019s DataBias monitoring to compute pre-training bias metrics, and then mitigates imbalance via a native transform or Processing job, minimizing custom code and meeting compliance and bias-mitigation requirements."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "During preprocessing, an ML engineer discovers a significant class imbalance: one protected class comprises only 1% of the training data. They need to generate additional synthetic samples for that class without introducing label noise and then validate the augmented dataset\u2019s quality. Which approach aligns best with AWS-recommended bias mitigation and data validation practices?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Clarify\u2019s synthetic data generator to produce minority-class examples and merge with original data.",
      "B": "Launch a SageMaker Processing job that uses scikit-learn\u2019s SMOTE to create synthetic minority-class samples, then run AWS Glue Data Quality jobs to validate data integrity.",
      "C": "Use AWS Glue DataBrew\u2019s \u201cGenerate rows\u201d transform to duplicate minority-class records until balance is achieved.",
      "D": "Ingest data into SageMaker Feature Store and use record augmentation in Feature Store to rebalance classes."
    },
    "explanation": "Option B applies SMOTE in a managed Processing job to generate realistic synthetic samples for the minority class, then validates the augmented dataset with Glue Data Quality, aligning with best practices for bias mitigation and data integrity."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "An ML engineer must deliver preprocessed, bias-mitigated training data to a SageMaker training job. The data must be encrypted at rest and in transit, and the training dataset split must be stratified by a protected attribute to avoid introducing bias. How should the engineer configure the data pipeline to meet these requirements?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Store the CSVs in S3 with SSE-S3 encryption, use File input mode in SageMaker training, and rely on built-in random shuffle for splits.",
      "B": "Provision an encrypted FSx for Lustre file system, copy preprocessed files there, and configure the training job with File mode and a JSON split manifest.",
      "C": "Mount an encrypted Amazon EFS volume in the training container, copy data via a Processing job that shuffles and splits, then train using File mode.",
      "D": "Store preprocessed data in S3 with SSE-KMS encryption, use Pipe input mode for the training job, and implement a stratified shuffle-and-split routine inside the training container to ensure splits respect the protected attribute."
    },
    "explanation": "Option D ensures end-to-end encryption with SSE-KMS, leverages Pipe mode to stream data efficiently, and uses a custom stratified shuffle-and-split routine in the container to maintain fairness across protected groups."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A fintech company is building a credit-scoring model to classify loan applicants as high or low risk. The dataset consists of 1 million rows of structured tabular data with a mix of numerical and categorical features of moderate cardinality. Regulatory requirements mandate that the model be highly interpretable for audit, and the application demands low-latency inference. Data scientists want to minimize manual feature engineering. Which SageMaker built-in algorithm should the ML engineer choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use the XGBoost built-in algorithm with default hyperparameters.",
      "B": "Use the Factorization Machines built-in algorithm to capture feature interactions.",
      "C": "Use the Linear Learner built-in algorithm configured for logistic regression.",
      "D": "Use the K-Nearest Neighbors built-in algorithm for classification."
    },
    "explanation": "Linear Learner in logistic regression mode provides a fully interpretable model with low inference latency and minimal feature engineering. XGBoost offers higher accuracy but less interpretability. Factorization Machines capture interactions but are harder to audit. KNN has high latency and is not suited for large datasets."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A legal analytics startup needs to automate abstractive summarization of large contract documents. The team lacks expertise in training sequence-to-sequence models and requires a fully managed, high-quality solution that supports few-shot prompting. Which modeling approach should the ML engineer select?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon Comprehend\u2019s extractive summarization API on the contracts.",
      "B": "Fine-tune a T5 sequence-to-sequence model on SageMaker using the Hugging Face framework from JumpStart.",
      "C": "Invoke a foundation model via Amazon Bedrock (for example, Titan) with a prompt template for abstractive summarization.",
      "D": "Build a custom TensorFlow sequence-to-sequence model from scratch on SageMaker training instances."
    },
    "explanation": "Amazon Bedrock provides managed foundation models that excel at few-shot abstractive summarization with minimal ML expertise. Comprehend only supports extractive summarization. Fine-tuning on SageMaker or building from scratch incurs significant development and infrastructure overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "An industrial IoT provider collects streaming time-series sensor data from thousands of devices and needs to detect anomalies in near real time. The provider wants a fully managed service requiring minimal ML development and configuration. Which AWS service or approach should the ML engineer choose?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Train and deploy a Random Cut Forest (RCF) model on SageMaker for anomaly detection.",
      "B": "Develop and train a custom LSTM-based anomaly detector on SageMaker using TensorFlow.",
      "C": "Use Amazon Lookout for Metrics to automatically detect anomalies in the time-series data.",
      "D": "Use the anomaly-detection recipe in SageMaker Studio JumpStart and deploy the generated pipeline."
    },
    "explanation": "Amazon Lookout for Metrics is a fully managed anomaly-detection service that automatically tracks time-series metrics with minimal configuration. Training an RCF or LSTM on SageMaker requires substantial ML development and ongoing maintenance. JumpStart recipes simplify development but still require managing model training and pipelines."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An ML engineer previously ran a hyperparameter tuning job on a deep learning model and wants to optimize a new tuning job by reusing the results of that earlier job. Additionally, to reduce overall training cost and time, the engineer needs to automatically stop underperforming training jobs during tuning. Which configuration will meet these requirements?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run a new hyperparameter tuning job using BayesianOptimization search, with MaxNumberOfTrainingJobs set to 100, and leave EarlyStoppingType unset.",
      "B": "Run a warm start tuning job with type TRANSFER_LEARNING and disable early stopping to leverage prior results only.",
      "C": "Run a warm start hyperparameter tuning job with type IDENTICAL_DATA_AND_ALGORITHM and set EarlyStoppingType to Auto.",
      "D": "Run a random search tuning job with MaxNumberOfTrainingJobs set to 50 and enable EarlyStoppingType to OfflineStopping."
    },
    "explanation": "A warm start of type IDENTICAL_DATA_AND_ALGORITHM reuses prior tuning results for the same algorithm and search space, and setting EarlyStoppingType to Auto applies SageMaker\u2019s median stopping rule to halt underperforming jobs early."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A data scientist has fine-tuned a 2 GB PyTorch BERT model for text classification on a GPU instance. The model must now be deployed to a CPU-based inference endpoint with a memory footprint below 500 MB while maintaining at least 95% of its original accuracy. Which approach best meets these requirements with the least development effort?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Export the model to ONNX, write custom quantization code to convert weights to int8, and then deploy the ONNX model on the CPU endpoint.",
      "B": "Compile and optimize the trained PyTorch model using Amazon SageMaker Neo to perform graph optimizations and int8 quantization, then deploy the Neo-compiled model to the CPU endpoint.",
      "C": "Rewrite the model architecture to reduce hidden layer sizes by 50%, retrain from scratch on CPU, and deploy the smaller model.",
      "D": "Use AWS Lambda with a custom container to dynamically load and prune model weights at inference time to reduce memory usage."
    },
    "explanation": "SageMaker Neo automates model graph optimization and precision quantization (e.g., int8), reducing size and improving CPU performance with minimal code changes while preserving accuracy."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A research team has trained a custom XGBoost model locally and now wants to integrate it into Amazon SageMaker for managed hosting, hyperparameter tuning, and CI/CD pipelines without rewriting the training code. What is the most appropriate way to import and serve this externally trained model in SageMaker?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Package the model artifact into a model.tar.gz, build a custom inference Docker container (BYOC) that loads the artifact, push it to Amazon ECR, and create a SageMaker Model referencing that container and S3 artifact.",
      "B": "Translate the local training code into a SageMaker Script Mode training script, run it in a built-in XGBoost container, and register the new model.",
      "C": "Use the SageMaker SDK to call CreateTrainingJob with LocalMode enabled to import and train the model artifact directly.",
      "D": "Upload the model artifact to Amazon SageMaker Model Registry without a container and deploy it to a serverless endpoint."
    },
    "explanation": "Bringing an externally trained model into SageMaker requires a Bring-Your-Own-Container (BYOC) that contains the inference logic; SageMaker Model Registry alone cannot host artifacts without a container."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "A financial services company is building a fraud detection model. The base fraud rate in production is 1%. On a validation set of 10,000 transactions (100 frauds, 9,900 non-frauds), two candidate models yield the following metrics:\n\nModel A: precision = 0.667, recall = 0.8\nModel B: precision = 0.8, recall = 0.6\n\nThe business cost of a false positive (FP) is $1 and the cost of a false negative (FN) is $10. Which model has the lower expected cost per 10,000 transactions, and which should the company choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Choose Model A: expected cost = (FP=40\u00d7$1)+(FN=20\u00d7$10) = $240",
      "B": "Choose Model B: expected cost = (FP=15\u00d7$1)+(FN=40\u00d7$10) = $415",
      "C": "Both models have the same cost",
      "D": "Cannot decide without additional metrics"
    },
    "explanation": "Compute TP, FP, FN for each: Model A: TP=0.8\u00d7100=80, FP=80\u00d7(1/0.667\u22121)=40, FN=20 \u2192 cost=40+200=240. Model B: TP=60, FP=60\u00d7(1/0.8\u22121)=15, FN=40 \u2192 cost=15+400=415. Model A yields lower cost."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "An ML engineer needs to detect if a production classification model begins to rely on different features over time due to data drift. Which SageMaker Clarify monitor class should the engineer configure to track changes in feature contributions (for example, SHAP values) between a baseline and production data?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ModelBiasMonitor",
      "B": "ModelExplainabilityMonitor",
      "C": "ModelQualityMonitor",
      "D": "DataQualityMonitor"
    },
    "explanation": "ModelExplainabilityMonitor tracks explainability metrics such as SHAP feature attributions over time. The other monitors handle bias metrics (ModelBiasMonitor), overall prediction quality (ModelQualityMonitor), or input data distributions (DataQualityMonitor)."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "A deep neural network trained on SageMaker displays stagnating training loss and high validation loss. The engineer enabled SageMaker Debugger\u2019s default rules and then enabled the vanishing gradient rule, which flagged anomalies in the earliest layers. Which action best addresses the vanishing gradient problem in these layers?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase the global learning rate to accelerate gradient propagation",
      "B": "Replace sigmoid/tanh activations with ReLU (and use He initialization)",
      "C": "Add L2 weight regularization to penalize large weights",
      "D": "Reduce batch size to increase gradient noise and variability"
    },
    "explanation": "Vanishing gradients in early layers are mitigated by using activation functions with constant gradients (ReLU) and appropriate weight initializations (He). Other options (higher learning rate, L2 penalty, smaller batches) do not directly solve vanishing gradient issues."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A fintech firm has developed a credit-scoring ML model (<50 MB) that must serve real-time API requests with no more than 20 ms 95th-percentile latency. Traffic patterns are unpredictable, ranging from 5 to 500 requests per minute. The firm wants to minimize infrastructure management overhead and pay only for the compute capacity it uses, while ensuring consistent low-latency performance. Which SageMaker deployment infrastructure should the ML engineer select?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Deploy the model to a SageMaker serverless endpoint with default concurrency limits.",
      "B": "Deploy the model to a multi-model real-time SageMaker endpoint on a single ml.m5.large instance with auto scaling.",
      "C": "Deploy the model to a SageMaker real-time endpoint on ml.c5.large instances with a provisioned concurrency configuration and target-tracking auto scaling.",
      "D": "Deploy the model to a SageMaker asynchronous inference endpoint with default VCPU provisioning."
    },
    "explanation": "A serverless endpoint can cold-start and breach latency requirements. A single-instance multi-model endpoint cannot guarantee sub-20 ms under unpredictable load. Asynchronous inference is batch-oriented. A real-time endpoint with provisioned concurrency keeps containers warm and uses target-tracking scaling to elastically adjust capacity while maintaining low latency."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An industrial IoT company needs to deploy a vision ML model to 1 000 ARM-based edge cameras with intermittent connectivity. The model must run local inference with latency <50 ms and accept periodic updates without manual intervention. Which deployment infrastructure should the ML engineer choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Containerize the model and deploy it to AWS Lambda functions on AWS Greengrass Core.",
      "B": "Use SageMaker Edge Manager to package and deploy the model directly to AWS IoT devices.",
      "C": "Compile and optimize the model with SageMaker Neo for the target ARM architecture and deploy it to devices using AWS IoT Greengrass.",
      "D": "Deploy the model to a SageMaker real-time endpoint and configure the devices to call the endpoint when connected."
    },
    "explanation": "Lambda on Greengrass incurs container startup and may not meet <50 ms. Edge Manager provides monitoring but still requires a runtime-optimized model. A real-time endpoint requires connectivity. SageMaker Neo compiles and optimizes the model for ARM, and AWS IoT Greengrass handles offline deployment and periodic updates with minimal ops overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A retail company requires an automated, end-to-end CI/CD pipeline for their ML workflow: data preprocessing, training with hyperparameter tuning, model registration and approval, and blue/green deployment to production endpoints. The pipeline must integrate with a Git repository for version control, provide step-level observability, and support easy rollback. Which orchestrator should the ML engineer select to meet these requirements with minimal custom infrastructure?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS CodePipeline with custom AWS Lambda functions for each ML workflow step.",
      "B": "Amazon Managed Workflows for Apache Airflow (MWAA) with DAGs defining each stage.",
      "C": "AWS Step Functions orchestrating AWS Batch jobs for training and deploying Step Functions tasks.",
      "D": "Amazon SageMaker Pipelines with integrated steps for data processing, training, model registry, approval, and deployment."
    },
    "explanation": "CodePipeline requires significant custom Lambda code for ML steps. MWAA is general-purpose and needs custom operators. Step Functions plus Batch require building and managing additional compute and monitoring. SageMaker Pipelines natively integrates preprocessing, training, tuning, registry, approval, and deployment, supports Git integration, lineage, monitoring, and rollback with minimal custom infrastructure."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "A machine learning team must deploy a SageMaker real-time multi-model endpoint in private subnets with no internet access. The models are packaged as custom Docker images stored in Amazon ECR. The team wants to automate provisioning of all networking, ECR, and SageMaker resources using infrastructure as code (IaC) with minimal operational overhead, and to enforce coding best practices and unit tests. Which approach best meets these requirements?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "Write an AWS CloudFormation template that defines the VPC, private subnets, ECR repository, SageMaker model, endpoint configuration, and endpoint resources.",
      "B": "Develop an AWS CDK application (in a supported language) that defines the VPC with interface endpoints, ECR repository, SageMaker model, endpoint configuration, and auto scaling policies, and run unit tests against the CDK constructs.",
      "C": "Use an AWS SAM template to define a Lambda function that creates the VPC and provisions the SageMaker endpoint when invoked.",
      "D": "Author a Terraform module to provision the VPC, ECR repository, and SageMaker resources, and manage state in an S3 backend."
    },
    "explanation": "AWS CDK provides a programmable IaC framework with built-in support for unit testing of constructs, reduces YAML/JSON boilerplate compared to raw CloudFormation, and automates resource provisioning including VPC interface endpoints and SageMaker auto scaling policies with minimal operational overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "A company\u2019s SageMaker real-time endpoint experiences sudden traffic spikes, causing increased latency. The current target-tracking auto scaling policy uses CPUUtilization. The operations team wants to trigger scale-out more quickly on incoming inference requests while avoiding over-provisioning during lulls. Which metric should they use in the target-tracking policy?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CPUUtilization",
      "B": "ModelLatency",
      "C": "InvocationsPerInstance",
      "D": "MemoryUtilization"
    },
    "explanation": "InvocationsPerInstance measures the number of inference requests handled per instance, providing a direct signal of request load and enabling faster scaling on bursts, whereas CPUUtilization and MemoryUtilization lag and ModelLatency may not correlate directly with load volume."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "An ML engineer needs an automated, end-to-end pipeline that builds custom inference container images on code changes, pushes them to ECR, and updates a SageMaker endpoint to use the new image\u2014all defined as code. Which design has the least operational overhead?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use EventBridge to trigger a CodeBuild project on Git push; CodeBuild builds and pushes the image to ECR; a CloudFormation stack update is triggered manually to point the endpoint to the new image.",
      "B": "Define a CodePipeline pipeline (in AWS CDK) that uses CodeCommit, a CodeBuild action to build/push the image, and a CloudFormation action to update the SageMaker endpoint; deploy the pipeline via the CDK app.",
      "C": "Install AWS CLI scripts on an EC2 instance to poll CodeCommit for changes, build and push the container, then run AWS CLI to update the endpoint.",
      "D": "Use CodePipeline with CodeCommit and a Lambda step to build the container image, then invoke an AWS SAM deployment to update the endpoint."
    },
    "explanation": "A CDK-defined CodePipeline with CodeCommit, CodeBuild, and CloudFormation actions provides a fully managed, declarative CI/CD pipeline with minimal custom code, built-in integration, and automated endpoint updates, reducing operational overhead compared to scripting or Lambda-based workarounds."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "An ML engineer is tasked with building a fully automated CI/CD pipeline for an Amazon SageMaker\u2013based model. The pipeline must: 1) pull code and configuration from a Git repository, 2) run unit tests and data validation, 3) execute data transformation in a SageMaker Processing job, 4) train the model, 5) register the trained model in the SageMaker Model Registry, and 6) perform a canary deployment to production with traffic shifting. The solution must minimize custom Lambda code and leverage native AWS services. Which pipeline architecture best meets these requirements?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS CodePipeline with stages: Source (GitHub), CodeBuild (run tests and data validation), CloudFormation (provision Processing and Training jobs and register model), Manual approval, CloudFormation (update endpoint).",
      "B": "Use AWS CodePipeline with stages: Source (GitHub), CodeBuild (run tests, invoke direct SDK calls to Processing and Training, register model), Manual approval, CodeBuild (use AWS CLI to update endpoint).",
      "C": "Implement a SageMaker Pipeline that includes Processing, Training, Evaluation, and Model Registry steps, and trigger it via EventBridge on git push. Use SageMaker Pipeline\u2019s built-in endpoint update step with canary configuration.",
      "D": "Use AWS CodePipeline with stages: Source (GitHub), CodeBuild (invoke a SageMaker Pipeline via AWS CLI to run Processing, Training, Evaluation, and register the model), Approval, CodeDeploy (configured for SageMaker endpoint canary deployment and traffic shifting)."
    },
    "explanation": "Option D minimizes custom code by delegating ML workflow orchestration to SageMaker Pipelines and uses native CodePipeline stages and CodeDeploy for canary traffic shifting. It cleanly separates CI (CodeBuild) from CD (CodeDeploy) without custom Lambdas."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A new CodePipeline contains a CodeBuild action that uses the AWS CLI to start SageMaker Training and Model Deployment jobs. The CodeBuild project\u2019s IAM role has permissions for sagemaker:CreateTrainingJob, sagemaker:CreateModel, and sagemaker:CreateEndpoint, but the build fails with an AccessDenied error stating that the role cannot be passed. What is the LEAST-privilege IAM change required to allow the CodeBuild stage to succeed?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add iam:PassRole permission for the SageMaker execution role to the CodeBuild project's IAM role policy.",
      "B": "Update the SageMaker execution role trust policy to allow sts:AssumeRole from codepipeline.amazonaws.com.",
      "C": "Add iam:PassRole permission for the CodeBuild project role to the CodePipeline service role policy.",
      "D": "Add sts:AssumeRole permission for the SageMaker execution role in the CodeBuild project's trust policy."
    },
    "explanation": "When CodeBuild calls SageMaker, it must pass the SageMaker service execution role. Granting iam:PassRole on that role in the CodeBuild project's IAM role policy satisfies the least-privilege requirement."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A financial services team uses SageMaker Model Monitor to emit CloudWatch metrics when data drift exceeds thresholds. They need to automatically retrain the model end-to-end (processing, training, evaluation, registry) and roll out the new model with a linear traffic shift as soon as drift is detected. Which combination of AWS services and configurations will satisfy these requirements with minimal custom code?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create a CloudWatch alarm on the Model Monitor drift metric and an EventBridge rule that triggers an AWS CodePipeline pipeline. In CodePipeline, invoke a SageMaker Pipeline (processing, training, evaluation, model registration) and add a CodeDeploy stage configured for SageMaker endpoint linear traffic shifting.",
      "B": "Subscribe an SNS topic to the Model Monitor violation notification and use an AWS Lambda function to start a SageMaker Pipeline and update the endpoint via SDK with gradual traffic shifting.",
      "C": "Configure Model Monitor to directly invoke a SageMaker Pipeline on drift and include a built-in traffic shifting step in the SageMaker Pipeline definition.",
      "D": "Schedule a daily CodeBuild job to query Model Monitor metrics, and if drift is detected, run a training job and invoke a CloudFormation change set to update the endpoint."
    },
    "explanation": "Option A uses native integration: CloudWatch\u2192EventBridge to trigger CodePipeline, SageMaker Pipelines for retraining, and CodeDeploy for linear traffic shifting. This minimizes custom code and leverages managed services end-to-end."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An e-commerce company deployed a recommendation model on SageMaker and needs to monitor both input data drift and model bias in production. They have a baseline training dataset for drift detection and fairness metrics for bias detection. To minimize operational overhead and leverage fully managed monitoring, which solution meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a single SageMaker Model Monitor job with default settings to detect both drift and bias; schedule hourly.",
      "B": "Configure SageMaker Model Monitor (DefaultModelMonitor) to run a data quality monitoring job with baseline statistics and constraints for input features, and SageMaker Clarify ModelBiasMonitor to run a bias monitoring job with baseline fairness metrics; configure both on a daily schedule.",
      "C": "Use SageMaker Clarify's ModelExplainabilityMonitor to detect both data drift and bias by specifying a SHAP baseline, and schedule it hourly.",
      "D": "Use AWS Lambda triggered every hour to run custom Python scripts that compute drift metrics and bias metrics against baselines; send alerts via Amazon SNS."
    },
    "explanation": "SageMaker Model Monitor (DefaultModelMonitor) is optimized for data drift and quality monitoring using baseline statistics and constraints. SageMaker Clarify\u2019s ModelBiasMonitor is designed to monitor bias against fairness baselines. Combining these two fully managed monitors meets both drift and bias requirements with minimal custom code and operational overhead. Options A and C misuse or over-simplify the services, and D introduces unnecessary custom code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A SaaS provider hosts a real-time image classification endpoint on Amazon SageMaker. They have observed intermittent spikes in 5XX invocation errors and increased inference latency impacting user experience. The operations team needs a solution with the least operational overhead to automatically detect and alert on these issues in near real time. Which approach should the team implement?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Enable Amazon CloudWatch Logs for the endpoint; configure a metric filter to count '5XX' errors; create CloudWatch alarms for '5XX' error count and for 'ModelLatency' metric.",
      "B": "Configure SageMaker Model Monitor to capture inference requests and responses; schedule a data quality monitoring job every 5 minutes with a custom script to check for errors and latency; publish custom metrics to CloudWatch.",
      "C": "Configure DataCaptureConfig on the endpoint to capture all invocations to S3; create an AWS Lambda function triggered by S3 events that calculates error rates and latency; publish custom CloudWatch metrics and alarms.",
      "D": "Use SageMaker Clarify ModelExplainabilityMonitor to detect anomalies in output embeddings, which will indirectly detect errors and latency issues; schedule it on an hourly basis."
    },
    "explanation": "SageMaker endpoints emit built-in CloudWatch metrics such as Invocation5XXErrors and ModelLatency. Creating CloudWatch alarms on these metrics provides near real-time monitoring with zero custom code and minimal overhead. Model Monitor (B) and Lambda/S3 solutions (C) introduce unnecessary complexity, and Clarify (D) is not designed for error or latency monitoring."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A financial analytics team deployed a credit-scoring model on SageMaker real-time endpoints. They want to detect univariate feature drift in critical numeric input features (such as \u2018annual_income\u2019 and \u2018debt_ratio\u2019) and alert if the distribution has shifted beyond acceptable thresholds. They already have training data to serve as a baseline. Which SageMaker feature and configuration should the team use to meet this requirement with minimal custom coding?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure SageMaker Clarify ModelBiasMonitor with the training dataset as a baseline and set the 'drift_detection' parameter for numeric features; schedule hourly.",
      "B": "Use SageMaker Model Monitor's DefaultModelMonitor to create a MonitorSchedule with a DataQualityJobDefinition that specifies baseline_statistics and baseline_constraints created from the training dataset; set the monitoring schedule to run hourly.",
      "C": "Deploy a Python-based Lambda function that loads the training dataset baseline, fetches the latest batch of inference requests, computes Kolmogorov\u2013Smirnov tests for each numeric feature, and sends alerts via Amazon SNS.",
      "D": "Use Amazon CloudWatch Metrics Insight to query the 'InputDataMean' metric for each feature and configure alarms when mean deviates by more than a threshold from the training data mean."
    },
    "explanation": "SageMaker Model Monitor\u2019s DefaultModelMonitor supports univariate feature drift detection by automatically generating baseline_statistics and baseline_constraints from training data, and scheduling the monitoring job. This fully managed solution minimizes custom coding. Clarify\u2019s ModelBiasMonitor (A) is for fairness, not general drift, and options C and D require significant custom development or rely on unsupported metric calculations."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A company hosts a SageMaker real-time inference endpoint that experiences predictable daily traffic spikes at 9 AM and 6 PM, plus unpredictable fluctuations throughout the day. The ML engineer needs to minimize costs during off-peak hours while ensuring the endpoint maintains a p95 latency below 100 ms. The solution must use built-in AWS services and require minimal custom code. Which configuration should the engineer implement?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure a step scaling policy in Amazon CloudWatch that scales out when CPU Utilization > 70% and scales in when CPU Utilization < 30%; set MinCapacity=1 and MaxCapacity=10.",
      "B": "Deploy an AWS Lambda function triggered by Amazon EventBridge schedule rules at 8:50 AM and 5:50 PM to call UpdateEndpointWeightsAndCapacities, and use a target tracking policy for unpredictable loads.",
      "C": "Define an Application Auto Scaling target tracking policy on the SageMaker endpoint based on p95 latency with MinCapacity=1, MaxCapacity=10, and add a scheduled scaling action to increase capacity to 8 at 8:50 AM and 5:50 PM.",
      "D": "Use Spot Instances by converting the endpoint to asynchronous inference, configure Spot for inference to de-provision overnight, and rely on target tracking for bursts."
    },
    "explanation": "Option C leverages built-in Application Auto Scaling for SageMaker to handle both scheduled and dynamic scaling in a single service with minimal custom code. Step and Lambda solutions are more complex and less precise; asynchronous/Spot inference cannot guarantee real-time p95 latency."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "An ML engineer needs to optimize cost and performance of multiple SageMaker real-time inference endpoints. Each endpoint has different traffic patterns and latency requirements. The engineer wants to identify the most cost-effective instance type for each endpoint, based on sample inference payloads, throughput, and latency SLOs. Which tool or workflow should the engineer use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Compute Optimizer to generate EC2 instance recommendations and manually map those to SageMaker endpoints.",
      "B": "Use SageMaker Inference Recommender to run profiling jobs with sample payloads and choose the instance types with the lowest cost per inference that meet the SLOs.",
      "C": "Use AWS Trusted Advisor to forecast inference endpoint costs and recommend Reserved Instances.",
      "D": "Use AWS Cost Explorer\u2019s RI purchase recommendations to apply savings to SageMaker endpoint instance types."
    },
    "explanation": "SageMaker Inference Recommender is designed to profile real-world inference workloads and recommend the optimal instance types for cost and performance. Compute Optimizer and Trusted Advisor do not profile SageMaker workloads at the application level."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A global team uses SageMaker across 10 AWS accounts under consolidated billing. They must enforce that every SageMaker training job, endpoint, and pipeline carries the tags Project and CostCenter, automatically deny creations without them, and generate a centralized monthly cost dashboard per project. Which solution meets these requirements with the least operational overhead?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy AWS Config rules in each account to audit SageMaker resource tags, use AWS Service Catalog with TagOptions to solicit the tags, and build QuickSight dashboards from Cost Explorer filtered by tags.",
      "B": "Implement AWS Organizations Tag Policies to require Project and CostCenter tags on SageMaker resources, enable AWS Config for compliance evaluation, and use Cost Explorer/AWS Budgets with tag filters for centralized dashboards.",
      "C": "Create Service Control Policies to forbid creation of untagged SageMaker resources, use CloudTrail + Lambda to remediate missing tags, and aggregate costs via Cost Explorer.",
      "D": "Enforce all SageMaker deployments through AWS CloudFormation StackSets with mandatory tags in the templates, and view costs in the AWS Billing console."
    },
    "explanation": "AWS Organizations Tag Policies provide native, centralized enforcement of required tags across accounts with minimal operational effort. Combined with AWS Config compliance checks and Cost Explorer/AWS Budgets, this delivers automated enforcement and centralized cost reporting. Other options require custom code or more operational overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "An enterprise mandates that all SageMaker training and inference workloads must run within a secured VPC without any public internet access. However, these workloads need to read training data from Amazon S3, pull container images from Amazon ECR, write logs to Amazon CloudWatch Logs, and use a customer-managed AWS KMS key. The security team requires the solution to minimize ongoing maintenance overhead while enforcing least-privilege network connectivity. Which architectural configuration meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable SageMaker network isolation and launch jobs outside any VPC; rely on network isolation to prevent internet access.",
      "B": "Launch SageMaker jobs inside a VPC with no internet gateway, configure interface VPC endpoints for S3, ECR (both API and Docker), CloudWatch Logs, and KMS, and remove any NAT gateways.",
      "C": "Launch SageMaker jobs in the default VPC and attach a security group that blocks 0.0.0.0/0; use public endpoints for AWS services.",
      "D": "Use a network ACL on the private subnet to deny all outbound traffic; allow S3 and ECR access by whitelisting their public IP ranges."
    },
    "explanation": "Enabling interface VPC endpoints for S3, ECR, CloudWatch Logs, and KMS in a VPC without an Internet Gateway ensures workloads have private, least-privilege access to required AWS services and keys, while preventing any public internet access. Network isolation alone (A) doesn\u2019t provide VPC-private access to AWS services. Security groups cannot filter based on AWS service endpoints (C). NACLs cannot reliably allow AWS service traffic by IP (D)."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A company\u2019s sensitive training data is stored in an S3 bucket encrypted with a customer-managed KMS key (DataKey). They must run SageMaker training jobs that decrypt the data and write model artifacts to a separate S3 bucket encrypted with another KMS key (OutputKey). The SageMaker execution role must follow the principle of least privilege. Which combination of IAM role policy statements and KMS key policy statements satisfies this requirement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "IAM role: Grant s3:GetObject and s3:PutObject on both buckets and kms:Decrypt, kms:Encrypt on both keys. KMS key policies: Trust the entire account principal for all operations.",
      "B": "IAM role: Grant s3:GetObject on the data bucket; s3:PutObject on the artifact bucket; kms:Decrypt and kms:GenerateDataKey on DataKey; kms:Encrypt and kms:GenerateDataKey on OutputKey. KMS key policies: Specify the SageMaker execution role as the only principal allowed to use each key.",
      "C": "IAM role: Grant s3:* on both buckets and kms:* on both keys. KMS key policies: No changes needed (account admins manage keys).",
      "D": "Use S3 bucket policies to grant SageMaker service principal full access, and KMS grants to allow any IAM principal in account to decrypt keys."
    },
    "explanation": "Least-privilege requires the IAM role only have s3:GetObject on the data bucket, s3:PutObject on the output bucket, and only the specific KMS operations. The key policies must explicitly allow the SageMaker execution role to use each key. Over-permissive (A, C) or relying on bucket policies alone (D) violates least-privilege."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A security team manages multiple AWS accounts using AWS Organizations. They want to ensure that data scientists in member accounts can only create, update, or delete SageMaker endpoints within their own accounts, and prevent any SageMaker endpoint operations in accounts outside their OU. Which mechanism should be implemented at the organization level to enforce this restriction?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use an AWS Organizations service control policy (SCP) at the OU level to Deny all sagemaker:CreateEndpoint, UpdateEndpoint, and DeleteEndpoint actions when the resource\u2019s account ID does not match the requesting account.",
      "B": "Apply IAM permission boundaries to the data scientists\u2019 roles in each member account that restrict sagemaker:* to specific endpoint ARNs.",
      "C": "Attach a resource-based policy to each SageMaker endpoint specifying allowed IAM principals from the same account.",
      "D": "Use the SageMaker service-linked role policy to restrict endpoint operations to certain accounts."
    },
    "explanation": "An AWS Organizations SCP can centrally deny SageMaker endpoint management actions across accounts outside the OU, enforcing the requirement. IAM permission boundaries (B) must be set per role and cannot prevent actions outside the OU. SageMaker endpoints do not support resource-based policies for management operations (C). Service-linked roles cannot enforce cross-account restrictions (D)."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A fintech company needs to ingest 1 million financial transaction events per second into Amazon SageMaker Feature Store for real-time fraud detection. The solution must support low latency, horizontal scalability, and minimal data loss. Which ingestion architecture meets these requirements?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "Use Amazon Kinesis Data Firehose to deliver events to S3 and batch import into SageMaker Feature Store Offline Store.",
      "B": "Use Amazon Kinesis Data Streams with enhanced fan-out consumers and AWS Lambda functions invoking the PutRecord API to ingest into the SageMaker Feature Store Online Store.",
      "C": "Use Amazon MSK with brokers backed by EBS volumes and use a Kafka consumer to push records into the Feature Store offline bucket.",
      "D": "Use AWS IoT Core to route transaction events to SageMaker Feature Store via a custom AWS Lambda integration."
    },
    "explanation": "Real-time, low-latency ingest into the Feature Store Online Store requires Kinesis Data Streams (for horizontal scaling and enhanced fan-out) combined with Lambda invoking the PutRecord API. Firehose and batch imports do not meet real-time SLAs; MSK and IoT Core approaches add unnecessary complexity and latency."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An analytics team has 200 GiB of nested JSON log files in Amazon S3. They require interactive profiling and transformation in SageMaker Data Wrangler with minimal cost and latency. Which file format should they convert the data into before ingestion?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CSV files (flattening nested structures into columns).",
      "B": "Apache Parquet with nested column support and predicate pushdown.",
      "C": "Avro files with JSON serialization.",
      "D": "RecordIO binary format."
    },
    "explanation": "Parquet is a columnar storage format with efficient compression, nested column support, and predicate pushdown, which significantly reduces I/O and speeds interactive profiling in Data Wrangler. CSV lacks nested support, Avro is row-oriented, and RecordIO is optimized for deep learning, not interactive ETL."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An ML engineer must merge hourly transaction updates from Amazon RDS and real-time clickstream events from DynamoDB Streams into a single dataset for training in SageMaker Data Wrangler, while automatically handling schema evolution in both sources. Which ingestion solution should the engineer choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Schedule an AWS Glue ETL job to export RDS to S3 and join with Lambda-pushed DynamoDB data once per hour.",
      "B": "Use SageMaker Data Wrangler\u2019s built-in connectors for Amazon RDS and for DynamoDB streams to import and join data in a single flow.",
      "C": "Provision an EMR Spark cluster to read from both sources nightly and write merged Parquet to S3.",
      "D": "Use AWS DMS to replicate both RDS and DynamoDB to Redshift and join via Redshift Spectrum in Data Wrangler."
    },
    "explanation": "Data Wrangler connectors natively support incremental read and schema inference for RDS and DynamoDB streams, simplifying merges and automatically handling schema changes. Glue jobs or EMR require more operational overhead, and DMS replication to Redshift adds extra cost and latency."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A genomics research team requires sub-millisecond file I/O and high throughput for a 500 GiB NFS-based dataset during preprocessing before model training. Which AWS storage service should they provision to meet performance and POSIX compatibility requirements?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "Amazon EFS One Zone SSD backed by NFS.",
      "B": "Amazon FSx for Lustre file system with link to S3.",
      "C": "Amazon FSx for NetApp ONTAP with SSD volumes.",
      "D": "Amazon S3 bucket mounted via VPC endpoint."
    },
    "explanation": "FSx for Lustre delivers the lowest POSIX read-latencies (sub-millisecond) and highest throughput for large NFS-based datasets. EFS has higher latencies; FSx ONTAP adds data management features but lower throughput; S3 is object storage, not POSIX."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An e-commerce company experiences high startup latency for SageMaker training jobs reading hundreds of small CSV files (~5\u201310 MiB each) from S3. Without modifying the training code, which solution will most reduce startup time?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable Amazon S3 Transfer Acceleration on the bucket.",
      "B": "Use AWS Glue to consolidate small CSV files into larger Parquet files and update the S3 prefix.",
      "C": "Upgrade to a SageMaker instance with higher network throughput.",
      "D": "Change the training input mode to Pipe mode."
    },
    "explanation": "Consolidating small files into fewer, larger Parquet files minimizes S3 list and open calls, dramatically reducing startup latency without code changes. Transfer Acceleration and network upgrades do not address metadata overhead; Pipe mode still requires metadata enumeration."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A media streaming platform requires exactly-once processing of real-time events into its ML ingestion pipeline for feature computation. Which combination of services and configurations provides this guarantee?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Amazon Kinesis Data Firehose with a retry buffer.",
      "B": "Amazon Kinesis Data Streams with Enhanced Fan-Out (guarantees at-least-once).",
      "C": "Amazon MSK (Kafka) configured with idempotent producers and transactional writes.",
      "D": "DynamoDB Streams with AWS Lambda and conditional writes to downstream store."
    },
    "explanation": "Kafka with idempotent producers and transactional writes supports exactly-once semantics end-to-end. Kinesis Data Streams and Firehose only guarantee at-least-once; DynamoDB Streams plus Lambda may still result in duplicates without complex idempotency logic."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An ML pipeline ingests unvalidated CSV files into Amazon S3, but inconsistent schemas and missing columns cause downstream failures. Which AWS service can enforce schema validation during ingestion and alert on violations?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Glue Schema Registry with schema-validation-enabled data producers.",
      "B": "AWS Config rules for S3 object schemas.",
      "C": "Amazon Macie classification jobs.",
      "D": "AWS Lake Formation schema enforcement."
    },
    "explanation": "AWS Glue Schema Registry can define, register and enforce schemas at ingestion for streaming and batch, and can reject or alert on malformed records. Lake Formation handles data access control, not row-level schema validation."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A large oil company must migrate 100 TiB of POSIX file-system data to Amazon S3 within 48 hours, with incremental file changes tracked and minimal management overhead. Which service meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Snowball Edge device shipment.",
      "B": "AWS DataSync configured for incremental sync to S3.",
      "C": "Amazon S3 Transfer Acceleration over the internet.",
      "D": "AWS Storage Gateway file gateway."
    },
    "explanation": "AWS DataSync optimizes incremental transfer of POSIX files to S3 with built-in scheduling and encryption, suitable for large, continuously changing data. Snowball has shipment delays; Transfer Acceleration and Storage Gateway don\u2019t handle incremental POSIX sync as efficiently."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An ML engineer wants to profile, clean, and transform streaming IoT telemetry data directly in SageMaker Data Wrangler without intermediate storage. Which ingestion method should the engineer use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use the Amazon Kinesis Data Streams connector in Data Wrangler to ingest data in real time.",
      "B": "Upload data to S3 and use the S3 connector for batch profiling.",
      "C": "Route data through AWS IoT Analytics to an S3 channel and ingest.",
      "D": "Crawl raw data with AWS Glue and import the Glue table."
    },
    "explanation": "The Kinesis Data Streams connector in Data Wrangler enables real-time profiling and transformation of streaming data. Other options introduce batch windows or extra services, increasing latency and operational overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "When mounting an Amazon EFS file system to a SageMaker Studio notebook in a VPC, which configuration step ensures secure, low-latency access?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Mount the EFS file system over the public internet using TLS.",
      "B": "Create EFS mount targets in the same VPC subnets as the SageMaker notebooks and attach security groups allowing NFS traffic (TCP/2049).",
      "C": "Use AWS Direct Connect to mount the EFS file system from on-premises.",
      "D": "Peer the notebook VPC to another VPC containing EFS and mount through a transit gateway."
    },
    "explanation": "To achieve secure, low-latency NFS access from Studio in the same VPC, you must create EFS mount targets in the same subnets and open NFS ports in security groups. Public mounts or cross-VPC peering add latency and complexity."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A cross-account pipeline must ingest data from an S3 bucket in Account A into SageMaker Studio in Account B. To minimize privileges and management, which approach should be used?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a bucket ACL granting Account B the s3:GetObject permission.",
      "B": "Create an IAM role in Account B with a trust policy for Account A\u2019s principal, and update the bucket policy to allow s3:GetObject for that role.",
      "C": "Use an AWS Organizations SCP to allow cross-account access.",
      "D": "Set up VPC peering between the two accounts and access the bucket over the interface endpoint."
    },
    "explanation": "Creating a cross-account IAM role in Account B with trust from Account A and granting that role permission in the bucket policy provides least-privilege, auditable access. Bucket ACLs are legacy, and Organizations SCP/VPC peering do not directly enable S3 access control."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A data ingestion job reading from S3 into SageMaker notebooks is intermittently throttled with HTTP 503 SlowDown errors. Which approach will most effectively reduce these errors while maintaining high throughput?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Implement exponential backoff with jitter in the client\u2019s S3 request retry logic.",
      "B": "Enable S3 Transfer Acceleration to optimize network path.",
      "C": "Configure the bucket as requester-pays.",
      "D": "Increase the IAM request quota for the S3 service."
    },
    "explanation": "503 SlowDown errors indicate S3 throttling; the recommended mitigation is implementing exponential backoff with jitter to retry requests gracefully. Transfer Acceleration and requester-pays do not address throttling, and IAM quotas are unrelated."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An Amazon SageMaker training job mounts an Amazon FSx for Lustre file system backed by an 8 TiB S3 data repository. After heavy parallel reads, the engineer observes metadata lookup latencies >100 ms for POSIX operations. How can the engineer optimize metadata performance?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase the Lustre file system\u2019s throughput capacity to provision more metadata IOPS.",
      "B": "Switch the workload to Amazon EFS for metadata caching.",
      "C": "Directly mount the underlying S3 bucket instead of Lustre.",
      "D": "Implement a local cache on the training instance\u2019s EBS volume."
    },
    "explanation": "FSx for Lustre provides scalable metadata throughput proportionate to the configured throughput capacity. Increasing capacity raises available metadata IOPS and reduces latency. EFS and direct S3 mounts do not meet the sub-millisecond metadata requirements."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A data science team must join millions of small S3 objects with a 50 GiB Redshift table for feature engineering before training. The join must complete in the lowest end-to-end latency. Which ingestion and compute architecture should they choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run an AWS Glue ETL job to merge the data into a Parquet file and reload into SageMaker.",
      "B": "Use Redshift Spectrum to query the S3 objects in place and join directly with the local Redshift table.",
      "C": "Provision an EMR Spark cluster to read both sources and write joined output to S3.",
      "D": "Use Athena CTAS to precompute the join and then read from S3 in SageMaker."
    },
    "explanation": "Redshift Spectrum can directly query external S3 data and join with internal tables, minimizing data movement and achieving the lowest latency. Glue, EMR, and Athena introduce additional data shuffles or write phases, adding latency."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "When performing offline ingestion into SageMaker Feature Store, which file format and partitioning scheme will maximize batch import throughput and query efficiency?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A single unpartitioned CSV file containing all records.",
      "B": "Many small gzipped JSON files partitioned by feature group name.",
      "C": "Parquet files partitioned by ingestion date (yyyy/mm/dd) and hour.",
      "D": "Avro files with no partitions."
    },
    "explanation": "Partitioning Parquet files by date and hour enables parallel reads by the batch import process and efficient predicate pushdown, maximizing throughput and query efficiency. Single large CSVs or unpartitioned formats become bottlenecks; Avro lacks columnar benefits."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You have a numeric feature in your dataset that is strongly right-skewed. You plan to preprocess the data in SageMaker Data Wrangler and then train a linear regression model. Which sequence of transformations in Data Wrangler will best reduce skewness and satisfy the assumptions of linear regression?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Standardize the feature first, then apply a log transform",
      "B": "Apply a log transform first, then standardize the resulting values",
      "C": "Min\u2013max scale the feature first, then apply a log transform",
      "D": "One-hot encode the feature, then normalize the encoded columns"
    },
    "explanation": "A log transform first reduces skew, and subsequent standardization yields zero mean and unit variance, aligning with linear regression assumptions."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "Your streaming sensor pipeline ingests data via Kinesis Data Streams. You need to filter out invalid readings and impute missing values in real time with sub-second latency. Which solution provides the lowest operational overhead and meets performance requirements?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Kinesis Data Analytics SQL to filter and invoke Lambda for imputation",
      "B": "Use a Lambda function to filter and then call SageMaker Data Wrangler batch job for imputation",
      "C": "Use Kinesis Data Analytics for Apache Flink to filter and impute within the streaming application",
      "D": "Use AWS Glue streaming ETL job to filter and impute before writing to S3"
    },
    "explanation": "Kinesis Data Analytics for Apache Flink supports stateful, low-latency streaming transforms including filter and impute within the service, minimizing custom infrastructure."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A categorical feature has 5,000 unique values. You intend to use a tree-based algorithm and want to avoid excessive dimensionality. Which encoding technique should you apply using SageMaker Data Wrangler?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "One-hot encoding of all categories",
      "B": "Label encoding (assigning integer codes)",
      "C": "Frequency encoding (replace category with its occurrence frequency)",
      "D": "Hashing trick with a large hash space"
    },
    "explanation": "Frequency encoding reduces cardinality to a single numeric value per category, retaining information without high-dimensional expansion."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You need to remove duplicate records from two large S3 datasets in preparation for feature engineering. Which AWS Glue Spark ETL approach ensures schema enforcement and minimal data loss?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Glue DynamicFrame.drop_duplicates on the combined DynamicFrame",
      "B": "Convert to Spark DataFrame and call DataFrame.distinct()",
      "C": "Use a SageMaker Processing job with custom dedupe code",
      "D": "Use a Glue DataBrew recipe step to drop duplicates"
    },
    "explanation": "Glue DynamicFrame.drop_duplicates preserves schema and metadata, integrates with the Glue catalog, and scales automatically."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You need to detect and remove outliers beyond 3 standard deviations for a numeric column using AWS Glue DataBrew. Which recipe step accomplishes this with minimal effort?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Add a Filter rows step with condition > mean + 3*stddev",
      "B": "Use the built-in Remove outliers step and set the z-score threshold to 3",
      "C": "Add a Cluster rows step and drop the smallest cluster",
      "D": "Use Impute missing step to replace values beyond 3 standard deviations"
    },
    "explanation": "The Remove outliers step in DataBrew natively handles z-score thresholds, detecting and dropping extreme values automatically."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "Your team needs to implement a custom feature transformation in a visual pipeline that includes Python code (e.g., complex binning logic). Which SageMaker tool supports this requirement and collaborative workflows?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Glue DataBrew",
      "B": "SageMaker Data Wrangler",
      "C": "AWS Glue Python shell job",
      "D": "Spark on Amazon EMR notebook"
    },
    "explanation": "Data Wrangler provides a visual flow with built-in support for custom Python steps, versioning, and collaboration in Studio."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You need to compute a 7-day rolling mean on a time-series feature for millions of records. Which approach in an AWS managed service offers the highest throughput with minimal code?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run a SageMaker Processing job with PySpark window functions",
      "B": "Write a Glue Spark ETL job using Spark SQL window functions",
      "C": "Use SageMaker Data Wrangler\u2019s rolling statistics transform",
      "D": "Use Lambda functions with batched DynamoDB lookups"
    },
    "explanation": "Data Wrangler\u2019s built-in rolling statistic transform executes efficiently in a managed environment without custom code or cluster management."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You have a free-text column that needs tokenization into word tokens and conversion into integer IDs for embedding. Which Data Wrangler transformation sequence should you apply?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use the Tokenize step (word level) and then Encode step (label encoding)",
      "B": "Use TextVectorization step with TF-IDF output",
      "C": "One-hot encode the text column directly",
      "D": "Write a custom PySpark transform in SageMaker Processing"
    },
    "explanation": "The Tokenize step splits text into words, and Label Encoding maps each token to a unique integer ID, preparing for embeddings."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "Before training, you must enforce schema constraints (data types, value ranges) programmatically. Which AWS feature will you use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Glue Data Quality rules leveraging Deequ",
      "B": "SageMaker Data Wrangler profile job",
      "C": "DataBrew column profiling summary",
      "D": "Glue Data Catalog crawler validation"
    },
    "explanation": "Glue Data Quality uses Deequ to define and evaluate table constraints and thresholds in code, enforcing schema at scale."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You plan to store engineered features for real-time inference in SageMaker Feature Store. Which Data Wrangler export option supports batching data into a Feature Group?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Use a SageMaker Processing job to call PutRecord API",
      "B": "Use a Lambda function to batch and call PutRecordBatch",
      "C": "Export directly from Data Wrangler to SageMaker Feature Store",
      "D": "Upload CSV to S3 and configure offline store only"
    },
    "explanation": "Data Wrangler has a direct export feature to batch-write transformed data into a Feature Store Feature Group."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You need to reduce a numeric feature\u2019s cardinality to 100 bins based on equal-frequency intervals using a no-code solution. Which DataBrew step do you choose?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Group by step with custom quantile aggregation",
      "B": "Bin numeric values step with equal-frequency option",
      "C": "Cluster rows step with K-means clustering",
      "D": "Custom recipe with Python UDF"
    },
    "explanation": "The Bin numeric values step with equal-frequency creates quantile-based bins automatically, reducing cardinality."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "To impute missing age values by country group using DataBrew, which step sequence is correct?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use Partition by country, then Impute missing step with median strategy",
      "B": "Aggregate to compute medians, then perform a join back",
      "C": "Apply global median imputation for the age column",
      "D": "Export data and impute in SageMaker Processing"
    },
    "explanation": "Partitioning by country followed by median imputation uses group-wise statistics directly in DataBrew with minimal extra steps."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "Two 1 TB CSV files in S3 must be inner-joined on a composite key before feature engineering. You want a serverless, managed service that handles scaling and scripting. Which approach is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run an AWS Glue ETL job using DynamicFrame.join",
      "B": "Launch an EMR cluster with custom PySpark code",
      "C": "Use a SageMaker Processing PySpark job",
      "D": "Perform a CTAS join in Amazon Athena"
    },
    "explanation": "AWS Glue ETL with DynamicFrame.join provides serverless scaling, schema enforcement, and integration with the Glue Data Catalog."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You need to anonymize email addresses in real time within a Kinesis Data Stream, ensuring <200 ms processing per record. Which architecture meets this requirement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Glue streaming ETL job with built-in masking",
      "B": "Kinesis Data Analytics for Apache Flink with a UDF to hash emails",
      "C": "AWS Lambda subscribed to the stream invoking a hashing library",
      "D": "SageMaker Data Wrangler in streaming mode"
    },
    "explanation": "Kinesis Data Analytics for Flink executes in-stream UDFs with low, consistent latency, meeting sub-200 ms requirements without cold starts."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "Duplicate events may occur within a 2-minute window in your Kinesis stream. You need to drop duplicates in real time before storing to S3. Which solution is most appropriate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Kinesis Data Analytics for Flink, keyBy event ID and apply a 2-minute deduplication window",
      "B": "Configure an AWS Glue streaming job with dedupe enabled",
      "C": "Trigger a Lambda function per record and check DynamoDB for prior IDs",
      "D": "Run an EMR streaming job with custom dedupe code"
    },
    "explanation": "Kinesis Data Analytics for Flink supports stateful windowed deduplication on event keys with minimal infrastructure overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "An ML engineer needs to set up automated, scheduled data quality checks on an Amazon S3 dataset to validate completeness, uniqueness, and detect numeric outliers over time. Which solution meets these requirements with the LEAST operational overhead?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Use AWS Glue DataBrew to author a recipe with transformations and schedule it as a DataBrew job.",
      "B": "Use AWS Glue Data Quality to define data quality rules and configure a schedule to run them.",
      "C": "Use SageMaker Clarify DataQualityCheckConfig in a SageMaker Processing job triggered by EventBridge.",
      "D": "Use Amazon Athena SQL queries inside a Lambda function scheduled by EventBridge."
    },
    "explanation": "AWS Glue Data Quality is designed for automated, rule-based validation and scheduling of data quality checks with minimal operational overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A data scientist must detect pre-training label disparity between male and female groups in a binary classification dataset. Which SageMaker Clarify metric should be used?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "pre_training_bias:CI (class imbalance)",
      "B": "pre_training_bias:DPL (difference in proportions of labels)",
      "C": "post_training_bias:CI",
      "D": "post_training_bias:DPL"
    },
    "explanation": "Difference in proportions of labels (DPL) is the appropriate pre-training metric to measure label distribution disparity between protected groups."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A dataset for fraud detection has severe class imbalance (0.5% positives). The engineer wants synthetic minority oversampling before training. Which AWS service/plugin should be used to implement SMOTE?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Specify class_weight in the training script and rely on model loss adjustment.",
      "B": "Use AWS Glue DataBrew sampling transform to oversample the minority class.",
      "C": "Use a SageMaker Processing job that implements imbalanced-learn\u2019s SMOTE algorithm.",
      "D": "Use SageMaker Clarify to automatically generate synthetic samples."
    },
    "explanation": "A SageMaker Processing job allows you to run custom code (e.g., imbalanced-learn) to generate SMOTE synthetic samples before training."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A tabular dataset containing PII must be anonymized before model training. The engineer wants to mask names, emails, and SSNs in CSV files with minimal coding. Which solution should be implemented?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use AWS Glue DataBrew recipe steps with the built-in PII Mask transform on the relevant columns.",
      "B": "Write a Lambda function to scan each file in S3, mask PII, and write back to S3.",
      "C": "Use SageMaker Clarify\u2019s bias analysis and misapply it for PII detection.",
      "D": "Use AWS KMS custom encryption to encrypt only the PII columns at rest."
    },
    "explanation": "AWS Glue DataBrew provides built-in PII Mask transforms that can mask specified columns with minimal code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "An ML engineer uses an Amazon EFS file system to store training data. To meet compliance, data must be encrypted at rest and in transit between the SageMaker training instance and EFS. Which configuration meets these requirements?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable only SSE-KMS encryption on the EFS file system.",
      "B": "Configure the EFS mount with NFSv4 and no encryption.",
      "C": "Use Amazon FSx for Lustre with default settings.",
      "D": "Enable EFS encryption at rest (SSE-KMS) and mount using EFS mount options with encryption in transit (TLS)."
    },
    "explanation": "To meet both requirements, enable SSE-KMS on EFS and use the EFS mount option to enforce TLS encryption in transit."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A dataset contains multiple records per user_id. To prevent data leakage, how should an ML engineer split the dataset into train and test subsets?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Randomly split 80/20 at the record level.",
      "B": "Use stratified split to preserve label proportions.",
      "C": "Use a group-based split (e.g., scikit-learn\u2019s GroupShuffleSplit) with user_id as the group key.",
      "D": "Split based on time, taking the latest 20% of records as test."
    },
    "explanation": "A group-based split ensures that all records for a given user_id are either in train or test, preventing leakage."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A text dataset with PII must be anonymized before training. The engineer wants automatic detection of names and SSNs. Which AWS service should be used in a SageMaker Processing job?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Glue DataBrew PII Masking",
      "B": "Amazon Comprehend PII detection API",
      "C": "SageMaker Clarify bias detection",
      "D": "AWS Lake Formation data labeling"
    },
    "explanation": "Amazon Comprehend\u2019s PII detection API can be called from a Processing job to automatically identify and anonymize PII in text."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "To detect statistical parity in a numeric target variable across demographic groups before training, which SageMaker Clarify component should be used?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "DataQualityCheckConfig in SageMaker Clarify",
      "B": "PreTrainingBiasCheckConfig in a SageMaker Clarify Processing job",
      "C": "ModelBiasMonitor",
      "D": "BatchTransform with a custom script"
    },
    "explanation": "PreTrainingBiasCheckConfig in a Clarify Processing job enables computation of pre-training bias metrics on the dataset."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "Before ingesting S3 data into SageMaker, an engineer needs to automatically discover and classify PII fields at scale. Which AWS service should be used?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Config",
      "B": "Amazon Athena",
      "C": "Amazon Macie",
      "D": "AWS CloudTrail"
    },
    "explanation": "Amazon Macie automatically discovers and classifies PII in S3 objects at scale."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A SageMaker training job must encrypt all EBS volumes with a customer-managed KMS key. How should the engineer configure the training job?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Specify the KmsKeyId parameter in the CreateTrainingJob API.",
      "B": "Configure the InputDataConfig to include an EncryptionKeyId.",
      "C": "Enable SSE-KMS on the S3 bucket only.",
      "D": "Use a SageMaker Pipeline with a default KMS key."
    },
    "explanation": "The CreateTrainingJob API\u2019s KmsKeyId parameter applies the specified customer-managed KMS key to encrypt all training EBS volumes."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A company must enforce fine-grained access control on PII columns in its Data Catalog tables. Which AWS capability enables this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "S3 bucket policies on the underlying data files.",
      "B": "AWS Lake Formation LF-Tags attached to Data Catalog columns.",
      "C": "IAM identity-based policies only.",
      "D": "AWS KMS key policies on the Data Catalog."
    },
    "explanation": "Lake Formation LF-Tags on Data Catalog columns provide column-level access control for sensitive data."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "An engineer needs to ensure that a string column contains only unique values before training. Which AWS service and rule type should be used?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Glue DataBrew with a completeness rule.",
      "B": "AWS Glue Data Quality with a passing ratio rule.",
      "C": "SageMaker Clarify DataQualityCheckConfig uniqueness check.",
      "D": "AWS Glue Data Quality with a uniqueness rule."
    },
    "explanation": "AWS Glue Data Quality supports a rule to check column uniqueness to enforce that values are unique."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "To prevent order-based bias in large S3 datasets during training, how can the engineer shuffle data in a SageMaker training job?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use File input mode (default shuffling).",
      "B": "Use RecordIO input mode (shuffles automatically).",
      "C": "Use Pipe input mode with ShuffleConfig set to an appropriate buffer size.",
      "D": "Download and shuffle in the training script only."
    },
    "explanation": "Pipe input mode with ShuffleConfig allows SageMaker to shuffle streaming records before training to avoid order bias."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A dataset contains missing numeric values that must be imputed with the median before feature engineering. Which tool provides a built-in transform for this?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS Glue DataBrew recipe step \u201cFill missing values\u201d with median.",
      "B": "SageMaker Debugger preprocessing hook.",
      "C": "SageMaker Feature Store ingestion transform.",
      "D": "AWS Glue Data Quality imputation rule."
    },
    "explanation": "AWS Glue DataBrew includes a \u201cFill missing values\u201d transform that can impute missing entries with statistics like median."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "An engineer wants to use a consistent, repeatable set of cleaned and validated features for both offline training and real-time inference. Which AWS capability should be used?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon S3 with versioned CSV files.",
      "B": "Amazon DynamoDB table with precomputed features.",
      "C": "SageMaker Feature Store to store and retrieve features.",
      "D": "Amazon Redshift external table."
    },
    "explanation": "SageMaker Feature Store provides a centralized store for features that ensures consistency between offline training and online inference."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A credit risk team needs a highly interpretable, low-latency regression model for predicting applicant default probability using 50 numerical and categorical features. They require clear feature coefficients for regulatory reporting. Which SageMaker built-in algorithm should they choose?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "XGBoost built-in algorithm with SHAP explanations",
      "B": "Linear Learner built-in algorithm in regression mode",
      "C": "Factorization Machines built-in algorithm",
      "D": "DeepAR forecasting algorithm"
    },
    "explanation": "Linear Learner in regression mode provides direct feature weights for interpretability and low latency. XGBoost can require SHAP post hoc analysis, and other algorithms aren\u2019t designed for regression interpretability."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "An online advertising platform must predict click-through rates with hundreds of categorical features of high cardinality. The team wants to capture pairwise interactions between sparse features efficiently. Which built-in SageMaker algorithm is most appropriate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "XGBoost built-in algorithm",
      "B": "Linear Learner built-in algorithm",
      "C": "Factorization Machines built-in algorithm",
      "D": "K-Means built-in clustering algorithm"
    },
    "explanation": "Factorization Machines are designed to model interactions among high-cardinality categorical features more efficiently than tree-based or linear models."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A retail company needs to forecast hourly product demand for 10,000 SKUs with seasonal patterns and occasional promotions. They require a probabilistic forecast of future demand. Which built-in SageMaker algorithm should they select?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "DeepAR forecasting algorithm",
      "B": "XGBoost regression algorithm",
      "C": "Linear Learner regression algorithm",
      "D": "K-Nearest Neighbors built-in algorithm"
    },
    "explanation": "DeepAR provides probabilistic time series forecasts at scale and handles seasonality and promotions. Other algorithms do not directly support probabilistic forecasting."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "An operations team needs to detect anomalies in streaming server CPU metrics. They require a one-class unsupervised method that adapts to drift. Which SageMaker built-in algorithm meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Isolation Forest built-in algorithm",
      "B": "Random Cut Forest built-in algorithm",
      "C": "One-Class SVM via script mode",
      "D": "K-Means built-in clustering algorithm"
    },
    "explanation": "Random Cut Forest is an unsupervised, streaming-friendly anomaly detection algorithm that adapts to drift; it\u2019s a SageMaker built-in algorithm."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A global publisher must classify news articles in 10 languages with minimal training data per language. They need an AWS managed NLP service with built-in support. Which service should they choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Custom BERT model on SageMaker",
      "B": "Custom TF-based text classifier in script mode",
      "C": "Amazon Comprehend DetectDominantLanguage and Custom Classification APIs",
      "D": "Amazon Translate to English plus custom classifier"
    },
    "explanation": "Amazon Comprehend offers multi-language classification with minimal training data and managed infrastructure. Custom models require more data and maintenance."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A biotech startup wants to identify cell types in microscopy images with minimal ML expertise. They prefer a no-code AWS solution that adapts to new classes. Which service should they use?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Custom CNN built in TensorFlow on SageMaker script mode",
      "B": "Deploy a PyTorch model on SageMaker endpoint",
      "C": "Use SageMaker AutoML via built-in algorithms",
      "D": "SageMaker Canvas image classification with Amazon Rekognition Custom Labels integration"
    },
    "explanation": "SageMaker Canvas provides a no-code interface, and integration with Rekognition Custom Labels supports labeling and incremental class adaptation without deep ML expertise."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A call-center analytics team needs real-time transcription and sentiment analysis of customer calls. They require a managed service with low operations overhead. Which combination should they use?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Custom DeepSpeech model on SageMaker",
      "B": "Amazon Transcribe for transcription and Amazon Comprehend for sentiment",
      "C": "Deploy open-source Kaldi on SageMaker",
      "D": "Amazon Lex chatbot for transcription and sentiment"
    },
    "explanation": "Amazon Transcribe and Comprehend are managed services for ASR and sentiment analysis, respectively, minimizing operational overhead compared to custom models."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A financial modeling team needs to generate synthetic financial reports using a foundation model. They require a generative text service with minimal training. Which AWS service best fits?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Bedrock with a foundation LLM",
      "B": "Custom GPT-2 on SageMaker",
      "C": "Use TensorFlow Sequence-to-Sequence on SageMaker",
      "D": "Amazon Translate with custom glossary"
    },
    "explanation": "Amazon Bedrock provides foundation LLMs for text generation, reducing heavy training and tuning efforts versus custom Seq2Seq models."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "An edge computing use case requires running an image classifier on IoT cameras with limited compute. They have a trained ResNet model. Which SageMaker feature should they use to optimize the model?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker built-in Image Classification algorithm",
      "B": "SageMaker Script Mode with TensorFlow",
      "C": "SageMaker Neo compilation to target edge architecture",
      "D": "Containerize model for SageMaker endpoint"
    },
    "explanation": "SageMaker Neo compiles and optimizes trained models for edge devices, reducing latency and resource usage on IoT cameras."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A team needs to quickly prototype a multi-label text classification for internal documents with zero code. They want to experiment with pre-trained models and fine-tune them. Which SageMaker capability should they use?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "SageMaker Clarify for data bias analysis",
      "B": "SageMaker Model Monitor for drift alerts",
      "C": "Notebook instance with pre-built scripts",
      "D": "SageMaker JumpStart solution templates for multi-label text classification"
    },
    "explanation": "SageMaker JumpStart provides pre-trained model solutions and templates for multi-label classification that can be fine-tuned with minimal code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A startup has a small tabular dataset (5,000 rows) and needs a quick binary classifier with built-in regularization and automated hyperparameter tuning. Which SageMaker built-in algorithm and mode is most appropriate?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Linear Learner in binary classification mode",
      "B": "XGBoost regression mode",
      "C": "K-Means clustering mode",
      "D": "DeepAR forecasting mode"
    },
    "explanation": "Linear Learner supports binary classification with built-in regularization and integrates with SageMaker Automatic Model Tuning for small datasets efficiently."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A fraud detection team requires an algorithm that handles class imbalance and produces probabilistic scores for each transaction. They prefer a tree-based approach. Which built-in algorithm should they select?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Linear Learner with L1 regularization",
      "B": "XGBoost built-in algorithm",
      "C": "Random Cut Forest",
      "D": "K-Nearest Neighbors"
    },
    "explanation": "XGBoost produces probabilistic outputs, handles class imbalance via objective weighting, and is a high-performance tree-based algorithm."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A medical imaging project needs semantic segmentation on X-ray images. The team has no pre-built algorithm in SageMaker. They want minimal development overhead. Which approach should they take?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker built-in Object Detection algorithm",
      "B": "Use Rekognition Custom Labels",
      "C": "Use JumpStart semantic segmentation pre-trained model in script mode",
      "D": "Develop a U-Net from scratch in a notebook"
    },
    "explanation": "JumpStart offers pre-trained semantic segmentation models that can be fine-tuned in script mode, reducing overhead compared to building models from scratch."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A business needs to extract key phrases from customer reviews in Spanish. They want a managed solution that supports custom phrase detection. Which service should they choose?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Custom spaCy model on SageMaker",
      "B": "Custom PyTorch NLP in script mode",
      "C": "Amazon Translate to English plus Comprehend key phrases",
      "D": "Amazon Comprehend with custom entity recognizer for Spanish"
    },
    "explanation": "Amazon Comprehend supports custom entity detection in multiple languages including Spanish, avoiding translation pipelines or heavy custom models."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A team must choose between deploying a SageMaker built-in algorithm versus a pre-packaged deep learning framework in script mode. They prioritize faster training on CPU-only instances for a moderate-sized tabular dataset. Which should they choose?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "SageMaker built-in XGBoost algorithm",
      "B": "TensorFlow in script mode",
      "C": "PyTorch in script mode",
      "D": "Bring-Your-Own container with Scikit-learn"
    },
    "explanation": "XGBoost built-in algorithm is optimized for CPU training on tabular data and will train faster with minimal configuration compared to deep learning frameworks."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A data scientist has an existing SageMaker automatic model tuning (AMT) job that produced optimal hyperparameter values for a regression model. New data arrives and the scientist wants to reuse the previous tuning results as a starting point for a new hyperparameter tuning job to save compute time. Which warm start configuration type should the scientist choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a warm start tuning job with type IDENTICAL_DATA_AND_ALGORITHM.",
      "B": "Use a warm start tuning job with type TRANSFER_LEARNING.",
      "C": "Use a warm start tuning job with type CURRENT_BEST.",
      "D": "Use a regular hyperparameter tuning job without warm start."
    },
    "explanation": "TRANSFER_LEARNING reuses previous tuning job results to inform new search on similar data or algorithm. IDENTICAL_DATA_AND_ALGORITHM reruns the same search space exactly. CURRENT_BEST and non\u2013warm-start jobs don\u2019t utilize prior results to reduce compute."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An ML engineer wants to reduce wasted compute and stop unpromising training jobs early during an AMT hyperparameter tuning job. Which configuration change will achieve this with minimal code changes?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set EarlyStoppingType to AUTO in the hyperparameter tuning job configuration.",
      "B": "Set MaxRuntimePerTrainingJob to a lower value.",
      "C": "Add a CloudWatch alarm to terminate long-running training jobs.",
      "D": "Wrap each training job in a custom script that kills the process if validation loss stops improving."
    },
    "explanation": "Setting EarlyStoppingType='Auto' enables SageMaker to automatically terminate unpromising training jobs. Lowering MaxRuntime limits total time but doesn\u2019t target unpromising jobs. CloudWatch alarms and custom scripts add operational overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A team trains a large PyTorch model on 8 GPU instances, but training is slow and network communication is a bottleneck. Which SageMaker feature will provide an efficient out-of-the-box distributed training solution to accelerate this job?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure Horovod for distributed training.",
      "B": "Enable smdistributed.dataparallel in Script Mode.",
      "C": "Use the built-in PyTorch Multi-GPU Estimator.",
      "D": "Use a parameter server architecture implemented in the training script."
    },
    "explanation": "smdistributed DataParallel integrates with Script Mode and optimizes gradient synchronization. Horovod requires extra setup, custom parameter server needs code changes, and there is no separate built-in PyTorch multi-GPU Estimator."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "During training of a convolutional neural network, an ML engineer observes overfitting: training accuracy far exceeds validation accuracy. Which single change to the training job will increase generalization with the least complexity?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase the dropout rate from 0.25 to 0.75.",
      "B": "Reduce the initial learning rate by a factor of ten.",
      "C": "Add L2 weight-decay regularization to the optimizer.",
      "D": "Switch to early stopping and set patience to 3 epochs."
    },
    "explanation": "L2 weight decay penalizes large weights and reduces overfitting without requiring new callbacks. Excessive dropout may underfit; reducing learning rate doesn\u2019t directly address overfitting; early stopping adds extra tuning and monitoring."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An ML engineer has trained both a random forest and an XGBoost model for the same classification task and wants to combine them to improve accuracy. Which ensemble approach should the engineer implement to learn an optimal combination of predictions?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a meta-learner on the model outputs (stacking).",
      "B": "Average the predictions from both models (bagging).",
      "C": "Train the XGBoost model to correct errors of the random forest (boosting).",
      "D": "Select the model with the higher validation accuracy for each input dynamically (voting)."
    },
    "explanation": "Stacking uses a meta\u00ad-model trained on base-model outputs to learn optimal weights. Bagging pools multiple instances of one algorithm. Boosting uses sequential training, not two distinct models. Voting is a simple unweighted ensemble."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A financial services firm needs to track, promote, and audit multiple versions of their ML models in SageMaker. Which feature should they use to centrally manage model versions and stages (e.g., Approved, Pending)?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon S3 object versioning on the model artifacts bucket.",
      "B": "SageMaker Model Registry.",
      "C": "SageMaker Experiments.",
      "D": "AWS CodeCommit repository."
    },
    "explanation": "SageMaker Model Registry is designed to store, version, and annotate model artifacts and their approval lifecycle. S3 versioning tracks raw objects but lacks metadata. Experiments track experiments, not production stages. CodeCommit is source control."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "When using smdistributed.dataparallel for PyTorch training, the network bandwidth is still limiting training throughput. Which additional configuration can reduce communication overhead?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch to Horovod with NCCL backend.",
      "B": "Enable gradient compression (fp16) in smdistributed.dataparallel.",
      "C": "Use MPI and ring-allreduce instead of reduce-scatter.",
      "D": "Increase batch size to amortize communication."
    },
    "explanation": "smdistributed DataParallel supports fp16 gradient compression to reduce bandwidth. Horovod setup is heavier, using MPI doesn\u2019t enable compression, and larger batches may help but don\u2019t directly reduce communication volume."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An ML engineer is constrained by budget and can run at most 50 training jobs for hyperparameter tuning. Which search strategy in SageMaker AMT will likely find the best solution within the least number of jobs?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Grid search.",
      "B": "Random search.",
      "C": "Bayesian optimization.",
      "D": "Genetic algorithm search."
    },
    "explanation": "Bayesian optimization balances exploration and exploitation, converging faster. Grid search is exhaustive, random search is less efficient, and genetic algorithms aren\u2019t natively supported in SageMaker AMT."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A company must deploy a deep learning model to edge devices that have strict memory limits. Which approach using SageMaker will produce the smallest model artifact with minimal manual optimization?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Apply manual weight pruning in the training script.",
      "B": "Use SageMaker Neo to compile and quantize the model for the target device.",
      "C": "Enable mixed-precision training and save fp16 weights.",
      "D": "Use model distillation to train a smaller student network."
    },
    "explanation": "SageMaker Neo automates compilation and quantization for the target hardware, reducing model size. Manual pruning requires custom code; mixed precision lowers memory at runtime but not artifact size; distillation requires additional training."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An ML engineer wants to fine-tune a large pre-trained Hugging Face transformer using SageMaker. Which method requires the least boilerplate code?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Bring your own container with transformers installed.",
      "B": "Use the SageMaker Hugging Face estimator in Script Mode.",
      "C": "Translate the model code to MXNet and use the built-in MXNet Estimator.",
      "D": "Use Amazon SageMaker built-in text classification algorithm."
    },
    "explanation": "The SageMaker Hugging Face estimator simplifies fine-tuning with minimal code. BYOC adds container management; translating to MXNet is error-prone; no built-in algorithm for large transformer fine-tuning exists."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A data scientist configures an AMT tuning job but forgot to configure early stopping. Which change should they make to the tuning configuration to enable automated early stopping of poor performing jobs?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set EarlyStoppingType='Auto' and choose an appropriate WaitInterval.",
      "B": "Lower the MaxJobs parameter to force quicker completion.",
      "C": "Define a CloudWatch rule to terminate jobs with low metrics.",
      "D": "Wrap the training script in a debugger rule to kill unresponsive jobs."
    },
    "explanation": "EarlyStoppingType='Auto' with WaitInterval allows SageMaker to stop jobs whose objective does not improve. Lowering MaxJobs only limits total count. CloudWatch rules and debugger scripts are more operationally complex."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A neural network training job shows high training accuracy but validation accuracy plateaus early. The engineer wants an automated mechanism to detect this during training. Which SageMaker Debugger rule should they enable?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "VanishingGradientDetector.",
      "B": "LossNotDecreasing.",
      "C": "OverfitDetector.",
      "D": "WeightUpdateVerifier."
    },
    "explanation": "OverfitDetector monitors training vs. validation metrics to detect early signs of overfitting. VanishingGradientDetector and LossNotDecreasing focus on gradients and loss. WeightUpdateVerifier checks parameter updates."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A team wants to track parameters, metrics, and artifacts across multiple training jobs and compare runs in SageMaker. Which feature should they use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Model Monitor.",
      "B": "SageMaker Experiments.",
      "C": "Amazon CloudWatch metrics.",
      "D": "AWS X-Ray."
    },
    "explanation": "SageMaker Experiments is built to organize, track, and compare training runs. Model Monitor observes deployed models. CloudWatch and X-Ray are for logs and traces, not training experiment management."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A neural network trained on tabular data shows many small nonzero weights and generalizes poorly. Which regularization change will encourage sparsity and reduce overfitting?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase the L2 (weight\u2010decay) coefficient.",
      "B": "Increase dropout to 0.8.",
      "C": "Switch to L1 regularization on weights.",
      "D": "Use batch normalization after every layer."
    },
    "explanation": "L1 regularization (Lasso) encourages many weights to become exactly zero, producing sparse models. L2 shrinks but doesn\u2019t enforce sparsity. Dropout and batch normalization address generalization but not weight sparsity."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You have trained a binary classifier on a dataset where the positive class constitutes only 1% of the data. On a held-out test set, the model achieves an AUC-ROC of 0.90, but its precision at low recall is poor. Which evaluation metric should you use to better assess the model\u2019s ability to identify the minority positive class?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Accuracy",
      "B": "AUC-ROC",
      "C": "Area Under the Precision-Recall Curve (AUC-PR)",
      "D": "Log Loss"
    },
    "explanation": "With extreme class imbalance, the precision-recall curve (AUC-PR) focuses on performance for the positive class and is more informative than AUC-ROC or accuracy."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "A regression model predicts house prices. Stakeholders are more concerned about a few very large prediction errors than many small ones. Which metric should you minimize?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Mean Absolute Error (MAE)",
      "B": "Root Mean Square Error (RMSE)",
      "C": "R\u00b2 (Coefficient of Determination)",
      "D": "Mean Absolute Percentage Error (MAPE)"
    },
    "explanation": "RMSE penalizes large errors more heavily than MAE, making it appropriate when large deviations are particularly undesirable."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "An ML engineer wants to run reproducible training experiments that track hyperparameters, metrics, and artifacts across multiple training jobs in Amazon SageMaker. Which AWS service should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon CloudWatch Logs",
      "B": "AWS CodePipeline",
      "C": "Amazon SageMaker Experiments",
      "D": "AWS Config"
    },
    "explanation": "SageMaker Experiments provides managed tracking of experiments, trials, hyperparameters, metrics, and artifacts to ensure reproducibility."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "During training, you notice that your model\u2019s loss on the training set stops decreasing early and plateaus. Which built-in SageMaker Debugger rule should you enable to detect this convergence issue?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "OverfitDetector",
      "B": "LossNotDecreasing",
      "C": "LearningRateFinder",
      "D": "WeightNormMonitor"
    },
    "explanation": "The LossNotDecreasing rule monitors training loss and raises an alert if it does not decrease sufficiently, indicating convergence problems."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You want to generate local feature\u2010level explanations (SHAP values) for your production model using SageMaker Clarify. Which configuration should you use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "DataBiasConfig in ClarifyProcessingJob",
      "B": "ModelBiasConfig in ClarifyProcessingJob",
      "C": "SHAPConfig in ClarifyProcessingJob",
      "D": "DriftConfig in ClarifyProcessingJob"
    },
    "explanation": "SHAPConfig enables Clarify to compute SHAP feature attributions for local explainability of model predictions."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "To select an optimal classification threshold that maximizes F1 score, what procedure should you perform on your validation data?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Evaluate F1 at various probability thresholds and choose the threshold yielding the highest F1",
      "B": "Select the threshold where ROC curve slope equals 1",
      "C": "Use the threshold where precision equals recall",
      "D": "Choose the threshold that balances true positive and false positive rates"
    },
    "explanation": "Maximizing F1 requires evaluating F1 at multiple thresholds on validation data and selecting the threshold with the highest F1."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your deep neural network exhibits vanishing gradient issues. Which SageMaker Debugger configuration helps you inspect gradient distributions during training?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable the base and gradient_histogram collections",
      "B": "Use the TorchLrFinder rule",
      "C": "Use the OverfitDetector rule",
      "D": "Configure only the losses collection"
    },
    "explanation": "The gradient_histogram collection captures gradient distributions so you can detect vanishing or exploding gradients."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You need to perform A/B testing between two model variants in production with minimal overhead. Which SageMaker feature supports traffic splitting between variants?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Deploy two separate endpoints and use Route 53 weighted routing",
      "B": "Configure two production variants in a single endpoint and set variant weights",
      "C": "Use an AWS Lambda function to proxy and split traffic",
      "D": "Use AWS CodePipeline for traffic routing"
    },
    "explanation": "SageMaker endpoints support multiple production variants with configurable weights for built-in traffic splitting (A/B testing)."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "For a multi-class classification problem with imbalanced classes, which evaluation metric gives equal importance to each class regardless of its frequency?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Micro-averaged F1 score",
      "B": "Macro-averaged F1 score",
      "C": "Overall accuracy",
      "D": "Weighted precision"
    },
    "explanation": "Macro-averaged F1 computes the F1 score for each class and averages them equally, treating all classes with equal importance."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You compare two regression models: Model A (RMSE = 5.0), Model B (RMSE = 4.8). How can you determine whether the observed difference is statistically significant?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Compare the RMSE values directly",
      "B": "Compute bootstrapped confidence intervals for the RMSE difference",
      "C": "Use one-way ANOVA on prediction residuals",
      "D": "Compare R\u00b2 values"
    },
    "explanation": "Bootstrapping provides confidence intervals on the RMSE difference to determine if the improvement is statistically significant."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Which SageMaker Clarify metric measures the difference in the proportion of positive outcomes between two demographic groups?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Class imbalance (CI)",
      "B": "Difference in Proportions of Labels (DPL)",
      "C": "Kolmogorov\u2013Smirnov (KS) statistic",
      "D": "Confusion matrix parity"
    },
    "explanation": "DPL quantifies the difference in label rates (positive outcome proportions) between a sensitive group and the baseline group."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your model\u2019s training loss decreases steadily while validation loss increases after a point. Which SageMaker Debugger rule can detect this behavior automatically?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "OverfitDetector",
      "B": "LossNotDecreasing",
      "C": "LearningRateFinder",
      "D": "GradientHistogram"
    },
    "explanation": "OverfitDetector monitors divergence between training and validation losses to detect overfitting during training."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You want to identify a suitable initial learning rate for faster convergence. Which SageMaker Debugger rule should you run?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "LearningRateFinder",
      "B": "LossNotDecreasing",
      "C": "OverfitDetector",
      "D": "EarlyStoppingRule"
    },
    "explanation": "LearningRateFinder systematically varies the learning rate to help identify an optimal learning rate for training convergence."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You need to compute evaluation metrics at multiple classification thresholds using SageMaker Clarify. Which configuration option enables this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Specify multiple thresholds in the ClarifyProcessor\u2019s ModelPredictionsConfig",
      "B": "Use SageMaker Model Monitor\u2019s default job",
      "C": "Run a SageMaker batch transform and manually compute metrics",
      "D": "Use SageMaker Automatic Model Tuning"
    },
    "explanation": "You can configure multiple thresholds in ModelPredictionsConfig for Clarify processing jobs to compute metrics (precision, recall, F1) at those thresholds."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "During training you observe that both training and validation losses are high and nearly identical, and the model fails to improve. What issue does this indicate?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Overfitting",
      "B": "Underfitting",
      "C": "Data leakage",
      "D": "Gradient explosion"
    },
    "explanation": "High and similar training/validation losses indicate underfitting, meaning the model is too simplistic to capture the underlying data patterns."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A startup has developed an ML model for an internal analytics tool that is invoked fewer than 100 times per day. Each inference must return within 200 ms, and total monthly cost must be minimized. Which SageMaker deployment option best meets these requirements?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Provision a real-time SageMaker endpoint with minimal instance capacity and auto scaling.",
      "B": "Deploy the model as a SageMaker serverless endpoint.",
      "C": "Use a SageMaker asynchronous endpoint with low concurrency settings.",
      "D": "Schedule a SageMaker batch transform job once per day."
    },
    "explanation": "A serverless endpoint incurs no idle instance cost, supports <200 ms latency for light workloads, and is optimal for very low invocation volume."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A machine learning team must host 500 small models (<50 MB each) behind a single API. All models share identical inference logic but have different weights. Which deployment infrastructure should be used to minimize cost and management overhead?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A multi-model SageMaker endpoint.",
      "B": "500 separate real-time SageMaker endpoints.",
      "C": "A SageMaker batch transform job per model.",
      "D": "Amazon ECS on Fargate with mounted S3 weights."
    },
    "explanation": "Multi-model endpoints load models on demand into a shared container, reducing cost and simplifying management compared to separate endpoints."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An automotive OEM wants to run an object-detection model on in-vehicle devices with limited compute and no internet connectivity. The model must start in <50 ms after request. Which deployment approach meets these requirements?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Host the model on a SageMaker real-time endpoint and stream data from the vehicle.",
      "B": "Use a SageMaker asynchronous endpoint with cached results on the vehicle gateway.",
      "C": "Compile the model with SageMaker Neo and deploy it to the edge container on the device.",
      "D": "Package the model in a Lambda@Edge function and invoke it from the vehicle."
    },
    "explanation": "SageMaker Neo compiles and optimizes models for specific hardware and enables sub-50 ms local inference without connectivity."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A document-processing model accepts 8 MB JSON payloads and may run up to 10 minutes per request. Customers must receive a response per request. Which SageMaker endpoint type is the most cost-effective?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Asynchronous SageMaker endpoint.",
      "B": "Real-time SageMaker endpoint.",
      "C": "Serverless SageMaker endpoint.",
      "D": "SageMaker batch transform job."
    },
    "explanation": "Asynchronous endpoints support large payloads, long processing durations, and return one response per request, avoiding persistent instance costs of real-time endpoints."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An e-commerce site needs <100 ms inference latency for its image-classification model at peak traffic. Daily traffic patterns vary widely. Which compute environment should the ML engineer choose?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CPU-based serverless SageMaker endpoint.",
      "B": "GPU-based serverless SageMaker endpoint.",
      "C": "CPU-based real-time SageMaker endpoint with fixed instance count.",
      "D": "GPU-based real-time SageMaker endpoint with auto scaling."
    },
    "explanation": "GPU real-time endpoints deliver required low latency, and auto scaling adjusts capacity to traffic variation."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A custom TensorFlow model requires specific OS libraries and drivers that are not available in SageMaker built-in containers. What is the least operationally intensive way to deploy the model?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Translate the model code to one compatible with a built-in container.",
      "B": "Run a batch transform job instead of hosting a real-time endpoint.",
      "C": "Build a custom Docker container, push to ECR, and use it in a SageMaker endpoint.",
      "D": "Use a Lambda function with layers containing the required libraries."
    },
    "explanation": "Custom containers in SageMaker allow full control over dependencies and integrate seamlessly with endpoint deployment."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A regulated enterprise requires all inference traffic to traverse a secured, private network with no public internet access. Which configuration satisfies this requirement with minimal changes?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy a public SageMaker endpoint and block internet via security groups.",
      "B": "Deploy a SageMaker real-time endpoint in the customer VPC with private subnets and no NAT gateway.",
      "C": "Use a serverless SageMaker endpoint and disable outbound access.",
      "D": "Host the model in EKS and restrict Internet at cluster level."
    },
    "explanation": "Launching the endpoint in private subnets of the customer VPC ensures no public internet access without additional NAT configuration."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An ML engineer must deploy a new model version and gradually shift 20% of production traffic to it for canary testing, with automatic rollback on errors. Which SageMaker feature accomplishes this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy a new real-time endpoint and use a Route 53 weighted record.",
      "B": "Create a second endpoint and manually switch after validation.",
      "C": "Use a SageMaker batch transform job for the canary.",
      "D": "Use SageMaker endpoint\u2010update with traffic shifting via endpoint configurations."
    },
    "explanation": "SageMaker\u2019s endpoint configuration supports traffic weights for A/B and canary deployments and automatic rollback on alarms."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "Your team uses Amazon Managed Workflows for Apache Airflow (MWAA) but wants tighter integration with SageMaker model deployments and lineage tracking. Which orchestrator should you choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Continue with MWAA and use custom operators.",
      "B": "SageMaker Pipelines.",
      "C": "AWS Step Functions directly.",
      "D": "AWS CodePipeline."
    },
    "explanation": "SageMaker Pipelines provides built-in integration for training, model registry, and deployment with lineage tracking."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A model requires a preprocessing container and a separate inference container on each request. How should you deploy this in SageMaker?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Multi-container real-time endpoint with both containers in the same inference pipeline.",
      "B": "Two separate endpoints chained via Lambda.",
      "C": "Batch transform job with a custom script calling both containers.",
      "D": "ECS service with multiple containers per task."
    },
    "explanation": "SageMaker multi-container endpoints allow a preprocessor and model container to run sequentially in the same endpoint."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A lightweight NLP model (<10 MB) must be invoked by other AWS services via API and handle occasional spikes of up to 50 requests per second. You want to avoid managing servers. Which deployment target is best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Serverless SageMaker endpoint.",
      "B": "Real-time SageMaker endpoint with a single instance.",
      "C": "Amazon Lambda container image with the model packaged and hosted behind an API Gateway.",
      "D": "ECS Fargate service."
    },
    "explanation": "A Lambda container image offers serverless scaling to handle spikes and exposes a simple API gateway endpoint without EC2 management."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "Your team needs to optimize inference performance on ARM-based instances in AWS. The model currently runs on x86 CPU. Which deployment approach will yield the greatest performance gain?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch to a serverless endpoint on ARM.",
      "B": "Compile the model with SageMaker Neo for ARM and host on ARM EC2 instances.",
      "C": "Use a GPU-based real-time endpoint on x86.",
      "D": "Deploy in ECS with ARM container images."
    },
    "explanation": "Neo compiles and optimizes for ARM hardware, delivering significant inference speedups compared to generic CPU execution."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A hybrid environment uses on-premises Kubernetes and AWS. You need to deploy an ML model so that it's available to both environments with unified CI/CD. Which deployment infrastructure is most appropriate?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker serverless endpoint with VPC peering.",
      "B": "SageMaker real-time endpoint in a public subnet.",
      "C": "ECS with Fargate in a public VPC.",
      "D": "Amazon EKS with GPU node group and a shared Terraform pipeline."
    },
    "explanation": "EKS provides Kubernetes compatibility on-prem and in AWS and integrates with existing CI/CD pipelines."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A data-science team must process nightly batches of 10 million records through an ML model. Sub-second latency is not required, but overall runtime must complete within 2 hours. Which deployment strategy is optimal?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High-capacity real-time SageMaker endpoint with multiple GPU instances.",
      "B": "Serverless SageMaker endpoint with high concurrency.",
      "C": "SageMaker batch transform job with GPU instances.",
      "D": "SageMaker asynchronous endpoint with high timeouts."
    },
    "explanation": "Batch transform jobs are optimized for high-throughput offline inference and can leverage large GPU fleets to meet deadlines without request-based endpoint costs."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "Your organization uses Kubernetes microservices on Amazon EKS. You must deploy an ML model as a microservice in this architecture with GPU acceleration. Which approach do you choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a SageMaker real-time endpoint and call it from EKS pods.",
      "B": "Deploy the model in a container on EKS with a GPU instance profile.",
      "C": "Package the model in a Lambda function behind an Application Load Balancer.",
      "D": "Use SageMaker multi-model endpoint within the VPC and proxy through EKS."
    },
    "explanation": "Hosting directly on EKS with GPU nodes integrates with existing microservices, avoids network hops, and provides GPU acceleration under Kubernetes control."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "You have two CloudFormation stacks: one provisioning a VPC with subnets and security groups, and another provisioning a SageMaker real-time endpoint. The endpoint stack needs to reference the VPC subnets and security group ARNs created by the VPC stack. What is the most maintainable way to share these values?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "In the VPC stack, export the subnet IDs and security group ARNs using Outputs with Export names. In the endpoint stack, import them with Fn::ImportValue.",
      "B": "Store the subnet IDs and security group ARNs in an S3 object in the VPC stack and download them in CloudFormation custom resources in the endpoint stack.",
      "C": "Pass the subnet IDs and security group ARNs as parameters manually each time you deploy the endpoint stack.",
      "D": "Use a Lambda-backed custom resource in the endpoint stack to call DescribeStacks on the VPC stack and parse the JSON output."
    },
    "explanation": "Cross-stack references via Outputs and Fn::ImportValue is the recommended, maintainable approach for sharing values between CloudFormation stacks."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "You need to configure application autoscaling for a SageMaker real-time endpoint variant using CloudFormation. Which CloudFormation resource type must you declare to register the endpoint variant with AWS Application Auto Scaling?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS::SageMaker::Endpoint",
      "B": "AWS::ApplicationAutoScaling::ScalableTarget",
      "C": "AWS::SageMaker::EndpointConfiguration",
      "D": "AWS::ApplicationAutoScaling::ScheduledAction"
    },
    "explanation": "AWS::ApplicationAutoScaling::ScalableTarget is required to register the SageMaker endpoint variant with Application Auto Scaling before you attach scaling policies."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "Your SageMaker endpoint experiences unpredictable spikes in traffic throughout the day. You want to configure auto scaling to trigger when each instance receives more than 100 requests per minute. Which scaling metric should you choose in your scaling policy?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CPUUtilization",
      "B": "ModelLatency",
      "C": "Invocation4xxErrors",
      "D": "InvocationsPerInstance"
    },
    "explanation": "InvocationsPerInstance is the supported Application Auto Scaling metric for SageMaker endpoints to scale based on request volume per instance."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "You deploy a SageMaker endpoint inside a VPC without a NAT gateway to save cost. The endpoint fails to pull the container from ECR and the model artifacts from S3. Which minimum set of VPC endpoints must you script to restore functionality?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "An interface endpoint for com.amazonaws.<region>.s3 only",
      "B": "Interface endpoints for com.amazonaws.<region>.ecr.api and com.amazonaws.<region>.ecr.dkr only",
      "C": "Interface endpoints for com.amazonaws.<region>.ecr.api, com.amazonaws.<region>.ecr.dkr, and a gateway endpoint for com.amazonaws.<region>.s3",
      "D": "Interface endpoints for com.amazonaws.<region>.sagemaker.api and com.amazonaws.<region>.sagemaker.runtime"
    },
    "explanation": "SageMaker endpoints in a private VPC need ECR API and DKR interface endpoints plus the S3 gateway endpoint to pull container images and model artifacts."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "You want to deploy a multi-model endpoint using CloudFormation to host multiple model artifacts in a single container. Which property must you use when defining AWS::SageMaker::Model to support this use case?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "PrimaryContainer",
      "B": "Containers",
      "C": "InferenceExecutionConfig",
      "D": "ModelPackageName"
    },
    "explanation": "The Containers property (a list) is required to define a multi-model endpoint in CloudFormation, whereas PrimaryContainer supports only a single container."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "Your inference container is stored in Amazon ECR and your SageMaker model execution role must pull the image. When scripting the IAM role in CloudFormation, which managed policy should you attach to grant the minimum required permissions?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "AmazonSageMakerFullAccess",
      "B": "AmazonEC2ContainerServiceforEC2Role",
      "C": "AmazonEC2ContainerRegistryReadOnly",
      "D": "AmazonS3ReadOnlyAccess"
    },
    "explanation": "AmazonEC2ContainerRegistryReadOnly grants the least-privilege permissions necessary for SageMaker to pull container images from ECR."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "Your organization hit the export limit for CloudFormation cross-stack references. You still need to share subnet IDs and security group IDs across stacks. What alternative approach should you use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use nested stacks instead of separate stacks.",
      "B": "Store the values in SSM Parameter Store and reference them with dynamic references.",
      "C": "Batch all values into a single export by concatenating comma-delimited strings.",
      "D": "Use AWS Organizations to share parameters between accounts."
    },
    "explanation": "Storing shared configuration in SSM Parameter Store avoids export limits and allows multiple stacks to reference values via dynamic references."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "You maintain several CloudFormation templates that define similar network resources for different teams. You want to reuse and standardize these resource definitions. Which CloudFormation feature is best suited for this purpose?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Custom resources",
      "B": "StackSets",
      "C": "Macros",
      "D": "Nested stacks"
    },
    "explanation": "Nested stacks allow you to factor out common resource definitions into reusable templates and include them in multiple parent stacks."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "When writing AWS CDK code for your SageMaker endpoint, you need to import an existing VPC by its tags so your endpoint can deploy into it. Which CDK method is most appropriate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ec2.Vpc.fromVpcAttributes()",
      "B": "ec2.Vpc.import()",
      "C": "ec2.Vpc.fromLookup()",
      "D": "ec2.Vpc.fromEnv()"
    },
    "explanation": "ec2.Vpc.fromLookup() performs a context lookup by tags or name at synthesis time to import existing VPCs into CDK apps."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "You are using AWS CDK to build and push a custom Docker container for SageMaker inference. Which CDK construct should you use to define and publish the image asset?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ecr.Repository",
      "B": "DockerImageAsset",
      "C": "ContainerImage.fromAsset",
      "D": "EcrDockerImage"
    },
    "explanation": "DockerImageAsset (from aws-cdk-lib/aws-ecr-assets) builds a Docker image from a local directory and publishes it to ECR automatically."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "You want to reduce training costs by using managed Spot Instances for a SageMaker training job defined in CloudFormation. Which property must you add to the AWS::SageMaker::TrainingJob resource?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "EnableManagedSpotTraining: true",
      "B": "UseSpotInstances: true",
      "C": "RuntimeSpotMode: Managed",
      "D": "SpotConfiguration: ManagedSpot"
    },
    "explanation": "EnableManagedSpotTraining set to true enables SageMaker managed Spot training for cost savings."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "You need to deploy a Lambda function in the same VPC as your SageMaker endpoint so it can invoke the endpoint privately. Which CloudFormation property must you include in the AWS::Lambda::Function resource?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "NetworkConfiguration",
      "B": "SecurityGroupIds",
      "C": "SubnetIds",
      "D": "VpcConfig"
    },
    "explanation": "VpcConfig ({ SubnetIds, SecurityGroupIds }) on AWS::Lambda::Function places the Lambda inside the specified VPC."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "You configured Application Auto Scaling for your SageMaker endpoint to use CPUUtilization, but you observe that scaling never occurs. Which explanation is correct?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "You must enable detailed monitoring on the endpoint to expose CPU metrics.",
      "B": "SageMaker endpoints only support InvocationsPerInstance as a built-in scaling metric.",
      "C": "CPUUtilization is supported only for asynchronous endpoints.",
      "D": "You must configure a CloudWatch alarm for CPUUtilization even when using Application Auto Scaling."
    },
    "explanation": "Application Auto Scaling for SageMaker real-time endpoints supports only the InvocationsPerInstance metric; CPUUtilization is not supported directly."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "When you register your SageMaker endpoint variant with AWS Application Auto Scaling in CDK, what is the correct format of the resourceId property?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "endpoint/YourEndpointName/variant/AllTraffic",
      "B": "endpoint/YourEndpointName/variantName/AllTraffic",
      "C": "sagemaker:endpoint:YourEndpointName:variant:AllTraffic",
      "D": "YourEndpointName/AllTraffic"
    },
    "explanation": "The resourceId for a real-time SageMaker endpoint variant must be specified as \"endpoint/<endpointName>/variant/<variantName>\"."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "You are deploying a new version of your VpcConfig for a SageMaker endpoint in CloudFormation. You modify the Subnets list in the template and redeploy, but the change is not applied. Why?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CloudFormation cannot update VpcConfig on an existing EndpointConfiguration; you must modify another property.",
      "B": "You must delete the EndpointConfiguration resource manually before CloudFormation can apply changes.",
      "C": "CloudFormation only creates a new AWS::SageMaker::EndpointConfiguration when the Endpoint resource\u2019s EndpointConfigName property is updated; you haven\u2019t changed it.",
      "D": "VpcConfig changes require an update to AWS::SageMaker::Model, not EndpointConfiguration."
    },
    "explanation": "Modifying VpcConfig in the EndpointConfiguration has no effect until you update the Endpoint resource\u2019s EndpointConfigName to reference the new configuration."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "You have an existing CloudFormation template defining your SageMaker model and endpoint. You decide to adopt AWS CDK but want to reuse the existing template without rewriting it. Which CDK construct allows you to embed and extend the existing template?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CfnModel",
      "B": "TemplatePart",
      "C": "IncludeTemplate",
      "D": "CfnInclude"
    },
    "explanation": "CfnInclude (from aws-cdk-lib/cloudformation-include) lets you import an existing CloudFormation template into a CDK app for extension."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A data science team needs to implement a CI/CD pipeline that automates model training, testing, and deployment for a SageMaker ML model. The pipeline should enforce a manual approval before deploying to production and use AWS CodePipeline with minimal custom code. Which pipeline configuration meets these requirements?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a manual approval action between the CodeBuild action that trains the model and the CloudFormation Deploy action that updates the production SageMaker endpoint.",
      "B": "Use a Lambda function as a CodePipeline action to pause for approval between the training and deployment stages.",
      "C": "Configure the SageMaker Model Training action to require manual confirmation before executing the SageMaker Model Deploy action.",
      "D": "Use a CloudWatch Events rule to trigger a manual deployment after the training stage finishes."
    },
    "explanation": "CodePipeline supports manual approval actions natively; inserting an Approval action between the training (CodeBuild) stage and the deployment (CloudFormation Deploy endpoint) stage provides a built-in manual gate with minimal custom code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "An ML engineer must trigger a CI/CD pipeline whenever new training data arrives in an S3 bucket. The pipeline uses CodePipeline and CodeBuild to preprocess data and train a SageMaker model. Which configuration best accomplishes this requirement with event-driven automation?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure the S3 bucket as the Source stage in CodePipeline and enable change detection polling.",
      "B": "Create a CloudWatch Events rule for s3:ObjectCreated:* that targets the CodePipeline API to start a pipeline execution.",
      "C": "Use an S3 Event Notification to invoke a Lambda function that calls StartPipelineExecution for the CodePipeline.",
      "D": "Configure EventBridge to listen for S3 notifications and invoke the CodeBuild project directly."
    },
    "explanation": "Using an S3 event notification to invoke a Lambda function that calls StartPipelineExecution provides event-driven triggering with fine-grained control and low latency."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A team wants to deploy updated SageMaker endpoints in a blue/green deployment fashion as part of their CI/CD pipeline. They need to shift traffic gradually to the new model and enable easy rollback if issues occur. Which approach using AWS native services meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a SageMaker CreateModel action in CodePipeline with the 'Blue/Green' deployment type and specify the traffic shifting percentage.",
      "B": "Use CloudFormation Deploy action in CodePipeline configured with CodeDeploy SafeMode for CloudFormation change sets to perform traffic shifting between endpoints.",
      "C": "Invoke the SageMakerUpdateEndpoint API in CodeBuild and script traffic weights in the buildspec.",
      "D": "Configure a Lambda function in a pipeline action that calls UpdateEndpointWeightsAndCapacities for traffic shifting."
    },
    "explanation": "Using a CloudFormation Deploy action with CodeDeploy-managed change sets enables native blue/green deployments and traffic shifting with rollback capabilities without custom scripts."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A CodeBuild project in a CI/CD pipeline needs to access resources inside a VPC (private RDS and SageMaker endpoint). Builds are failing because the project cannot reach VPC-only endpoints. How should you modify the CodeBuild configuration to allow access while maintaining least privilege?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add the CodeBuild project to the same security group as the RDS instance and SageMaker endpoint.",
      "B": "Configure the CodeBuild project with VPC configuration specifying the subnets and security groups that allow access to the private endpoints.",
      "C": "Create a NAT gateway in the VPC and enable public access for the CodeBuild project.",
      "D": "Enable CodeBuild network isolation and whitelist the VPC CIDR block."
    },
    "explanation": "Configuring the CodeBuild project with the correct VPC, subnets, and security groups allows it to access private resources securely and maintain least privilege."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "You need to add automated testing stages to your ML CI/CD pipeline: unit tests for preprocessing code, integration tests against a development SageMaker endpoint, and performance validation of the new model. Which pipeline actions should you include to satisfy these requirements?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Three CodeBuild actions: one running pytest for code, one invoking the dev endpoint via AWS CLI, and one running a custom performance test script.",
      "B": "One CodeBuild action with sequential buildspec phases for unit, integration, and performance tests.",
      "C": "Unit tests as a CodeBuild action, integration tests as a Lambda invoke action, and performance tests as a SageMaker Batch Transform action.",
      "D": "Integration tests first, then unit tests, then performance validation, all in a single CodeBuild action."
    },
    "explanation": "Separating testing into distinct CodeBuild actions for unit, integration, and performance ensures isolation and clear visibility of failures, and uses the native build environment to invoke tests."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "An ML engineer has a SageMaker Pipeline defined for data preprocessing, model training, and evaluation. To include this SageMaker Pipeline in a larger CodePipeline CI/CD workflow, which native CodePipeline action type should they use to start and monitor the SageMaker Pipeline execution?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "AWS CloudFormation action with a custom resource to invoke the SageMaker Pipeline.",
      "B": "AWS Lambda invoke action that calls StartPipelineExecution.",
      "C": "SageMakerPipeline action type provided by AWS CodePipeline.",
      "D": "CodeBuild action using AWS CLI to start the SageMaker Pipeline."
    },
    "explanation": "CodePipeline provides the SageMakerPipeline action to natively integrate SageMaker Pipelines as a stage, handling execution and status monitoring without custom scripts."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your ML CI/CD pipeline uses CodeBuild to package and push Docker images to a private ECR repository encrypted with a customer-managed KMS key. Builds are failing when CodeBuild attempts to push the image. What minimum IAM policy change should you apply to the CodeBuild service role to resolve this issue?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Add kms:Decrypt permission on the KMS key used to encrypt the ECR repository.",
      "B": "Add kms:GenerateDataKey permission on the KMS key.",
      "C": "Add kms:Encrypt and kms:Decrypt permissions on the KMS key.",
      "D": "Add kms:DescribeKey permission on the KMS key."
    },
    "explanation": "CodeBuild needs kms:GenerateDataKey to encrypt the image layers before pushing to ECR. Decrypt is not required for push operations."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A team adopts GitFlow branching for ML code and infrastructure definitions. How should branches map to CodePipeline stages to implement dev, test, and prod environments with automated promotion?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Map the 'develop' branch as the Source for the dev pipeline, 'release' for test, and 'master' for prod, each with its own CodePipeline using source triggers.",
      "B": "Use 'feature' branches for dev, 'develop' for test, and 'release' for prod, all in a single pipeline with multiple source actions.",
      "C": "Use 'master' branch for all environments and control deployments with manual approval actions.",
      "D": "Use 'hotfix' branches to promote code directly to production pipeline."
    },
    "explanation": "GitFlow maps develop \u2192 dev pipeline, release \u2192 test pipeline, and master \u2192 production pipeline, enabling automated promotions based on branch."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "You configured SageMaker Model Monitor to detect data drift for a production endpoint. You want your CI/CD pipeline to automatically retrain the model when drift is detected. Which event pattern and target configuration should you use in EventBridge to integrate drift detection with your pipeline?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Event pattern: SageMaker Model Monitor DataQualityCheckNotification; Target: CodePipeline StartPipelineExecution action.",
      "B": "Event pattern: CloudWatch Alarm for drift metric; Target: Lambda that updates the SageMaker endpoint.",
      "C": "Event pattern: SageMaker TrainingJobStateChange; Target: EventBridge rule that triggers retraining.",
      "D": "Event pattern: S3:ObjectCreated for captured data; Target: CodeBuild to start training."
    },
    "explanation": "Model Monitor emits DataQualityCheckNotification events; capturing these in an EventBridge rule targeting StartPipelineExecution automates retraining when drift is detected."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "You want to define your entire CI/CD pipeline in AWS CDK, including stages for building, testing, and deploying a SageMaker model. Which CDK construct and patterns should you use to best represent stages and actions?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use pipelines.CodePipeline construct with pipelines.CodeBuildStep and pipelines.ShellStep.",
      "B": "Use aws_codepipeline.Pipeline with aws_codepipeline_actions.CodeBuildAction and aws_codepipeline_actions.CloudFormationCreateUpdateStackAction.",
      "C": "Use aws_sagemaker.CfnPipeline and embed CodePipeline definitions as metadata.",
      "D": "Use a single CodeBuild project in CDK and run all steps in buildspec."
    },
    "explanation": "Using aws_codepipeline.Pipeline with native CodeBuild and CloudFormation actions allows explicit definition of CI/CD stages and is the recommended pattern in CDK for pipelines."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "During a CodePipeline execution, the CodeBuild step that builds the Docker container for a custom SageMaker algorithm fails due to insufficient privileges for Docker. Which setting in the CodeBuild project should you enable to allow Docker builds?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set privileged mode to true in the CodeBuild project settings.",
      "B": "Assign the CodeBuild role to the DockerUsers group in IAM.",
      "C": "Enable inbound and outbound network access in CodeBuild.",
      "D": "Grant CodeBuild service role permissions for sagemaker:CreateAlgorithm."
    },
    "explanation": "Privileged mode allows CodeBuild to run Docker commands needed for building and pushing container images."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "You need to validate model performance metrics generated during training before deploying the model in the CI/CD pipeline. Which CodePipeline action can you use to automatically compare the new metrics against a baseline and fail the pipeline if the model does not meet thresholds?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a Lambda invoke action that reads metrics from S3 and throws an error if thresholds are not met.",
      "B": "Use a CloudWatch Alarm action in CodePipeline to evaluate metrics.",
      "C": "Use the built-in SageMaker Model Quality check action in CodePipeline.",
      "D": "Use a CodeBuild action with a buildspec that runs a custom validation script."
    },
    "explanation": "CodePipeline does not have a native metric gating action; invoking a Lambda to assert metric thresholds provides a serverless, automated gate."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "In your pipeline, the output of the model training stage is a model artifact location in S3. You need to pass this dynamic S3 path to the SageMaker CreateModel deployment stage in CodePipeline. How should you configure the deployment action to consume this artifact location?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use CodePipeline artifact variables and parameter overrides in the SageMaker CreateModel action configuration.",
      "B": "Hardcode the S3 path in the CreateModel action ARN.",
      "C": "Write the path to Systems Manager Parameter Store and reference it in the deployment stage.",
      "D": "Use a Lambda function to retrieve the location and call CreateModel."
    },
    "explanation": "Artifact variables allow CodePipeline stages to access outputs from previous stages and dynamically pass them to action parameters without custom code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your team is evaluating whether to use SageMaker Pipelines or AWS CodePipeline for end-to-end ML workflows. They need automated data preprocessing, hyperparameter tuning, model evaluation, and deployment gates. Which is the most appropriate orchestration tool?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Pipelines for ML steps and embed it as a stage in AWS CodePipeline for approvals and multi-account deployments.",
      "B": "Use AWS CodePipeline exclusively for all ML and deployment tasks.",
      "C": "Use AWS Step Functions to orchestrate both ML and deployment steps.",
      "D": "Use Amazon Managed Workflows for Apache Airflow (MWAA) to orchestrate SageMaker training and deploy."
    },
    "explanation": "SageMaker Pipelines provides first-class support for ML workflows; embedding it in CodePipeline adds enterprise-grade CI/CD features like approvals and cross-account deployments."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "To implement automated rollback on a SageMaker endpoint if post-deployment smoke tests fail, which deployment pattern should you configure in CodePipeline and CodeDeploy?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "All-at-once deployment with a Lambda test hook in CodeDeploy that triggers rollback on failure.",
      "B": "Canary deployment type in CloudFormation Deploy action with pre-traffic and post-traffic validation Lambda hooks.",
      "C": "Linear deployment with time-based rollout and manual approval for rollback.",
      "D": "In-place deployment with CodeBuild test stage preceding deployment."
    },
    "explanation": "Using a canary deployment with pre- and post-traffic validation hooks in CodeDeploy allows automated testing and rollback if smoke tests fail."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An ML engineer has deployed a real-time SageMaker inference endpoint and needs to detect both input feature distribution drift and prediction quality degradation in production with minimal custom code. Which combination of SageMaker services and configurations should the engineer use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure DataCaptureConfig on the endpoint, deploy a DefaultModelMonitor job to detect data drift, and schedule a ModelQualityMonitor job with ground truth to detect prediction drift.",
      "B": "Configure DataCaptureConfig on the endpoint and schedule a Clarify DataBiasMonitor job to detect both data and prediction drift.",
      "C": "Stream inference events to Kinesis Data Firehose and use AWS Glue to analyze drift for both features and predictions.",
      "D": "Use SageMaker Clarify ModelExplainabilityMonitor to detect distribution shifts and model accuracy degradation."
    },
    "explanation": "Use DataCaptureConfig + DefaultModelMonitor for input drift and ModelQualityMonitor with ground truth for prediction/concept drift; other options do not cover both aspects or require more custom work."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "Before enabling continuous data drift detection on a production SageMaker endpoint, an ML engineer must establish baseline statistics and constraints. Which approach should the engineer use to generate these baselines automatically?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run a SageMaker Processing job using the DefaultModelMonitor container on a representative historical dataset in S3.",
      "B": "Use SageMaker Clarify DataBiasMonitor to create baseline drift constraints on the training data.",
      "C": "Write custom code to compute statistics and manually author the constraint JSON file.",
      "D": "Use AWS Glue DataBrew to profile the data and export a constraint file to S3."
    },
    "explanation": "The DefaultModelMonitor Processing container provides built-in functionality to compute baseline statistics and constraints; other options either don\u2019t generate constraints in the required format or don\u2019t support SageMaker\u2019s drift detectors."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A financial services company must monitor inference data while excluding PII fields from retention. Which configuration in SageMaker Model Monitor allows the engineer to capture inference payloads but filter out PII attributes?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set DataCaptureConfig sample_percentage to 0 and use encryption to prevent PII capture.",
      "B": "Use Clarify ModelBiasMonitor with an exclude_columns parameter for PII fields.",
      "C": "Configure DataCaptureConfig with a CaptureFilter to exclude PII JSON paths before writing to S3.",
      "D": "Enable a processing container script to mask PII after the monitor job runs."
    },
    "explanation": "DataCaptureConfig\u2019s CaptureFilter lets you specify JSONPath or CSV column filters so only non-PII fields are captured; other methods either capture PII or require post-processing."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A Model Monitor violation report shows a numeric feature\u2019s distribution exceeding baseline constraints for variance only. What is the most appropriate next step?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Adjust the variance threshold in the baseline constraint to suppress false positives.",
      "B": "Investigate the production data distribution change, update training data or retrain the model if the shift reflects new valid patterns.",
      "C": "Disable monitoring for that feature to reduce alert noise.",
      "D": "Regenerate the baseline constraints on the current production data without retraining."
    },
    "explanation": "A constraint violation indicates real drift; you should investigate and retrain or augment training data if needed. Changing thresholds or disabling the monitor risks missing true drift."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An operations team requires near real-time alerts whenever model input data drift is detected. Which integration provides automated notifications upon Model Monitor constraint violations?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Have a Lambda function poll the S3 violation reports folder every minute.",
      "B": "Create an Amazon EventBridge rule for SageMaker MonitoringExecutionStatus change events and target an SNS topic.",
      "C": "Subscribe an SNS notification directly to the Model Monitor S3 bucket.",
      "D": "Use CloudWatch Logs Insights to search for violations and trigger alarms."
    },
    "explanation": "EventBridge can capture SageMaker monitoring execution status change events and forward them to SNS; polling or log-based solutions introduce delay or complexity."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A team wants to detect anomalies in inference latency, error rates, and invoke rates on a SageMaker real-time endpoint. Which approach will meet these requirements with the least operational overhead?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use Amazon CloudWatch built-in metrics (Latency, Invocations, 4xx/5xx error counts) and configure CloudWatch Alarms.",
      "B": "Use SageMaker Model Monitor to detect infrastructure anomalies.",
      "C": "Use SageMaker Clarify to monitor model performance metrics.",
      "D": "Enable AWS CloudTrail on the endpoint and analyze logs in S3."
    },
    "explanation": "CloudWatch automatically captures endpoint performance and error metrics; Model Monitor focuses on data quality, and Clarify focuses on bias/explainability."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "Which statement best describes concept drift in a deployed ML model?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A change in the input feature distribution compared to baseline data.",
      "B": "A change in the statistical relationship between input features and target labels over time.",
      "C": "An imbalance in class label frequencies in production data.",
      "D": "A bias introduced during model training that only affects edge cases."
    },
    "explanation": "Concept drift refers to changes in P(Y|X), i.e., the relationship between features and labels; data drift is about P(X) changes."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An ML engineer needs to gradually shift traffic to a canary variant of a SageMaker endpoint and automatically roll back if error rates exceed a threshold. Which deployment mechanism should they use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker multi\u2010variant endpoints with AWS CodeDeploy canary traffic\u2010shifting and configure a CloudWatch Alarm on 5xx error rate.",
      "B": "Deploy two separate endpoints and manually switch DNS when errors are low.",
      "C": "Use SageMaker asynchronous endpoints with weighted routing.",
      "D": "Use SageMaker batch transform jobs scheduled hourly and compare error rates."
    },
    "explanation": "SageMaker multi\u2010variant endpoints integrated with CodeDeploy support canary traffic shifts and automatic rollback via CloudWatch Alarms."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "To detect shifts in feature attributions of a production model over time, which SageMaker monitoring job should an ML engineer schedule?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A Clarify ModelExplainabilityMonitor job with SHAP baseline and monitoring configuration.",
      "B": "A Clarify DataBiasMonitor job on the inference data.",
      "C": "A DefaultModelMonitor job focusing on data quality constraints.",
      "D": "A ModelQualityMonitor job comparing accuracy against ground truth."
    },
    "explanation": "ModelExplainabilityMonitor (SHAP) jobs track how feature attributions change; data bias or quality monitors don\u2019t measure attribution shifts."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "What is the minimum supported frequency for scheduling a SageMaker MonitoringSchedule to detect production data drift?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Every 5 minutes",
      "B": "Every 1 minute",
      "C": "Every hour",
      "D": "Every 24 hours"
    },
    "explanation": "SageMaker MonitoringSchedule supports a minimum interval of 5 minutes; shorter intervals are not allowed."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An engineering team uses batch transform jobs for asynchronous inference and needs to monitor prediction quality drift against offline ground truth. Which solution requires the least operational overhead?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable DataCaptureConfig on the batch transform endpoint and run DefaultModelMonitor.",
      "B": "Implement a custom Lambda to compare S3 outputs to ground truth and publish metrics.",
      "C": "Schedule a ModelQualityMonitor ProcessingJob using the batch transform output folder and S3 ground truth labels.",
      "D": "Use Clarify DataBiasMonitor on the batch transform results."
    },
    "explanation": "Scheduling a ModelQualityMonitor ProcessingJob directly on transform outputs and labels uses built-in monitoring; other options require custom polling or inappropriate monitors."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A production endpoint\u2019s data quality monitor runs but model accuracy has degraded without any data drift reported. What additional monitoring configuration is required?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a Clarify DataBiasMonitor to detect label shifts.",
      "B": "Increase the sampling percentage in DataCaptureConfig.",
      "C": "Enable multivariant monitoring on the same monitor.",
      "D": "Configure and schedule a ModelQualityMonitor job with ground truth labels to detect accuracy degradation."
    },
    "explanation": "DataQuality monitors P(X) shifts, not P(Y|X); ModelQualityMonitor with ground truth is required to detect concept or accuracy drift."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A company must monitor production inference outputs for fairness drift (e.g., change in demographic parity) over time. Which SageMaker Clarify monitor should they schedule?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Clarify DataBiasMonitor",
      "B": "Clarify ModelBiasMonitor",
      "C": "ModelExplainabilityMonitor",
      "D": "DefaultModelMonitor"
    },
    "explanation": "ModelBiasMonitor detects fairness metrics (demographic parity, equalized odds) on inference data; DataBiasMonitor analyzes training set biases."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An ML engineer must filter out sensitive fields from inference data before it reaches the Model Monitor processing container. Which component should they customize?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provide a custom preprocessing script in the MonitoringSchedule\u2019s ProcessingJobConfig.",
      "B": "Write a post-monitoring ETL job to remove sensitive columns from violation reports.",
      "C": "Modify the DefaultModelMonitor container image to drop PII.",
      "D": "Configure DataCaptureConfig to drop features via sample_percentage."
    },
    "explanation": "A custom preprocessing script in MonitoringSchedule\u2019s ProcessingJobConfig lets you transform or filter data before constraint evaluation; other methods occur after capture or require image modification."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An ML engineer wants to integrate drift detection into a SageMaker Pipelines workflow and automatically trigger retraining when drift is detected. Which pipeline step should they include to evaluate drift violations before invoking the retraining branch?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a MonitoringStep with fail_on_violation=True to stop the pipeline on any drift.",
      "B": "Use a ConditionStep to check if the MonitoringStep status equals 'Failed'.",
      "C": "Use a RegisterModel step to register the model only if no drift is detected.",
      "D": "Use a ConditionStep to inspect the MonitoringStep output property 'BaselineViolations' > 0 and branch accordingly."
    },
    "explanation": "A ConditionStep can examine the MonitoringStep output (e.g., violated constraint count) and route to retraining; this avoids hard failures and enables branching logic."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "An ML team needs to right-size SageMaker inference endpoints to minimize cost while maintaining performance. They want to benchmark actual model inference performance (latency and under various loads) to choose optimal instance types. Which AWS service should they use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Compute Optimizer",
      "B": "AWS SageMaker Inference Recommender",
      "C": "AWS Cost Explorer",
      "D": "AWS Trusted Advisor"
    },
    "explanation": "SageMaker Inference Recommender runs performance benchmarks on your model to recommend optimal instance families and sizes. Compute Optimizer provides general EC2/EBS recommendations, not ML-specific inference benchmarks."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A financial organization observes unexpected monthly spikes in SageMaker inference costs due to seasonal usage. They need to receive near real-time alerts when cost anomalies occur in their AWS account. Which solution meets this requirement with minimal operational overhead?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create a CloudWatch alarm on the AWS/Billing namespace EstimatedCharges metric",
      "B": "Configure an AWS Budget with email notifications when forecasted spend exceeds a threshold",
      "C": "Enable AWS Cost Anomaly Detection and configure alerts for anomaly events",
      "D": "Schedule daily Cost Explorer reports and parse them with Lambda for thresholds"
    },
    "explanation": "AWS Cost Anomaly Detection uses machine learning to detect unusual cost spikes and can send near real-time alerts without manual report parsing or budget forecasting."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A deployed SageMaker endpoint is experiencing intermittent increases in tail latency. The ML engineer needs to trace requests through the network stack and service mesh to pinpoint network bottlenecks. Which AWS service or feature should they enable?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run CloudWatch Logs Insights on the endpoint logs",
      "B": "Monitor EC2 CPUUtilization metrics in CloudWatch",
      "C": "Enable AWS X-Ray integration with SageMaker",
      "D": "Enable VPC Flow Logs on the endpoint\u2019s ENIs"
    },
    "explanation": "AWS X-Ray provides distributed tracing across network and service calls, enabling end-to-end latency analysis. VPC Flow Logs show packet metadata but not distributed service traces."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "For compliance, the security team wants to audit all Amazon SageMaker CreateEndpoint API calls over the past 30 days and retain logs for 90 days. Which configuration achieves this with the LEAST administrative overhead and ensures immutability?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Enable CloudTrail management events for SageMaker and send to an S3 bucket with Object Lock enabled",
      "B": "Create a CloudWatch Events rule for CreateEndpoint and log events to CloudWatch Logs",
      "C": "Enable CloudTrail Data events on CreateEndpoint and stream to Kinesis Data Firehose",
      "D": "Enable CloudTrail Insights to capture anomalous CreateEndpoint activity"
    },
    "explanation": "CloudTrail management events capture all CreateEndpoint API calls; delivering them to an S3 bucket with Object Lock provides immutability. Data events and Insights are unnecessary for standard API logging."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A director wants a single dashboard that shows per-endpoint invocation count, average 95th percentile latency, and cost per hour for each SageMaker endpoint in the account. Which solution meets this requirement with minimal development effort?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Build a custom web app that calls CloudWatch metrics API and Cost Explorer API",
      "B": "Use Amazon QuickSight to visualize a dataset combining AWS Cost and Usage Reports and CloudWatch metrics",
      "C": "Use CloudWatch Dashboards with metric math combining performance and billing metrics",
      "D": "Use the AWS Pricing API to fetch rates and combine with CloudWatch metrics in dashboards"
    },
    "explanation": "QuickSight can natively ingest both Cost and Usage Report data (cost per resource) and CloudWatch metrics to build a unified dashboard with minimal custom code. CloudWatch Dashboards cannot display cost per resource granularity."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "To reduce off-peak costs, an ML engineer wants to automatically update SageMaker endpoint instance types to smaller ones every night and revert to original sizes each morning. Which solution requires the LEAST operational overhead?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Use SageMaker Pipelines with a scheduled pipeline to call UpdateEndpointConfig",
      "B": "Create EventBridge cron rules that trigger Lambda functions invoking UpdateEndpointConfig",
      "C": "Configure AWS Step Functions with Wait states and Lambda tasks to update endpoints",
      "D": "Use Systems Manager Automation documents scheduled via State Manager"
    },
    "explanation": "Using EventBridge with cron schedules triggering lightweight Lambda functions to call UpdateEndpointConfig is the simplest and lowest-overhead scheduling solution."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A SageMaker real-time endpoint is exhibiting memory pressure, but memory utilization is not visible in CloudWatch by default. To monitor container-level memory for this endpoint, what should the ML engineer do?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable SageMaker Debugger profiling to collect memory metrics",
      "B": "Deploy the endpoint on EC2 instances where memory metrics are published by default",
      "C": "Install and configure the CloudWatch agent via a container lifecycle configuration",
      "D": "Use CloudWatch metric filters on container logs to estimate memory usage"
    },
    "explanation": "The CloudWatch agent must be installed inside the container via a lifecycle configuration script to collect OS-level metrics such as memory usage. Debugger profiles model internals, not OS memory."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "An ML application experiences periodic bursts of inference requests that exceed the 70% CPU utilization threshold. The ML engineer wants to add step scaling to the existing auto scaling policy to provision two additional instances when InvocationsPerInstance exceeds 100 for 2 minutes, and remove one instance when it drops below 50 for 5 minutes. Which configuration meets these requirements?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Define two CloudWatch alarms on the SageMakerVariantInvocationsPerInstance metric with thresholds 100 (2-minute evaluation) and 50 (5-minute evaluation), and attach a step scaling policy with +2 and -1 adjustments",
      "B": "Use CPUUtilization alarms instead of InvocationsPerInstance and a target-tracking policy",
      "C": "Use a single target-tracking policy on InvocationsPerInstance with a target value of 75",
      "D": "Configure two AWS Budgets for usage and link them to auto scaling actions"
    },
    "explanation": "Step scaling requires separate CloudWatch alarms on the SageMakerVariantInvocationsPerInstance metric with specified evaluation periods and corresponding step adjustments (+2, -1). Using CPUUtilization or budgets would not meet the specified invocation-based requirements."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A financial ML endpoint must maintain 99th percentile latency under 200 ms. To automate scaling to achieve this SLO, which Application Auto Scaling policy should be configured?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Target tracking policy on CPUUtilization at 70%",
      "B": "Target tracking policy on SageMakerVariantInvocationLatency at the p99 200 ms target",
      "C": "Step scaling policy on InvocationsPerInstance thresholds",
      "D": "Step scaling policy on ModelLatencyAvg metric"
    },
    "explanation": "A target-tracking policy on the p99 lifecycle metric of SageMakerVariantInvocationLatency targeting a 200 ms threshold will automatically adjust capacity to maintain the latency SLO. CPUUtilization or average latency would be less precise for the p99 requirement."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "After deploying a model, the team wants to trigger a retraining SageMaker pipeline whenever a new training dataset file is uploaded to S3. They also want to log all such retraining triggers for audit. Which combination of AWS services should they use?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Configure an S3 EventBridge notification on object upload to start the pipeline, and rely on CloudTrail to log the pipeline StartPipelineExecution API call",
      "B": "Use a CloudWatch scheduled rule to scan S3 daily and start the pipeline if new files exist, logging via CloudWatch Logs",
      "C": "Deploy a Lambda function to poll S3 every hour, invoke the pipeline, and log to DynamoDB",
      "D": "Send S3 events to SNS and have the pipeline poll SNS, with audit in CloudWatch Metrics"
    },
    "explanation": "Using an S3 EventBridge notification provides immediate trigger of the pipeline on new data. CloudTrail automatically logs the StartPipelineExecution API call for auditing."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "Which AWS service provides generalized compute resource recommendations, including SageMaker endpoints and EC2 instances, based on historical utilization metrics and can be applied across accounts with minimal configuration?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Inference Recommender",
      "B": "AWS Compute Optimizer",
      "C": "AWS Cost Explorer rightsizing recommendations",
      "D": "AWS Trusted Advisor"
    },
    "explanation": "AWS Compute Optimizer analyzes historical utilization across EC2, SageMaker endpoints, and other resources to recommend optimal instance types, whereas Inference Recommender focuses specifically on ML inference benchmarking."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "An engineering team needs to break down monthly SageMaker spend by project. They require cost allocation by tagging endpoints and jobs, and reporting at tag granularity. Which steps should they take?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Tag SageMaker endpoints and training jobs with project identifiers, activate those tags in AWS Billing Cost Allocation Tags, and use Cost Explorer with tag filters",
      "B": "Create separate AWS accounts per project and use consolidated billing",
      "C": "Use CloudWatch metric dimensions to filter cost metrics",
      "D": "Group SageMaker resources into separate CloudFormation stacks and view stack costs"
    },
    "explanation": "Activating resource tags for cost allocation and filtering in Cost Explorer is the standard way to break down costs by project without needing separate accounts."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "To optimize inference costs for stable production workloads with predictable traffic, which purchasing option should an ML engineer choose?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "On-Demand SageMaker Instances",
      "B": "SageMaker Savings Plans",
      "C": "EC2 Reserved Instances attached to SageMaker endpoints",
      "D": "SageMaker Spot Instances"
    },
    "explanation": "SageMaker Savings Plans offer a commitment discount on SageMaker compute usage for stable workloads. Spot Instances are not supported for real-time endpoints and Reserved Instances apply only to EC2, not SageMaker directly."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A model training workflow uses Amazon FSx for Lustre as input storage. The ML engineer notices frequent I/O throttling. To monitor FSx performance and identify bottlenecks, which CloudWatch metrics should they add to their dashboard?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "BurstCreditBalance and DataReadIOBytes",
      "B": "FreeStorageCapacity and NumberOfConnections",
      "C": "DataWriteIOPS and MetadataOperations",
      "D": "PercentIOPSUtilization and NetworkThroughput"
    },
    "explanation": "BurstCreditBalance shows available throughput credits and DataReadIOBytes shows actual read throughput, key metrics for diagnosing FSx for Lustre I/O throttling issues."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "An ML engineer is troubleshooting intermittent 5XX errors from inference endpoints. The engineer has enabled data capture to S3. To quickly identify patterns in the errors, which approach provides the fastest operational insight?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure a CloudWatch Logs subscription filter to Lambda to parse S3 logs",
      "B": "Use CloudWatch Logs Insights with a query on the /aws/sagemaker/Endpoints log group to count HTTPStatus5XX occurrences",
      "C": "Download the S3 logs locally and run custom scripts",
      "D": "Use AWS X-Ray traces to calculate error percentages"
    },
    "explanation": "CloudWatch Logs Insights can rapidly query large volumes of log data in the /aws/sagemaker/Endpoints log group to identify and aggregate 5XX errors without moving data."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "An ML engineer has deployed a SageMaker inference endpoint that writes inference results and debug logs to an S3 bucket encrypted with a customer-managed KMS key. The security team requires that only this endpoint can write to the bucket and decrypt the objects. Which configuration meets these requirements with the least administrative effort?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Attach an IAM policy to the SageMaker execution role allowing s3:PutObject on the bucket and kms:Decrypt on the CMK.",
      "B": "Configure the S3 bucket policy to allow s3:PutObject only from the VPC endpoint used by SageMaker.",
      "C": "Configure the S3 bucket policy with a condition aws:SourceArn equal to the SageMaker endpoint ARN, and update the KMS key policy to grant only the SageMaker execution role decrypt permissions.",
      "D": "Create an S3 ACL that grants the SageMaker service principal write access and rely on the IAM role for decryption."
    },
    "explanation": "A resource-based S3 bucket policy using aws:SourceArn ensures only that SageMaker endpoint can write objects, and the CMK key policy must explicitly grant the SageMaker execution role kms:Decrypt. IAM identity policies alone or ACLs cannot enforce both write and decrypt at the resource level as simply."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A security audit finds that any IAM user in the account can create SageMaker Studio user profiles, but the security team wants only members of the IAM group ProdMLUsers to be allowed. Which approach enforces this requirement at the SageMaker domain level?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add an identity-based IAM policy to the ProdMLUsers group allowing sagemaker:CreateUserProfile.",
      "B": "Attach a Service Control Policy in AWS Organizations denying sagemaker:CreateUserProfile unless the user is in ProdMLUsers.",
      "C": "Attach a resource-based policy to the SageMaker Domain that allows CreateUserProfile only when aws:PrincipalIsInGroup equals ProdMLUsers.",
      "D": "Use an S3 bucket policy to block studio creation from principals not in the group."
    },
    "explanation": "A SageMaker Domain resource policy can restrict CreateUserProfile to principals in a specific IAM group via aws:PrincipalIsInGroup. SCPs or identity policies alone cannot enforce at the domain resource level."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "An organization wants to run SageMaker training jobs in a VPC with no internet egress, but the training data resides in S3. Which network configuration meets these requirements while minimizing cost and operational overhead?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provision a NAT Gateway in the private subnet to access S3.",
      "B": "Attach an Internet Gateway to the VPC and allow routing to S3.",
      "C": "Create a VPC Gateway endpoint for S3 and update the private route table to direct S3 traffic through it.",
      "D": "Create an Interface VPC endpoint for SageMaker in the VPC and rely on AWS private networking for S3 access."
    },
    "explanation": "A Gateway VPC endpoint for S3 allows private, cost-effective access to S3 from a VPC with no internet egress. An interface endpoint for SageMaker does not enable S3 data access, and a NAT Gateway adds cost."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A data science team wants to share a SageMaker Model Registry package with a partner AWS account. The model artifacts are encrypted by a customer-managed CMK. The partner must be able to deploy the model from their account. Which combination of steps is required?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "In the CMK console, add the partner account as a key administrator, and update the model package resource policy to grant DescribeModelPackage.",
      "B": "Add the partner AWS account principal to the CMK key policy with decrypt permissions, and attach a resource policy to the model package allowing that account DescribeModelPackage and CreateModel.",
      "C": "Enable cross-account sharing in SageMaker Model Registry settings and share the CMK ARN.",
      "D": "Create a cross-account IAM role in the partner account with sagemaker:CreateModel permission and trust the partner\u2019s account; no changes to CMK."
    },
    "explanation": "Both the CMK and the model package need resource policies. The key policy must allow the partner account to decrypt, and the model package policy must allow DescribeModelPackage and CreateModel so they can register and deploy it."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A centralized CodePipeline in Account A builds ML packages and needs to deploy a trained model to a SageMaker endpoint in Account B. The security team requires least privilege. How should the cross-account permissions be configured?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "In Account A, grant the CodePipeline role sagemaker:* across all resources in Account B and trust Account B\u2019s principals.",
      "B": "Use CloudFormation StackSets from Account A to provision SageMaker resources in Account B without IAM role assumption.",
      "C": "In Account B, create an IAM role with sagemaker:CreateEndpoint and related actions, trust policy allowing assumption by the CodePipeline role\u2019s ARN in Account A, and configure the pipeline to assume that role.",
      "D": "In Account B, add the CodePipeline service principal from Account A to an IAM group with full SageMaker privileges."
    },
    "explanation": "The pipeline in Account A should assume a dedicated IAM role in Account B that has only the permissions needed to deploy the SageMaker endpoint. This follows least-privilege and standard cross-account role assumption practices."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "The security team requires that all SageMaker data-plane API calls (for example, CreateModel, InvokeEndpoint) be logged in CloudTrail. By default, only management events are recorded. What must the ML engineer do to capture these data-plane operations?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable CloudTrail management events in all regions.",
      "B": "In the CloudTrail console, enable data events for Amazon SageMaker to record data-plane API calls.",
      "C": "Enable AWS Config recording for SageMaker resource types.",
      "D": "Create CloudWatch Logs metric filters for SageMaker API calls."
    },
    "explanation": "To log data-plane API operations such as CreateModel and InvokeEndpoint, you must enable CloudTrail data events specifically for the SageMaker service. Management events alone do not capture these."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "An enterprise wants to expose a private SageMaker inference endpoint to on-premises clients over their VPN without using the internet. Which architecture meets this requirement securely with minimal overhead?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy an Amazon NLB in front of the SageMaker endpoint and configure an Interface VPC endpoint (AWS PrivateLink) for the NLB; route on-prem VPN traffic to it.",
      "B": "Create an Internet-facing Application Load Balancer in front of the endpoint and restrict access via CIDR.",
      "C": "Peer the SageMaker VPC with the on-premises network and access the endpoint via peering.",
      "D": "Use AWS Transit Gateway to route to a public-facing endpoint with strong security groups."
    },
    "explanation": "Using a Network Load Balancer with a PrivateLink interface endpoint allows on-premises VPN clients to connect privately without exposing the endpoint to the internet, and with minimal additional components."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A security policy requires that every SageMaker training job include a CostCenter tag and use the execution role arn:aws:iam::123456789012:role/MLExecRole. Which feature can the ML engineer use to enforce both requirements at job creation?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use an IAM permissions boundary on ML users to restrict tag values.",
      "B": "Deploy an AWS Organizations SCP that denies sagemaker:CreateTrainingJob when tags or role do not match.",
      "C": "Enable an AWS Config rule for SageMaker jobs to require tags and roles.",
      "D": "Attach an IAM policy to users with a condition on sagemaker:RequestTag/CostCenter and sagemaker:ResourceTag/CostCenter and a condition requiring the UseServiceRole parameter equals MLExecRole."
    },
    "explanation": "An IAM identity policy with sagemaker:RequestTag and sagemaker:ResourceTag conditions can prevent job creation if the CostCenter tag is missing or if the specified ServiceRole is not MLExecRole. SCPs cannot inspect request tags."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A processing job running in SageMaker needs to mount an Amazon EFS file system for intermediate data. The security team requires encryption at rest and in transit for EFS, and only the processing job\u2019s subnets should be able to mount it. Which configuration meets these requirements?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create an unencrypted EFS, rely on encryption in transit only, and restrict mount via security group.",
      "B": "Create an encrypted EFS with SSE-KMS, mount it in the processing job with encryption in transit disabled.",
      "C": "Use FSx for Lustre with default encryption, and mount via a VPN.",
      "D": "Create an EFS file system encrypted at rest with a CMK, enable encryption in transit (TLS) on the mount target, and restrict the EFS security group to only allow mount traffic from the processing job\u2019s security group."
    },
    "explanation": "An Amazon EFS file system can be encrypted at rest using a CMK and can enforce TLS encryption in transit. Security groups can restrict mount access so that only SageMaker processing job instances can connect."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "The security team wants only members of the SecurityAudit role to read SageMaker logs in CloudWatch Logs. Other users must be denied. How can this be implemented?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Attach a resource-based policy to the specific CloudWatch Log Group that allows only the SecurityAudit role to GetLogEvents and FilterLogEvents.",
      "B": "Encrypt the log group with a CMK and grant decrypt only to the SecurityAudit role.",
      "C": "Use an IAM identity policy denying CloudWatch Logs actions unless aws:PrincipalArn equals the SecurityAudit role.",
      "D": "Move the log group to a different AWS account used by security."
    },
    "explanation": "CloudWatch Log Groups support resource-based policies that can explicitly allow only a principal to perform log-reading actions. This is the most direct way to restrict access at the log-group level."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A CI/CD pipeline in CodePipeline uses an ECR repository for custom container images. The security team requires that only this pipeline can pull images and no other IAM principals. Which configuration enforces this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Attach an IAM policy to the pipeline\u2019s role allowing ecr:BatchGetImage on the repo.",
      "B": "Add an ECR repository resource policy that allows ecr:BatchGetImage for the CodePipeline service role ARN and denies all others.",
      "C": "Use SCP to deny ecr:BatchGetImage globally for all principals except the pipeline.",
      "D": "Rely on IAM identity policies on all users to not allow ECR actions."
    },
    "explanation": "An ECR repository policy can directly specify which principals (the pipeline role) are allowed to pull images and deny everyone else, enforcing container image access at the resource level."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "The security team wants to prevent developers from creating any SageMaker notebook instances or Studio domains that are internet-facing. Which control can achieve this across the organization?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use an IAM permissions boundary for all developers disallowing sagemaker:CreateDomain.",
      "B": "Apply an AWS Organizations Service Control Policy that denies sagemaker:CreateDomain or CreateNotebookInstance when NetworkIsolation is false.",
      "C": "Attach an identity policy to each developer denying notebook creation.",
      "D": "Use AWS Config to detect and remediate non-VPC notebooks."
    },
    "explanation": "A Service Control Policy can centrally deny creation of SageMaker notebooks or domains unless the NetworkIsolation parameter is set to true, preventing internet access organization-wide."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "An automated retraining workflow triggers a Lambda function via EventBridge. The function needs only permissions to DescribeEndpoint and CreateTrainingJob for a specific endpoint and training job prefix. How should the IAM role be defined to follow least privilege?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Grant sagemaker:* on all resources in the account.",
      "B": "Grant sagemaker:DescribeEndpoint and sagemaker:CreateTrainingJob on all SageMaker ARNs.",
      "C": "Grant sagemaker:DescribeEndpoint on the specific endpoint ARN and sagemaker:CreateTrainingJob with a resource ARN pattern matching the training-job-prefix*, and no other actions.",
      "D": "Attach AWS managed SageMaker full-access policy."
    },
    "explanation": "Defining resource-level permissions for only the specific endpoint and a wildcard training job prefix ensures the Lambda role has the minimum permissions required for its function."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "Security requirements dictate that SageMaker notebook instances access internal services only on port 443 and cannot initiate outbound traffic to the internet. Which combination of controls enforces this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure the notebook instance\u2019s security group to allow outbound 443 to specific subnets and attach an Internet Gateway to the VPC.",
      "B": "Use a NAT Gateway with a route table restricting to internal CIDR.",
      "C": "Configure the SG to allow outbound TCP 443 to internal subnet CIDR, set the route table for the private subnet with no internet gateway, and use a VPC Endpoint for required AWS service calls.",
      "D": "Deploy a firewall appliance in the VPC to filter outbound traffic."
    },
    "explanation": "Removing an internet gateway and not configuring a NAT Gateway prevents internet egress. A security group can allow only TCP 443 to internal subnets. VPC endpoints enable needed AWS service access without the internet."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "An ML engineer needs to capture inference requests and responses in a SageMaker endpoint. The security team requires that the data capture archive in S3 be encrypted at rest with a CMK and that all data in transit use TLS. Which configuration achieves this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable S3 default encryption on the bucket and configure DataCaptureConfig without specifying KMS.",
      "B": "Specify a KMS key in the SageMaker ModelRegistry settings and enable DataCaptureConfig.",
      "C": "Use an S3 bucket policy to require server-side encryption and rely on endpoint defaults.",
      "D": "In the endpoint\u2019s DataCaptureConfig, set EnableCapture true and specify the CMK KmsKeyId, and ensure the endpoint uses HTTPS (TLS) invocations."
    },
    "explanation": "DataCaptureConfig allows specifying a customer-managed KMS key for server-side encryption of captured data. SageMaker endpoints always use HTTPS for traffic, ensuring TLS in transit."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A data engineering team must transfer 100 TB of historical log files from an on-premises data center to Amazon S3 within 48 hours. The network bandwidth is limited to 1 Gbps and is cost-constrained. Which ingestion method meets the requirement with the lowest operational overhead?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS DataSync over the 1 Gbps link with parallel transfers.",
      "B": "Use S3 Transfer Acceleration over the internet connection.",
      "C": "Ship multiple AWS Snowball Edge devices and import data.",
      "D": "Use a custom multi-threaded upload client over the VPN."
    },
    "explanation": "At 1 Gbps, 100 TB over the network exceeds 48 hours; Snowball Edge avoids bandwidth constraints and minimizes operational complexity."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An ML workflow requires high-throughput reading of a 200 GB dataset during training in SageMaker. The data is stored in CSV format in S3. Training jobs are currently I/O-bound. Which change yields the greatest reduction in I/O time?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable S3 Transfer Acceleration on the bucket.",
      "B": "Convert the dataset to Parquet with Snappy compression and use Apache Arrow for reading.",
      "C": "Increase the SageMaker instance EBS volume throughput.",
      "D": "Enable S3 request rate optimization on the bucket."
    },
    "explanation": "Parquet is a columnar, compressed format that reduces data transfer size and read time, outperforming acceleration or EBS tweaks."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A streaming ingestion pipeline uses Amazon Kinesis Data Streams with 10 shards. Downstream training jobs require data partitioned by customer ID. The engineer notices uneven data distribution and hot shard errors. Which solution evenly distributes load?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase to 20 shards without changing partition key.",
      "B": "Use a composite primary key combining customer ID and timestamp.",
      "C": "Switch to Amazon MSK and let Kafka partition dynamically.",
      "D": "Hash the customer ID with a random suffix as the partition key."
    },
    "explanation": "Appending a random suffix to the customer ID spreads records across shards evenly, avoiding hot shards while preserving grouping on average."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A SageMaker Data Wrangler flow reads multiple small JSON files (~100 MB each) from S3. The job startup and cataloging latency dominates time. How can the engineer minimize this overhead?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Consolidate the small files into larger partitioned Parquet files.",
      "B": "Increase the SageMaker Data Wrangler instance type.",
      "C": "Use S3 Select on each JSON file.",
      "D": "Enable S3 Requester Pays on the bucket."
    },
    "explanation": "Consolidating files into fewer larger Parquet partitions reduces per-object overhead and speeds up scanning and schema inference."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An application uses Amazon Kinesis Data Firehose to deliver JSON records to S3. The delivery latency spikes under peak load. Which Firehose configuration change will smooth delivery latency while controlling costs?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set buffer size to minimum and buffer interval to default.",
      "B": "Set buffer size to maximum and buffer interval to minimum.",
      "C": "Increase buffer size to max and buffer interval to 300 seconds.",
      "D": "Decrease buffer size to minimum and buffer interval to 300 seconds."
    },
    "explanation": "Larger buffer size with a longer interval results in fewer deliveries, smoothing spikes and reducing egress costs."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A team must ingest near-real-time IoT telemetry (5 KB messages, 5,000 msg/sec) into S3 for batch training. Which AWS service combination best meets throughput and minimal operational overhead?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS IoT Core + Lambda writing to S3.",
      "B": "Kinesis Data Streams + Kinesis Data Firehose to S3.",
      "C": "MSK cluster + custom consumer that writes to S3.",
      "D": "Directly upload from devices to S3 using the AWS SDK."
    },
    "explanation": "Kinesis Streams scales to handle throughput and Firehose auto-batches and delivers to S3 without infra management."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A data scientist merges customer profile data from Amazon RDS and clickstream logs stored in S3. They need an automated, serverless solution with minimal code. Which approach is optimal?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Deploy an EMR cluster with Spark to join RDS and S3 datasets.",
      "B": "Use AWS Glue ETL jobs on a scheduled basis.",
      "C": "Use SageMaker Processing with custom Pandas scripts.",
      "D": "Use AWS Glue DataBrew recipe to connect to RDS and S3 and join datasets."
    },
    "explanation": "DataBrew provides serverless, no-code recipes to join RDS and S3 datasets, minimizing code and infra."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A company requires secure ingestion of PII data into Amazon S3. Data must be encrypted in transit and at rest, and decryption only occurs in SageMaker. Which configuration satisfies these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable SSE-S3 on the bucket and HTTPS for uploads.",
      "B": "Configure HTTPS for uploads, enable SSE-KMS with a CMK granting only SageMaker decryption.",
      "C": "Use client-side encryption and store keys in AWS Secrets Manager.",
      "D": "Enable SSE-KMS and grant all users read access to the CMK."
    },
    "explanation": "SSE-KMS with CMK access restricted to SageMaker ensures encrypted transit and at-rest encryption, with decryption only in SageMaker."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An ML pipeline uses Amazon MSK to stream data into S3 via Kafka Connect. During peak, Kafka Connect tasks lag behind. Which change will improve ingestion throughput?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase Kafka Connect workers and partitions for the connector topic.",
      "B": "Switch connector batch.size to 1.",
      "C": "Use a single large Kafka Connect worker.",
      "D": "Decrease connector retries to zero."
    },
    "explanation": "More workers and partitions allow parallelism in Kafka Connect, increasing ingestion throughput and reducing lag."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A training job on SageMaker needs to read a dataset stored on Amazon FSx for Lustre for low-latency I/O. The dataset originates in S3. Which configuration provides the fastest initial data access?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Mount FSx for Lustre and copy data manually from S3.",
      "B": "Use AWS DataSync to mirror S3 to FSx for Lustre.",
      "C": "Use FSx\u2019s native S3 integration by specifying the S3 bucket as the data repository.",
      "D": "Use a SageMaker Processing job to prefetch data into EBS."
    },
    "explanation": "FSx for Lustre native S3 integration transparently caches and streams S3 data into the file system without manual copy."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A dataset stored in S3 is consumed by multiple SageMaker training instances concurrently. To minimize read latency and S3 request costs, what ingestion pattern should be used?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Each instance downloads the full dataset to its EBS volume.",
      "B": "Use S3 Select API within training scripts.",
      "C": "Use a shared EFS mount backed by S3.",
      "D": "Use S3 Replication with Cross-Region Replication disabled and parallel GETs with HTTP Keep-Alive."
    },
    "explanation": "Parallel GETs with HTTP Keep-Alive reuse connections, reducing latency and request overhead compared to duplicated EBS or EFS."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A company needs to ingest 10 TB of data daily from S3 into SageMaker Feature Store with minimal lag. Which approach scales ingestion while controlling costs?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use service API PutRecord sequentially for each feature.",
      "B": "Use the batch ingest offline feature with RecordIO protobuf files.",
      "C": "Use real-time endpoint calls for each record.",
      "D": "Use Glue Streaming to write directly to Feature Store."
    },
    "explanation": "Batch ingest with RecordIO is optimized for large-scale ingestion into Feature Store and is cost-effective compared to per-record API calls."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An engineer must ingest data from multiple AWS accounts into a central S3 data lake. Data producers may intermittently lose network connectivity. Which ingestion pattern ensures reliability and scalability?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use S3 Cross-Account replication with retry policies.",
      "B": "Use a central Firehose delivery stream with producers as PUT clients.",
      "C": "Use AWS DataSync agents in each account.",
      "D": "Use AWS Transfer Family SFTP endpoints in the central account."
    },
    "explanation": "Cross-Account replication handles intermittent producers with built-in retry and scales to multi-account sources without agents."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A SageMaker processing job fails intermittently reading from an S3 bucket due to 503 throttling errors. Which change reduces the throttling while optimizing cost?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable S3 Transfer Acceleration.",
      "B": "Increase the processing instance count.",
      "C": "Enable S3 Requester Pays and use signed requests with exponential backoff in the SDK.",
      "D": "Switch the bucket to use SSE-C encryption."
    },
    "explanation": "Signed requests with retry/backoff reduce throttling; Requester Pays can shift cost but throttling reduction via SDK backoff is key."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A compliance team requires that logs ingested into S3 by Kinesis Firehose are immutable and versioned. Which S3 bucket configuration achieves this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable MFA Delete and SSE-S3.",
      "B": "Enable Object Lock in Governance mode with versioning.",
      "C": "Enable default encryption with SSE-KMS.",
      "D": "Enable Lifecycle rules to transition logs to Glacier."
    },
    "explanation": "Object Lock with versioning set to Governance mode enforces immutability on ingested S3 objects."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "During a large dataset migration, objects are written to an S3 bucket in parallel. The team observes increased 503 errors. Which bucket-level configuration resolves this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable S3 Transfer Acceleration.",
      "B": "Enable default encryption with SSE-KMS.",
      "C": "Enable MFA Delete on the bucket.",
      "D": "Enable S3 request rate optimization on the bucket."
    },
    "explanation": "Request rate optimization (prefix deprecation) reduces 503 errors under high parallel write rates by distributing keys."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A pipeline uses AWS Glue to extract data from an Amazon Aurora MySQL database to S3. The Glue job times out during snapshot extraction. Which approach resolves the timeout while minimizing developer effort?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Increase the connection timeout in the Glue job script.",
      "B": "Use AWS Database Migration Service with full load to S3.",
      "C": "Split the Glue job into multiple smaller jobs.",
      "D": "Export Aurora snapshot to S3 using native export."
    },
    "explanation": "Using DMS for full load to S3 handles large data extracts reliably with minimal custom code versus Glue timeouts."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An ML engineer must ingest encrypted CSV files from S3 to SageMaker Processing. The files are encrypted with SSE-KMS using a custom CMK. The processing role lacks decrypted access. What adjustment grants the least privilege?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Attach an IAM policy to the processing role allowing kms:Decrypt on the CMK.",
      "B": "Re-encrypt the files with SSE-S3.",
      "C": "Use client-side encryption and share the key.",
      "D": "Grant the role AmazonS3FullAccess."
    },
    "explanation": "Granting kms:Decrypt on the CMK to the processing role grants only needed permissions rather than broad S3 access."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A data lake uses S3 with AWS Lake Formation. Data ingestion from streaming sources must respect table-level LF permissions. Which integration ensures permissions enforcement?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Kinesis Data Firehose direct to S3 outputs.",
      "B": "Use Lambda to write to S3 then catalog with Glue.",
      "C": "Use Lake Formation transactions with AWS Glue streaming jobs.",
      "D": "Use Athena to INSERT INTO Lake Formation tables."
    },
    "explanation": "Lake Formation transactions in Glue streaming enforce table-level LF permissions during ingestion."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An ML pipeline ingests files via S3 Event Notifications to Lambda. Under high throughput, some events are lost. Which change ensures all records are ingested?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase the Lambda concurrency limit.",
      "B": "Use SQS as the Event Notification target before invoking Lambda.",
      "C": "Switch to SNS Event Notifications.",
      "D": "Enable retry behavior in S3."
    },
    "explanation": "Using SQS between S3 and Lambda buffers events, ensuring delivery and retry rather than direct event loss."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A company plans to centralize logs from multiple regions into a single S3 bucket. Logs must remain in native partition folders. How should a cross-region replication rule be configured?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a replication rule with IncludePuts and prefix replication per region.",
      "B": "Use replication time control to replicate objects hourly.",
      "C": "Use batch Athena CTAS to copy logs across regions.",
      "D": "Use DataSync to mirror entire buckets."
    },
    "explanation": "A replication rule with region-specific prefixes replicates objects into same partition paths in central bucket."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A secure pipeline must ingest data from Amazon DynamoDB to S3 for ML training. The transfer must be encrypted, serverless, and cost-efficient. Which service accomplishes this?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Use EMR Spark job to export DynamoDB to S3.",
      "B": "Use AWS DataSync to copy table exports.",
      "C": "Use AWS Glue with DynamoDB connector.",
      "D": "Use DynamoDB Export to S3 with SSE-KMS encryption."
    },
    "explanation": "DynamoDB native Export to S3 is serverless, encrypted, and cost-efficient compared to EMR or Glue."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An ML pipeline needs to ingest image files from multiple SFTP servers into S3. The process must be fully AWS managed and scalable. Which solution meets these requirements?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Deploy EC2 instances running custom SFTP-to-S3 scripts.",
      "B": "Use AWS DataSync agents on EC2.",
      "C": "Use AWS Transfer Family SFTP endpoints with custom identity providers and S3 backing.",
      "D": "Use Glue FTP connector."
    },
    "explanation": "AWS Transfer Family provides managed SFTP endpoints that store files directly in S3, scaling without custom servers."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A pipeline needs to ingest sensor data at 100 MB/s into S3 for real-time analytics. Which combination of services and configurations meets throughput while controlling costs?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Kinesis Data Streams with 100 shards directly writing to S3.",
      "B": "MSK with 5 broker nodes, MirrorMaker to Firehose with gzip compression.",
      "C": "Direct Firehose delivery with default shard count.",
      "D": "Lambda triggered by S3 upload events."
    },
    "explanation": "MSK scales to high throughput; MirrorMaker populates Firehose which batches and compresses data to S3 cost-effectively."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "During ingest of nested JSON event data into S3, the team needs to infer schema and partition by event date. Which AWS Glue configuration fulfills this with minimal code?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Use a custom PySpark Glue job to parse JSON and write Parquet.",
      "B": "Use Athena CTAS to convert and partition data.",
      "C": "Use SageMaker Processing to flatten and write to S3.",
      "D": "Use AWS Glue JSON classifier with crawler partitioning on event_date field."
    },
    "explanation": "A Glue crawler with JSON classifier can infer schema and apply partitioning on a specified JSON field automatically."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A dataset must be ingested daily from an external partner to S3 via the internet. The partner cannot use AWS SDK. How should the company securely and reliably ingest files?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provision an AWS Transfer Family SFTP endpoint backed by the S3 bucket.",
      "B": "Provide S3 PUT URLs with public write ACL.",
      "C": "Use HTTP POST to S3 REST API with credentials.",
      "D": "Ask partner to email files and upload manually."
    },
    "explanation": "Transfer Family SFTP endpoint allows partners to upload over SFTP without SDK, delivering files directly and securely to S3."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "The team needs to ingest 500 GB of data daily from an on-prem Hadoop cluster to S3 under network constraints. Which approach provides incremental ingestion and optimizes bandwidth?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Snowball Edge weekly ingestion.",
      "B": "Deploy AWS DataSync agent with incremental sync enabled.",
      "C": "Use a Glue job with JDBC connector.",
      "D": "Use AWS Glue DataBrew to pull data."
    },
    "explanation": "DataSync incremental sync only transfers changed data daily, optimizing bandwidth and automating ingestion."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An ML pipeline reads an S3 directory of Avro files during training. The engineer wants to reduce startup latency when listing thousands of files. Which S3 configuration reduces LIST cost?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable SSE-KMS on the bucket.",
      "B": "Enable bucket versioning.",
      "C": "Use S3 Inventory to generate daily manifest and use the manifest for file listing.",
      "D": "Enable S3 Transfer Acceleration."
    },
    "explanation": "S3 Inventory provides a manifest file listing objects, eliminating costly API ListObjects calls during job startup."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A team needs to ingest HTTP logs into S3 for historical analysis. Logs are generated at 10 GB/hour. The solution must auto-scale and require no server management. Which ingestion architecture is best?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "EMR cluster with Flink reading logs.",
      "B": "Kinesis Data Firehose with Lambda data transformation to S3.",
      "C": "Custom ECS service tailing logs and writing to S3.",
      "D": "S3 multipart upload from application servers."
    },
    "explanation": "Firehose auto-scales ingestion and can deliver data to S3 with optional Lambda transformation, requiring no servers."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A multi-region application produces data that must land in a centralized S3 bucket in the same AWS Region as processing. Replication must preserve object metadata and ACLs. Which configuration meets this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use CloudTrail replication to central bucket.",
      "B": "Configure Firehose across regions.",
      "C": "Use DataSync cross-region replication.",
      "D": "Use cross-region replication with replicate object metadata and ACL."
    },
    "explanation": "S3 cross-region replication configured to replicate metadata and ACLs preserves all object attributes in central bucket."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A SageMaker training job must read millions of small JSON files from S3. The engineer notices high GET request costs. Which solution reduces GETs and maintains low latency?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Combine JSON files into larger S3 objects, e.g., Parquet, using a Glue job.",
      "B": "Enable S3 Intelligent-Tiering.",
      "C": "Use S3 Select for each JSON file.",
      "D": "Use Lifecycle rules to consolidate files."
    },
    "explanation": "Combining small files into larger Parquet objects reduces individual GET calls and lowers request costs."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "To ingest clickstream events into S3 with exactly-once semantics, an engineer uses Kinesis Data Streams and Lambda to write to S3. At times, duplicates occur. Which pattern ensures exactly-once writes?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use PutObject with versioning enabled.",
      "B": "Use S3 Multipart upload with unique upload IDs.",
      "C": "Use Kinesis Data Firehose with S3 delivery and deduplication via record ID.",
      "D": "Use DynamoDB streams and batch writes to S3."
    },
    "explanation": "Firehose supports exactly-once delivery semantics to S3 when configured with record IDs for deduplication."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A data lake's S3 bucket is configured with CloudWatch Events to start Glue crawlers upon new object creation. Under heavy load, some crawlers start late and miss events. How ensure timely ingestion?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase crawler concurrency in Glue.",
      "B": "Introduce a Step Functions state machine triggered by EventBridge to queue and throttle crawler invocations.",
      "C": "Use Lambda to start crawlers directly.",
      "D": "Enable S3 Intelligent-Tiering."
    },
    "explanation": "Using Step Functions to queue and throttle ensures orderly crawler invocations without overload compared to direct triggers."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An ML engineer is training a PyTorch model in SageMaker script mode on a single GPU instance. The training job often runs out of GPU memory when using a batch size of 256. To continue training with minimal code changes and without incurring additional infrastructure cost, which action should the engineer take?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable mixed precision training (automatic mixed precision) in the training script.",
      "B": "Switch to SageMaker built-in algorithm XGBoost with the same batch size.",
      "C": "Decrease the number of training epochs to reduce memory footprint.",
      "D": "Use a larger EC2 instance type with more GPU memory."
    },
    "explanation": "Mixed precision reduces memory usage with minimal code changes. Switching algorithms or decreasing epochs doesn\u2019t reduce per-batch memory; upgrading instance increases cost."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A data scientist uses SageMaker Automatic Model Tuning (AMT) with Bayesian optimization to tune a deep neural network. The search is converging too slowly. The search space contains 10 continuous and 5 categorical hyperparameters. What adjustment will most speed up convergence?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch from Bayesian optimization to random search.",
      "B": "Increase the maximum number of training jobs in the tuning job.",
      "C": "Reduce the hyperparameter search space by fixing low-impact parameters to default values.",
      "D": "Use an IDENTICAL_DATA_AND_ALGORITHM warm start tuning job type."
    },
    "explanation": "Reducing search space focuses Bayesian optimization on impactful parameters. Random search may be less efficient; warm start of type IDENTICAL_DATA_AND_ALGORITHM duplicates prior runs, not solving slow convergence."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An ML engineer trains a large transformer model using SageMaker script mode with TensorFlow. Training takes 48 hours. The engineer wants to halve training time with minimal hyperparameter changes. Which approach meets this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Double the batch size and halve the number of epochs.",
      "B": "Enable distributed data parallel training across two GPU instances.",
      "C": "Use L2 regularization with higher weight to speed up convergence.",
      "D": "Decrease learning rate and increase number of epochs."
    },
    "explanation": "Distributed training splits workload across GPUs, reducing wall-clock time. Changing regularization or learning rate affects training dynamics, not time directly; doubling batch/halving epochs may hurt convergence."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A tabular dataset shows early overfitting: validation loss increases after 5 epochs. Which combination of techniques best reduces overfitting while preserving model capacity?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase batch size and remove dropout layers.",
      "B": "Decrease learning rate and increase number of layers.",
      "C": "Add L2 regularization with high weight and increase epochs.",
      "D": "Implement dropout, add L2 weight decay, and enable early stopping after no validation improvement for 3 epochs."
    },
    "explanation": "Dropout+L2 regularization reduce overfitting; early stopping avoids unnecessary epochs. Increasing layers or removing dropout worsens overfitting."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A company uses a SageMaker built-in XGBoost algorithm to classify credit risk. The model size on disk is 1.5 GB, exceeding edge device storage limit of 500 MB. Which strategy best reduces model size without retraining from scratch?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Decrease max_depth and num_round in a new training job.",
      "B": "Use post-training model pruning and quantization using SageMaker Neo compilation.",
      "C": "Enable L1 regularization in the current model registry version.",
      "D": "Decrease the learning rate and increase the number of trees."
    },
    "explanation": "SageMaker Neo can prune and quantize a compiled model without retraining. Changing training hyperparameters requires retraining."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An ML engineer fine-tunes a pre-trained BERT model on SageMaker JumpStart. To prevent catastrophic forgetting of pre-trained weights, which configuration should they apply?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use lower learning rate for pre-trained layers and higher rate for newly added classification head.",
      "B": "Initialize all layers randomly before fine-tuning.",
      "C": "Freeze the classification head and only train transformer layers.",
      "D": "Increase batch size by fourfold to stabilize gradients."
    },
    "explanation": "Lower LR on pretrained layers preserves learned features; random init or freezing head loses benefits."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "During hyperparameter tuning with AMT, an engineer observes that many training jobs fail due to out-of-memory errors for large batch sizes. To optimize tuning efficiency, which AMT configuration change is best?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase the maximum parallel training jobs.",
      "B": "Switch from Bayesian to grid search.",
      "C": "Decrease the minimum number of training jobs.",
      "D": "Add a conditional statement in the training script to skip batch sizes causing OOM and return low objective metric."
    },
    "explanation": "Skipping invalid hyperparameter combinations via script invalidation avoids wasted jobs; changing search strategy or job counts doesn\u2019t prevent failures."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A deep CNN trained in SageMaker exhibits slow convergence and high variance among epoch losses. Which combination of techniques addresses both issues?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Remove data augmentation and add more layers.",
      "B": "Increase learning rate and decrease batch size.",
      "C": "Apply batch normalization, use learning rate scheduler, and add dropout layers.",
      "D": "Switch optimizer from Adam to stochastic gradient descent with fixed LR."
    },
    "explanation": "Batch norm and LR scheduling stabilize training; dropout reduces variance. Removing augmentation or switching optimizer without scheduler may worsen convergence."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An ML engineer needs to integrate a scikit-learn RandomForest model trained locally into SageMaker for batch transform inference. Which step is required?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Wrap the model in a SageMaker built-in algorithm container.",
      "B": "Create a custom inference script and container using SageMaker script mode to load the pickle file.",
      "C": "Use SageMaker Clarify to register the model directly from local filesystem.",
      "D": "Upload the model artifact to SageMaker Model Registry via console."
    },
    "explanation": "Local models require a custom container/script in script mode. Built-in algorithms can\u2019t import arbitrary pickles."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A large NLP model fine-tuned via Hugging Face in SageMaker shows slow throughput per epoch. Which SageMaker feature reduces training time without code changes?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable early stopping in AMT.",
      "B": "Use SageMaker Model Monitor to detect drift.",
      "C": "Enable managed spot training with distributed training.",
      "D": "Switch to a built-in algorithm for NLP tasks."
    },
    "explanation": "Managed distributed spot training uses multiple instances to speed training; early stopping and monitoring don\u2019t affect raw throughput."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An engineer uses SageMaker script mode to train a PyTorch model. The training time per epoch increases over time due to GPU memory fragmentation. How to mitigate this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable gradient checkpointing to reduce peak memory usage.",
      "B": "Increase batch size to better pack memory.",
      "C": "Disable data parallelism to reduce fragmentation.",
      "D": "Switch from PyTorch to TensorFlow."
    },
    "explanation": "Gradient checkpointing trades compute for memory, alleviating fragmentation. Increasing batch size worsens it; switching frameworks is heavy-handed."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A SageMaker AMT job tunes hyperparameters including learning_rate and optimizer choice. The team needs to incorporate prior tuning results to guide the new job. Which warm start type is appropriate?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "IDENTICAL_DATA_AND_ALGORITHM",
      "B": "TRANSFER_LEARNING",
      "C": "STANDARD",
      "D": "TRANSFER_LEARNING with previous job as parent"
    },
    "explanation": "TRANSFER_LEARNING warm start uses prior results from parent tuning jobs; IDENTICAL_DATA_AND_ALGORITHM reuses only matching configs."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An ML researcher wants repeated reproducible experiments in SageMaker. Which practice ensures identical results across training runs?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use random search in AMT.",
      "B": "Use spot instances to save costs.",
      "C": "Enable early stopping after fixed epochs.",
      "D": "Set and log explicit random seeds, fix training hyperparameters, use the Model Registry for versioning."
    },
    "explanation": "Explicit seed and consistent config plus registry ensure reproducibility; early stopping and spot instances introduce variability."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A team notices high variance between ensemble members when combining models. To improve ensemble performance, which strategy is best?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use identical architecture and data splits for each member.",
      "B": "Use diverse base learners and bootstrap sampling to increase diversity.",
      "C": "Increase depth of all tree-based models.",
      "D": "Use only the top-performing single model."
    },
    "explanation": "Ensemble benefits from diverse learners; identical architectures reduce benefit."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An engineer needs to fine-tune a large foundation model from Amazon Bedrock in SageMaker for a specialized downstream task. Which approach integrates seamlessly?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Export the model weights and import into a SageMaker built-in algorithm.",
      "B": "Download the model artifacts and retrain from scratch.",
      "C": "Use SageMaker JumpStart to fine-tune the model with custom dataset.",
      "D": "Convert Bedrock model to TensorFlow SavedModel and deploy."
    },
    "explanation": "JumpStart provides ready fine-tuning pipelines for Bedrock foundation models."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A deep learning model exhibits underfitting on training and validation sets. To improve fit, which hyperparameter adjustment is most effective?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Increase model capacity by adding layers or units.",
      "B": "Increase dropout rate.",
      "C": "Increase L2 regularization weight.",
      "D": "Decrease number of training epochs."
    },
    "explanation": "Underfitting requires more capacity; dropout and L2 worsen underfitting."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An automatic model tuning job uses AMT with a goal metric of validation error. The team now needs to optimize two metrics: inference latency and accuracy. Which solution supports this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run two separate AMT jobs and manually choose trade-off.",
      "B": "Use Bayesian optimization with a composite metric manually computed in script.",
      "C": "Switch search strategy to grid search for multi-objective coverage.",
      "D": "Use SageMaker multi-objective hyperparameter tuning-enabled job."
    },
    "explanation": "Multi-objective tuning in SageMaker supports simultaneous optimization of metrics."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A training job using Hugging Face Transformers in SageMaker uses a JSON Lines dataset on S3. The engineer observes slow data loading. Which training configuration change accelerates data throughput?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch input mode to Pipe mode.",
      "B": "Use File mode with Amazon FSx for Lustre linked to the S3 bucket.",
      "C": "Decrease number of worker processes in DataLoader.",
      "D": "Disable shuffling of the dataset."
    },
    "explanation": "FSx for Lustre caches data for high throughput; pipe mode streams raw S3, slower for many small records."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An ML engineer wants to prune a TensorFlow model during SageMaker training to reduce inference latency while preserving >98% of accuracy. Which training callback should be used?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "EarlyStopping on validation loss.",
      "B": "LearningRateScheduler.",
      "C": "TensorFlow Model Optimization pruning callback.",
      "D": "ReduceLROnPlateau."
    },
    "explanation": "TF Model Optimization pruning callback prunes weights during training; others adjust learning rate or stop early."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A model trained on SageMaker exhibits high training throughput but low GPU utilization. Which configuration change most directly improves GPU utilization?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase per-instance batch size.",
      "B": "Decrease the number of GPUs.",
      "C": "Enable early stopping.",
      "D": "Use a smaller instance type."
    },
    "explanation": "Batch size affects GPU utilization; larger batches fill GPU compute better."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A team uses AMT with max_jobs=50 and max_parallel_jobs=5. They need to reduce total run time but keep exploring same search space. Which change achieves this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase search space cardinality.",
      "B": "Increase max_parallel_jobs to 10.",
      "C": "Decrease max_jobs to 25.",
      "D": "Switch to random search strategy."
    },
    "explanation": "Increasing parallel jobs reduces wall-clock time; random search doesn\u2019t guarantee faster convergence."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An existing model version in SageMaker Model Registry is retrained with new hyperparameters. To automate promoting the version when validation accuracy improves by >1%, which approach is best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Manually compare metrics and click Promote in console.",
      "B": "Use SageMaker Experiments to tag and promote.",
      "C": "Define a SageMaker Pipeline with conditional ModelRegistry promotion step.",
      "D": "Write a Lambda triggered by CloudWatch alarm on metrics."
    },
    "explanation": "Pipelines support conditional promotion; Lambda approach requires custom code and integration."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "During distributed training on multiple GPU instances, training noise leads to divergence. What change reduces variance across replicas?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase initial learning rate.",
      "B": "Disable gradient clipping.",
      "C": "Use smaller batch size per replica.",
      "D": "Enable synchronized BatchNorm across replicas."
    },
    "explanation": "Synchronized BatchNorm ensures consistent statistics; other changes may worsen instability."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A SageMaker TensorFlow training job time per epoch is dominated by data preprocessing in training script. How to offload preprocessing to speed training?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Processing or Glue to preprocess and save TFRecords before training.",
      "B": "Enable profiler for training jobs.",
      "C": "Switch to script mode with Python SDK.",
      "D": "Use hyperparameter tuning to find optimal preprocessing parameters."
    },
    "explanation": "Preprocessing offline reduces per-epoch overhead; tuning and profiling don\u2019t offload work."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An engineer needs to train a model with imbalanced classes. To ensure balanced gradient updates per batch, which DataLoader or training config change is best?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Shuffle dataset each epoch.",
      "B": "Use a weighted random sampler to form balanced batches.",
      "C": "Increase number of epochs.",
      "D": "Add L1 regularization to loss."
    },
    "explanation": "Weighted sampler balances classes per batch; shuffling alone doesn\u2019t ensure balance."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A team wants to combine three diverse models into a final prediction in SageMaker. They need an ensemble pipeline that retrains all members and the meta-learner automatically. Which SageMaker feature should they use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Batch Transform.",
      "B": "SageMaker Clarify.",
      "C": "SageMaker Neo.",
      "D": "SageMaker Pipelines with Ensemble step implementations."
    },
    "explanation": "Pipelines can orchestrate multi-step ensemble training; other services unrelated."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You have a dataset with highly skewed numeric features and missing values partitioned across multiple S3 prefixes. You need a pipeline that imputes missing values with the median per feature, applies a log transform to reduce skew, and writes the cleaned data to Amazon SageMaker Feature Store. Which approach meets these requirements with the least custom code?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use an AWS Glue Spark ETL job to compute medians via AWS Glue Data Quality, apply a PySpark log transform, and write to Feature Store using the SageMaker SDK.",
      "B": "Use a SageMaker Data Wrangler flow: add a fill-missing recipe with median, add a built-in log transform step, then export directly to Feature Store.",
      "C": "Use AWS Glue DataBrew: create a project, add fill-missing and log transform recipes, and publish to S3 for later ingestion into Feature Store.",
      "D": "Run a SageMaker Processing job with a custom Scikit-Learn script that imputes medians, applies log transforms, and writes to Feature Store."
    },
    "explanation": "Data Wrangler provides built-in median imputation and log transform steps and can export directly to Feature Store, minimizing custom code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A streaming application ingests JSON records into Amazon Kinesis Data Streams. Each record contains nested user attributes and a categorical field with dozens of categories. You must extract the nested field, one-hot encode the categorical feature, and persist the output in S3 in Parquet format. Which solution is most operationally efficient?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Kinesis Data Analytics for Apache Flink with SQL to flatten JSON, then implement a user-defined function for one-hot encoding, and sink to S3.",
      "B": "Trigger an AWS Lambda function on Kinesis shards to parse JSON, use pandas.get_dummies for encoding, and write Parquet to S3.",
      "C": "Use SageMaker Data Wrangler with a streaming data source, apply transforms, and export to S3.",
      "D": "Use AWS Glue Streaming ETL with Apache Spark: apply JSON extract, use Spark ML OneHotEncoderEstimator, and write Parquet to S3."
    },
    "explanation": "AWS Glue Streaming ETL natively supports Spark transformations on streaming data, including JSON flattening and OneHotEncoder, with minimal custom code and built-in S3 sink."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You have a multi-class categorical feature with 1000 unique values, most of which appear in fewer than 5% of the records. You need to encode it for a tree-based model without creating high dimensionality. Which feature-engineering technique should you apply in SageMaker Data Wrangler?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use one-hot encoding with drop-last to reduce one dimension.",
      "B": "Apply label encoding directly to assign integer codes.",
      "C": "Group infrequent categories into an \"Other\" bucket, then apply one-hot encoding on the top categories.",
      "D": "Apply ordinal encoding based on average target probability per category."
    },
    "explanation": "Combining infrequent categories into \"Other\" reduces cardinality before one-hot encoding, balancing dimensionality and model interpretability."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A text column contains customer reviews that must be tokenized and vectorized for an NLP model. You need to integrate this into a SageMaker feature-engineering pipeline with minimal custom code. Which service and sequence accomplishes this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Glue DataBrew: add tokenization recipe, export to S3, and then train with a custom tokenizer.",
      "B": "Use a SageMaker Processing job with the Hugging Face tokenizer, then push token IDs to Feature Store.",
      "C": "Use SageMaker Data Wrangler: add a built-in text tokenizer transform, then export token counts to Feature Store.",
      "D": "Stream reviews through Lambda to call Amazon Comprehend for tokenization, then store results in S3."
    },
    "explanation": "A SageMaker Processing job with Hugging Face tokenizer integrates easily into pipelines and can write token IDs directly to Feature Store."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You need to discretize a continuous feature into bins of equal frequency and standardize another feature to zero mean and unit variance, using managed AWS services. Which combination is most appropriate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Data Wrangler recipe: add quantile binning for equal-frequency bins and Z-score normalization for standardization.",
      "B": "Use AWS Glue DataBrew: add equal-width binning recipe and MinMax scale recipe.",
      "C": "Run a SageMaker Processing job with a Scikit-Learn pipeline that uses KBinsDiscretizer and StandardScaler.",
      "D": "Use AWS Glue ETL: write PySpark code to bucketBy frequency and use VectorAssembler with StandardScaler."
    },
    "explanation": "Data Wrangler supports quantile binning (equal frequency) and Z-score normalization out of the box, minimizing custom code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "Your dataset contains duplicate records due to multiple ingestion systems. You need a low-code AWS solution to deduplicate by a composite key and write the result back to S3. Which approach is best?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Use SageMaker Data Wrangler: add a drop duplicates recipe on the composite key, then export to S3.",
      "B": "Run a Glue Spark ETL job with custom PySpark dropDuplicates call on the composite key.",
      "C": "Use AWS Glue DataBrew: create a project, add a dedup recipe on the composite key, and publish the output to S3.",
      "D": "Implement a SageMaker Processing job with Pandas drop_duplicates and write to S3."
    },
    "explanation": "DataBrew provides a built-in deduplication recipe that can drop duplicates on any key and export to S3 without custom code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You must engineer features that capture hour-of-day and day-of-week from a timestamp column for downstream modeling. The solution must integrate with SageMaker Pipelines and Feature Store. Which components do you use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A SageMaker ProcessingStep using a custom Pandas script to extract features and call PutRecord to Feature Store.",
      "B": "A SageMaker Data Wrangler step in the pipeline: use built-in ExtractTimestamp transforms for hour and weekday, and export to Feature Store.",
      "C": "An AWS Glue job to run Python code, write to S3, then a ProcessingStep to ingest into Feature Store.",
      "D": "Lambda functions behind EventBridge that trigger on S3 updates to compute features and write to Feature Store."
    },
    "explanation": "Data Wrangler can be used as a ProcessingStep in SageMaker Pipelines, with built-in timestamp extract transforms and direct export to Feature Store."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A dataset contains outliers in multiple numeric columns. You need to detect and cap them at the 1st and 99th percentiles before modeling. Which AWS Workflow is most efficient?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a SageMaker Processing job with custom code to compute percentiles and cap outliers.",
      "B": "Use SageMaker Data Wrangler: add an outlier detection transform to compute percentiles and use a value mapping recipe to cap values.",
      "C": "Write a Glue Spark ETL job using approxQuantile to find percentiles and withColumn to cap values.",
      "D": "Use AWS Glue DataBrew: add a percentile filter recipe then a replace recipe to cap extremes."
    },
    "explanation": "Data Wrangler outlier detection can compute percentiles and supports value mapping recipes to cap outliers without custom code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You have a categorical feature with high cardinality and need to generate target-guided encoding (mean target value per category) without leaking test data information. Which pipeline design prevents leakage?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a single SageMaker Processing job to compute global means on full dataset, apply to all splits.",
      "B": "Use Data Wrangler to compute target means on full dataset and join back to features.",
      "C": "Use Glue ETL to compute means on training set and apply same mapping to test set in the same job.",
      "D": "Define two ProcessingSteps in a SageMaker Pipeline: first compute category-target means on the training split only, store mapping; second step applies mapping to train and test splits separately."
    },
    "explanation": "Separating computation on training split from application to test split in different steps prevents target leakage."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A feature column contains text with inconsistent casing and punctuation. You need to normalize the text (lowercase, remove punctuation), tokenize, and compute TF-IDF vectors, using managed AWS services. Which sequence is correct?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker ProcessingStep: run a Scikit-Learn script for lowercasing, regex to remove punctuation, CountVectorizer with TF-IDF transformer.",
      "B": "Use Data Wrangler: add lowercase and remove-pattern transforms, then export tokens to S3 for external TF-IDF calculation.",
      "C": "Use Glue DataBrew recipes: add text lowercase and remove punctuation recipes, then a custom recipe for TF-IDF.",
      "D": "Use AWS Lambda triggered on S3: call Amazon Comprehend to normalize and tokenize, then compute TF-IDF in Lambda."
    },
    "explanation": "A ProcessingStep gives full control to use Scikit-Learn pipeline for normalization and TF-IDF vectorization directly in SageMaker Pipelines."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You need to enrich your dataset with rolling window features (7-day sum and average) on time-series data stored in S3. Which AWS service and pattern is most appropriate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Glue Streaming ETL with sliding window in Spark Structed Streaming to compute rolling features.",
      "B": "Use SageMaker Data Wrangler to define window aggregation transforms directly.",
      "C": "Schedule a SageMaker Processing job with PySpark code that reads S3 parquet, applies window functions, and writes enriched data back to S3.",
      "D": "Use AWS Lambda triggered by new data to update rolling aggregates in Amazon DynamoDB."
    },
    "explanation": "SageMaker Processing with PySpark supports Spark SQL window functions for rolling aggregates in a batch fashion, suitable for scheduled enrichment."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "Your dataset has a highly imbalanced binary label. You need to generate synthetic minority samples using SMOTE within a managed AWS pipeline. Which service and pattern achieves this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Data Wrangler\u2019s built-in synthetic data generation transforms.",
      "B": "Use a SageMaker Processing job with Imbalanced-Learn\u2019s SMOTE, then export to S3.",
      "C": "Use AWS Glue ETL with custom PySpark code to perform SMOTE.",
      "D": "Use AWS Glue DataBrew with a custom recipe for synthetic minority sampling."
    },
    "explanation": "Only a Processing job supports custom Python libraries like Imbalanced-Learn to run SMOTE within a managed pipeline."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You have feature interactions that must be generated (pairwise products) for a logistic regression model. You want to do this at scale on a large dataset in S3. What is the most serverless approach?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run a SageMaker Processing job on large ML instances with a custom script to generate interactions.",
      "B": "Use AWS Glue ETL on Spark with PySpark to compute pairwise products and write back to S3.",
      "C": "Use SageMaker Batch Transform with a script that outputs interactions.",
      "D": "Use AWS Glue DataBrew: create a recipe to generate new columns via formula for each interaction and publish to S3."
    },
    "explanation": "DataBrew scales serverlessly with partitions and can generate new columns via formulas for interactions without managing infrastructure."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You need to normalize a set of numeric features using robust scaling (subtract median and divide by IQR) within SageMaker Pipelines. Which component do you include?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A ProcessingStep that calls AWS Glue Data Quality for robust scaling.",
      "B": "A ProcessingStep that uses a DataBrew project for IQR scaling.",
      "C": "A ProcessingStep with a custom Scikit-Learn RobustScaler script.",
      "D": "A TransformStep using built-in SageMaker XGBoost with robust scaling parameters."
    },
    "explanation": "A ProcessingStep with a custom script is required for RobustScaler since no built-in recipe exists in Data Wrangler or DataBrew."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You must anonymize PII in free-text user comments by replacing names and emails, then vectorize text. Which AWS-managed solution minimizes custom code?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Processing with a custom regex script for PII removal and TF-IDF vectorization.",
      "B": "Use a SageMaker Processing job combining Amazon Comprehend PII entity detection API for redaction, then Scikit-Learn vectorizer.",
      "C": "Use AWS Glue DataBrew with PII detection recipes and export tokens to S3 for vectorization.",
      "D": "Use AWS Lambda to call Comprehend for redaction, store in S3, then run DataWrangler for vectorization."
    },
    "explanation": "A single Processing job can orchestrate Comprehend API calls and vectorization in one managed step."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You need to transform multi-value categorical fields (lists of tags per item) into indicator features and store them for low-latency inference. Which pipeline configuration should you choose?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "In SageMaker Data Wrangler, use the split multi-value transform to explode tags into rows, one-hot encode them, pivot back to wide format, and export to online Feature Store.",
      "B": "In Glue ETL, use PySpark explode then OneHotEncoderEstimator and write to S3 for offline Feature Store.",
      "C": "Use Lambda functions to parse tags into booleans and write JSON to DynamoDB.",
      "D": "Use SageMaker Processing with pandas to create indicator columns and write to S3."
    },
    "explanation": "Data Wrangler supports split multi-value transforms and direct export to online Feature Store for low-latency inference."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A dataset contains decimal representations of categorical codes. You need to convert these codes to binary bit-vector features (one bit per significant bit) for modeling. Which approach is most straightforward using Data Wrangler?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Use a ProcessingStep with custom Python bit-operation code.",
      "B": "Use Glue ETL with PySpark bit operations in withColumn.",
      "C": "Use SageMaker Data Wrangler\u2019s custom transform node: write a small Pandas UDF to convert codes to bit vectors.",
      "D": "Use DataBrew with multiple formula recipes to extract bits manually."
    },
    "explanation": "A custom transform node in Data Wrangler allows concise Pandas UDF bit extraction without full ETL management."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You want to join two large datasets in S3 by a composite key, perform feature computations (ratio of two columns), and write the result in parquet. Which solution scales most cost-effectively?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Data Wrangler to join and compute ratio then export.",
      "B": "Use AWS Glue ETL on Apache Spark with pushdown predicates and write to S3.",
      "C": "Use a SageMaker Processing job with PySpark on large instances.",
      "D": "Use AWS Glue DataBrew: import both datasets, join in project, and publish result."
    },
    "explanation": "AWS Glue ETL offers serverless Spark with pushdown and partition pruning, minimizing cost for large joins."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "Your feature store offline table has outdated records. You need to batch ingest new features while preserving historical records for audit. Which pattern do you follow?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Overwrite the offline store using Feature Group import mode FULL Refresh.",
      "B": "Use a Glue job to append to the offline table in the Feature Store\u2019s backing S3 bucket.",
      "C": "Use a ProcessingJob to write new features to the same S3 prefix without Feature Store APIs.",
      "D": "Use SageMaker Feature Store Batch PutRecord API to append new records, which preserves history if record identifier and event time increase."
    },
    "explanation": "Batch PutRecord appends new feature values and keeps older versions for audit based on eventTime."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You need to generate polynomial features (up to degree 3) on a subset of continuous variables for a regression model, using SageMaker Pipelines. Which step do you include?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A ProcessingStep that runs a Scikit-Learn PolynomialFeatures transformer on the selected columns.",
      "B": "A TransformStep using XGBoost\u2019s built-in polynomial feature parameters.",
      "C": "A Data Wrangler step with custom Python recipe for polynomial features.",
      "D": "A Glue ETL job scheduled separately and results stored in S3."
    },
    "explanation": "A ProcessingStep allows direct use of Scikit-Learn PolynomialFeatures within the pipeline."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "Your data contains timestamp strings in multiple formats. You must standardize to ISO8601 before downstream feature extraction. Which AWS service and method handle this with minimal code?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Processing with a custom regex parser to normalize formats.",
      "B": "Use Glue ETL with custom Python UDF in Spark to parse and format timestamps.",
      "C": "Use SageMaker Data Wrangler: add multiple parse-date recipes with input patterns, unify output to ISO8601, then export.",
      "D": "Use DataBrew: define multiple date parsing recipes and publish standardized data."
    },
    "explanation": "Data Wrangler supports multiple parse-date recipes and an output format selection, reducing custom code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You must encode a hierarchical categorical feature (e.g., Country > State > City) to capture both levels for a tree model. Which sequence in Data Wrangler best preserves hierarchy?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "One-hot encode City only.",
      "B": "Create combined category codes (Country_State_City) then one-hot encode top N combined categories and bucket rest as Other.",
      "C": "Label encode each level separately.",
      "D": "Use Hash encoding on City and ignore higher levels."
    },
    "explanation": "Creating combined codes preserves hierarchy and one-hot encoding top combinations captures joint distribution with controlled dimensionality."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A dataset includes text descriptions with HTML markup. You must remove tags, lowercase, and compute the length of each description. Which managed flow meets this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Data Wrangler: add a custom transform node that runs a small Python function to strip HTML, lowercase, and compute length.",
      "B": "Use a Glue ETL job with BeautifulSoup and Python to process, then write to S3.",
      "C": "Use AWS Lambda triggered on S3 to clean text and store lengths in DynamoDB.",
      "D": "Use Glue DataBrew recipes: remove HTML pattern, lowercase, compute new column via formula."
    },
    "explanation": "DataBrew\u2019s formula for length may be limited; Data Wrangler custom node allows running Python code for HTML stripping and length calculation."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "Your model benefits from interaction terms between a date and a numeric feature. You need to encode weekday \u00d7 metric interactions. Which pipeline component choice is correct?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use DataBrew: create numeric \u00d7 weekday formula recipes.",
      "B": "Use Glue ETL: write PySpark to compute interaction terms.",
      "C": "Use a SageMaker ProcessingStep with a Pandas script to extract weekday and multiply by the metric, then export.",
      "D": "Use Data Wrangler: extract weekday and use the \u201cGenerate feature combinations\u201d transform."
    },
    "explanation": "Data Wrangler does not auto-generate cross features; a ProcessingStep with Pandas gives precise control for interaction terms."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You have high-cardinality ordinal data where label encoding might mislead the model. You need ordinal encoding that respects order but maps to a uniform distribution. Which transform do you apply?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use one-hot encoding after sorting ordinal values.",
      "B": "Use Quantile ordinal encoding: map ordinal rank to quantile value between 0 and 1 via Data Wrangler\u2019s custom transform.",
      "C": "Use label encoding directly.",
      "D": "Use binary encoding to reduce dimensionality."
    },
    "explanation": "Quantile ordinal encoding maps each ordinal category to its quantile position, preserving order without magnitude assumptions."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You need to integrate SageMaker Ground Truth labeled data into your feature-engineering flow in Data Wrangler, then merge those labels with features. Which pattern works?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Import the Ground Truth output manifest into Data Wrangler via import manifest, join on record IDs, and proceed with transforms.",
      "B": "Use Glue ETL to read manifest JSON and your features, join, then export to S3.",
      "C": "Use a ProcessingJob to read S3 manifest and feature data and merge in code.",
      "D": "Configure DataBrew to read manifest and features and merge via project join recipes."
    },
    "explanation": "Data Wrangler can directly import a Ground Truth manifest and join on record identifiers within the flow."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "During feature engineering, you need to normalize numeric fields per group (e.g., by customer segment) to zero mean within each group. Which service supports this grouping and transform without custom code?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "SageMaker Processing job with Pandas groupby and transform.",
      "B": "AWS Glue Streaming ETL or Glue batch ETL? Actually Data Wrangler? But grouping per segment: Data Wrangler supports groupby transforms in custom? It doesn't. So correct: ProcessingStep.",
      "C": "SageMaker Data Wrangler: add an \u201cAggregate\u201d transform with groupby and join back.",
      "D": "AWS Glue DataBrew: add \u201cGroup\u201d recipe with normalization."
    },
    "explanation": "Data Wrangler does not natively support group-based normalization; you need a custom script in a ProcessingStep to group and normalize."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You want to cleanse geographical coordinate outliers using interquartile range and then project lat/long into distance features from a fixed point. Which pipeline step do you use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A SageMaker ProcessingStep running a Python script that computes IQR, filters outliers, then applies haversine distance formula.",
      "B": "Data Wrangler recipes: outlier removal and geospatial transform.",
      "C": "Glue DataBrew: percentile filter and custom geospatial formula.",
      "D": "Lambda functions for geospatial cleaning and a Data Wrangler step for distance."
    },
    "explanation": "No managed transform for geospatial distance; ProcessingStep with Python gives full control for outlier filtering and distance computation."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A categorical feature contains hierarchical JSON structures per row. You need to flatten to multiple indicator features at any depth. Which approach scales best with minimal code?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Processing with a recursive JSON flattener in Python.",
      "B": "Use AWS Glue ETL on Spark with from_json and explode operations to flatten, then OneHotEncoderEstimator.",
      "C": "Use DataBrew JSON flatten recipe then one-hot encode.",
      "D": "Use Data Wrangler custom transform to flatten JSON manually."
    },
    "explanation": "Glue ETL\u2019s built-in from_json and explode support scalable JSON flattening; Spark\u2019s OneHotEncoder completes encoding."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "You must generate time-lagged features (t-1, t-2) for a time series dataset with irregular intervals. Which solution handles irregular sampling properly?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Use Glue ETL with window functions assuming regular partitions.",
      "B": "Use Data Wrangler\u2019s lag transform with fixed intervals.",
      "C": "Use a ProcessingStep with pandas shift (but assumes uniform index).",
      "D": "Use a SageMaker Processing job with custom script that resamples by timestamp and computes lag based on nearest previous record."
    },
    "explanation": "Irregular intervals require custom resampling and lag logic in code, best done in a ProcessingStep."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "Your model requires polynomial and interaction features, missing-value flags, and one-hot encodings in one cohesive flow. Which AWS-managed solution is most integrated?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Glue ETL job with custom PySpark combining all transformations.",
      "B": "SageMaker Processing job orchestrating multiple scripts.",
      "C": "SageMaker Data Wrangler flow: use built-in polynomial, fill missing flag, encoding, and export to Feature Store.",
      "D": "DataBrew project with custom recipes for each transformation."
    },
    "explanation": "Data Wrangler supports polynomial features, missing-value flagging, and categorical encodings in a single flow with direct exports."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A bank is building a credit approval model. The training dataset contains a binary \u201capproved\u201d label and a protected \u201cgender\u201d feature. Before training, the ML engineer needs to quantify label imbalance (difference in approval rates) between male and female applicants. Which SageMaker Clarify configuration correctly measures this metric?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run a Clarify \u201cdataset_analysis\u201d job and specify the \u201cmean_difference\u201d metric for the \u201cgender\u201d feature.",
      "B": "Run a Clarify pre_training_bias job, set facet_name to \u201cgender\u201d, label_values_or_threshold to ['approved'], and include the \u201cdifference_in_proportions\u201d metric.",
      "C": "Run a Clarify post_training_bias job after model training and view SHAP importances for \u201cgender.\u201d",
      "D": "Use SageMaker Model Monitor baseline for data quality to detect label skew."
    },
    "explanation": "Pre-training bias jobs with facet_name and difference_in_proportions compute DPL for the protected feature before training."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A healthcare provider must de-identify PHI in free-text clinical notes stored on S3 before any ML processing. Which AWS service and approach automatically finds and redacts PHI entities in text files?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon Comprehend Medical de-identification API to detect and redact PHI, then store de-identified text back in S3.",
      "B": "Use AWS Glue DataBrew PII masking transform to remove PHI patterns.",
      "C": "Use Amazon Macie to classify S3 objects and replace PHI entities.",
      "D": "Run a SageMaker Clarify data quality job to identify and filter PHI."
    },
    "explanation": "Comprehend Medical de-identification API is purpose-built for PII/PHI redaction in clinical text."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "An ML engineer uses an Amazon EFS file system to store HIPAA-regulated data for SageMaker training. To meet compliance, they must encrypt data at rest and in transit. Which configuration satisfies both requirements?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable EFS encryption at rest with a customer-managed CMK, create EFS mount targets with TLS, and reference the EFS file system in the training job.",
      "B": "Enable S3 SSE-KMS on the EFS bucket and configure SageMaker network isolation.",
      "C": "Switch to FSx for Lustre with in-transit encryption enabled and mount in training job.",
      "D": "Enable encryption on the training instance root volume and use HTTPS to access EFS."
    },
    "explanation": "EFS supports encryption at rest via CMKs and in-transit TLS mount targets; training jobs can mount encrypted EFS directly."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A regression dataset has 20% missing values in the numeric \u201cincome\u201d column. The engineer must impute missing values by median without leaking test information. Using AWS DataBrew, which sequence of steps is correct?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Profile full dataset to compute global median, then impute \u201cincome\u201d on combined data and split into train/test.",
      "B": "First split dataset into train/test in DataBrew, then apply a recipe step on both sets using the median calculated from the training set.",
      "C": "Run a SageMaker Processing job with scikit-learn SimpleImputer on combined data, then split.",
      "D": "Use AWS Glue ETL to aggregate median and fill missing values, then split datasets."
    },
    "explanation": "To avoid leakage you must split first, compute median on training data, and apply same imputation to test."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "Before training, an engineer must verify that the \u201cuser_id\u201d column in CSV files is non-null and unique. Which AWS Glue Data Quality configuration accomplishes this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Define a Data Quality ruleset with NOT_NULL and UNIQUENESS rules on \u201cuser_id\u201d, run a Data Quality job and inspect violations.",
      "B": "Use SageMaker Data Wrangler profile and manually inspect unique count for \u201cuser_id.\u201d",
      "C": "Write a SageMaker Processing pandas script to drop nulls and duplicates.",
      "D": "Use AWS Glue DataBrew with a data quality job and assertions on \u201cuser_id.\u201d"
    },
    "explanation": "AWS Glue Data Quality jobs with built-in rules handle NOT_NULL and UNIQUENESS assertions at scale."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A large S3 dataset may contain anomalous rows that degrade model accuracy. Which AWS tool combination efficiently detects and removes outliers across multiple features during preprocessing?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Data Wrangler to profile the dataset, apply the Outlier Detection transform, and add recipe steps to filter anomalies.",
      "B": "Write custom Spark in AWS Glue to compute z-scores and drop outliers then load cleaned data to S3.",
      "C": "Run a SageMaker Clarify data bias job to detect feature outliers and filter.",
      "D": "Use AWS Glue Data Quality to generate anomaly reports and manually remove rows."
    },
    "explanation": "Data Wrangler\u2019s built-in outlier detection recipe automates detection and removal within the ETL workflow."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "The S3 training data files are partitioned by date, causing temporal ordering bias. To randomize samples across partitions in SageMaker training with minimal overhead, which option is best?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Specify ShuffleConfig with \u201cFullyReplicated\u201d in the S3 input channel of the training job.",
      "B": "Use AWS Glue DataBrew to shuffle rows within each file and write to new S3 files.",
      "C": "Run a SageMaker Processing job using AWS Wrangler to shuffle and rewrite the dataset.",
      "D": "Use Lambda to concatenate and shuffle data before training."
    },
    "explanation": "SageMaker\u2019s native ShuffleConfig on S3 inputs randomizes across objects without extra ETL."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A dataset contains an \u201cssn\u201d column that must be irreversibly hashed before training. Which AWS service and transform securely performs this operation with minimal code?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use AWS Glue DataBrew \u201cMaskValue\u201d transform on \u201cssn\u201d column with SHA-256 salt and export the recipe.",
      "B": "Write a SageMaker Processing job in Python to hash values and save to S3.",
      "C": "Use AWS Glue Spark ETL with hash functions to overwrite values.",
      "D": "Use SageMaker Clarify anonymization to drop the \u201cssn\u201d field."
    },
    "explanation": "DataBrew\u2019s MaskValue step supports cryptographic hashing with salt in a low-code recipe."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "In a fraud dataset, the minority class is only 1% of records. The engineer wants to quantify this imbalance via SageMaker Clarify and then mitigate it. Which metric and mitigation approach should be used?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use the ClassImbalance metric to measure ratio and apply SMOTE to oversample the minority class.",
      "B": "Use DifferenceInProportions metric and apply random undersampling of the majority class.",
      "C": "Use KLDivergence metric to detect drift and add class weights during training.",
      "D": "Use MutualInformation to rank features and drop low-importance ones."
    },
    "explanation": "ClassImbalance gives the minority ratio; SMOTE is a standard synthetic oversampling method to correct it."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "An organization mandates that all S3 input data for SageMaker training be encrypted with a customer-managed CMK. How should the training job be configured to honor SSE-KMS on input channels?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "In InputDataConfig for each S3 channel, set S3DataDistributionType to \u201cFullyReplicated\u201d and specify KmsKeyId with the CMK ARN.",
      "B": "Enable EncryptVolume on the training instance and rely on default S3 key.",
      "C": "Set EnableNetworkIsolation to true to force KMS encryption.",
      "D": "Configure OutputDataConfig KmsKeyId \u2013 input will inherit the same key."
    },
    "explanation": "Specifying KmsKeyId on InputDataConfig ensures SageMaker uses SSE-KMS to decrypt input data with the CMK."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A deployed ML model processes IoT telemetry. To detect production input drift and alert when numeric features shift beyond a threshold, which AWS service and metric should be used?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Model Monitor with a data quality JobDefinition that uses the KLDivergence metric.",
      "B": "Use SageMaker Clarify explainability to monitor SHAP drift in features.",
      "C": "Use AWS Config to evaluate drift rules on input buckets.",
      "D": "Use AWS CloudWatch anomaly detection on CPU usage of endpoint."
    },
    "explanation": "Model Monitor\u2019s data quality jobs with KLDivergence compare production vs. baseline distributions to detect drift."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A dataset contains an ordinal \u201crating\u201d feature encoded as text (\u201cpoor\u201d, \u201caverage\u201d, \u201cgood\u201d, \u201cexcellent\u201d). To prepare for numeric modeling and avoid bias, which AWS DataBrew transform sequence is correct?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "One-hot encode \u201crating\u201d directly in DataBrew recipe.",
      "B": "Apply label encoding mapping \u201cpoor\u201d\u21920,\u2026\u201cexcellent\u201d\u21923 then one-hot encode in training.",
      "C": "Define custom map transform in DataBrew to assign ordinal values (0\u20133) preserving order, and leave as numeric.",
      "D": "Use SageMaker Clarify to auto-encode text categories during training."
    },
    "explanation": "Ordinal features should be mapped to numeric preserving order; one-hot would lose ordinality."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A regulated dataset includes random noise in a \u201csalary\u201d column to preserve privacy. Before training, the engineer needs to detect abnormal noise levels that might distort the model. Which tool and check accomplish this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Glue DataBrew to profile mean and std deviation of \u201csalary\u201d across batches.",
      "B": "Use SageMaker Clarify data quality job with \u201cstatistics\u201d check to compare batch distributions to baseline.",
      "C": "Use SageMaker Model Monitor feature drift job with PSI metric on \u201csalary.\u201d",
      "D": "Use AWS Glue Data Quality anomaly detection rule on \u201csalary.\u201d"
    },
    "explanation": "Clarify data quality jobs support statistics checks to compare current vs. baseline distributions for numeric features."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A ML pipeline ingests user-submitted text that may contain offensive words. The engineer must remove any rows containing profanity before training. Which approach is most automated?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Glue DataBrew FilterRows with a profanity regex.",
      "B": "Run a custom SageMaker Processing job calling an external profanity API.",
      "C": "Use AWS Glue DataBrew with built-in \u201cReplaceText\u201d transform referencing a profanity wordlist to flag and drop rows.",
      "D": "Use SageMaker Clarify to identify toxic text and filter."
    },
    "explanation": "DataBrew\u2019s ReplaceText transform and conditional filter can flag profanity and drop offending rows in a recipe."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A sensitive column \u201cemail\u201d must be pseudonymized by replacing with reversible tokens, ensuring only the training job can reverse it. Which design below meets this requirement securely?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Use DataBrew MaskValue transform with a static salt stored in S3.",
      "B": "Use SageMaker Processing with Python to hash emails and store hash key on the notebook.",
      "C": "Use AWS Glue ETL to encrypt email using default KMS key without rotation.",
      "D": "Use a SageMaker Processing job that calls AWS KMS GenerateDataKey to encrypt each email, storing ciphertext and encrypted data key columns."
    },
    "explanation": "GenerateDataKey provides a data key for encrypt/decrypt; processing job can encrypt email with the data key and store encrypted data key per row."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A health insurance company needs to predict claim costs using tabular data with dozens of numeric and categorical features. The model must be explainable for compliance and deployed quickly without custom code. Which modeling approach should the ML engineer choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a deep neural network in SageMaker with a custom PyTorch script.",
      "B": "Use the SageMaker built-in XGBoost linear learner in linear-learner mode.",
      "C": "Deploy an Amazon Bedrock foundation model for tabular regression.",
      "D": "Use SageMaker JumpStart to fine-tune a random forest model."
    },
    "explanation": "The SageMaker linear learner in linear mode offers both speed of built-in algorithms and coefficient-based explainability, meeting compliance without custom code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A startup has millions of short text support tickets. They need to automatically categorize them into about ten categories. They require near real-time inference and minimal infrastructure management. Which approach is most appropriate?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Use SageMaker built-in Latent Dirichlet Allocation (LDA) to extract topics.",
      "B": "Train a K-means clustering model on bag-of-words features.",
      "C": "Use Amazon Comprehend custom classification endpoints.",
      "D": "Fine-tune a SageMaker JumpStart GPT model for classification."
    },
    "explanation": "Amazon Comprehend custom classification provides a managed, low-latency service optimized for text classification without heavy infrastructure or custom training."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A financial firm must model customer churn using a dataset of 100,000 rows and 200 features, many of which are sparse. They need both high accuracy and moderate interpretability. Which algorithm should they select?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Regularized gradient-boosted trees with SHAP explainability in SageMaker.",
      "B": "Deep neural network with attention layers in SageMaker Training.",
      "C": "K-nearest neighbors classifier on the full feature set.",
      "D": "SageMaker linear learner without feature interactions."
    },
    "explanation": "Gradient-boosted trees provide strong accuracy on structured data and SHAP integration yields moderate interpretability, balancing both requirements."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A retailer wants to forecast hourly product demand across 500 stores. They need a model that handles seasonality and multiple time series simultaneously with minimal custom development. Which AWS capability should they use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Build a Seq2Seq model in PyTorch with SageMaker script mode.",
      "B": "Use SageMaker built-in DeepAR forecasting.",
      "C": "Fine-tune a Bedrock LLM for time series forecasting.",
      "D": "Use Amazon Forecast with a custom recipe for seasonal demand."
    },
    "explanation": "Amazon Forecast is a managed service specialized in multi-dimensional time series forecasting with built-in seasonality handling and minimal development."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "An e-commerce company needs to detect anomalous orders in real time. The data has 50 continuous and categorical features. They want full control over algorithm selection and hyperparameters. Which modeling approach is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train an Isolation Forest using SageMaker built-in with batch inference enabled.",
      "B": "Deploy Amazon Lookout for Metrics for anomaly detection.",
      "C": "Use SageMaker KNN anomaly detection with default parameters.",
      "D": "Fine-tune a JumpStart anomaly detection model on Amazon Bedrock."
    },
    "explanation": "SageMaker built-in Isolation Forest gives full control over hyperparameters and supports custom batch inference while being managed."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A logistics company wants to segment customers into behavioral clusters using both shipment frequency and cost. They expect clear cluster centroids for interpretability. Which algorithm should they choose?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "DBSCAN clustering in SageMaker Processing.",
      "B": "Hierarchical clustering in SageMaker Spark.",
      "C": "Gaussian Mixture Model with EM in SageMaker.",
      "D": "K-means clustering with the SageMaker built-in algorithm."
    },
    "explanation": "K-means provides clear centroids and is available as a built-in SageMaker algorithm, balancing interpretability and managed operation."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A media company needs to recommend movies using collaborative filtering on user-item ratings. They require a scalable solution integrated with SageMaker. Which modeling approach fits?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a matrix factorization model in Scikit-Learn on SageMaker Training.",
      "B": "Use Amazon Personalize batch recommendations.",
      "C": "Use SageMaker built-in factorization machines algorithm.",
      "D": "Fine-tune a JumpStart recommender system foundation model."
    },
    "explanation": "SageMaker factorization machines are optimized for collaborative filtering, scalable, and integrated into SageMaker workflows."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A biotech startup must classify protein sequences. Labeled data is scarce, and interpretability is not critical. They want to leverage pretrained models. Which approach is most appropriate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a CNN from scratch in SageMaker script mode.",
      "B": "Fine-tune a JumpStart protein foundation model.",
      "C": "Use SageMaker built-in XGBoost with k-mer features.",
      "D": "Develop a random forest on physicochemical features."
    },
    "explanation": "JumpStart protein foundation models leverage pretrained knowledge, requiring limited data and engineering, ideal for scarce labels without interpretability needs."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A compliance-driven enterprise must predict default risk on personal loans. They need to justify each decision to auditors. Which modeling approach balances accuracy and full explainability?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker built-in linear learner and present feature coefficients.",
      "B": "Train an XGBoost model and approximate SHAP explanations.",
      "C": "Deploy an Amazon Bedrock LLM to explain historical decisions.",
      "D": "Use DeepAR to forecast default probabilities."
    },
    "explanation": "Linear learner provides transparent coefficients for direct auditing, ensuring full explainability though with modest accuracy trade-off."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A gaming company needs to classify images of player avatars into categories. They want to minimize GPU cost and only require coarse-grained accuracy. Which SageMaker option should they choose?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Fine-tune a JumpStart ResNet model.",
      "B": "Train a custom CNN in PyTorch on GPU.",
      "C": "Use SageMaker built-in image classification algorithm on CPU.",
      "D": "Deploy Amazon Rekognition with custom labels."
    },
    "explanation": "The built-in image classifier can run on CPU for coarse tasks with lower cost, avoiding GPU and heavy customization."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "An IoT company collects multivariate sensor time series and needs to detect anomalies streaming. They prefer managed services and minimal model maintenance. Which approach is best?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Train an LSTM autoencoder in SageMaker script mode.",
      "B": "Use the SageMaker built-in random cut forest algorithm in batch.",
      "C": "Fine-tune an LLM for time series anomaly detection.",
      "D": "Use Amazon Lookout for Equipment for streaming anomalies."
    },
    "explanation": "Amazon Lookout for Equipment is managed, supports streaming anomaly detection on sensor data with minimal maintenance."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A marketing team wants to segment emails by topic automatically. They need a fully managed NLP solution that adapts over time. Which AWS service should they use?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Train LDA in SageMaker Processing.",
      "B": "Use Amazon Comprehend topic modeling APIs.",
      "C": "Fine-tune a JumpStart GPT-3 model for topic extraction.",
      "D": "Use Amazon SageMaker BlazingText for unsupervised topic clusters."
    },
    "explanation": "Amazon Comprehend provides managed topic modeling that adapts to new data without custom training."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A telecom wants to predict customer churn with an imbalanced dataset and needs to prioritize recall over precision. They plan to tune thresholds. Which algorithm and metric combination should they choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train XGBoost in SageMaker and optimize for recall using hyperparameter tuning with custom F-beta metric.",
      "B": "Use logistic regression and evaluate accuracy.",
      "C": "Deploy random forest and adjust class weights only at inference.",
      "D": "Use built-in factorization machines with default evaluation."
    },
    "explanation": "XGBoost with custom metric optimization allows tuning hyperparameters for recall, matching business priority on false negatives."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A media analytics team needs to extract entities from video transcripts. They require deep customization of entity categories and managed model hosting. What should they choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a custom spaCy NER model on SageMaker Processing.",
      "B": "Fine-tune a JumpStart language model and deploy on SageMaker.",
      "C": "Use Amazon Comprehend custom entity recognition endpoints.",
      "D": "Use Amazon Textract to extract text and regex on transcripts."
    },
    "explanation": "Comprehend custom entity recognition offers managed training and hosting with customizable entities, minimizing infra effort."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A fraud detection use case has a growing number of streaming events per second and requires sub-millisecond inference. They need high throughput and low latency with minimal operational overhead. Which approach fits?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "Deploy XGBoost model on SageMaker real-time endpoint on CPU.",
      "B": "Use SageMaker serverless inference with ProvisionedConcurrency.",
      "C": "Deploy model on ECS Fargate behind API Gateway.",
      "D": "Use Lambda to host model via Docker image."
    },
    "explanation": "Serverless inference with provisioned concurrency handles variable volumes and low latency without managing servers."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A retailer wants price elasticity estimates per product category, requiring interpretable coefficients. They have limited compute budget. Which modeling approach should they select?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Train a tree-based ensemble and use SHAP.",
      "B": "Fine-tune a Bedrock regression foundation model.",
      "C": "Use SageMaker XGBoost regressor default.",
      "D": "Use SageMaker linear learner with L1 regularization for sparse features."
    },
    "explanation": "Linear learner yields direct coefficients for price elasticity, with L1 handling sparsity and minimal compute overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "An ML engineer must automatically classify customer reviews into positive or negative sentiment. They need near zero code and managed endpoints. Which AWS service should they choose?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Build a logistic regression in SageMaker script mode.",
      "B": "Use Amazon Comprehend sentiment analysis endpoints.",
      "C": "Fine-tune a JumpStart BERT model.",
      "D": "Train SageMaker built-in BlazingText skip-gram model."
    },
    "explanation": "Comprehend provides out-of-the-box sentiment analysis API with no code and managed hosting."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A company has sparse high-dimensional clickstream data and wants a scalable algorithm for binary classification. They prioritize L1 regularization to perform feature selection. Which SageMaker built-in algorithm should they choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker built-in linear learner with L1 mode.",
      "B": "Built-in XGBoost with default settings.",
      "C": "Built-in KNN.",
      "D": "Built-in K-means."
    },
    "explanation": "Linear learner supports L1 regularization for feature selection and scales to high dimensions."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A healthcare provider needs to detect disease from X-ray images with regulatory constraints on data privacy. They want to use a managed service and avoid moving data off-premises. Which solution is best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker built-in image classification in public endpoints.",
      "B": "Deploy a JumpStart Vision transformer in SageMaker Studio.",
      "C": "Configure SageMaker Inference on AWS Outposts with the built-in image classification algorithm.",
      "D": "Use Amazon Rekognition for custom labels."
    },
    "explanation": "SageMaker on AWS Outposts runs locally, preserving data privacy without sending images off-premises."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A startup needs to detect default probability from loan applications with limited GPU budget. They require class probability outputs and fast training. Which algorithm should they pick?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deep AR forecasting model.",
      "B": "Built-in XGBoost classifier.",
      "C": "Built-in K-means clustering.",
      "D": "Built-in PCA."
    },
    "explanation": "XGBoost classifier offers fast training on CPU/GPU, outputs probabilities, and is cost-efficient."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A legal firm wants to identify key contract clauses in documents. They need a managed NLP solution they can tailor without hosting infrastructure. Which should they use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Comprehend custom classification for clause categories.",
      "B": "Fine-tune JumpStart GPT-3 and host on SageMaker.",
      "C": "Use SageMaker built-in BlazingText.",
      "D": "Train a custom BERT model in SageMaker script mode."
    },
    "explanation": "Comprehend custom classification lets them define clause categories and uses managed endpoints without infra overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A search engine needs to rank results by relevance using user click logs. They require pairwise ranking and integration with SageMaker. Which algorithm is suitable?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Built-in random forest classifier.",
      "B": "Built-in KNN algorithm.",
      "C": "DeepAR forecasting.",
      "D": "SageMaker Factorization Machines with ranking objective."
    },
    "explanation": "Factorization Machines support pairwise ranking objectives and integrate seamlessly in SageMaker pipelines."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "An agricultural startup wants to detect crop diseases from leaf images. They have limited labeled data and need transfer learning with minimal fine-tuning. Which approach should they choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a CNN from scratch in PyTorch on SageMaker.",
      "B": "Use SageMaker built-in XGBoost on flattened image features.",
      "C": "Fine-tune a JumpStart transfer learning image classification model.",
      "D": "Deploy Amazon Rekognition Custom Labels."
    },
    "explanation": "JumpStart transfer learning models require minimal labeled data and fine-tuning for image tasks, reducing effort."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A robotics company collects multi-modal sensor data (images and lidar). They need a model that natively handles heterogeneous inputs. Which modeling approach is best?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "Ensemble separate built-in algorithms for each modality.",
      "B": "Train a multimodal PyTorch model in SageMaker script mode.",
      "C": "Use Amazon SageMaker built-in Linear Learner with concatenated features.",
      "D": "Fine-tune a JumpStart text foundation model."
    },
    "explanation": "A custom PyTorch script enables combining different data types in a single model, which built-ins cannot natively handle."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A voice assistant service must detect five spoken commands in streaming audio. They need low latency and a managed solution. Which AWS offering should they use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a CNN in SageMaker to classify spectrograms.",
      "B": "Use SageMaker built-in KNN on MFCC vectors.",
      "C": "Fine-tune a JumpStart Transformer.",
      "D": "Use Amazon Transcribe to convert speech to text and Comprehend custom classification."
    },
    "explanation": "Transcribe plus Comprehend custom classification offers managed low-latency command detection without building audio models."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A rideshare company needs to price surge multipliers based on real-time location and demand. They require sub-second inference at scale. Which modeling approach and deployment should they choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a decision tree in SageMaker and deploy on CPU endpoint.",
      "B": "Use SageMaker real-time multi-model endpoints with XGBoost model.",
      "C": "Host a neural network on ECS Fargate.",
      "D": "Use Amazon Forecast predictor endpoint."
    },
    "explanation": "XGBoost on multi-model endpoints provides low-latency at scale with minimal endpoint overhead for multiple regions."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "A financial services company observes that their fraud-detection model has high overall accuracy but low recall on high-value transactions. They suspect the model underfits rare, high-value cases. Which of the following evaluation strategies best identifies this issue?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Plot a single ROC curve on the full dataset and inspect AUC",
      "B": "Compute macro-averaged precision across all transaction values",
      "C": "Segment the test set by transaction value and compute per-segment recall",
      "D": "Evaluate overall F1 score without stratification"
    },
    "explanation": "Segmenting by transaction value and computing recall highlights underperformance on high-value cases. Overall AUC or aggregated metrics mask segment-specific issues."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "An ML engineer deploys two XGBoost models: a control and a new variant. After 1M predictions, the variant shows a slightly higher latency but 2% higher recall. How should they evaluate if this trade-off is acceptable?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Compare single-sample inference logs for outliers",
      "B": "Plot precision-recall curves and latency distributions for both variants",
      "C": "Compute overall accuracy for both and pick model with higher accuracy",
      "D": "Use confusion matrix of the slower model only"
    },
    "explanation": "Precision-recall curves reveal how recall gain trades off against precision, while latency distributions quantify performance cost. Overall accuracy and single-sample latency don\u2019t capture distribution details."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "During hyperparameter tuning of a neural network, SageMaker Debugger reports gradient vanishing in early layers. Which metric or visualization should you use to pinpoint the affected layers?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Plot TensorBoard histograms of weight and gradient distributions per layer",
      "B": "Inspect model\u2019s overall loss curve only",
      "C": "Compute validation accuracy after each epoch",
      "D": "View average prediction confidence on test data"
    },
    "explanation": "Layer-wise weight and gradient histograms directly show vanishing or exploding gradients. Loss curves and accuracy trends are too coarse to locate the problem."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "A classification model shows 95% accuracy but customers complain of frequent false positives. Which metric will best capture the operational false-positive rate?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ROC AUC",
      "B": "Accuracy",
      "C": "Macro F1 score",
      "D": "False positive rate (FP/(FP+TN))"
    },
    "explanation": "False positive rate directly measures proportion of non-events incorrectly flagged. Aggregate metrics and AUC do not quantify FP in production terms."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "An ML engineer wants to compare a shadow variant\u2019s performance to production. They have stored logs with predictions and ground truth over a week. What\u2019s the most reproducible approach?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Re-run inference on production endpoint and compute metrics",
      "B": "Compute metrics in real time on live data",
      "C": "Use the stored logs to batch calculate identical evaluation metrics for both models",
      "D": "Use shadow variant accuracy reported in CloudWatch metrics"
    },
    "explanation": "Batch calculating evaluation metrics on the same logged data ensures reproducibility. CloudWatch reports may differ in metric definitions, and re-running inference can produce drift."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "After deploying a regression model, business users report that error increases when feature \"loan_amount\" exceeds $100k. Which diagnostic step is most appropriate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Check overall RMSE",
      "B": "Plot residuals against loan_amount",
      "C": "Compute R\u00b2 on training data",
      "D": "Re-train model without loan_amount"
    },
    "explanation": "Residual-vs-feature plots reveal heteroscedasticity or feature-specific error patterns. Overall RMSE or training R\u00b2 mask inequalities across feature ranges."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "SageMaker Clarify indicates a high feature importance contribution from \"zipcode\" in loan approval model. Finance team worries about proxy bias. What evaluation should be done next?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Compute disparity metrics for protected groups defined by zipcode demographics",
      "B": "Remove zipcode and retrain immediately",
      "C": "Ignore since importance doesn\u2019t imply bias",
      "D": "Compute overall feature correlation matrix"
    },
    "explanation": "High importance for zipcode may proxy demographic bias; computing disparity metrics for protected groups reveals actual bias. Simply removing the feature may degrade performance without confirming bias."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "A model\u2019s AUC on validation is 0.85 but drops to 0.75 in production. Which step best isolates the cause?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase regularization to reduce overfitting",
      "B": "Retrain with more epochs",
      "C": "Change the threshold to match validation TPR",
      "D": "Compare feature distribution between validation and production using Clarify data drift reports"
    },
    "explanation": "Distribution drift analysis identifies covariate changes causing performance drop. Regularization and threshold tuning don\u2019t address data mismatch."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "During batch experiments, an engineer notices that increasing tree depth raises precision but lowers recall. Which plot best illustrates this trade-off?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Precision-recall curve parametrized by depth",
      "B": "ROC curve at each depth",
      "C": "Confusion matrix for the deepest tree only",
      "D": "Histogram of prediction confidences"
    },
    "explanation": "Precision-recall curves for each depth show the precision/recall trade-off directly. ROC curves and single-depth confusion matrices do not illuminate that trade dynamically."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "After hyperparameter tuning, two models have identical validation loss but different generalization gaps. Which metric indicates stronger overfitting?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Higher training loss",
      "B": "Larger difference between training and validation accuracy",
      "C": "Higher validation AUC",
      "D": "Lower inference latency"
    },
    "explanation": "A large gap between training and validation accuracy indicates overfitting. Validation AUC and latency don\u2019t measure overfitting directly."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "An NLP classification model shows significantly higher F1 for short texts versus long texts. How do you quantify performance disparity?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Overall F1 score",
      "B": "Word count distribution histogram",
      "C": "Group test samples by text length bins and compute per-bin F1",
      "D": "Pearson correlation between length and predicted label"
    },
    "explanation": "Bin by text length and compute F1 per bin to quantify disparity. Overall metrics and correlation don\u2019t isolate the problem."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You need to determine if a drop in model accuracy after a code change is due to random seed variance or genuine performance loss. Which practice ensures reproducibility?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Fix random seed across training, log model parameters, and compare artifacts",
      "B": "Re-run training with different seeds and average",
      "C": "Use early stopping to stabilize performance",
      "D": "Only compare inference results on live data"
    },
    "explanation": "Fixing seeds and logging artifacts ensures reproducible runs. Averaging across seeds helps smooth noise but doesn\u2019t isolate change impact."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "An engineer suspects label leakage because a high validation AUC is too good to be true. Which diagnostic will reveal leakage?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase regularization",
      "B": "Shuffle labels and retrain to observe AUC",
      "C": "Add more layers",
      "D": "Compute feature correlations only"
    },
    "explanation": "Shuffling labels destroys true signal; if AUC remains high, leakage exists. Feature correlations alone may miss leakage patterns."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "A deployed model exhibits high memory consumption during inference. Which SageMaker Debugger profiler metric helps pinpoint bottlenecks?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Overall endpoint invocation count",
      "B": "CloudWatch memory usage graphs",
      "C": "Inference latencies per request",
      "D": "Model container\u2019s PeakHostMemoryUsage metric"
    },
    "explanation": "PeakHostMemoryUsage from Debugger profiling identifies model container memory peaks. Invocation counts and latency don\u2019t measure memory usage.\n"
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "When comparing two regression models, you find one has lower MAE but higher RMSE. What does this indicate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "The second model has larger outlier errors",
      "B": "The second model performs better on average",
      "C": "The first model is overfitting",
      "D": "The first model has higher variance"
    },
    "explanation": "Higher RMSE than MAE implies presence of larger outlier errors, since RMSE penalizes them more. Average performance may be similar.\n"
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You deploy an image classifier and monitor per-class recall. Class A recall drops below SLA overnight. Logs show no data drift. What is your next diagnostic?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase batch size",
      "B": "Retrain with more epochs",
      "C": "Analyze inference input distribution for class A by sampling inputs",
      "D": "Reduce learning rate"
    },
    "explanation": "Sampling inputs checks if unexpected inputs are arriving for Class A. No drift in overall features doesn\u2019t guarantee class-specific input change.\n"
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your model\u2019s production log shows many low-confidence predictions. Which threshold-based method helps maintain production SLA?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use maximum probability per sample as a calibration correction",
      "B": "Implement a confidence threshold to send low-confidence cases to manual review",
      "C": "Retrain until all predictions exceed a fixed threshold",
      "D": "Always return top-2 classes to clients"
    },
    "explanation": "A confidence threshold with fallback to manual review maintains SLA. Calibration alone won\u2019t meet SLA guarantees.\n"
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "An ML pipeline reruns hyperparameter tuning daily. You want to compare results over time. Which practice aids trend analysis?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Log only best tuning job\u2019s metrics",
      "B": "Store only final model artifacts",
      "C": "Compare only today's and yesterday's tuning graphs",
      "D": "Archive tuning job metrics and hyperparameter configurations in SageMaker Experiments"
    },
    "explanation": "SageMaker Experiments archives metrics and configs over time, enabling trend and retrospective analysis. Comparing partial data isn\u2019t sufficient.\n"
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "After retraining a model with more data, confusion matrix shows increased FN but decreased FP. Business impact of FNs is higher. How do you adjust the model?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Shift decision threshold to favor recall over precision",
      "B": "Increase regularization",
      "C": "Reduce training data size",
      "D": "Switch to a different algorithm"
    },
    "explanation": "Threshold adjustment trades precision and recall bias to reduce FNs, aligning with higher cost of FNs. Algorithm shift or data size changes are heavier-handed.\n"
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "SageMaker Clarify reports a DPL (difference in proportions of labels) of 0.12 for a protected group. Which conclusion is correct?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model is fair, since DPL < 0.2",
      "B": "There is moderate demographic bias requiring mitigation",
      "C": "DPL only applies to continuous features",
      "D": "Bias can be ignored if overall accuracy is high"
    },
    "explanation": "DPL of 0.12 indicates non-trivial bias that should be addressed. Fairness thresholds vary, but non-zero DPL warrants mitigation. Accuracy doesn\u2019t nullify bias.\n"
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You need to diagnose why training loss plateaus early. Which combined SageMaker Debugger rule is most helpful?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "LossNotDecreasing and LargeGradient",
      "B": "VanishingGradient and OverTraining",
      "C": "LossNotDecreasing and VanishingGradient",
      "D": "OverTraining and HighThroughput"
    },
    "explanation": "LossNotDecreasing flags plateau, VanishingGradient identifies small gradients. Together they pinpoint stalled training due to gradient issues.\n"
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "After enabling endpoint data capture, you observe data skew for feature X. Which remediation reduces skew immediately?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Retrain online with captured data",
      "B": "Increase endpoint instance count",
      "C": "Switch to a different instance type",
      "D": "Implement input feature transformation to normalize X at inference"
    },
    "explanation": "Applying transformation at inference normalizes skewed feature values immediately. Retraining is longer-term and won\u2019t reduce skew on streaming data.\n"
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your binary classifier has high variance between folds in cross-validation. What change will most reduce variance?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use ensembles like random forests",
      "B": "Increase learning rate",
      "C": "Use fewer features",
      "D": "Reduce training set size"
    },
    "explanation": "Ensembling reduces model variance by averaging multiple learners. Lower LR or fewer features may increase bias. Reducing data increases variance.\n"
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "A class-imbalanced dataset yields high recall but poor precision. Which metric-guided tuning step helps restore balance?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase tree depth",
      "B": "Use mean squared error loss",
      "C": "Optimize model using precision-recall AUC as objective",
      "D": "Switch to accuracy objective"
    },
    "explanation": "Using PR AUC aligns hyperparameter tuning to both precision and recall on imbalanced data. MSE or accuracy objectives aren\u2019t appropriate for classification imbalance.\n"
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "An LSTM model overfits after 10 epochs. You add dropout but performance still degrades. Which debugging step helps find root cause?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase L2 regularization",
      "B": "Use Debugger\u2019s tensor shape rule to confirm no unintended dimension mismatch causing overfitting",
      "C": "Increase learning rate",
      "D": "Reduce batch size"
    },
    "explanation": "Tensor shape mismatches can cause incorrect weight updates that appear as overfitting. Debugger shape rules detect such bugs. Regularization alone may mask deeper issues.\n"
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your model produces inconsistent outputs when served on GPU versus CPU instances. How do you isolate source?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Retrain separately on GPU and CPU",
      "B": "Enable inference latency metrics",
      "C": "Use SageMaker Clarify to detect drift",
      "D": "Run inference deterministically on both environments with same input batch and compare element-wise outputs"
    },
    "explanation": "Deterministic inference comparison on CPU vs GPU pinpoints numerical instability or library inconsistency. Drift detection and latency metrics don\u2019t isolate environment differences.\n"
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "A regression model\u2019s residuals exhibit non-constant variance. Which transformation corrects this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Standardize all features",
      "B": "Apply log transform to the target variable",
      "C": "Increase polynomial degree of predictors",
      "D": "Use one-hot encoding for categorical features"
    },
    "explanation": "Log-transforming the dependent variable often stabilizes variance (heteroscedasticity). Feature standardization doesn\u2019t address residual variance issues.\n"
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "After changing preprocessing, model performance unexpectedly improved in test but dropped in validation. Which practice uncovers this data leakage?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Inspect preprocessing pipelines for operations applied before train-test split",
      "B": "Increase test set size",
      "C": "Add more regularization",
      "D": "Use k-fold CV on original split"
    },
    "explanation": "Applying preprocessing (e.g., normalization) before splitting leaks test information. Reviewing pipeline order reveals leakage.\n"
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your model\u2019s precision increases when you add a new feature but recall drops. You need to quantify the net business impact. What analysis should you perform?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Compute new overall F1 score",
      "B": "Plot ROC curve",
      "C": "Estimate expected cost change using per-error cost matrix",
      "D": "Compare AUC-PR before/after"
    },
    "explanation": "Cost matrix analysis translates precision/recall changes into business impact. F1 and curves are abstract performance measures without cost context."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A fintech startup has an image-based document verification model that requires sub-100ms inference latency at the edge. They must deploy the optimized model to ARM-based devices with intermittent connectivity. Which deployment infrastructure best meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Host the model on a SageMaker real-time endpoint in a VPC and use IoT Greengrass to pull predictions.",
      "B": "Compile the model with SageMaker Neo for ARM, deploy to IoT Greengrass devices for local real-time inference.",
      "C": "Package the model in a custom Docker container, deploy to AWS Lambda on ARM architecture.",
      "D": "Use a SageMaker serverless endpoint and cache responses on edge devices."
    },
    "explanation": "SageMaker Neo produces highly optimized ARM binaries for local (<100ms) inference. IoT Greengrass ensures offline capability."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An e-commerce application has a recommendation model that sees large traffic spikes during flash sales. It runs in a private VPC, uses GPU instances, and needs multi-model hosting with auto-scaling. Which architecture is optimal?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Deploy separate single-model SageMaker real-time endpoints per model with target-tracking scaling.",
      "B": "Use a SageMaker serverless endpoint with model registry versioning.",
      "C": "Containerize models and host on ECS Fargate behind an ALB with custom scaling scripts.",
      "D": "Use a multi-model SageMaker real-time endpoint in the VPC with provisioned concurrency and auto-scaling policies."
    },
    "explanation": "Multi-model endpoint reduces overhead, shares GPU, and auto-scaling in VPC meets spikes."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A research team needs to batch-process 10 TB of genomic data weekly. They want to minimize cost, use the same container as real-time inference, and avoid idle pods. Which infra is best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create a SageMaker asynchronous endpoint with batch transform inside it.",
      "B": "Deploy models on EKS pods with KNative autoscaling.",
      "C": "Use SageMaker batch transform jobs with the real-time container and spot instances.",
      "D": "Host on a long-running ECS Fargate service and schedule container tasks."
    },
    "explanation": "Batch transform jobs spin up only for jobs, reuse container, support spot to reduce cost."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A global SaaS company requires GDPR compliance. Their NLP model must serve EU-only traffic in dedicated subnets and scale on demand. Which deployment solution is most appropriate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy regional SageMaker real-time endpoints in EU-based subnets with endpoint auto scaling.",
      "B": "Deploy a single SageMaker serverless endpoint and enforce subnet restrictions.",
      "C": "Host on ECS Fargate in one EU cluster and route all traffic through it.",
      "D": "Use Lambda functions behind API Gateway with VPC peering to subnets."
    },
    "explanation": "Regional real-time endpoints in EU subnets ensure data residency and scalable performance."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A startup needs to test a new fraud-detection model in production without risking customer impact. They want to route 5% of traffic to the new model, track performance, and easily roll back. Which deployment strategy and infra should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy two SageMaker asynchronous endpoints and split payloads manually.",
      "B": "Use a SageMaker serverless endpoint for the new model and shift DNS entries.",
      "C": "Deploy the new model to a SageMaker real-time endpoint variant in the same endpoint, configure a canary traffic shift.",
      "D": "Host the new model in ECS Fargate and use Application Load Balancer weighted routing."
    },
    "explanation": "Real-time endpoint variants support canary deployments and easy rollback within the same endpoint."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An advertising platform has multiple small models (<100MB) that share similar preprocessing code. They need to deploy 50 models, optimize resource utilization, and minimize cost. What is the best approach?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy 50 separate real-time SageMaker endpoints.",
      "B": "Use ECS Fargate and spin up a container per model.",
      "C": "Bundle all models into one Docker image and host on a single serverless endpoint.",
      "D": "Use a SageMaker multi-model endpoint with shared container in a single VPC endpoint."
    },
    "explanation": "Multi-model endpoints share the container and infra, reducing cost and overhead for many small models."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A biotech firm must run long-running inference jobs (up to 30 minutes each) that process large data packages. They want predictable scaling and pay-per-use cost. Which SageMaker deployment fits best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure a SageMaker asynchronous inference endpoint with appropriate max payload size.",
      "B": "Use a SageMaker real-time endpoint and extend Lambda timeouts.",
      "C": "Submit each job as a batch transform.",
      "D": "Host it on ECS Fargate with long-running tasks."
    },
    "explanation": "Asynchronous endpoints handle long payloads, non-blocking, scale predictably and charge per call."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A media company needs GPU acceleration for video-related inference but wants to minimize idle GPU costs during off-hours. They can tolerate 1\u20132 second cold start. Which infra is optimal?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Keep a provisioned GPU SageMaker real-time endpoint running with auto-scaling.",
      "B": "Use SageMaker serverless endpoints configured with GPU support.",
      "C": "Deploy on ECS Fargate GPU instances with manual scaling schedules.",
      "D": "Host on Lambda GPU functions behind an ALB."
    },
    "explanation": "Serverless endpoints with GPU avoid idle costs and manage cold starts within tolerance."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An auto manufacturer needs to deploy multiple model versions concurrently for A/B tests, with equal traffic share and easy rollback. They must run in the same VPC. Which infra supports this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy separate ECS services per version behind an ALB.",
      "B": "Use Lambda versions and aliases in VPC.",
      "C": "Deploy separate SageMaker serverless endpoints per version.",
      "D": "Use a SageMaker real-time endpoint with two production variants and traffic weights."
    },
    "explanation": "Production variants in one real-time endpoint allow weighted traffic splitting and quick rollback."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A retailer needs to deploy a recommendation model to handle unpredictable holiday season traffic. They need <50ms latency, GPU inference, and minimal ops overhead. Which solution is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker serverless endpoints with GPU acceleration and auto-scaling.",
      "B": "Deploy a spot-based ECS GPU cluster with custom autoscaler.",
      "C": "Maintain a provisioned GPU real-time endpoint with scheduled scaling.",
      "D": "Run inference in Lambda with EFS-mounted model files."
    },
    "explanation": "Serverless GPU endpoints auto-scale without manual capacity management and meet latency."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A healthcare application requires batch scoring of PHI data with encryption at rest and in transit. They want to reuse existing VPC resources and avoid cross-account data movement. Best infra?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker serverless endpoint in VPC with data capture.",
      "B": "Real-time endpoint in VPC with KMS encryption.",
      "C": "Batch transform jobs configured in the same VPC and encrypted S3.",
      "D": "ECS Fargate with EFS encryption."
    },
    "explanation": "Batch transform in VPC uses S3 encrypted data, no constant endpoint and reuse VPC security."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A gaming company needs to deploy a scoring model to hundreds of edge kiosks with no always-on internet and limited compute. They need occasional connectivity to sync metrics. Which infra fits?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a SageMaker real-time endpoint through AWS IoT Core.",
      "B": "Compile via SageMaker Neo, deploy to Lambda@Edge on Greengrass devices.",
      "C": "Host containerized model on small EC2 instances at each kiosk.",
      "D": "Deploy inference code as AWS Lambda functions invoked by MQTT."
    },
    "explanation": "Neo-compiled model on Greengrass devices runs locally, syncs periodically over IoT Core."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A logistics company has a micro-batch inference requirement: group deliveries into sets of 100 and process every 5 minutes. They want minimal latency overhead and no idle endpoints. What infrastructure should they choose?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy a real-time endpoint and buffer requests into batches.",
      "B": "Use serverless endpoint with batched invocation.",
      "C": "Submit jobs to ECS Fargate tasks scheduled every 5 minutes.",
      "D": "Configure a SageMaker asynchronous inference endpoint with batching config."
    },
    "explanation": "Asynchronous endpoint supports batching and scheduled triggers without idle infra."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A startup wants to experiment with GPU- and CPU-based inference to compare cost-performance trade-offs. They need a common orchestration tool and seamless model promotion. Which approach is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Pipelines to deploy the same model to GPU and CPU real-time endpoints.",
      "B": "Script deployments in Terraform to provision ECS GPU and CPU clusters.",
      "C": "Use AWS Batch for GPU and Lambda for CPU inference.",
      "D": "Manually launch EC2 GPU and CPU instances and copy containers."
    },
    "explanation": "SageMaker Pipelines automates deployment steps, tracks versions, and supports multiple compute targets."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A financial modeling team needs to deploy Python-based models that require custom system libraries. They want minimal management and autoscaling. Which deployment target is most appropriate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy as a SageMaker serverless endpoint with layers.",
      "B": "Use Lambda with custom runtime layer.",
      "C": "Build a custom Docker container and host on SageMaker real-time endpoint in VPC.",
      "D": "Use ECS Fargate with code-build pipelines."
    },
    "explanation": "Custom container on SageMaker real-time endpoint supports all dependencies, auto-scales, VPC support."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A data science team wants to automate their ML workflow so that any code commit on the \u2018develop\u2019 branch runs unit tests, data validation, and triggers a SageMaker training pipeline. After successful training, the model should be registered and automatically deployed to a staging endpoint for integration tests, before manual approval pushes to production. Which CI/CD design best meets these requirements?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use CodePipeline with a Source action on GitHub \u2018develop\u2019, CodeBuild to run unit tests and data validation scripts, a CodeBuild action to invoke a SageMaker Pipeline for training, a stage to register the model in the Model Registry, a CloudFormation action to deploy to staging, an integration-test CodeBuild action, followed by a manual approval and a Lambda action to swap the staging endpoint to production.",
      "B": "Use SageMaker Pipelines with Git integration triggering on \u2018develop\u2019 for training and registration, then use a separate CodePipeline triggered by new Model Registry entries to deploy to staging and run integration tests, with manual approval for production.",
      "C": "Use CodePipeline with a GitHub source on \u2018develop\u2019, a CodeBuild stage to run unit tests and data quality checks, a SageMaker Pipeline action to train and register the model, a CloudFormation stage to deploy to a staging endpoint, a CodeBuild integration-test stage, then a manual approval stage and a CloudFormation canary-deployment stage for production.",
      "D": "Use CodeDeploy directly to push code changes to SageMaker endpoints, adding a CodeDeploy pre-deployment hook to run tests and trigger training before deployment."
    },
    "explanation": "Option C uses CodePipeline native actions for each stage, integrates SageMaker Pipelines, staging, tests, manual approval, and a canary production deployment with minimal glue code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A compliance policy requires monthly retraining of an ML model using new data uploaded to s3://company/data/monthly/ at 1 AM on the first of each month. The workflow must run data quality checks, trigger SageMaker model training, register the model, and send an email if any stage fails. How should you configure this with minimal overhead?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create an EventBridge scheduled rule that invokes a Lambda function which runs data quality checks, starts a SageMaker training job, registers the model, and publishes to SNS on failure.",
      "B": "Create an EventBridge scheduled rule to start a CodePipeline at 1 AM monthly. In CodePipeline, add: a Source stage (S3 polling), a CodeBuild stage for data quality checks, a SageMaker Pipeline invoke stage for training and registration, and a CloudWatch Events action to notify via SNS on failure.",
      "C": "Use SageMaker Pipelines with a Schedule trigger for monthly runs, include DataQualityCheck, Train, Register steps, and configure an SNS Alarm for failed steps.",
      "D": "Use AWS Batch scheduled on EventBridge to run a container that executes the entire workflow and sends SNS notifications."
    },
    "explanation": "Option B leverages EventBridge\u2192CodePipeline schedule, built-in actions (S3 source, CodeBuild, SageMaker Pipeline, SNS) with minimal custom code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "An organization maintains separate dev and prod AWS accounts. They want a centralized CI/CD in dev that, upon merging to master in CodeCommit, triggers a deployment pipeline in prod account to update the SageMaker endpoint. How should they configure this cross-account pipeline?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "In dev account create a CodePipeline with a CloudFormation action targeting prod account using StackSets.",
      "B": "Push artifacts from dev S3 bucket to prod S3 bucket, then have a prod pipeline polling that bucket to start deployment.",
      "C": "Use AWS CodeDeploy cross-account deployments from dev to prod.",
      "D": "In dev CodePipeline, add an IAM role action with a role ARN in prod account. Grant the dev pipeline role permission in prod to assume that role. In prod, that role\u2019s trust policy allows the dev pipeline service role to assume it and run deployment actions."
    },
    "explanation": "Option D correctly uses IAM role assumption to allow a pipeline in one account to execute actions in another account without custom polling."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A team follows Gitflow: feature/* branches, develop, release/*, and master. They want two pipelines: one for develop that runs tests and staging deployments on merges to develop, and one for production on merges to master. How should they filter source triggers in CodePipeline?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use two CodePipelines with GitHub source actions configured: one with branch filter pattern '^refs/heads/develop$' and webhooks enabled, the other with '^refs/heads/master$'.",
      "B": "Use a single pipeline triggered on all branches; in the first stage inspect the branch name and abort if not develop or master.",
      "C": "Use CloudWatch Events on push events with wildcard branch patterns to start pipelines via Lambda.",
      "D": "Configure CodePipeline to poll S3 artifacts tagged with branch names to distinguish branches."
    },
    "explanation": "Option A uses CodePipeline\u2019s built-in branch filters in source actions to isolate triggers per pipeline."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A data engineering pipeline produces a delta file in S3 upon completion. You must ensure that your ML CI/CD pipeline ingests this file, validates schema against a baseline, and fails if mismatched, before triggering training. How do you integrate this into CodePipeline?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Have CodePipeline poll the S3 prefix for the delta file and pass it to a Lambda that does schema validation and returns success/fail.",
      "B": "Trigger CodePipeline with an S3 event notification directly to the pipeline\u2019s StartPipelineExecution API.",
      "C": "Use an EventBridge rule matching the S3 PutObject event to call StartPipelineExecution, then in CodePipeline add a CodeBuild stage that runs a schema-validation script against the new file and fails on mismatch.",
      "D": "Use Lambda to copy the file to a Kinesis stream, use a CodePipeline source action on the stream, and a CodeBuild validation stage."
    },
    "explanation": "Option C uses EventBridge\u2192CodePipeline to start on S3 writes and uses CodeBuild to validate schema, failing if mismatched."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your ML pipeline requires a secret API key available to training jobs at build time. You must not expose the secret in plain text or logs. Which CodePipeline stage configuration meets this requirement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Store the API key in S3 encrypted with KMS. In CodeBuild, download and decrypt it inside the build script.",
      "B": "Store the API key in AWS Secrets Manager. In the CodeBuild project definition, set an environment variable that references the Secrets Manager ARN and enable 'Reveal secrets in build logs' to false.",
      "C": "Hard-code the secret in an encrypted parameter in Systems Manager Parameter Store and pull it during the build.",
      "D": "Pass the secret as a plaintext environment variable in the CodePipeline action configuration, relying on IAM to protect it."
    },
    "explanation": "Option B uses Secrets Manager with environment variables in CodeBuild, ensuring secrets aren\u2019t logged."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A reloadable data pipeline updates feature data daily. You want to trigger model retraining automatically whenever a new featureset lands in the SageMaker Feature Store. What\u2019s the least\u2010maintenance way to integrate this into your CI/CD pipeline?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Modify your daily ETL job to call the CodePipeline StartPipelineExecution API at its end.",
      "B": "Use S3 event notifications on the Feature Store underlying S3 bucket to trigger a Lambda that starts the pipeline.",
      "C": "Add a CloudWatch alarm on Feature Store PutRecord metrics and have it trigger pipeline via SNS.",
      "D": "Create an EventBridge rule matching the SageMaker Feature Store PutRecord API via CloudTrail, target StartPipelineExecution for your training pipeline."
    },
    "explanation": "Option D leverages CloudTrail events in EventBridge to detect new records and start the pipeline with zero custom code in the ETL job."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "You need to implement a canary deployment for a real-time SageMaker endpoint in your pipeline. It must shift 5% traffic to the new model initially, then 50%, then 100%, with automatic rollback on errors. Which combination of actions do you use in CodePipeline?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Use CloudFormation deploy action with TYPE=BLUE_GREEN and trafficRoutingConfig CanaryInterval (5%) and CanaryInterval (50%), and a CloudWatch alarm action to auto-rollback.",
      "B": "Use CodeDeploy with application type 'AWS Lambda' against the SageMaker endpoint and set canaryDeploymentConfig.",
      "C": "Invoke a Lambda function in each stage to call UpdateEndpointWeightsAndCapacities on the endpoint.",
      "D": "Use a custom CodeBuild action to call the SageMaker API to shift traffic and monitor latency; if latency rises, call rollback API."
    },
    "explanation": "Option A uses CloudFormation blue/green with canary traffic routing and automatic rollback via alarms, requiring no custom code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A governance requirement mandates that any pipeline change must be peer-reviewed before execution. You want to enforce code reviews for the buildspec.yaml and pipeline definition in CDK. What CI/CD pattern accomplishes this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Allow developers to commit directly to main; have a pre-build hook that runs only if PR label 'reviewed' exists.",
      "B": "Store CDK pipeline code and buildspec in a feature branch, require PR approval in CodeCommit/GitHub, merge only after two approvers, then trigger pipeline on merge to main.",
      "C": "Use CodeBuild PreBuild commands to validate a 'reviewed-by' tag in Git metadata before proceeding.",
      "D": "Configure a manual approval action at the start of every pipeline run to verify that PRs were reviewed externally."
    },
    "explanation": "Option B leverages standard Git-based code-review gating before code merges and pipeline triggers."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your pipeline runs SageMaker training jobs in parallel for hyperparameter tuning. Sometimes runs collide and exceed resource quotas, causing failures. How do you prevent concurrent training executions?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Limit the maximum concurrent CodeBuild builds in the project settings to 1.",
      "B": "Use a Lambda in your pipeline to list current SageMaker jobs and fail fast if any RUNNING jobs exist.",
      "C": "Configure the CodePipeline stage for training to use an ActionConfiguration including a custom concurrency token in your state machine so that only one execution at a time proceeds.",
      "D": "Set the SageMaker training action\u2019s maximum concurrency to 1 in the pipeline definition."
    },
    "explanation": "Option C uses a concurrency token to serialize that action\u2019s invocations; CodePipeline supports concurrencyToken to prevent parallel runs."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "You want to include a data-skew check using SageMaker Clarify in your CI/CD before training. Which pipeline stage sequence and actions accomplish this without custom containers?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "After unit tests, add a CodeBuild stage that invokes SageMaker Clarify ProcessingStep via AWS CLI, passing baseline and new dataset; fail build on detected skew.",
      "B": "Add a custom Docker build to run a Python script that calls Clarify SDK inside CodeBuild.",
      "C": "Trigger SageMaker Pipelines containing a ClarifyBaselineStep; ingest results via SNS into CodePipeline.",
      "D": "Use Lambda action in CodePipeline to call Clarify\u2019s API and post results to S3, then have pipeline poll for results."
    },
    "explanation": "Option A uses CodeBuild with the AWS CLI to call SageMaker Clarify ProcessingJob, avoiding custom Docker images."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your pipeline must run unit tests, then integration tests against a temporary SageMaker endpoint after deployment, and delete the endpoint automatically when tests complete or fail. What do you add?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A CodeDeploy preTraffic hook to run tests and a postRollback hook to delete the endpoint.",
      "B": "A Lambda in CodePipeline to run tests and then call DeleteEndpoint.",
      "C": "A CodeBuild test stage that spins up the endpoint, tests, then calls DeleteEndpoint in the same build.",
      "D": "Two CodeBuild actions: one for integration tests against the staging endpoint, then one that deletes the endpoint by calling AWS CLI delete-endpoint; add them sequentially and configure failureAction=Abort."
    },
    "explanation": "Option D cleanly separates testing and teardown in CodeBuild stages, ensuring teardown always runs after tests if preceding stages succeed."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A developer accidentally pushed sensitive data to a feature branch. You need to prevent that branch from ever triggering your CI/CD pipeline. How do you modify your pipeline\u2019s source action?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a whitelist of allowed branch patterns in the buildspec to abort on unallowed patterns.",
      "B": "Configure the GitHub source action to include an exclude filter 'refs/heads/feature/sensitive*'.",
      "C": "Use a Lambda in the source stage to inspect commits and abort if the file pattern matches.",
      "D": "Add a pre-build CodeBuild action that checks commit history for sensitive files."
    },
    "explanation": "Option B uses the source action\u2019s exclude filter to prevent specific branch patterns from triggering the pipeline."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your ML pipeline needs to use Spot Instances for cost-saving on training. How do you enable this in your CodePipeline/SageMaker training integration?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "In the SageMaker Pipeline definition YAML, set ResourceConfig.AuxiliarySpotCapacity to the desired percentage and invoke via CodePipeline\u2019s SageMakerPipelineAction.",
      "B": "Add a CodeBuild stage that runs 'aws sagemaker create-training-job' CLI with --use-spot-instances flag.",
      "C": "Wrap training in a Lambda function configured with capacityType=SPOT and call it from CodePipeline.",
      "D": "Switch the CodePipeline action type to AWS Batch and configure Spot in the ComputeEnvironment."
    },
    "explanation": "Option A leverages SageMaker Pipelines spot training support via ResourceConfig and CodePipeline\u2019s SageMakerPipelineAction without custom code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "You need to trigger your CI/CD pipeline whenever a pull request is opened or updated, and only for PRs targeting develop. How is this configured?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure CodePipeline Source with GitHub webhook on all PR events and in the build stage inspect the target branch.",
      "B": "Use EventBridge rule for GitHub PullRequest events filtering on repository name.",
      "C": "Enable GitHub Pull Request webhooks in the source action and set the targetReference filter to 'refs/heads/develop'.",
      "D": "Use a Lambda subscriber to GitHub webhooks to call StartPipelineExecution when PR target is develop."
    },
    "explanation": "Option C uses the source action\u2019s webhook pull request event support with targetReference filters to limit triggers to PRs against develop."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your staging endpoint runs expensive benchmarks as part of integration tests. To avoid excessive cost, you only want to run tests when changes affect inference code. How do you optimize your pipeline?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Cache build artifacts between runs to speed up deployment.",
      "B": "Use a manual approval before integration tests.",
      "C": "Use a Lambda pre-check in CodePipeline to compare diff of inference directory and skip tests stage if no changes.",
      "D": "Split your repository: place inference code in its own folder and configure two pipelines\u2014one that triggers full workflow on inference folder changes (using source filters) and one for other changes."
    },
    "explanation": "Option D uses separate pipelines with source filtering to avoid running expensive tests when unrelated code changes."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your pipeline fails intermittently at the SageMaker Pipeline invocation stage due to transient throttling errors. How do you make the pipeline more resilient?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Wrap the SageMakerPipelineAction in a CodeBuild action with a retry loop using exponential backoff on throttling errors.",
      "B": "Increase the service quota for StartPipelineExecution calls.",
      "C": "Add a manual retry approval stage after the failure.",
      "D": "Use a Lambda-based custom action that catches throttles and auto-retries once."
    },
    "explanation": "Option A uses a CodeBuild wrapper to implement retries with backoff, improving resilience without manual intervention."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "You want your CI/CD pipeline to automatically rollback to the last known good model if performance metrics degrade after deployment. How can you implement this in CodePipeline?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Include a manual approval stage that runs a performance test, and if it fails, manually invoke rollback.",
      "B": "Use a Lambda in the post-deploy stage to run a test; if metrics below threshold, call UpdateEndpoint to the previous variant.",
      "C": "Use a CloudWatch alarm on performance metrics to trigger a CodePipeline retrigger of the deploy stage with the previous model via CodeDeploy automatic rollback.",
      "D": "Publish the previous model as an alias in Model Registry and switch alias in a Lambda action on failure."
    },
    "explanation": "Option C ties CloudWatch alarms to CodePipeline\u2019s deploy stage with a configured rollback in CodeDeploy, automating fallback."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your team uses GitHub Flow, pushing directly to main and using feature branches only for experiments. They want every merge to main to trigger a full pipeline. But they also use semantic version tags (e.g., v1.2.0) to denote releases. How do you ensure only tag pushes trigger production deployments?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure CodePipeline source to only trigger on main branch updates.",
      "B": "Use two pipelines with GitHub source actions: one listening to refs/tags/v* (for production) and another to refs/heads/main (for tests), and in the production pipeline restrict to tag pattern '^refs/tags/v[0-9]+.*$'.",
      "C": "Add logic in the build stage to parse GIT_REF and only continue if it\u2019s a tag.",
      "D": "Use EventBridge pattern matching on GitHub Tag events to start the pipeline."
    },
    "explanation": "Option B cleanly separates pipelines by source filters: one triggers on tag pushes for production deployments."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A pipeline source stage uses AWS CodeCommit. Developers occasionally force-push and rewrite history, causing source metadata mismatches. How do you make the pipeline robust against force pushes?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable polling instead of webhooks in the CodeCommit source action.",
      "B": "Add a pre-build stage to Git fetch --prune and reset --hard origin/branch.",
      "C": "Configure the CodePipeline source action to use 'FullClone' fetch method to always get the current tip regardless of history rewrite.",
      "D": "Disable code commit history rewrite in the repository settings."
    },
    "explanation": "Option C uses the FullClone fetch mode so the pipeline always pulls the latest commit independent of history changes."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "You need to integrate model explainability tests into your pipeline: after training, fail the pipeline if any feature attribution for protected classes exceeds a threshold. Which stage should you add?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a CodeBuild stage that runs a SageMaker Clarify ModelExplainabilityProcessingJob via AWS CLI with the trained model, parses the results JSON, and exits non-zero if attribution drift exceeds the threshold.",
      "B": "Invoke a Lambda function in a pipeline action that calls Clarify SDK and uses CloudWatch metrics to decide pass/fail.",
      "C": "Embed the Clarify step inside the SageMaker Pipeline and then have CodePipeline poll for model registry tags to decide.",
      "D": "Use a CloudFormation custom resource to run the Clarify job and roll back the stack on failure."
    },
    "explanation": "Option A uses CodeBuild to orchestrate Clarify explainability checks, parse results, and enforce pipeline success/failure cleanly."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A pipeline stage builds and pushes a Docker image to ECR, then a SageMaker training action uses that image. Occasionally, the training job starts before the image is available, causing errors. How do you prevent this race condition?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a fixed sleep delay in the training action\u2019s custom script.",
      "B": "Use an EventBridge rule on ECR image-push to trigger the pipeline second stage.",
      "C": "Configure CodeBuild to push to a different tag and have training wait on tag propagation.",
      "D": "Split the build-and-push into one CodeBuild stage that outputs the image URI as an artifact, and configure the next SageMakerPipelineAction to consume that artifact, which enforces sequencing."
    },
    "explanation": "Option D uses CodePipeline artifact dependency to guarantee that the training stage does not start until the image has been successfully pushed and the URI passed along."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your pipeline\u2019s CodeBuild unit-test stage frequently downloads the entire repository, slowing builds. How do you optimize clone behavior?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure the GitHub source action to use 'ShallowClone' with a depth of 1, and enable CodeBuild\u2019s source cache for the repo.",
      "B": "Use Git LFS to store large binaries outside the repo.",
      "C": "Split the repo into micro-repos each with its own pipeline.",
      "D": "Switch the source action to S3 and upload only changed files manually."
    },
    "explanation": "Option A combines shallow cloning at the source action with CodeBuild caching to speed up fetch time without repo restructuring."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A CI/CD pipeline should only deploy models when both unit tests and integration tests pass. However, you need integration tests to run against a live endpoint, which itself requires deployment. How do you model this in CodePipeline?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run both unit and integration tests in the same CodeBuild action after deployment.",
      "B": "Chain two separate pipelines: one for tests, one for deployment, and only call the deploy pipeline if tests pass.",
      "C": "In a single pipeline: Stage1 run unit tests via CodeBuild, Stage2 deploy to staging via CloudFormation, Stage3 run integration tests via CodeBuild, Stage4 manual approval and production deploy.",
      "D": "Use a SageMaker Pipeline that supports validation steps natively and call it from CodePipeline."
    },
    "explanation": "Option C lays out a linear pipeline with distinct stages for unit tests, deployment, integration tests, and production approval."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "You want to notify your team in Slack whenever any pipeline stage fails, including the stage name and error message. How do you configure this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Attach an SNS topic to pipeline notifications and configure an AWS Chatbot integration for Slack.",
      "B": "Create a CloudWatch EventBridge rule matching CodePipeline 'Stage Execution Failed' events, target a Lambda that formats the message and posts to Slack via webhook.",
      "C": "In each CodePipeline stage\u2019s onFailure hook, invoke a Lambda to post to Slack.",
      "D": "Enable CloudTrail logs for CodePipeline and stream failures to Slack using Log Insights."
    },
    "explanation": "Option B uses a single EventBridge rule and a Lambda to capture failure events and send formatted Slack messages, minimizing per-stage configuration."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "An executive insists that production deployments require two separate approvers. How can you enforce this in CodePipeline?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add two successive ManualApproval actions in the production deployment stage, each assigned to a different user group.",
      "B": "Configure the ManualApproval action with 'approvalThreshold' set to 2.",
      "C": "Use IAM policies to require MFA for invoking the production stage.",
      "D": "Chain two identical manual approval actions but restrict both to the same group."
    },
    "explanation": "Option A ensures two distinct approvals by having two manual approval actions in series, each can be assigned to different user sets."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your CodePipeline deploys a SageMaker model via CloudFormation. When the deployment fails, the pipeline remains stuck in a FAILED state and you must manually clean up. How do you configure automatic rollback of the CloudFormation stack?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a CloudFormation DeleteStack action after the deploy action conditioned on failure.",
      "B": "Use a Lambda in the onFailure hook to delete the stack.",
      "C": "Set the CloudFormation action\u2019s 'ActionMode' to CHANGE_SET_REPLACE and 'RollbackOnFailure' parameter to true.",
      "D": "Wrap the CloudFormation deploy in an AWS Step Functions state machine with a Catch to delete the stack."
    },
    "explanation": "Option C leverages CloudFormation action parameters to rollback automatically, avoiding manual cleanup."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "You need to run parallel hyperparameter tuning and data quality steps in your pipeline to reduce total runtime. How do you implement parallel actions in CodePipeline?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create multiple pipelines and start them simultaneously from a Lambda.",
      "B": "Use SageMaker Pipelines which natively parallelizes steps.",
      "C": "Run one CodeBuild action that multiplexes both tasks in background processes.",
      "D": "Define two actions within the same pipeline stage; CodePipeline will execute them in parallel, then proceed only when both succeed."
    },
    "explanation": "Option D uses CodePipeline\u2019s support for parallel actions in a single stage to run tasks concurrently."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An online retailer deploys a real-time inference endpoint for product recommendations. Shortly after launch, they notice that the distribution of a key categorical feature \"user_region\" has shifted, although model accuracy remains within SLA. Which automated monitoring solution should they implement to detect and alert on this feature distribution change with minimal custom coding?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure a SageMaker Model Quality Monitor job to check for data drift on \"user_region\" using ground truth labels.",
      "B": "Use SageMaker Model Monitor DataQuality baselining to establish constraints on \"user_region\" and schedule a drift detection job.",
      "C": "Deploy SageMaker Clarify to compute SHAP values for \"user_region\" on each inference and alert when feature importance changes.",
      "D": "Write an AWS Lambda that polls CloudWatch logs, computes a KS test on \"user_region\", and sends SNS alerts."
    },
    "explanation": "Model Monitor DataQuality jobs can baseline feature distributions and automatically detect drift with minimal custom code. ModelQualityMonitor requires labels; Clarify focuses on bias/importance rather than raw distribution; a Lambda solution requires more custom work."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A financial institution uses a SageMaker real-time endpoint for credit scoring. They need to monitor inference latency degradation and data schema violations. Which combination of SageMaker features meets both requirements natively?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure SageMaker Model Debugger to capture latency and data quality metrics.",
      "B": "Use CloudWatch Contributor Insights for the endpoint to detect schema violations and latency issues.",
      "C": "Enable Data Capture on the endpoint and run SageMaker Model Monitor with both DataQuality and ModelQuality checks.",
      "D": "Deploy AWS X-Ray for latency tracing and AWS Glue Data Quality for schema validation."
    },
    "explanation": "Data Capture plus Model Monitor can track payload schema violations via DataQuality checks and latency via custom metrics. Model Debugger focuses on training/debug; Contributor Insights and Glue don\u2019t natively integrate inference monitoring as well."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A company wants to detect concept drift in predicted probabilities of a binary classifier in production in near real-time. They also want to minimize cost. Which monitoring configuration should they choose?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run a SageMaker Clarify ModelExplainabilityMonitor on each batch of incoming requests.",
      "B": "Configure an asynchronous SageMaker endpoint with built-in data drift detection.",
      "C": "Use SageMaker Model Quality Monitor with ground truth labels and a 5-minute schedule.",
      "D": "Enable Data Capture on the endpoint and schedule a SageMaker Model Monitor DataQuality drift check on predicted score distribution."
    },
    "explanation": "Without ground truth, ModelQualityMonitor isn\u2019t applicable. Clarify ExplainabilityMonitor focuses on SHAP. Asynchronous endpoints have no built-in drift. DataQuality drift checks on predictions detect concept drift cost effectively."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An ML engineer deployed two production variants (A and B) for A/B testing. They need to monitor emerging data skew between the variants for a continuous categorical feature \"device_type\". Which approach best surfaces variant-specific data skew?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure two separate Model Monitor DataQuality jobs, each capturing and analyzing \"device_type\" for its variant.",
      "B": "Use a single Model Monitor DataQuality job that collects data from both variants and uses filters on \"variant_name\".",
      "C": "Use SageMaker Clarify to detect bias between variants on \"device_type\".",
      "D": "Aggregate CloudWatch logs across variants and run a post-processing job to compare distributions."
    },
    "explanation": "Separate DataQuality jobs allow per-variant baselines and alerts. A single job with filters isn't supported; Clarify is for bias interpretation, not raw skew; log aggregation is more custom work."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "During inference, a regression model returns NaN for target predictions intermittently. Which Model Monitor configuration will automatically detect this anomaly and notify the team?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable SageMaker Model Debugger to capture NaNs during inference.",
      "B": "Use Model Monitor DataQuality baseline constraints to flag NaN values in the prediction column.",
      "C": "Set up Amazon CloudWatch anomaly detection on inference response logs to catch NaNs.",
      "D": "Configure SageMaker Clarify to test for invalid prediction values."
    },
    "explanation": "DataQuality monitors can enforce no-NaN constraints on any column. Debugger focuses on training tensors; CloudWatch anomaly detection requires custom log parsing; Clarify is for bias/explainability."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A language translation model hosted on SageMaker Edge Manager runs on devices with intermittent connectivity. The team needs to detect and report when local input data statistics drift significantly. Which architecture is most appropriate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Model Monitor DataQuality jobs in the AWS cloud to poll edge logs hourly.",
      "B": "Install SageMaker Debugger on devices to log input metrics to CloudWatch.",
      "C": "Embed the SageMaker Edge Manager monitoring SDK on device to emit metrics to AWS IoT, then trigger a Model Monitor drift check in the cloud.",
      "D": "Deploy AWS Greengrass Lambda functions to run Clarify drift checks locally."
    },
    "explanation": "Edge Manager SDK plus IoT ingestion allows capture of local stats; cloud Model Monitor then runs drift. Debugger isn\u2019t for inference; Greengrass + Clarify unsupported; cloud polling logs is inefficient."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An ML engineer must monitor prediction skew between two ensembles serving the same traffic for anomaly detection. They require automatic, per-minute alerts if the divergence of prediction distributions exceeds a threshold. How should they implement this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable Data Capture for both ensemble endpoints, schedule two Model Monitor drift checks on predictions with a 1-minute interval, and configure CloudWatch alarms on the job results.",
      "B": "Run SageMaker Clarify on both ensembles every minute to compute PSI on prediction distributions.",
      "C": "Write a custom Lambda that pulls logs from both endpoints, computes KS test, and pushes CloudWatch metrics.",
      "D": "Use CloudWatch Contributor Insights to track prediction distribution per minute."
    },
    "explanation": "Two Data Capture + Model Monitor drift checks with CloudWatch alarms automates per-minute divergence alerts. Clarify isn\u2019t for raw distribution drift; custom Lambda is higher overhead; Contributor Insights not suited."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "After deploying a multi-model SageMaker endpoint, a sudden spike in malformed JSON requests is observed. The engineer wants to monitor schema violations per model. Which solution provides the most granular alerts?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure a single Model Monitor DataQuality job on the endpoint to catch all schema violations.",
      "B": "Enable Data Capture and configure a separate DataQuality job per model container with JSON schema constraints.",
      "C": "Use AWS WAF JSON body inspection rules to log violations to CloudWatch.",
      "D": "Parse inference logs with CloudWatch Logs Insights and trigger SNS alerts."
    },
    "explanation": "Separate DataQuality jobs per model in the multi-model endpoint allow per-container schema checks. A single job cannot differentiate models; WAF and custom log parsing add external complexity."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A streaming inference pipeline uses an asynchronous endpoint. They want to detect if more than 1% of requests take longer than 10 seconds. Which AWS-native feature combination best achieves this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Model Monitor latency metrics with custom Python script to parse logs.",
      "B": "Configure AWS X-Ray on the endpoint and set up CloudWatch alarms for trace duration.",
      "C": "Enable Data Capture and schedule Model Quality Monitor to evaluate latency distribution.",
      "D": "Enable Data Capture on the endpoint, configure CloudWatch metric filters for invocation duration, and set alarms for >1% above 10s."
    },
    "explanation": "Data Capture plus CloudWatch metric filters allow latency distribution monitoring and alarms without custom training jobs. Model Quality Monitor isn\u2019t for latency; X-Ray traces require code hooks."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A healthcare inference service must monitor PII leakage: requests and responses must contain no PHI tokens. The model returns free-form text. Which monitoring approach will detect policy violations?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Model Monitor DataQuality to enforce a regex constraint rejecting PHI patterns in responses.",
      "B": "Configure AWS WAF with a custom rule to block PHI regex in JSON payload.",
      "C": "Enable Data Capture, run a scheduled SageMaker Clarify bias check with custom detectors for PHI tokens.",
      "D": "Write a Lambda for each inference to scan for PHI and generate CloudWatch events."
    },
    "explanation": "Clarify allows custom text analyzers via pre-built detectors to detect sensitive tokens. DataQuality regex can catch patterns but is limited; WAF doesn\u2019t inspect model responses; Lambda is higher overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A model predicts customer churn and uses 50 numeric features. They need to detect multivariate drift (covariate shift) across those features. Which monitoring setup is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Model Monitor DataQuality with a multilook at a multivariate drift check (using dataset_format=\u201cJSON\u201d with drift_check_categories).",
      "B": "Configure SageMaker Clarify to compute pairwise SHAP drift metrics for all feature pairs.",
      "C": "Schedule a batch transform job daily and compare covariance matrices via AWS Glue.",
      "D": "Write custom Spark on EMR to compute Mahalanobis distance across features."
    },
    "explanation": "Model Monitor supports multivariate drift checks during DataQuality monitoring. Clarify focuses on feature importance; batch transform and custom Spark are inefficient and higher overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A retailer serves an image-classification model via SageMaker. They need to detect when the distribution of image size (in bytes) drifts and when inference latency spikes. Which out-of-the-box features should they enable?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Clarify DataBias monitors on image_size and SageMaker Model Debugger for latency.",
      "B": "Enable Data Capture and schedule Model Monitor DataQuality jobs to track image_size distribution and custom CloudWatch latency metrics.",
      "C": "Configure a Lambda pre-processor to log image_size and usage of SageMaker Profiler for latency.",
      "D": "Deploy a Step Functions workflow around the endpoint to log metrics and analyze separately."
    },
    "explanation": "Data Capture plus Model Monitor DataQuality tracks input size drift; CloudWatch latency metrics catch spikes. Clarify and Debugger don\u2019t cover both needs; lambda and Step Functions add complexity."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An enterprise requires that no inference job runs outside their VPC and that monitoring data remains within their private subnet. How can they deploy Model Monitor under these constraints?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable network isolation on Model Monitor jobs.",
      "B": "Run Model Monitor on a public subnet with NAT gateway to pull data.",
      "C": "Use VPC endpoints for S3 and CloudWatch and run jobs on public instances.",
      "D": "Configure Model Monitor Processing Job with VPC configuration in private subnets and use VPC S3 endpoints."
    },
    "explanation": "Model Monitor jobs run as SageMaker Processing Jobs and support VPC config. Private subnets plus S3 VPC endpoints ensure data stays in VPC. Network isolation is for training/inference only."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A financial model returns probability scores. The team needs to detect if the median score changes by more than 5% compared to baseline daily. Which feature and configuration accomplish this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Model Monitor DataQuality drift check on the prediction field with a median tolerance constraint of \u00b15%.",
      "B": "Schedule a Clarify ModelBiasMonitor job to compute median changes.",
      "C": "Extract daily predictions to Athena and run SQL analytics to compare medians.",
      "D": "Enable CloudWatch percentile-based alarms on the endpoint\u2019s inference metric."
    },
    "explanation": "DataQuality drift constraints support median checks on numeric fields. Clarify bias monitors aren\u2019t for central tendency; Athena SQL is custom; CloudWatch latency alarms only handle infrastructure metrics."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "To comply with regulations, a bank must audit all inference inputs and outputs and ensure they cannot be modified post-hoc. Which combination of services and features meets this requirement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable Data Capture to write logs to an encrypted S3 bucket and use ACLs for write-once.",
      "B": "Use Data Capture to store inputs/outputs in an S3 bucket with S3 Object Lock in Compliance mode and encryption by KMS.",
      "C": "Configure CloudTrail to log inference API calls and point logs to S3 with versioning.",
      "D": "Use Athena for logging inputs/outputs and store results in a write-only DynamoDB table."
    },
    "explanation": "Data Capture plus S3 Object Lock (Compliance) and KMS encryption provides immutable audit trail. CloudTrail logs API calls but not payloads; ACLs alone don\u2019t enforce write-once."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A data scientist wants to spot-time batch inference data quality issues such as missing values and invalid formats before consuming results. Which approach should they apply?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Clarify to compute bias metrics on batch responses.",
      "B": "Schedule a CloudWatch Logs Insights query on batch transform logs.",
      "C": "Run a SageMaker Processing job with Model Monitor DataQuality checks on batch output.",
      "D": "Validate outputs manually in Jupyter notebooks after batch jobs complete."
    },
    "explanation": "Model Monitor Processing jobs can validate batch outputs via DataQuality checks. Clarify is for bias; Logs Insights doesn\u2019t parse output payload; manual validation lacks automation."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An autonomous driving model processes high-frequence sensor data. They need to detect when any input channel has degraded signal quality (e.g., constant zeros) in real time. Which solution best addresses this?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Use Model Debugger to capture input tensor anomalies.",
      "B": "Set up CloudWatch metric filters on inference logs to detect zero-value readings.",
      "C": "Implement an AWS IoT rule to check sensor message contents for zeros.",
      "D": "Enable Data Capture on the endpoint, schedule a Model Monitor DataQuality job with custom tolerances for each channel."
    },
    "explanation": "Data Capture and DataQuality allow custom numeric constraints per feature. Debugger isn\u2019t for inference; CloudWatch requires custom parsing; IoT rules don\u2019t integrate inference model context."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A model returns a confidence score as a float between 0 and 1. The team must ensure no inference returns a score outside this range. Which monitoring configuration should they use?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Write a custom CloudWatch metric filter on response logs to detect invalid scores.",
      "B": "Use Model Monitor DataQuality constraints to flag scores <0 or >1.",
      "C": "Apply SageMaker Clarify to validate output distributions within range.",
      "D": "Include assertion logic in the inference container to throw errors on invalid values."
    },
    "explanation": "DataQuality constraints can enforce numeric ranges on columns. Clarify doesn\u2019t enforce hard constraints; custom filter or container logic increases maintenance."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A marketing model predicts click-through probability and requires monitoring of high skew in the \"campaign_id\" feature usage. Which Service Lens should they use in Model Monitor to inspect this categorical feature?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use the Categorical Lens in SageMaker Model Monitor DataQuality to monitor \"campaign_id\".",
      "B": "Use the Label Lens in SageMaker Clarify to monitor campaign labeling.",
      "C": "Use the Numeric Lens in Model Monitor to inspect category counts.",
      "D": "Use the Bias Lens in Clarify to track category fairness."
    },
    "explanation": "The Categorical Lens in Model Monitor DataQuality jobs specializes in tracking categorical feature distribution. Numeric Lens monitors only numeric features; Clarify lenses focus on bias/explainability."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An engineering team notices that model inference failures often coincide with sudden changes in request payload size. They want to correlate error rate with payload size anomalies. Which approach is simplest?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Upload logs to CloudWatch Logs and manually correlate with payload sizes stored in S3.",
      "B": "Instrument the container to push custom CloudWatch metrics for payload size and errors.",
      "C": "Enable Data Capture with custom JSONPath for payload_size and error_code, then run Model Monitor DataQuality job with both fields.",
      "D": "Use AWS X-Ray annotations to trace payload size and exceptions."
    },
    "explanation": "Data Capture with custom JSONPath can extract both payload_size and error_code; DataQuality jobs can detect anomalies and correlations. Container instrumentation or X-Ray require more custom code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A translation service uses a multi-model endpoint. They need to monitor latency at the container level for each model separately. What\u2019s the most direct AWS-native way?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure Model Monitor on the endpoint to track per-model latency.",
      "B": "Enable per-model invocation logging with Data Capture and use CloudWatch metric filters to calculate latency.",
      "C": "Deploy a CloudWatch Agent on the endpoint hosts to capture container metrics.",
      "D": "Use SageMaker Debugger profiling on inference containers."
    },
    "explanation": "Data Capture logs invocation metadata per model; CloudWatch logs filters can compute latency per container. Model Monitor and Debugger don\u2019t provide per-container latency; CloudWatch Agent isn\u2019t supported on managed endpoints."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A company must detect malicious adversarial inputs that cause anomalous model outputs. They define an acceptable range for each output. Which monitoring solution enforces this at scale?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model Monitor DataQuality constraints on the output columns with defined min/max thresholds.",
      "B": "Deploy a WAF rule inspecting response bodies against known attack signatures.",
      "C": "Use Clarify Explainability to detect adversarial perturbations.",
      "D": "Stream inference logs to Lambda for custom anomaly detection."
    },
    "explanation": "DataQuality constraint checks on output enforce numeric ranges and can scale. WAF and Clarify aren\u2019t designed for content-based validation; Lambda adds overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An endpoint serves an NLP model that occasionally returns excessively long generated text (>1000 tokens). They want real-time alerts when this happens. Which setup is most efficient?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Model Monitor to check response length length via DataQuality.",
      "B": "Run Clarify to measure token counts and alert.",
      "C": "Enable CloudWatch RUM to capture response sizes.",
      "D": "Configure Data Capture to extract response_text, set a DataQuality constraint for max token count, and schedule frequent jobs."
    },
    "explanation": "Data Capture plus DataQuality constraint handles response length checks. Clarify and RUM unsuited; CloudWatch RUM is for front-end metrics."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A pharmaceutical company must catch silent inference failures where the model returns a default value of 0. They want automated detection. Which AWS feature configuration accomplishes this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure SageMaker Debugger to flag constant zero outputs.",
      "B": "Use Model Monitor DataQuality constraints to detect when output equals 0 more than a threshold.",
      "C": "Enable CloudWatch metric filters on logs for zero values and alert.",
      "D": "Instrument the model container to throw errors on zeros."
    },
    "explanation": "DataQuality constraints detect when a column value matches an anomaly pattern. Debugger is for training; logs filtering is custom; container instrumentation has higher overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "To monitor inference drift for streaming predictions without ground truth, which monitor type and category should they choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ModelQualityMonitor with concept drift category.",
      "B": "Clarify ModelBiasMonitor on predictions.",
      "C": "Model Monitor DataQuality with drift_check_categories set to \"data\" on the prediction field.",
      "D": "Clarify ModelExplainabilityMonitor for prediction distribution."
    },
    "explanation": "DataQuality monitors support drift_check_categories on arbitrary fields. ModelQualityMonitor requires labels. Clarify focuses on bias/explainability."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A company needs to ensure that feature engineering code doesn\u2019t introduce new features unseen during training. They want to monitor for any new column names in inference payloads. What\u2019s the best strategy?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Model Monitor DataQuality with input schema constraints listing allowed column names.",
      "B": "Implement a Lambda authorizer on API Gateway to validate JSON keys.",
      "C": "Enable SageMaker Clarify DatasetDriftMonitor on input keys.",
      "D": "Use AWS Config rules on the SageMaker endpoint inference setting."
    },
    "explanation": "DataQuality constraints can enforce allowed input schema. Lambda authorizer works but is outside SageMaker; Clarify and Config aren\u2019t for schema enforcement."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An ML engineer observes that the distribution of a numerical feature is bimodal in training but has become unimodal in production. They must detect this change automatically. Which Model Monitor configuration handles multimodal distribution drift?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Clarify ModelBiasMonitor to detect distribution shape changes.",
      "B": "Schedule a batch transform and post-process histograms in Athena.",
      "C": "Use CloudWatch anomaly detection on the feature metric.",
      "D": "Configure Model Monitor DataQuality with DistributionLens on the feature and enable drift checks."
    },
    "explanation": "DistributionLens in DataQuality monitors supports multiple bins for drift detection. Clarify bias monitors don\u2019t assess raw distribution shape; Athena/CloudWatch require custom coding."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A subscription service uses asynchronous batch endpoints. They need to monitor end-to-end inference job failures and durations. How can they instrument this without modifying model code?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Use Model Monitor DataQuality to track batch job status and duration.",
      "B": "Configure Clarify to track job failures.",
      "C": "Configure CloudWatch Events on SageMaker BatchTransform job state change and subscribe to SNS.",
      "D": "Deploy GuardDuty to detect anomalies in batch failures."
    },
    "explanation": "CloudWatch Events for job state and duration require no code changes. Model Monitor doesn\u2019t monitor batch job lifecycle; Clarify doesn\u2019t cover jobs; GuardDuty is for security threats."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An experiment uses shadow deployments: incoming traffic is mirrored to a new model. They want to compare output distributions in near real time and alert if divergence >2%. Which design is simplest?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy two DataQuality jobs separately and post-process alerts via Lambda.",
      "B": "Enable Data Capture on both endpoints, schedule a single DriftsCheck Job that references both datasets as baseline and target.",
      "C": "Use Clarify to compute PSI between baseline and candidate in near real time.",
      "D": "Implement a Step Functions workflow to fetch logs and compare distributions."
    },
    "explanation": "A single DriftCheck job can accept two datasets as baseline and target. DataQuality jobs per endpoint or Step Functions add complexity; Clarify mismatched purpose."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A biotech startup requires monitoring of model feature importance drift in production. They want to detect if SHAP importance of any feature changes by >10%. Which AWS feature combination achieves this?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Use Model Monitor DataQuality drift_check on SHAP values.",
      "B": "Schedule a Model Quality Monitor job with SHAP-based metrics.",
      "C": "Configure Clarify ModelBiasMonitor with feature_importance thresholds.",
      "D": "Enable SageMaker Clarify ModelExplainabilityMonitor on the endpoint with max_absolute_shap_change constraint."
    },
    "explanation": "Clarify\u2019s ModelExplainabilityMonitor captures SHAP importance drift and supports constraints. DataQuality and ModelQualityMonitoring don\u2019t handle SHAP; ModelBiasMonitor focuses on bias not drift."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A team must implement an alert if any inference job experiences input payload with more than 50 unique categorical feature values. Which approach scales best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Config custom rules to inspect payloads.",
      "B": "Deploy WAF rate-limit rules on JSON body parser.",
      "C": "Enable Data Capture and configure a Model Monitor DataQuality custom constraint on cardinality >50.",
      "D": "Run Athena queries on S3 logs and send alerts via Lambda."
    },
    "explanation": "DataQuality custom cardinality constraints enforce unique value limits. AWS Config and WAF don\u2019t inspect inference payloads; Athena+Lambda is higher latency."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A model is deprecated but still receives stale requests. The team wants to detect any inference calls to the old endpoint and retire it promptly. Which CloudWatch configuration helps?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Set up a CloudWatch metric filter on SageMakerAPI logs for InvokeEndpointOldModel API and alert.",
      "B": "Use Model Monitor to detect zero-inference values.",
      "C": "Configure a Custom Resource in CloudFormation to log invocations.",
      "D": "Enable SageMaker Clarify to flag calls to old endpoint."
    },
    "explanation": "CloudWatch metric filters on API logs for specific endpoint names detect calls. Model Monitor and Clarify aren\u2019t for endpoint invocation detection; custom resource is unnecessary."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A gaming app uses an ML endpoint that must maintain 95th percentile latency under 200ms. They need to alert if the SLA is broken. Which AWS native setup is recommended?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Run Model Monitor with a latency constraint on the endpoint logs.",
      "B": "Use CloudWatch percentile-based alarms on the AWS/SageMaker metric InferenceLatency.",
      "C": "Deploy SageMaker Debugger to capture latency tensors.",
      "D": "Implement a Lambda to sample requests and measure SLA."
    },
    "explanation": "CloudWatch percentile-based alarms directly monitor InferenceLatency p95. Model Monitor isn\u2019t designed for latency; Debugger and Lambda require extra work."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "An e-commerce company uses a SageMaker real-time endpoint for TensorFlow models with a strict p95 latency SLA. During peak traffic hours, end-to-end latency spikes intermittently even though CPU and memory utilization remain under thresholds. Which monitoring approach and remediation provides the deepest insight into latency sources and optimizes cost?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Enable AWS X-Ray tracing on the SageMaker endpoint, analyze service map latencies, identify cold-start segments, then provision minimal concurrency and configure auto scaling.",
      "B": "Increase the instance type to a larger CPU-optimized instance and create a CloudWatch Alarm on CPU utilization.",
      "C": "Enable SageMaker endpoint invocation metrics in CloudWatch and auto scale based on p95 latency.",
      "D": "Convert the endpoint to a multi-model endpoint to share a single container across models and reduce overhead."
    },
    "explanation": "X-Ray reveals where time is spent (initialization vs. inference). Provisioned concurrency mitigates cold starts. CPU scaling alone misses framework overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A financial services company runs a batch transform job on EC2 instances via SageMaker. Transform jobs run once per day, but 80% of capacity is idle for most of the job. The team wants to reduce cost without increasing job duration. Which solution meets this requirement?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch to CPU-optimized On-Demand instances sized for peak utilization and scale cluster size dynamically.",
      "B": "Use larger GPU instances to process in parallel and decrease overall run time.",
      "C": "Run the batch transform job on Lambda functions to parallelize tasks.",
      "D": "Configure the batch transform job to use Spot Instances with a fallback to On-Demand and set instance count for peak load."
    },
    "explanation": "Spot with fallback saves cost during idle periods and matches peak capacity, while On-Demand-only wastes money."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "Your team deployed multiple SageMaker endpoints in three regions. They must tag endpoints for cost allocation and alert if untagged resources appear. Which configuration achieves this with the least operational overhead?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Write a Lambda polling script to list endpoints daily, check tags, and send notifications.",
      "B": "Use CloudWatch Logs subscription to trigger a Lambda on CreateEndpoint API events to validate tags.",
      "C": "Create an AWS Config rule (required-tags) scoped to SageMaker::Endpoint resources and set SNS notifications.",
      "D": "Implement CloudTrail log analysis in CloudWatch Logs Insights to detect untagged CreateEndpoint events and alert."
    },
    "explanation": "AWS Config rule automatically checks tags on creation with built-in functionality and SNS notifications, minimal custom code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A gaming company uses SageMaker Processing to prepare game telemetry data. They notice sporadic high I/O wait times on EBS volumes attached to processing jobs. Which combination of monitoring metrics and actions will identify and mitigate the issue?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Monitor ProcessingJobIOBytes metric in Amazon CloudWatch and increase instance count.",
      "B": "Enable CloudWatch EBS Volume metrics (VolumeQueueLength, VolumeThroughputPercentage), use higher IOPS gp3 volumes and adjust volume throughput settings.",
      "C": "Monitor SageMaker Processing CPUUtilization and switch to GPU instance.",
      "D": "Enable CloudTrail logging on processing containers and analyze volume errors."
    },
    "explanation": "VolumeQueueLength and throughput % diagnose EBS bottleneck; gp3 allows tuning of IOPS separately and resolves I/O wait."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A retail ML pipeline uses SageMaker endpoints and ECS-based preprocessing. The finance team needs a single dashboard showing endpoint cost, CPU utilization, ECS Fargate memory utilization, and Lambda durations. Which solution meets this requirement with minimal latency?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Build separate CloudWatch dashboards per service and share links.",
      "B": "Export all metrics to an external monitoring tool via Fluentd.",
      "C": "Configure CloudTrail Lake queries to join logs and visualize in QuickSight.",
      "D": "Use CloudWatch unified metric namespace with metric math and create a combined CloudWatch dashboard with all resources."
    },
    "explanation": "CloudWatch dashboards support service metrics and metric math in one place without external tools or manual stitching."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A biotech startup has a GPU-based SageMaker endpoint used intermittently. The CFO demands a cost-optimized solution that does not sacrifice startup latency. Which approach balances both?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use provisioned concurrency with a small number of GPU instances, enable endpoint auto scaling to scale down to zero using asynchronous endpoints.",
      "B": "Switch to smaller CPU instances and batch requests to save cost.",
      "C": "Use Spot Instances for the real-time endpoint to lower cost.",
      "D": "Migrate to multi-model CPU endpoint and load one model at a time."
    },
    "explanation": "Provisioned concurrency preserves low latency, auto scaling to zero reduces idle cost; spot not supported for real-time."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A team observes their inference pipeline fails under burst traffic due to hitting service quotas. They want proactive alerts when approaching SageMaker endpoint invocation quota. How should they implement this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Configure a CloudWatch alarm on SageMaker API ThrottledRequests metric.",
      "B": "Create a CloudWatch alarm on the ThrottledRequests metric in the AWS/SageMaker namespace with threshold at 80% of quota.",
      "C": "Use EventBridge to detect service quota event and call SNS.",
      "D": "Poll DescribeEndpoint API hourly via Lambda and compare against known quotas."
    },
    "explanation": "ThrottledRequests metric directly signals when quota is approached; threshold at 80% gives early warning."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "Your model training jobs on GPU EC2 instances often underutilize GPU memory, leaving 40% unused for most jobs. You want better rightsizing. Which tool or service gives an automated recommendation?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Trusted Advisor\u2019s cost optimization checks.",
      "B": "CloudWatch anomaly detection on GPU utilization.",
      "C": "AWS Compute Optimizer for EC2 instances.",
      "D": "SageMaker Model Monitor for resource utilization analytics."
    },
    "explanation": "Compute Optimizer analyzes EC2/GPU instance metrics and recommends right-sized instance types and sizes."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A media company streams video analysis via SageMaker batch transform jobs. They want to optimize data transfer costs between S3 and training instances. What combination of configurations reduces cost?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use cross-region replication to move data closer to training instances.",
      "B": "Enable S3 Transfer Acceleration to speed transfer and reduce egress cost.",
      "C": "Configure batch transform to download from a public S3 bucket.",
      "D": "Place instances and S3 in same AZ and use VPC endpoints for S3 traffic."
    },
    "explanation": "Same AZ VPC endpoints keeps traffic on AWS network, avoids cross-AZ data transfer charges."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A regulated healthcare application must log all changes to SageMaker model endpoints for audit. They also want to trigger cost alerts when monthly spend exceeds budget. Which combined setup achieves both?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable AWS CloudTrail data events for SageMaker API and create an AWS Budget with SNS notifications.",
      "B": "Use CloudWatch Logs to collect API calls and set a Lambda to parse logs and send alerts.",
      "C": "Configure CloudWatch Events for CreateEndpoint and UpdateEndpoint events and notify SNS.",
      "D": "Integrate AWS Config for resource changes and rely on CloudWatch billing alarms."
    },
    "explanation": "CloudTrail data events capture every API call for audit; AWS Budgets integrates natively with SNS for cost alerts."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "Your SageMaker endpoint sees memory out-of-memory (OOM) errors for large images. You need to diagnose the frame where OOM occurs across many hosts. Which logs and queries should you use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Download Docker container logs from CloudWatch Logs and grep for \"OOM\".",
      "B": "Use CloudWatch Logs Insights on /aws/sagemaker/Endpoints/* to query ERROR messages with \"OutOfMemoryError\" and filter by hostID.",
      "C": "Enable SageMaker Debugger hook for memory profiling.",
      "D": "Switch to X-Ray and trace memory allocations."
    },
    "explanation": "CloudWatch Logs Insights can aggregate and filter OOM errors across endpoint hosts quickly; Debugger is for training."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A data pipeline uses EMR for preprocessing and then writes to S3 for SageMaker. During heavy loads, EMR tasks throttle due to hitting S3 request quotas. How do you monitor and remediate this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable EMRFS consistent view and increase retries.",
      "B": "Use CloudTrail to monitor S3 503 errors and back off EMR jobs.",
      "C": "Enable CloudWatch anomaly detection on EMRTaskDuration metric.",
      "D": "Monitor S3 4XX/5XX request metrics in CloudWatch and add S3 request rate limiting or use S3 prefix sharding."
    },
    "explanation": "S3 request metrics reveal throttling; prefix sharding or client-side rate limiting reduces 503 errors."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "Your team deployed a multi-model endpoint behind an Application Load Balancer. They need to track per-model invocation counts for billing and auto scaling. Which approach is most effective?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable ALB access logs and parse them in Athena.",
      "B": "Add custom CloudWatch metrics inside the inference container to emit per-model Invoke counts.",
      "C": "Use CloudTrail logs for InvokeEndpoint calls with model name filters.",
      "D": "Tag each model and use SageMaker built-in metrics."
    },
    "explanation": "Containers can push custom metrics (model=inference) to CloudWatch for precise counts; ALB logs parsing is more complex."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A startup runs GPU-based SageMaker training nightly. They want to reduce EC2 instance costs by 30% without increasing job time. Which purchasing option should they choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Reserved Instances for a one-year term.",
      "B": "EC2 Savings Plans covering all compute usage.",
      "C": "Spot Instances with checkpointing in S3 and automatic retry.",
      "D": "Dedicated Hosts to benefit from volume discounts."
    },
    "explanation": "Spot Instances offer largest discounts (up to 90%) and with checkpointing maintain job duration; RIs and SPs only 50%."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A large dataset processing job on SageMaker Processing has unpredictable CPU loads. They want to automatically adjust compute within a run. Which strategy works?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Processing with auto scaling instance type list.",
      "B": "Orchestrate multiple processing jobs in Step Functions with dynamic instance count based on CloudWatch metrics.",
      "C": "Embed CPU usage watchers in the processing script and call the AWS API to change instance count.",
      "D": "Switch to EMR on EKS with dynamic allocation."
    },
    "explanation": "Step Functions can branch logic mid-workflow and launch new Processing jobs sized by metrics; Processing itself doesn\u2019t auto scale."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "Your SageMaker endpoint is deployed in a VPC. Data scientists observe increased inference latency. Which VPC configuration issue should you monitor and how?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Monitor NAT Gateway CloudWatch NetworkPacketsOut to detect throttling and add NAT Gateways for scale.",
      "B": "Monitor VPC Flow Logs for high packet loss and increase MTU.",
      "C": "Monitor Security Group rejected packet metrics and open more ports.",
      "D": "Monitor Elastic IP usage and allocate more addresses."
    },
    "explanation": "VPC endpoints without dedicated NAT may bottleneck at NAT Gateway; NetworkPacketsOut metric shows if throughput is hitting limits."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A global SaaS product needs to track inference costs per customer across multiple SageMaker endpoints. What tagging and billing configuration should you apply?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add customer ID as an environment variable in the container and parse logs.",
      "B": "Use separate AWS accounts per customer and consolidate billing.",
      "C": "Instrument the SDK client to send customer metadata in X-ray trace.",
      "D": "Tag each endpoint with a customer cost center tag, activate cost allocation tags in Billing console, and use Cost Explorer grouped by tag."
    },
    "explanation": "Cost allocation tags are the AWS best practice for per-resource cost tracking in Cost Explorer."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A team uses Lambda functions for preprocessing before SageMaker. They receive occasional OutOfMemory errors in Lambda, causing increased retries and cost. How should they monitor and prevent this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase Lambda memory and rely on CloudWatch alarms on Duration.",
      "B": "Enable CloudWatch Logs Insights on Lambda error logs to filter OOM, set a CloudWatch alarm on Errors metric, and adjust memory allocation.",
      "C": "Switch to a Step Functions workflow to control memory.",
      "D": "Use X-Ray to trace memory usage of the function."
    },
    "explanation": "Errors metric combined with Logs Insights identifies OOM events; alarms notify and right-size memory to eliminate retries."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "Your preprocessing uses an ECS service on Fargate that sporadically fails CPU credit balance. Which CloudWatch metrics and configuration should you monitor and adjust?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Monitor Fargate MemoryReservation metric and increase CPU share.",
      "B": "Enable Container Insights and watch host CPUUtilization, then switch to EC2 launch type.",
      "C": "Monitor Fargate CPUCreditBalance and CPUUtilization in CloudWatch; adjust the CPU configuration (e.g., move from 0.25 vCPU to 0.5 vCPU) to maintain credits.",
      "D": "Use CloudTrail to log throttled tasks and set a budget alert."
    },
    "explanation": "Fargate tasks use CPU credits; monitoring CPUCreditBalance prevents throttling, and adjusting vCPU allocation maintains balance."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "An ML endpoint in production has highly variable traffic. The team configured auto scaling on CPU utilization. They see scale-in immediately after scale-out, causing thrashing and increased cost. How should they fix it?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add scale-in and scale-out cooldown periods and use target tracking on average invocation per instance.",
      "B": "Switch to step scaling based on p90 latency.",
      "C": "Increase the CPU utilization threshold for scale-in.",
      "D": "Use a multi-model endpoint instead to reduce container startup time."
    },
    "explanation": "Cooldown prevents rapid scale-in/out oscillations; target tracking on per-instance invocations stabilizes scaling."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "Your SageMaker Model Monitor baseline job generates drift violation alerts, but you also want to correlate them with infrastructure metrics to determine if CPU saturation causes data delays. How do you integrate these?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Ingest Model Monitor CloudWatch logs into ElasticSearch and join with CPU metrics.",
      "B": "Use CloudWatch metric math to overlay ModelMonitorDataQualityViolationCount and SageMakerEndpoint CPUUtilization in a unified dashboard.",
      "C": "Export Model Monitor S3 reports to Athena and join with CPU metrics exported by CloudWatch.",
      "D": "Implement a Lambda to capture both metrics on violation events and store in DynamoDB."
    },
    "explanation": "Metric math in CloudWatch provides immediate combining of application and infra metrics in one graph without custom code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A company is charged cross-account for S3 data egress when SageMaker in Account A reads from Account B buckets. They want to avoid egress charges. Which networking option solves this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable inter-account VPC peering and route traffic through private IP.",
      "B": "Use S3 Transfer Acceleration.",
      "C": "Create VPC Interface Endpoints (AWS PrivateLink) for S3 in Account A\u2019s VPC.",
      "D": "Copy data periodically to Account A\u2019s S3 bucket."
    },
    "explanation": "VPC Interface endpoints for S3 keep traffic within AWS network and avoid cross-account egress fees; bucket copy still incurs egress."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "Your nightly SageMaker hyperparameter tuning jobs sporadically hit EBS throughput limits, causing tunings to fail. Which combination of monitoring and remediation addresses this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable CloudTrail on CreateTrainingJob and throttle job submission.",
      "B": "Monitor EC2 Instance CPUUtilization and switch to instance with higher compute.",
      "C": "Use CloudWatch anomaly detection on TrainingJobRuntime metric.",
      "D": "Monitor EBS VolumeThroughputPercentage metrics and migrate to gp3 volumes with provisioned throughput."
    },
    "explanation": "VolumeThroughputPercentage shows EBS bottleneck; gp3 lets you tune throughput independently of volume size."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A data science team regularly creates and deletes dozens of notebooks in SageMaker Studio. The finance team wants to detect orphaned EBS volumes to stop incurring storage costs. What is the simplest solution?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Enable AWS Config managed rule for unattached EBS volumes and set automatic remediation to delete.",
      "B": "Write a Lambda to query DescribeVolumes for unattached volumes and delete.",
      "C": "Set CloudWatch Events on DeleteNotebookInstance to trigger volume clean up.",
      "D": "Use AWS Trusted Advisor\u2019s underutilized EBS check."
    },
    "explanation": "AWS Config rule for unattached volumes and auto-remediation offers built-in, no-code solution; Trusted Advisor only alerts."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A model training team wants to track costs per experiment across multiple SageMaker jobs, notebooks, and endpoints. Which tagging strategy and AWS service combination provides accurate reporting?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Tag resources at creation with ExperimentID and use CloudTrail logs to calculate costs.",
      "B": "Use SageMaker Experiments API to tag runs and rely on EMR cost reports.",
      "C": "Enable cost allocation tags for ExperimentID, tag all SageMaker resources, and view groupings in AWS Cost Explorer.",
      "D": "Import cost data into Quicksight and join on ExperimentID from logs."
    },
    "explanation": "Cost allocation tags in Cost Explorer directly associate resource costs with tags; Experiments API doesn\u2019t feed billing data automatically."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "Your preprocessing job uses AWS Glue and feeds data into SageMaker. You need to monitor Glue job CPU, Glue Data Catalog calls, and track Glue errors alongside SageMaker job failures in a single pane. Which tool do you choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CloudWatch dashboards per service linked manually.",
      "B": "CloudWatch Unified Dashboards with metrics from AWS/Glue and AWS/SageMaker plus logs insights widgets.",
      "C": "Athena querying Glue and SageMaker logs in S3.",
      "D": "QuickSight pulling billing data."
    },
    "explanation": "CloudWatch dashboards can incorporate multiple metrics and logs insights widgets for near real-time observability without custom ETL."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A SageMaker endpoint in a private subnet loses internet connectivity after a VPC route table change and fails to download model artifacts. How do you monitor and alert on such failures?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable CloudWatch Logs for the endpoint container and search for network exceptions.",
      "B": "Monitor CloudTrail events for GetObject failures on S3.",
      "C": "Set a CloudWatch alarm on EndpointInvocationErrors metric.",
      "D": "Create a CloudWatch anomaly detection alarm on SageMakerEndpointInvocation5XXErrors to catch connectivity failures."
    },
    "explanation": "5XX errors include network and container issues. Anomaly detection spots sudden error spikes without log parsing."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "Your inference pipeline uses Lambda behind API Gateway calling SageMaker endpoints. You notice high tail latencies occasionally. Which combined metrics should you monitor and how to differentiate between GP and HTTP latencies?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Monitor API Gateway Latency and IntegrationLatency in CloudWatch to separate client-facing vs. backend invocation delays.",
      "B": "Monitor SageMaker Endpoint ModelLatency and InvocationsPerInstance metrics only.",
      "C": "Enable X-Ray for Lambda and use cold start latency metrics.",
      "D": "Use CloudWatch Synthetics to measure end-to-end latency."
    },
    "explanation": "API Gateway Latency measures total, IntegrationLatency isolates backend; ModelLatency doesn\u2019t include network overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "Your team\u2019s SageMaker batch transform jobs intermittently fail due to insufficient vCPU quotas in a new AWS Region. What is the recommended monitoring step and long-term mitigation?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use CloudWatch to monitor ThrottledRequests and request a service quota increase manually when alarms fire.",
      "B": "Implement retries in the job script.",
      "C": "Create a CloudWatch alarm on ServiceQuotaExceeded metric via Service Quotas namespace and automate quota requests via AWS Service Quotas APIs.",
      "D": "Switch to GPU instances which have separate quotas."
    },
    "explanation": "ServiceQuotaExceeded metric alerts on hitting quotas. Automating via Service Quotas APIs ensures timely increases."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A global inference workload runs on SageMaker endpoints in multiple regions. They need to compare cost per inference across regions. What\u2019s the best way to collect and visualize this data?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Aggregate SageMaker billing metrics into CloudWatch using PutMetricData and graph them.",
      "B": "Enable cost allocation tags for Region on endpoints and use Cost Explorer filtering by tag.",
      "C": "Export Cost & Usage report to S3 and query with Athena manually.",
      "D": "Use QuickSight to connect to Billing API."
    },
    "explanation": "Cost Explorer with allocation tags gives near real-time region-based cost per resource without manual report queries."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "An ML endpoint behind a Network Load Balancer exhibits higher packet drop during bursts. Which VPC metric should you monitor, and what corrective action should you take?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Monitor NetworkBytesIn and move to larger instance.",
      "B": "Monitor HostNetworkRxErrors and increase MTU.",
      "C": "Monitor Flow Logs for REJECT entries and open more ports.",
      "D": "Monitor Elastic Network Interface (ENI) ReceivePacketDrops and use enhanced networking (ENA) instances or increase ENI count."
    },
    "explanation": "ReceivePacketDrops signals NIC saturation; ENA or more ENIs improves network performance for burst traffic."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.2",
    "stem": "A team using SageMaker Debugger for training also wants to monitor GPU utilization and EC2 metrics in one place. Which monitoring solution should they implement?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use CloudWatch Container Insights for the training job\u2019s underlying EC2 instances alongside Debugger tensor metrics.",
      "B": "Send EC2 metrics to CloudTrail for correlation.",
      "C": "Parse Debugger logs with CloudWatch Logs Insights and join with EC2 metrics in Athena.",
      "D": "Load all metrics into QuickSight and build a dashboard."
    },
    "explanation": "Container Insights collects EC2 host metrics and container-level metrics in CloudWatch seamlessly, enabling side-by-side visualization with Debugger metrics."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A team must restrict a SageMaker notebook instance so that it can read and write data only in a specific S3 bucket prefix. The notebook uses its execution role to access S3. Which configuration enforces least privilege?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Attach an IAM policy to the notebook execution role allowing s3:GetObject and s3:PutObject only on arn:aws:s3:::marketing-data/train/*",
      "B": "Add an S3 bucket policy granting the notebook role full access to the bucket and deny other principals",
      "C": "Create a VPC gateway endpoint for S3 with a policy restricting access to that prefix",
      "D": "Modify the KMS key policy to allow decryption only for that bucket prefix"
    },
    "explanation": "Least privilege is enforced by restricting the notebook\u2019s execution role with an inline IAM policy on the specific prefix. S3 endpoint policies or bucket policies can work but are more complex and less direct; KMS policy controls only encryption."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A security requirement mandates that SageMaker training jobs must not have internet access, yet must pull training images from Amazon ECR. How should you configure the training job?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Launch the training job in public subnets with a restrictive security group",
      "B": "Provide a customer-managed NAT gateway in the VPC used by the job",
      "C": "Specify VpcConfig with private subnets that have an ECR VPC endpoint and no NAT",
      "D": "Use a service-linked role that disables internet access by default"
    },
    "explanation": "By placing the training job in private subnets with an ECR VPC endpoint and no NAT gateway, you prevent internet access but allow ECR pulls. Public subnets or NAT would allow internet egress."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A SageMaker endpoint must be private within a VPC and only allow traffic from instances in another VPC over AWS PrivateLink. Which steps meet these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Launch endpoint without VPCConfig and restrict with security groups referencing peer VPC",
      "B": "Create interface VPC endpoint for SageMaker runtime in the consumer VPC, deploy the endpoint in a private subnet, and apply SG allowing traffic only from peer VPC CIDR",
      "C": "Use NAT gateway and public endpoint but restrict source IPs in SG",
      "D": "Enable endpoint public access and add route53 private hosted zone entries"
    },
    "explanation": "Deploying the endpoint in a VPC with no public access plus creating an interface endpoint in the consumer VPC over PrivateLink and restricting with SG enforces private-only access."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A DevOps engineer needs to grant AWS CodePipeline permission to deploy SageMaker models and endpoints but must follow least privilege. Which IAM policy attachment is correct?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Attach AmazonSageMakerFullAccess to the CodePipeline role",
      "B": "Allow sagemaker:* on resource '*' for the pipeline role",
      "C": "Create an SCP denying all actions except sagemaker:CreateEndpoint",
      "D": "Attach a scoped IAM policy allowing only sagemaker:CreateModel, sagemaker:CreateEndpointConfig, sagemaker:CreateEndpoint on identified resource ARNs"
    },
    "explanation": "Granting only the specific deploy actions on the exact model and endpoint ARNs enforces least privilege. FullAccess or wildcard would be overly permissive; SCP is for accounts."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "An organization requires CloudTrail logs of all SageMaker API calls to be encrypted using a KMS key managed by SecurityOperations. Which configuration satisfies this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable CloudTrail for SageMaker and leave default encryption",
      "B": "Configure CloudTrail with the SecurityOperations KMS key ARN for SSE-KMS encryption of logs",
      "C": "Apply an S3 bucket policy to encrypt objects using SSE-S3",
      "D": "Modify Trail event selectors to include SSE-KMS"
    },
    "explanation": "Specifying the KMS key ARN in the CloudTrail configuration ensures that logs are encrypted with that key. SSE-S3 or bucket policy alone cannot enforce use of a customer-managed key."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A notebook lifecycle configuration retrieves secrets from AWS Secrets Manager. You must ensure the lifecycle lambda can decrypt the secret but no other SageMaker service can. How do you tighten permissions?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Attach AWSSecretsManagerReadWrite to the notebook role",
      "B": "Add a resource policy on the secret allowing the notebook role",
      "C": "Add a KMS key policy granting decrypt only to the notebook lifecycle role principal",
      "D": "Enable automatic rotation on the secret"
    },
    "explanation": "Tightening access at the KMS key policy level restricts decryption to only the specified lifecycle role. Resource policies on the secret do not control the KMS decryption step."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A SOC team requires that any SageMaker endpoint creation must be approved by a central admin. How can you enforce this in CodePipeline?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a Manual Approval action before the CreateEndpoint stage",
      "B": "Use a CloudWatch rule to detect CreateEndpoint and block it",
      "C": "Implement an SCP to deny sagemaker:CreateEndpoint",
      "D": "Require MFA in the pipeline role"
    },
    "explanation": "A Manual Approval action in CodePipeline forces human approval. SCP would prevent creation globally; CloudWatch can only alert after the fact."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A model registry in SageMaker stores sensitive models. You need to prevent direct API deletes of models, but allow deletion through a daily cleanup Lambda. Which IAM policy accomplishes this?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Deny sagemaker:DeleteModel to everyone",
      "B": "Remove sagemaker:DeleteModel from all roles except Lambda by attaching a policy to the Lambda role only",
      "C": "Create an SCP denying DeleteModel for all principals",
      "D": "Attach a deny statement in a permission boundary that applies to principals unless aws:PrincipalArn equals the Lambda function execution role"
    },
    "explanation": "A permission boundary with a conditional deny unless the principal is the cleanup Lambda enforces that only that role may delete models, without affecting Service roles."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A compliance rule requires that only approved subnets are used in SageMaker jobs. How can you enforce this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use an SCP to deny SageMaker calls in unapproved subnets",
      "B": "Write an IAM permissions policy with Deny on sagemaker:CreateTrainingJob when sagemaker:VpcConfig.Subnets \u2260 approved IDs",
      "C": "Enforce via CloudWatch alarms and manual remediation",
      "D": "Use AWS Config managed rules without enforcement"
    },
    "explanation": "Embedding a Deny in the IAM policy conditioned on the VpcConfig.Subnets attribute blocks jobs launched in unapproved subnets. SCP cannot inspect parameters."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A training job needs to write to a DynamoDB table, but only to specific items. How do you restrict the SageMaker execution role accordingly?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Grant dynamodb:* on the table ARN",
      "B": "Attach AmazonDynamoDBFullAccess",
      "C": "Grant dynamodb:PutItem only on arn:aws:dynamodb:\u2026:table/MyTable with a Condition on dynamodb:LeadingKeys for the key prefix",
      "D": "Use an SCP to restrict all DynamoDB writes"
    },
    "explanation": "Using a resource-level IAM policy with a condition on LeadingKeys ensures writes only to items matching the key prefix. Full table or wildcard actions are too broad."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A DevSecOps requirement: all SageMaker model artifacts in S3 must be encrypted with a specific KMS key. Which is the simplest enforcement?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add SSE-KMS in the CreateModel call parameters",
      "B": "Use a bucket policy requiring x-amz-server-side-encryption header",
      "C": "Implement a CloudWatch rule that checks artifact uploads",
      "D": "Add an S3 bucket policy Deny PutObject unless x-amz-server-side-encryption-aws-kms-key-id matches the key"
    },
    "explanation": "An S3 bucket policy that denies PutObject requests unless the correct KMS key ID header is present enforces encryption at write time."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A SageMaker pipeline stores intermediate data in S3. The security team wants to prevent access to these artifacts except by the pipeline service. How do you enforce this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Grant pipeline roles wide access to the bucket",
      "B": "Use an S3 bucket policy allowing only the SageMaker Pipelines service principal and the pipeline execution role ARN",
      "C": "Encrypt with a KMS key and rely on default key policy",
      "D": "Use a resource-based IAM policy on the pipeline role"
    },
    "explanation": "An S3 bucket policy that permits only the SageMaker Pipelines service principal and the specific execution role to access the bucket restricts all other access."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A model deployed in SageMaker must only be invoked by authenticated users in an Amazon Cognito identity pool. How can you secure the endpoint?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use an endpoint policy with Allow for principal '*' and check Token in code",
      "B": "Attach a resource policy to Cognito to allow SageMakerInvoke",
      "C": "Create an endpoint policy requiring aws:PrincipalOrgID and aws:RequestContext.authorizer.claims.sub from Cognito identity",
      "D": "Use a VPC endpoint and IP-based SG only"
    },
    "explanation": "An endpoint policy can restrict invocation to principals from the specific Cognito identity pool by checking the principal\u2019s token claims in aws:RequestContext."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A notebook lifecycle script clones code from CodeCommit. The IAM role has ListRepos and GitPull permissions but the clone fails. On inspecting the trust policy, you find no conditions. What\u2019s the missing configuration?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add an SCP allowing codecommit:GitPull",
      "B": "Add a Condition in the IAM trust policy restricting the service principal to sagemaker.amazonaws.com",
      "C": "Attach AWSCodeCommitPowerUser to the role",
      "D": "Enable cross-account access in CodeCommit"
    },
    "explanation": "Notebook lifecycle functions are invoked by the SageMaker service, so the role\u2019s trust policy must allow sagemaker.amazonaws.com as a principal to assume the role. Without it, the role cannot be assumed and clone fails."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A security audit reveals CloudWatch Logs for SageMaker were publicly accessible. How do you block public access while preserving individual user access?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Delete the log group ACLs",
      "B": "Enable KMS encryption on the log group",
      "C": "Apply an SCP to prevent DescribeLogGroups",
      "D": "Attach a resource policy to the log group denying Principal '*' for log:Describe and log:Get"
    },
    "explanation": "A CloudWatch Logs resource policy can explicitly deny calls from anonymous principals (*) while allowing authenticated IAM principals to continue accessing the logs."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A data scientist left a SageMaker endpoint open to the internet. You need to remediate: lock down to a VPC in us-east-1 and allow only IPs from 10.0.0.0/16. What is the fastest remediation?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Delete the endpoint and recreate within a VPC",
      "B": "Update the endpoint configuration with VpcConfig and apply a security group that allows only 10.0.0.0/16",
      "C": "Attach an endpoint policy restricting CIDR",
      "D": "Apply a bucket policy on model artifacts"
    },
    "explanation": "Updating the existing endpoint\u2019s VpcConfig and security group is fastest. Deletion and recreation is slower; endpoint policies do not control network access."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "Your CodeBuild project for ML uses Docker containers on shared hardware. You must ensure build logs are encrypted with a custom KMS key. Which change meets this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable CloudWatch Logs for the project",
      "B": "Attach AWSKeyManagementServicePowerUser to CodeBuild role",
      "C": "Specify project\u2019s logsConfig.cloudWatchLogs.encryptionDisabled=false and kmsKey in CloudFormation",
      "D": "Use SSE-S3 for the CodeBuild logs bucket"
    },
    "explanation": "In the CodeBuild logsConfig, setting encryptionDisabled to false and specifying the KMS key ensures logs are encrypted with the customer-managed key."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A compliance mandate: all secrets accessed by SageMaker (e.g., DB credentials) must require KMS grant usage via key grants rather than policy. How to implement?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Use KMS grants to give the SageMaker execution role temporary decrypt privileges for the secret\u2019s CMK",
      "B": "Modify the key policy to allow the SageMaker role Decrypt",
      "C": "Add the SageMaker role to a KMS key alias",
      "D": "Enable automatic rotation on the secret"
    },
    "explanation": "KMS grants provide temporary, auditable decrypt permissions. Modifying key policies is static and not per-request as required."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A model registry must only allow promotion of approved models. You need to restrict who can call CreateModelPackageVersion. What\u2019s the best practice?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use an SCP denying sagemaker:CreateModelPackageVersion",
      "B": "Enable AWS Config rule for non-approved models",
      "C": "Create a SageMaker model registry tag rule",
      "D": "Attach an IAM policy requiring a specific request tag (e.g., ApprovedBy) for CreateModelPackageVersion"
    },
    "explanation": "Requiring a request tag via IAM policy ensures that only requests including the ApprovedBy tag can create new versions, enforcing the approval workflow."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A Dev team\u2019s pipeline stores its CloudFormation templates in S3. You must ensure only specific pipeline roles can read and write these templates. Which is correct?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Grant public read and limit write via signed URLs",
      "B": "Apply S3 bucket policy allowing List, Get, Put only to the IAM roles of the pipeline stages",
      "C": "Use KMS encryption only",
      "D": "Restrict via VPC endpoint policy"
    },
    "explanation": "An S3 bucket policy scoped to only the pipeline\u2019s IAM role ARNs for List, Get, and Put enforces that no other principals can read or write."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A post-deployment check must validate that SageMaker Notebook Instances are encrypted at rest. Which automated check fulfills this?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "CloudWatch alarm on unencrypted volumes",
      "B": "Enable AWS Config managed rule EC2_ENCRYPTED_VOLUMES",
      "C": "Enable AWS Config managed rule SAGEMAKER_NOTEBOOK_INSTANCE_ENCRYPTED_VOLUME_CHECK",
      "D": "Use GuardDuty on notebooks"
    },
    "explanation": "AWS Config\u2019s SageMaker-specific managed rule checks encryption of notebook volumes directly. EC2 rules do not cover SageMaker notebook volumes."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A SageMaker Processing job uses a role that also has S3ListAllMyBuckets permission. Security wants to remove ListAll permission but still allow list on two buckets. How to implement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Replace wildcard list permission with explicit s3:ListBucket on the two bucket ARNs",
      "B": "Add an SCP denying s3:ListAllMyBuckets",
      "C": "Use bucket policies on S3 buckets",
      "D": "Rotate IAM role credentials daily"
    },
    "explanation": "Restricting the role\u2019s IAM policy to only s3:ListBucket on the two specified buckets removes global list permission while allowing needed buckets."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A pipeline deploys a SageMaker model using AWS CDK. The synth step fails due to lack of KMS decrypt for the secret used in a CDK context. How can you fix this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Grant the CDK role kms:GenerateDataKey*",
      "B": "Attach SecretsManagerReadWrite to CDK role",
      "C": "Use plaintext secret in code",
      "D": "Grant the CDK execution role kms:Decrypt on the secret\u2019s CMK in the key policy"
    },
    "explanation": "The CDK synth needs kms:Decrypt rights on the CMK to read the secret. Adding that to the key policy resolves the failure."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A regulatory requirement: all IAM role assumptions by SageMaker must be logged and alerts sent on unusual assume-role calls. How do you implement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable CloudWatch Logs on SageMaker",
      "B": "Create CloudTrail trail capturing IAM:AssumeRole, create CloudWatch metric filter for the SageMaker service principal, and alarm",
      "C": "Use AWS Config rule to check unusual assume-role calls",
      "D": "Rotate the SageMaker service role daily"
    },
    "explanation": "A CloudTrail trail logs all AssumeRole calls; a metric filter on the CloudWatch Logs for the SageMaker principal can trigger alarms for any assumption events."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A data scientist requests temporary credentials for S3 to use inside a notebook, without exposing long-lived keys. What is the most secure pattern?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use AWS STS GetSessionToken in the notebook to generate time-limited credentials",
      "B": "Embed an IAM user access key in the notebook",
      "C": "Use an S3 pre-signed URL for each object access",
      "D": "Use root account credentials stored in Secrets Manager"
    },
    "explanation": "Using STS GetSessionToken provides temporary credentials with limited lifetime. Pre-signed URLs are object-specific and not convenient; root or static keys are insecure."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A SageMaker Endpoint in VPC must only communicate with ECR and CloudWatch Logs, not the public internet. How do you enforce this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Attach a bucket policy on ECR",
      "B": "Use NAT Gateway with restrictive route tables",
      "C": "Deploy endpoint in private subnets with interface endpoints for ECR and CloudWatch Logs and no NAT",
      "D": "Use Security Group to block 0.0.0.0/0"
    },
    "explanation": "Interface VPC endpoints allow access to AWS services without internet. Deploying in private subnets without NAT ensures no egress to public internet."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A build in CodeBuild requires access to a VPC-only RDS instance. The CodeBuild service role has all necessary DB permissions, but builds fail to connect. What must you configure?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Open the DB\u2019s security group to 0.0.0.0/0",
      "B": "Configure the CodeBuild project\u2019s VpcConfig with the DB\u2019s subnets and security group",
      "C": "Use a NAT gateway in the project\u2019s VPC",
      "D": "Attach AWSLambdaVPCAccessExecutionRole to the CodeBuild role"
    },
    "explanation": "CodeBuild must be configured with VpcConfig to attach ENIs in the same subnets/security groups as the RDS. Without this, it runs in default network and cannot reach the DB."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "An audit finds that some SageMaker endpoint access logs contain PII. You must restrict retrieval of those logs to a security analysis role. How?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Attach a resource policy on the CloudWatch Logs group allowing only the security role principal to get logs",
      "B": "Encrypt logs with SSE-S3",
      "C": "Use an SCP to deny DescribeLogStreams",
      "D": "Disable logging on the endpoint"
    },
    "explanation": "A CloudWatch Logs resource policy scoped to the security analysis role ensures only that role can retrieve the logs; others will be denied."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A new security control: SageMaker projects must only use predefined IAM permission boundaries. How to ensure new roles created by SageMaker pipelines comply?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use an SCP to deny iam:CreateRole",
      "B": "Tag all new roles and enforce with AWS Config",
      "C": "Use CodePipeline approval stage",
      "D": "Attach the managed permission boundary policy in the SageMaker pipeline role so that any new role inherits the boundary"
    },
    "explanation": "Permission boundaries attached to the role that creates new roles propagate the boundary to child roles, ensuring they cannot be more permissive."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A SageMaker Model Monitor job must write drift reports to S3 but only to a single bucket. The monitor role has wild-card S3 permissions. How do you tighten permissions without affecting other workflows?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a bucket policy to deny PutObject from that role",
      "B": "Replace the role\u2019s S3 wildcard with a policy granting PutObject only on the monitor reports bucket",
      "C": "Implement an SCP",
      "D": "Encrypt the bucket with default encryption"
    },
    "explanation": "Updating the monitor role\u2019s IAM policy to allow only the specific bucket reduces blast radius without impacting other S3 workflows."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.3",
    "stem": "A security guideline: all SageMaker processing jobs must use a VPC endpoint to S3. One job failed to access data. Which misconfiguration is most likely?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "The role lacks s3:GetObject",
      "B": "The bucket policy denies all traffic",
      "C": "VpcConfig lacked the S3 gateway endpoint in the subnet route tables",
      "D": "The job ran in a public subnet"
    },
    "explanation": "If the S3 gateway endpoint is not added to the subnet route tables in VpcConfig, the job cannot reach S3. A public subnet or role error would yield different symptoms."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "A data science team needs to deploy a SageMaker real-time inference endpoint in a private VPC. They want to script the entire environment using AWS CloudFormation. Which resources and configuration should they include to ensure proper VPC connectivity and auto scaling of the endpoint?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Define an AWS::SageMaker::Endpoint with VpcConfig, and rely on SageMaker\u2019s built-in auto scaling by setting EnableAutoScaling: true.",
      "B": "Create AWS::SageMaker::EndpointConfig with VpcConfig.SecurityGroupIds and Subnets, an AWS::SageMaker::Endpoint that references that config, an AWS::ApplicationAutoScaling::ScalableTarget for sagemaker:variant:DesiredInstanceCount, and an AWS::ApplicationAutoScaling::ScalingPolicy.",
      "C": "Use AWS::EC2::VPCEndpoint for SageMaker API, then AWS::AutoScaling::AutoScalingGroup to scale instances behind the endpoint.",
      "D": "Use AWS::Lambda-backed custom resource to modify the SageMaker endpoint configuration after deploy for VPC and scaling settings."
    },
    "explanation": "CloudFormation requires separate AppAutoScaling resources; SageMaker endpoint resource does not directly support scaling."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "An ML engineer wants to provision a multi-container SageMaker endpoint via AWS CDK in Python. They need one container for model A, one for model B, each with different IAM roles and different environment variables. Which CDK constructs and pattern should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use sagemaker.CfnModel for each container with respective environment and roles, then sagemaker.CfnEndpointConfig listing both models, and sagemaker.CfnEndpoint.",
      "B": "Use sagemaker.Model for each container, then sagemaker.MultiModelEndpoint to combine them automatically.",
      "C": "Define two sagemaker.CfnModel constructs with separate ExecutionRoleArn and Environment properties, create a single sagemaker.CfnEndpointConfig with both ModelName entries under ProductionVariants, then a sagemaker.CfnEndpoint.",
      "D": "Use sagemaker.EndpointBatchTransform with multiple TransformJobDefinitions for each container."
    },
    "explanation": "Multi-container real-time endpoints require separate CfnModel and listing in a single EndpointConfig."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "A team uses AWS CDK to deploy their SageMaker endpoint, but CloudFormation reports drift in the EndpointConfig when they manually update the environment variables via console. How can they prevent drift and enforce configuration as code?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Remove manual updates; enforce changes only through CDK by maintaining environment variables in code and deploying via CDK pipelines.",
      "B": "Set EnableDriftCorrection: true on the CfnEndpointConfig resource.",
      "C": "Use AWS Config to ignore drift on SageMaker endpoint resources.",
      "D": "Use a CloudFormation macro to override the console changes at deployment time."
    },
    "explanation": "Prevent drift by managing all changes in code; CloudFormation cannot auto-correct SageMaker endpoints."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "An organization requires that all SageMaker endpoint logs be sent to CloudWatch Logs in a separate AWS account. They want to script this via AWS CloudFormation. Which approach satisfies least-privilege and cross-account delivery?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Grant the SageMaker execution role cross-account CloudWatchPutLogEvents permission.",
      "B": "Attach AWS::Logs::SubscriptionFilter on the endpoint\u2019s log group to the destination account.",
      "C": "Use AWS::CloudWatch::Destination in the SageMaker stack to forward logs directly.",
      "D": "Deploy a CloudWatch Log Group export task via AWS::Logs::Destination in the central account and use resource policy on the SageMaker log group to allow PutSubscriptionFilter to the destination."
    },
    "explanation": "Cross-account subscription requires a CloudWatch Logs destination with resource policy granting permissions."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "A dev team needs to automate creation of multiple SageMaker endpoints with different instance types and scaling policies using AWS CDK. They want to avoid duplication of code. Which CDK pattern is most appropriate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Write separate stacks for each endpoint and instantiate them individually.",
      "B": "Define a high-level construct class that takes instance type and scaling parameters as properties, and reuse it for each endpoint.",
      "C": "Use a single CfnInclude to import an existing CloudFormation template with parameters.",
      "D": "Leverage AWS::CloudFormation::Stack resource within the same template for each endpoint."
    },
    "explanation": "A custom CDK construct promotes reuse and parameterization across multiple endpoints."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "A security audit requires that SageMaker endpoints be deployed only within private subnets and routed through NAT for internet access. How should this be scripted in AWS CDK to satisfy both requirements?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Specify subnetSelection with subnetType.PUBLIC in VpcConfig so endpoints use NAT automatically.",
      "B": "Set EndpointConfig.KmsKeyId to use private subnets.",
      "C": "Pass private subnet IDs and security group IDs in VpcConfig when defining CfnEndpointConfig in CDK, ensuring they are Private isolated subnets with NAT configured on the VPC.",
      "D": "Use VpcLink to attach the endpoint to a private Network Load Balancer."
    },
    "explanation": "VpcConfig must reference private subnets and SGs; NAT is configured at VPC level, not endpoint."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "An ML team wants to integrate spot instances into their inference fleet behind an Application Load Balancer with automatic scaling. They decide to use ECS on Fargate Spot. Which CloudFormation resource definitions must they include to ensure the desired capacity and scaling behavior?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS::ECS::CapacityProvider with AutoScalingGroupProvider using Spot Instances, AWS::ECS::Cluster defaultCapacityProviderStrategy including that capacity provider, and AWS::ECS::Service with desiredCount parameter.",
      "B": "AWS::AutoScaling::AutoScalingGroup with MixedInstancesPolicy using ON_DEMAND_ONLY, and AWS::ECS::Service.",
      "C": "AWS::ECS::Service with LaunchType FARGATE only, and AWS::ApplicationAutoScaling::ScalableTarget for ECS service.",
      "D": "AWS::Batch::ComputeEnvironment configured for Spot, and AWS::Batch::JobDefinition for inference jobs."
    },
    "explanation": "ECS Fargate Spot requires a capacity provider defined, then attach to the service for scaling on Spot."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "Your organization mandates that all ML model container images be stored in Amazon ECR with image scanning enabled at push time. You must script this using AWS CDK. Which steps are required?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Define an ECR repository in CDK with imageScanningConfiguration enabled, grant SageMaker execution role pull permissions, then use that repository URI in CfnModel.",
      "B": "Use ecr.Repository with imageScanOnPush: true in CDK, add repository.grantPull(sagemakerRole), then pass repository.repositoryUri to SageMaker Model property in CfnModel or CfnModelPackage.",
      "C": "Create AWS::ECR::LifecyclePolicy in CloudFormation and set scanOnPush: ENABLED.",
      "D": "Use AWS CLI in a custom resource to enable image scan on existing repo."
    },
    "explanation": "CDK\u2019s ecr.Repository supports imageScanOnPush; then grant pull to the role used by SageMaker."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "A SageMaker endpoint fails health checks intermittently due to VPC ENI provisioning delays, causing AWS CloudFormation deployment to rollback. How can you modify your CloudFormation template to handle this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set EndpointConfig.DeleteAfterUse: false.",
      "B": "Add CreationPolicy with ResourceSignal on AWS::SageMaker::Endpoint.",
      "C": "Wrap the CfnEndpoint resource in a Custom::SageMakerEndpoint that retries.",
      "D": "Use DependsOn with a custom wait condition Lambda that polls DescribeEndpointStatus until InService before signaling success."
    },
    "explanation": "CloudFormation offers no native retry for SageMaker endpoints; implement a wait condition via Lambda or custom resource."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "Your infra-as-code pipeline uses AWS CDK to deploy a SageMaker inference endpoint. During a deployment, you need to update the endpoint's variant weight to shift 20% traffic to a new model variant. How do you script this with least downtime?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Update the existing EndpointConfig in-place via CfnUpdatePolicy in CloudFormation.",
      "B": "Delete and recreate the endpoint with the new variant weight settings.",
      "C": "Create a new AWS::SageMaker::EndpointConfig with both variants and desired weights, then update the AWS::SageMaker::Endpoint to point to the new config.",
      "D": "Use AWS CLI in a CDK CustomResource to call UpdateEndpointWeight directly."
    },
    "explanation": "Endpoint updates must reference a new EndpointConfig; creating a new config and pointing the endpoint to it avoids downtime."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "A DevOps engineer must ensure that SageMaker endpoint deployments are idempotent and environment-specific. They decide to use CloudFormation parameter overrides. Which of these strategies will achieve both requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Store instance type and count in template metadata and reference via Fn::GetAtt.",
      "B": "Define Parameters for InstanceType, InitialInstanceCount, VPC Subnet IDs, and SecurityGroup IDs; reference them in CfnEndpointConfig, ensuring different values per environment.",
      "C": "Use Mappings section keyed by environment name to look up all values.",
      "D": "Use a separate template per environment to hardcode values."
    },
    "explanation": "Parameters allow idempotent, repeatable deployments with all settings externalized per environment."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "You need to deploy a SageMaker multi-model endpoint for cost-effective hosting of hundreds of small models. You want to automate provisioning with CloudFormation and enforce container image immutability. What is the correct approach?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create one AWS::SageMaker::ModelPackage for each container with ModelApprovalStatus set to Approved and InferenceSpecification referencing ECR image digests, then one AWS::SageMaker::EndpointConfig with MultiModelConfig.",
      "B": "Use AWS::SageMaker::Model with LocalCode option and set ImageConfig.AutoUpdate: false.",
      "C": "Deploy a custom multi-model server on an EC2 Auto Scaling group instead.",
      "D": "Use a single AWS::SageMaker::Endpoint with MultiModelEndpoint property."
    },
    "explanation": "ModelPackage with approved inference spec and image digest ensures immutability; EndpointConfig multi-model supports dynamic loading."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "A regulated environment requires all SageMaker endpoints to be deployed via AWS CloudFormation and reviewed via change sets. When a change to endpoint instance type is approved, CloudFormation update fails because of immutable properties. How should you script your template to allow instance-type changes without replacing resources?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set UpdateReplacePolicy: Retain on the Endpoint resource.",
      "B": "Use AWS::SageMaker::EndpointConfig with EndpointConfigName generated by a GUID to force replacement.",
      "C": "Use a nested stack for the endpoint and update only nested stacks.",
      "D": "Decouple endpoint from endpoint config: script EndpointConfig as separate resource and update the EndpointConfigName in the Endpoint resource, so only the config is replaced, not the endpoint."
    },
    "explanation": "EndpointConfig can be replaced independently; endpoint resource simply points to new config name."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "Your team wants to use AWS CDK in TypeScript to deploy SageMaker endpoints and test them automatically post-deployment. They need to add an automated post-deploy test hook in the same pipeline. Which CDK pattern should they adopt?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use CDK aspect to inject Lambda-backed custom resources.",
      "B": "Utilize CDK Pipelines\u2019 testing framework: add CodeBuild Step after deployment stage invoking a smoke-test Lambda or script.",
      "C": "Add AWS::CodeDeploy::DeploymentGroup in the CDK template.",
      "D": "Write a CDK Custom Resource that runs tests as part of deployment."
    },
    "explanation": "CDK Pipelines support post-deploy tests via CodeBuild steps integrated into the pipeline."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "An engineer must ensure that SageMaker endpoint container images are built, pushed to ECR, and deployed in one IaC deployment. Using AWS CDK, which sequence of constructs accomplishes this atomically?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Define a CodeBuild Project in CDK that builds the image, then a separate CDK app that references the ECR repository.",
      "B": "Use a custom resource to call AWS CLI to build and push, then define the CfnModel.",
      "C": "Use CodePipeline CDK constructs: define a pipeline with Source stage, Build stage (CodeBuild building and pushing to ECR), then a Deploy stage with CDK deploy of the SageMaker endpoint stack referencing image URI as pipeline output.",
      "D": "Use CDK Bundling API on the sagemaker.Model construct."
    },
    "explanation": "CDK Pipelines can orchestrate build and deploy stages, ensuring atomic transition from image build to endpoint deployment."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "Your infra team must grant least-privilege permissions for SageMaker to pull container images from a private ECR in another AWS account. Which IAM policy should you script in CloudFormation on the SageMaker execution role?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Allow ecr:BatchGetImage, ecr:GetDownloadUrlForLayer on the repository Arn directly in the role policy.",
      "B": "Policy allowing sts:AssumeRole on a role in the ECR account that has ecr:GetAuthorizationToken, BatchGetImage, GetDownloadUrlForLayer, and in the ECR account\u2019s repository policy grant the SageMaker role\u2019s AWS principal those actions.",
      "C": "Grant AmazonEC2ContainerRegistryFullAccess to the SageMaker execution role.",
      "D": "Add AWS-managed policy AmazonSageMakerFullAccess which includes ECR pull permissions."
    },
    "explanation": "Cross-account ECR pull requires AssumeRole into the ECR account and corresponding repository policy; broad managed policies violate least privilege."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "A team uses AWS::CloudFormation::Stack resource to encapsulate SageMaker endpoint provisioning in a nested stack. They now need to expose the endpoint name to the parent stack for AutoScaling configuration. Which method achieves this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "In the nested stack template, declare an Output for EndpointName; in the parent stack, reference it via Fn::GetAtt on the AWS::CloudFormation::Stack resource.",
      "B": "Use ImportValue to pull it from the nested stack without declaring Output.",
      "C": "Write a Lambda-backed custom resource in the parent to call DescribeStacks.",
      "D": "Use AWS::SSM::Parameter to store the endpoint name and read it in the parent."
    },
    "explanation": "Outputs and Fn::GetAtt on nested stacks pass values to parent stacks."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "Your company requires that all SageMaker endpoint configurations be version controlled and reviewed. Which AWS CDK and CloudFormation features should you use to allow change visibility and prevent unreviewed drift?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable termination protection on CfnEndpoint resources.",
      "B": "Use sagemaker.Endpoint.fromEndpointName to reference existing endpoint without managing it.",
      "C": "Use CDK to synthesize CloudFormation change sets and require manual approval before execution in pipelines.",
      "D": "Enable rolling updates in CloudFormation on the SageMaker stack."
    },
    "explanation": "Change sets provide visibility into what will change before applying; CDK pipelines can enforce manual approvals."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "An ML engineer needs to script blue/green deployments of a SageMaker endpoint using AWS CloudFormation and minimize invocation errors during traffic shift. Which pattern should they implement?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS::SageMaker::EndpointVariants property to specify both blue and green variants and weights.",
      "B": "Define two separate Endpoint resources and swap DNS records in Route53.",
      "C": "Use CodeDeploy with blue/green strategy for SageMaker service.",
      "D": "Use AWS::SageMaker::EndpointConfig to define two production variants with weights, attach AppAutoScaling policy to adjust weights gradually as part of CloudFormation update."
    },
    "explanation": "Defining both variants and adjusting weights via scaling policies enables gradual traffic shift without downtime."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "A DevOps pipeline must deploy a SageMaker batch transform job definition and a serving endpoint in one CloudFormation template. They encounter circular dependency between the IAM role and the endpoint config. How can they resolve this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Duplicate an IAM role resource for each service to break the cycle.",
      "B": "Use separate CloudFormation nested stacks: one for IAM role definition, one for endpoint and transform referencing an exported role ARN.",
      "C": "Use DependsOn between the role and endpoint config to force order.",
      "D": "Use AWS::IAM::ManagedPolicy to attach permissions instead of inline policies."
    },
    "explanation": "Splitting into nested stacks allows exporting the role ARN to avoid circular resource references."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "Your infrastructure uses AWS CDK to deploy SageMaker endpoints. A CloudFormation rollback occurs because the endpoint creation takes longer than the default timeout. How can you adjust the timeout in CDK?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set cfnOptions.creationPolicy with a Timeout value on the CfnEndpoint CDK construct.",
      "B": "Set the AWS SDK client timeout in CDK context.",
      "C": "Use AWS::CloudFormation::WaitCondition within the same stack.",
      "D": "Wrap the deployment in a longer-running AWS Step Functions state machine."
    },
    "explanation": "CDK\u2019s cfnOptions.creationPolicy allows specifying timeout for resource creation; other methods don\u2019t apply to SageMaker endpoints."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "An ML platform team must deploy hundreds of endpoints with varying instance counts. They choose AWS CDK to loop constructs. How should they write their code to maintain performance and minimize synth time?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a for loop in CDK to create each endpoint construct directly in the stack.",
      "B": "Use AWS::CloudFormation::Stack for each endpoint to parallelize.",
      "C": "Generate endpoint configurations via a CDK Construct that iterates over a configuration array at synthesis time, rather than at runtime, and emits only necessary CloudFormation resources.",
      "D": "Use AWS Step Functions to call CDK synth for each endpoint separately."
    },
    "explanation": "Constructs iterate at synth time to produce a single CF template; runtime loops in Lambda or Step Functions are inappropriate."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "A compliance requirement mandates that all SageMaker endpoints be deployed in dedicated subnets per environment. You already have a CDK VPC with multiple isolated subnet groups. How do you guarantee subnet selection per environment?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Hardcode subnet IDs in the CDK code per environment.",
      "B": "Define CDK Stack context values for subnetGroupNames per environment and use vpc.selectSubnets({subnetGroupName: ...}).",
      "C": "Use AWS::TaggedResource tagging on the VPC and filter subnets via CloudFormation Intrinsics.",
      "D": "Use defaultSubnets selection and rely on AWS to pick isolated ones."
    },
    "explanation": "CDK context allows environment-specific parameters; vpc.selectSubnets filters by groupName tag."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.2",
    "stem": "Your team wants to use AWS CDK to produce YAML CloudFormation templates for all resources. They need to validate their template for SageMaker endpoint VPC settings before deployment. Which CDK command and plugin should they use?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "cdk synth --no-staging and the cfn-lint plugin to lint the synthesized template.",
      "B": "cdk diff and AWS::SageMaker::ValidateEndpoint API.",
      "C": "cdk deploy --dry-run.",
      "D": "cdk docs to generate CloudFormation schema and manually inspect."
    },
    "explanation": "cdk synth outputs the template for cfn-lint validation; diff shows changes but not schema errors."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A healthcare provider has a highly imbalanced dataset (1% positive cases) stored in Amazon S3. They need to generate synthetic minority samples to train a disease detection model, ensure PHI compliance (no real patient data leakage), and prevent inversion attacks on synthetic data. Which approach best meets these requirements?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "Use AWS Glue DataBrew to oversample the minority class and enable record-level masking to anonymize PHI.",
      "B": "Implement a CTGAN model in a SageMaker notebook with differential privacy (DP-SGD), generate synthetic samples within a VPC-backed FSx for Lustre, and use a KMS key for encryption.",
      "C": "Use SageMaker Clarify\u2019s synthetic data feature to generate samples and store results in an S3 bucket with default encryption.",
      "D": "Subscribe to a third-party synthetic health dataset via Amazon Data Exchange and merge it with your S3 data."
    },
    "explanation": "CTGAN with DP-SGD ensures differential privacy (protects against inversion), runs in SageMaker within the VPC, uses FSx for high-performance storage, and KMS encryption prevents PHI leakage, satisfying all requirements."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A bank wants to detect selection bias in its loan application dataset (applicants by ZIP code). They must quantify the difference in proportions of approval rates between urban and rural ZIP codes before training. Which SageMaker Clarify pre-training bias configuration is most appropriate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use bias_config with 'label'='approval', 'facet_name'='ZIP_code_type', and 'metrics'=['dipl','ci'].",
      "B": "Use ModelMonitor with DataQualityMonitor to compute missing values in the ZIP_code_type feature.",
      "C": "Use bias_config with 'facet_name'='approval', 'label_values_or_threshold'=['urban','rural'], and 'metrics'=['feature_attribution'].",
      "D": "Use Clarify post-training explainability job to get SHAP values for ZIP_code_type."
    },
    "explanation": "Pre-training bias_config must specify label and facet_name (ZIP_code_type) to compute DPL (difference in proportions of labels) and CI (class imbalance) before training. Other options misuse metrics or post-training tools."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "An online retailer stores customer data in DynamoDB with encrypted PII (customer_id, email). They export it to S3 for cleaning. They need to mask PII during data cleansing using AWS Glue DataBrew. Which recipe action and configuration should they use?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Use 'maskValue' on columns [customer_id,email] with a static masked value.",
      "B": "Use 'hash' on columns [customer_id] and 'tokenize' on [email] with default salts.",
      "C": "Use 'encrypt' on columns [customer_id,email] using the DynamoDB table key.",
      "D": "Use 'maskValue' with 'Mask Type'='Random Character', 'Character Set'='Alphanumeric', on [customer_id,email]."
    },
    "explanation": "DataBrew\u2019s maskValue recipe action with Random Character on specified PII columns properly masks values without exposing patterns. Hash and encryption aren\u2019t available as DataBrew recipe actions, and static mask would reveal structure."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A credit-scoring dataset has 10% missing values in the 'employment_length' feature. You need to impute missing values to reduce measurement bias and avoid skew in training. The distribution is right-skewed. What is the best imputation strategy using AWS Glue Data Quality rules?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Impute missing 'employment_length' with the mean value computed over all records.",
      "B": "Use median imputation per gender group via a DataBrew 'Fill with median' action partitioned by 'gender'.",
      "C": "Use a Data Quality job to compute the median per 'employment_type' and then apply DataBrew 'Fill with value from column aggregation' partitioned by 'employment_type'.",
      "D": "Apply mode imputation globally for 'employment_length' using the most frequent category."
    },
    "explanation": "Median per employment_type addresses right skew within similar segments and reduces bias. A Data Quality job can compute segment medians and DataBrew can apply partitioned imputation accordingly."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "You must ensure all S3 data at rest and in transit is encrypted for an ML training job. Data is loaded from S3 into a SageMaker Training job and stored on an FSx for Lustre file system. Which configuration satisfies both requirements?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable SSE-KMS on the S3 bucket, attach an IAM role with KMS decrypt, launch FSx with transit encryption enabled and a customer-managed KMS key.",
      "B": "Use SSE-S3 on the S3 bucket and launch FSx with default AWS-managed KMS key.",
      "C": "Configure SageMaker training input to use SSL and store data in unencrypted FSx.",
      "D": "Enable client-side encryption for S3 uploads and use FSx encryption at rest without transit encryption."
    },
    "explanation": "SSE-KMS on S3 and FSx with transit encryption via customer-managed KMS ensure encryption at rest and in transit. SSE-S3 lacks customer KMS control, and client-side or missing transit encryption fails one requirement."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A telco data pipeline streams CDR logs into S3, with PII fields (phone_number) encrypted via KMS. A data scientist needs to preprocess and anonymize phone numbers before modeling using SageMaker Processing. Which approach ensures the data scientist cannot decrypt original values?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provide the processing job with KMS decrypt permissions and apply one-way hash.",
      "B": "Use a KMS grant to decrypt phone_number, then pseudonymize with reversible mapping.",
      "C": "Use Lambda triggered by S3 events to replace phone_number with SHA-256 digest before encrypting with a separate KMS key that processing job lacks decrypt permissions for.",
      "D": "Configure SageMaker Processing to use a private KMS key different from S3\u2019s and decrypt in job."
    },
    "explanation": "Lambda replaces PII with irreversible SHA-256 digest, re-encrypts with a key the job can use, ensuring the processing job cannot decrypt the original KMS-encrypted phone numbers."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "An image dataset stored in S3 contains location metadata in EXIF that constitutes PII. You must strip this metadata in a SageMaker Processing job, standardize image sizes, and then encrypt outputs in transit to FSx. Which pipeline achieves this with minimal steps?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Lambda to strip EXIF and resize, store intermediate in S3, then run a Processing job to copy to FSx with SSL.",
      "B": "Run a SageMaker Processing job with a custom container that uses exiftool to strip metadata, Pillow to resize, and output directly to FSx for Lustre with HTTPS mount.",
      "C": "Download images to EFS, run an EC2 batch job to preprocess and re-upload to encrypted S3, then mount S3 to FSx.",
      "D": "Use AWS Glue Spark job to strip EXIF and resize, write to FSx using AWS SDK with SSL."
    },
    "explanation": "A SageMaker Processing job with a custom container can handle EXIF stripping and resizing in one step and write directly over HTTPS to FSx for Lustre, minimizing components."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "You need to detect data drift and bias before training on new monthly data. The monthly datasets arrive in S3. Which combination of SageMaker Clarify configuration and workflow ensures automated pre-training bias and data drift detection?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Clarify post-training job, schedule via EventBridge to run monthly.",
      "B": "Use Clarify ModelBiasMonitor as part of ModelMonitor, schedule daily.",
      "C": "Use DataQualityMonitor to detect drift, then manually analyze with Clarify.",
      "D": "Configure Clarify pre-training bias job with baseline from last month, schedule with SageMaker Pipelines monthly to compute DPL, CI, and feature distribution drift."
    },
    "explanation": "Pre-training Clarify job in SageMaker Pipelines scheduled monthly compares new data to a historic baseline, computes DPL and CI for bias and distribution drift, and automates detection."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A startup must comply with GDPR: remove or mask 'email' and 'user_ip' fields before any ML training. They need an auditable automated solution using AWS services. Which design meets GDPR requirements?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create a SageMaker Processing pipeline that uses a DataBrew job for masking email and user_ip, store masked data in a GDPR-compliant S3 bucket with audit logs via CloudTrail.",
      "B": "Have developers write custom Python in a notebook to drop fields, then manually approve and upload to S3.",
      "C": "Use Amazon Macie to discover PII and instruct DataSync to exclude those columns.",
      "D": "Encrypt email and user_ip with SSE-KMS and train model on encrypted features."
    },
    "explanation": "DataBrew recipe for masking with audit logs in CloudTrail ensures fields are irreversibly masked and provides an auditable, automated workflow. Encryption doesn\u2019t remove PII for GDPR."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "You must validate that numeric features shipped to training satisfy min/max thresholds (e.g., age between 18 and 100). Violations should block training. How can you implement this in SageMaker?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a Lambda pre-processing check triggered by S3 upload events to reject invalid datasets.",
      "B": "Write custom validation code in the training script and raise errors if thresholds are breached.",
      "C": "Configure an AWS Glue Data Quality job to run pre-training via SageMaker Processing, fail the Pipeline step if thresholds outside range.",
      "D": "Use SageMaker Model Monitor DataQualityMonitor in real time to monitor feature thresholds."
    },
    "explanation": "Glue Data Quality job integrated into SageMaker Pipeline allows rule-based validation (min/max), and the pipeline can be configured to stop if validation fails, preventing invalid data from training."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A telecom dataset has missing values in time-series features. You need to impute missing timestamps using forward fill within each subscriber partition, ensure no cross-subscriber leakage, and provide a clean CSV to FSx. Which solution accomplishes this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Clarify to detect missing timestamps and fill globally with median intervals.",
      "B": "Use AWS Glue ETL with a window function to forward fill across all records.",
      "C": "Use SageMaker Processing with a Spark container, group by subscriber and apply forward fill, write to S3.",
      "D": "Deploy a SageMaker Processing job with a custom pandas script that partitions by subscriber_id, forward fills timestamps, and writes output to FSx for Lustre."
    },
    "explanation": "A SageMaker Processing job with pandas partitioned by subscriber_id isolates subscribers and forward fills, writing the clean CSV to FSx. Clarify doesn\u2019t impute and Glue window could leak between partitions if misconfigured."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "Your new dataset contains categorical features with high cardinality (10,000 unique values). You need to reduce dimensionality before modeling to prevent overfitting. Which AWS tool and method should you use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use DataBrew 'One-hot encode' action on the categorical column.",
      "B": "Use SageMaker Feature Store offline store and apply a frequency encoding transform in a Processing job.",
      "C": "Use Clarify bias detection to filter out rare categories.",
      "D": "Use Glue Data Quality to drop categories with low frequency."
    },
    "explanation": "Frequency encoding in a SageMaker Processing job reduces cardinality by mapping to numeric frequencies, avoids one-hot explosion. DataBrew one-hot creates huge feature space. Clarify and Data Quality don\u2019t transform features for modeling."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A sensitive dataset requires K-anonymity (k=10) on PII columns before ML. Which AWS service or combination can enforce k-anonymization in an automated pipeline?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Use AWS Glue DataBrew with the 'Anonymize' recipe action for k-anonymity.",
      "B": "Use Amazon Macie to detect PII and then use Lambda to drop records.",
      "C": "Build a SageMaker Processing job using ARX (or Python library) to enforce 10-anonymity and write to encrypted FSx.",
      "D": "Use SageMaker Clarify to generate anonymized synthetic data with k-anonymity."
    },
    "explanation": "No native AWS service enforces k-anonymity; a custom SageMaker Processing job using an open-source library like ARX or a Python package can implement k-anonymity and integrate with FSx for encrypted storage."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "During training, a job fails due to data corruption in Amazon FSx. You need to verify data integrity before each training run and fail fast if corruption occurs. What is the best way to implement this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Integrate a SageMaker Processing step using a checksum validation script to compare file checksums against known-good values stored in AWS Secrets Manager before training.",
      "B": "Enable FSx data compression to detect corrupted blocks.",
      "C": "Use ModelMonitor DataQualityMonitor to detect corrupted data during training.",
      "D": "Rely on SageMaker Training job logs to find errors post-start."
    },
    "explanation": "A pre-training Processing step that computes checksums and validates against stored values (in Secrets Manager) will detect corruption and allow the pipeline to fail fast. FSx compression and post-training monitors are insufficient."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A dataset is subject to data residency regulations requiring that no raw data leaves the EU. You have a multi-region SageMaker Pipeline. How do you ensure compliance when copying data from an S3 bucket in us-east-1 to an FSx file system in eu-west-1?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set S3 replication to eu-west-1 for raw data and run training in us-east-1.",
      "B": "Use S3 Cross-Region Replication to an eu-west-1 bucket, then use FSx for Lustre in eu-west-1 and run the pipeline entirely in eu-west-1.",
      "C": "Use AWS DataSync to transfer data directly from us-east-1 S3 to FSx in eu-west-1.",
      "D": "Use SageMaker Training with input_mode='Pipe' to stream data across regions securely."
    },
    "explanation": "Cross-Region Replication ensures raw data is copied to an EU bucket. Training and FSx in eu-west-1 ensure raw data doesn\u2019t leave the EU region. DataSync or streaming across regions violates residency rules."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "Your text dataset uses masked PII (<NAME>, <EMAIL>) but downstream tokenization treats these tokens as real vocabulary and biases the model. How do you preprocess these masks to avoid model bias?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use DataBrew to replace <NAME> and <EMAIL> with random real names and emails.",
      "B": "Use SageMaker Clarify to drop samples containing masked tokens.",
      "C": "Use a custom tokenizer to treat <NAME> and <EMAIL> as unique IDs.",
      "D": "In a SageMaker Processing job, replace all masked tokens with null or a uniform [MASK] token recognized by the model\u2019s vocabulary."
    },
    "explanation": "Replacing with a uniform [MASK] token recognized by the model avoids bias from inconsistent placeholders. Random real names reintroduce PII, dropping samples reduces data, unique IDs still bias."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "An ML project requires 50 million tabular records. You must ensure that when shuffling and splitting into train/val/test, no data skew occurs across Amazon FSx shards. Which strategy ensures uniform distribution?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Use a SageMaker Processing job with Spark: read data, apply 'repartition(nShards)' and 'randomSplit' for train/val/test, then write to FSx.",
      "B": "Use FSx distributed copy and rely on random file ordering in S3.",
      "C": "Use SM Channel input with ShuffleConfig in a Training job for splitting.",
      "D": "Use SageMaker Clarify pre-processing to shuffle and split data."
    },
    "explanation": "A Spark repartition(nShards) before randomSplit ensures uniform distribution across shards; FSx copy or ShuffleConfig doesn\u2019t guarantee uniform splits; Clarify is not for splitting."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "Your numeric feature 'income' has extreme outliers. You need to detect and treat them in an automated pipeline using AWS services before training. Which multi-step process is best?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Use a SageMaker Clarify Explainability job to identify outliers and then manually remove them.",
      "B": "Use DataBrew to compute z-scores and drop records with |z|>3 in a recipe, then feed to training.",
      "C": "In a SageMaker Processing job, run AWS Glue Data Quality rule to detect numeric outliers, replace values beyond 1st and 99th percentiles with percentile caps, and write to FSx.",
      "D": "Use ModelMonitor DataQualityMonitor to capture outliers at inference time and then feed back corrections."
    },
    "explanation": "Glue Data Quality rule can detect percentile-based outliers, and a Processing job can cap outliers automatically in a pipeline. Clarify and ModelMonitor don\u2019t enforce pre-training data corrections."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A legal requirement mandates that social security numbers (SSNs) be tokenized with a one-way hash before leaving a secure environment. Which architecture satisfies this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Trigger a Lambda in the secure subnet to hash SSNs with SHA-256 and write tokens to an S3 VPC endpoint bucket.",
      "B": "Encrypt SSNs in S3 with SSE-KMS before training.",
      "C": "Use SageMaker Processing to call DynamoDB to fetch hashed SSNs.",
      "D": "Use Glue Crawlers to detect SSNs and mask them."
    },
    "explanation": "A Lambda in the secure subnet with VPC endpoint for S3 can perform one-way SHA-256 hashing before data leaves, ensuring SSNs are never exposed. Encryption alone is reversible."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "You want to measure dataset imbalance (CI) and difference in proportions (DPL) for a multiclass image dataset stored in S3. You need to generate these metrics at scale before training. Which solution is most efficient?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a SageMaker Processing job with a custom Python script listing S3 image prefixes, counting labels, and computing CI and DPL.",
      "B": "Use SageMaker Clarify pre-training bias job pointed at the manifest file in S3, specifying 'label' and facet, for multiclass metrics.",
      "C": "Use Glue ETL to load metadata into Redshift and run SQL to compute counts.",
      "D": "Use Athena queries against S3 to compute label distributions and calculate DPL post-hoc."
    },
    "explanation": "Clarify pre-training bias job can compute CI and DPL at scale across a manifest without custom coding. Athena or custom scripts require manual metric computation; Clarify automates it."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "Your dataset includes an IP address feature that you must anonymize with consistent tokenization (same IP always same token) but irreversibly. Which technique in a SageMaker Processing job should you implement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Apply SHA-256 hashing with a secret salt stored in Secrets Manager.",
      "B": "Use Python\u2019s md5 without salt to hash the IP addresses.",
      "C": "Use DataBrew\u2019s encrypt action with KMS.",
      "D": "Use SageMaker Clarify to mask feature and record mapping."
    },
    "explanation": "SHA-256 with salt ensures consistent, one-way mapping and prevents rainbow table attacks; md5 is weak without salt; DataBrew encrypt is reversible with KMS; Clarify doesn\u2019t mask."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A data scientist suspects measurement bias due to inconsistent units in a numeric feature (some records in cm, some in inches). They need to detect and correct this automatically before training. How can this be done in AWS?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Clarify data bias to detect inconsistent unit distributions.",
      "B": "Use Glue Data Quality to enforce unit ranges and manual corrections.",
      "C": "Use Athena to query and tag inconsistent units for manual review.",
      "D": "Write a custom SageMaker Processing step that reads the 'unit' column, applies conversions to a standard unit, and writes back to FSx."
    },
    "explanation": "Measurement bias due to units requires a transformation step. A custom Processing job can read the unit metadata column and convert values programmatically. Other AWS services don\u2019t perform unit conversion."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A global dataset contains phone numbers in various formats. You need to standardize formatting and mask all but last four digits, while preserving relational joins via hashed salted values. Which pre-training pipeline components accomplish this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use DataBrew recipe to script regex formatting and static masking of phone numbers.",
      "B": "In a SageMaker Processing job: parse with libphonenumber, format E.164, generate SHA-256 hash with salt from Secrets Manager for join key, and mask all but last four digits in output.",
      "C": "Use AWS Glue Crawlers to detect phone patterns and then use Lambda to transform.",
      "D": "Use SageMaker Clarify to identify PII and then use DataBrew to mask."
    },
    "explanation": "A custom Processing job can use libphonenumber for E.164 formatting, apply salted SHA-256 for join preservation, and mask digits, integrating with Secrets Manager for salt. DataBrew and Clarify can\u2019t fully meet these steps."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "You need to guarantee that no training dataset has more than 5% of records from any single customer_id to prevent overfitting. How do you enforce this in an automated AWS pipeline?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a SageMaker Processing step with Spark: group by customer_id, sample up to 5% of that group into the final dataset, then write to FSx.",
      "B": "Use ModelMonitor to detect customer_id skew post-training and retrain.",
      "C": "Use Glue Data Quality to drop records exceeding threshold.",
      "D": "Use Clarify pre-training to detect over-representation and fail training."
    },
    "explanation": "A Spark-based Processing job can enforce maximum sampling per customer_id before training. Data Quality and Clarify detect but don\u2019t enforce sampling limits; ModelMonitor is post-training."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "Your model requires a balanced training set across 4 classes. The raw dataset in S3 is stored with per-class prefixes. You want SageMaker Training to automatically balance classes at job launch. What configuration achieves this?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Enable input data channel 'ShuffleConfig' and rely on equal distribution.",
      "B": "Use Clarify to rebalance classes pre-training.",
      "C": "Create a Processing job that downsamples over-represented prefix folders to match counts of the smallest class and writes balanced data to FSx for Training input.",
      "D": "Use Training hyperparameters to set 'class_weight' parameter during training."
    },
    "explanation": "SageMaker training doesn\u2019t automatically rebalance files by prefix. A custom Processing job must downsample to equal class counts. class_weight adjusts loss but doesn\u2019t rebalance data."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "A dataset with both categorical and numeric features needs to be split into train/validation/test, ensuring each split reflects the same categorical distribution. Which AWS-based method guarantees stratified sampling at scale?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Glue ETL to randomSplit on the full dataset.",
      "B": "Run a SageMaker Processing Spark job: stratified sampling with DataFrame.stat.sampleBy on the category column, then write splits to FSx.",
      "C": "Use Athena CTAS queries with WHERE RAND() thresholds.",
      "D": "Use SageMaker Clarify preprocessing to stratify."
    },
    "explanation": "Spark\u2019s sampleBy in a Processing job enables stratified sampling by category. Athena or randomSplit don\u2019t guarantee identical category proportions; Clarify doesn\u2019t provide sampling utilities."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.3",
    "stem": "An ML pipeline triggers monthly. You must automatically generate a data quality report that includes the number of nulls, duplicates, and schema drift before training. Which integration sequence in SageMaker Pipelines accomplishes this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Invoke an AWS Glue Data Quality Check job as a Pipeline step (via Lambda), retrieve results, fail Pipeline on violations, then proceed to training.",
      "B": "Use DataBrew Profile job in Pipeline to generate report, manually check.",
      "C": "Use SageMaker Clarify DataQualityMonitor for pre-training schema drift detection.",
      "D": "Use ModelMonitor after training to analyze data quality."
    },
    "explanation": "AWS Glue Data Quality Check can profile nulls, duplicates, and detect schema drift. Integrating it in Pipelines (via Lambda or a custom step) automates reports and halts on violations. Clarify and ModelMonitor are not designed for pre-training data quality."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A retail company needs to deploy a deep learning model for product image classification to an endpoint. They expect highly variable traffic: peak of 2000 requests per minute during sales and near zero at night. Latency must stay under 200ms. Cost minimization is critical. Which deployment infrastructure meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy the model on a real-time SageMaker endpoint with two ml.p3.2xlarge instances and configure target tracking auto scaling based on CPU utilization.",
      "B": "Use a SageMaker serverless inference endpoint with configurable concurrency limits to scale automatically and only incur cost when the model processes requests.",
      "C": "Deploy a multi-model SageMaker endpoint on ml.m5.xlarge instances behind an Application Load Balancer and scale manually.",
      "D": "Containerize the model on ECS Fargate with fixed CPU and memory settings and use CloudWatch alarms to trigger CloudFormation stack updates at scale events."
    },
    "explanation": "Serverless endpoints automatically scale down to zero between peaks, minimizing cost, and handle variable load with low latency under SageMaker serverless SLA."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An IoT startup must deploy a computer vision model at the edge device that has limited compute (ARM CPU) and must process frames at 15 FPS. The model is currently a standard PyTorch model requiring a GPU. They need minimal code changes. Which infrastructure choice satisfies requirements?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy the PyTorch model to an EC2 g4dn.xlarge instance and have the edge device stream images to it for inference.",
      "B": "Use SageMaker real-time GPU endpoint with a Lambda function forwarding requests from edge device.",
      "C": "Convert the PyTorch model to TensorFlow and use SageMaker Neo to compile for ARM, deploy on AWS IoT Greengrass core.",
      "D": "Use SageMaker Neo to compile the PyTorch model for ARM architecture and deploy the optimized model on AWS IoT Greengrass core."
    },
    "explanation": "SageMaker Neo supports PyTorch, compiles to ARM, and Greengrass core runs inference on device, meeting FPS and minimal code change."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A financial analytics platform requires batch inference on 10 TB of historical data weekly. The job takes too long on EC2 instances using a custom endpoint. They want to reduce run time and maintenance. Which infrastructure change is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch to SageMaker Batch Transform with optimized ML instances and spot instances for cost and performance.",
      "B": "Convert the batch job into a real-time endpoint and stream data through it.",
      "C": "Deploy the model as a Lambda function and invoke in parallel using Step Functions Express Workflows.",
      "D": "Use ECS Fargate tasks to run the custom container in parallel across multiple tasks."
    },
    "explanation": "SageMaker Batch Transform is optimized for large-scale offline inference, supports spot instances, and manages infrastructure automatically."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A healthcare application must deploy a fraud-detection model requiring compliance isolation. The model must serve 500 TPS with latency <50ms. Deployment must reside in a VPC with no internet access. Which infrastructure choice is most appropriate?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Deploy a serverless SageMaker endpoint and attach a VPC endpoint for network isolation.",
      "B": "Use ECS Fargate on a private subnet with an Application Load Balancer in the VPC.",
      "C": "Deploy a real-time SageMaker endpoint in the customer VPC with VPC-only mode and provision ml.c5.2xlarge instances behind an internal ALB.",
      "D": "Deploy a multi-model endpoint on SageMaker using ml.m5.large instances with VPC mode enabled."
    },
    "explanation": "Real-time endpoints in VPC-only mode on ml.c5.2xlarge satisfy high TPS, low latency, and strict isolation; multi-model ml.m5.large cannot handle 500 TPS."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An e-commerce site uses a text classification model and expects constant moderate load (100 requests/sec). They want lowest latency and minimal infrastructure management. Cost sensitivity is medium. Which deployment type is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy a multi-model SageMaker real-time endpoint on an ml.c5.large instance and preload the model for minimal inference latency.",
      "B": "Use a SageMaker serverless endpoint to scale automatically with moderate management.",
      "C": "Deploy as a Lambda function behind API Gateway to avoid managing servers.",
      "D": "Containerize the model on ECS with Fargate and auto scale."
    },
    "explanation": "Multi-model endpoints reduce cold-start latency by preloading, minimize cost by serving multiple models on same instances, and provide lowest latency."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A media analytics company has deployed an object-detection model on SageMaker real-time endpoints. They notice high idle time cost overnight. Available traffic is zero after business hours. They need to reduce cost without affecting daytime performance. Which solution achieves this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch to a larger instance and use downscaling to one instance overnight.",
      "B": "Use a multi-container endpoint and move the model to a cheaper container.",
      "C": "Configure a scheduled CloudWatch event to delete and redeploy the endpoint daily.",
      "D": "Migrate to a serverless inference endpoint that scales to zero at night and autos-scales during the day."
    },
    "explanation": "Serverless inference endpoints scale to zero when idle and up during demand automatically, minimizing idle costs."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A biotech firm needs to deploy a variant of their genomic model for two customer segments. Both share 80% of code, but differ in last layer. They want cost-effective multi-tenancy with strict data isolation. Which deployment strategy should they choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy two separate real-time endpoints on dedicated instances for each segment.",
      "B": "Use a multi-model endpoint with separate containers, one for base model and two variants, and IAM policies for isolation.",
      "C": "Deploy a single container with conditional logic to switch layers at runtime.",
      "D": "Use SageMaker serverless inference and deploy two endpoints sharing the same container."
    },
    "explanation": "Multi-model endpoints host multiple containers on same instances, reducing cost, while IAM and container separation ensure data isolation."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An autonomous vehicle company needs to deploy their sensor fusion model on GPU clusters with InfiniBand to meet <20ms latency. Which SageMaker infrastructure meets these requirements?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Use SageMaker CPU instances in a cluster with Enhanced Networking.",
      "B": "Deploy on ECS GPU tasks with ENA-enabled instances.",
      "C": "Deploy a SageMaker real-time endpoint on ml.p4d.24xlarge instances with Elastic Fabric Adapter (EFA) for high-performance networking.",
      "D": "Use EC2 p2.xlarge instances and manage GPUs manually."
    },
    "explanation": "ml.p4d.24xlarge with EFA supports InfiniBand-like throughput and low-latency networking required for sensor fusion."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A startup needs to deploy a model that uses proprietary libraries not supported by SageMaker built-in containers. They require both batch and real-time inference with minimal custom ops. Which deployment approach is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Build a custom Docker container with the libraries, store in ECR, and deploy to SageMaker real-time and Batch Transform endpoints.",
      "B": "Use a Lambda function with layers containing the custom libraries.",
      "C": "Deploy on ECS Fargate with scheduled tasks for batch and ALB for real-time.",
      "D": "Migrate model to supported framework to use built-in SageMaker containers."
    },
    "explanation": "Custom container in ECR lets you include proprietary libraries and deploy to both real-time and batch endpoints with minimal overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A financial trading firm requires synchronous inference latency under 5ms. They want maximum CPU throughput and cannot risk cold starts. They also want to optimize for cost. Which combination should they choose?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "SageMaker serverless endpoint with provisioned concurrency of 100.",
      "B": "Provision a real-time multi-model endpoint on bare-metal ml.inf1.24xlarge Inf1 instances with model pinned in memory.",
      "C": "Deploy as Lambda functions with provisioned concurrency of 200.",
      "D": "Use ECS Fargate GPU tasks with pre-warmed containers behind an ALB."
    },
    "explanation": "Inf1 bare-metal reduces virtualization overhead, multi-model pins model, real-time endpoint avoids cold starts, maximizing throughput and low latency."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A gaming company wants to deploy multiple language translation models for different regions. Total model memory footprint exceeds instance GPU memory. They need low-latency translation. Which infra is most appropriate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy all models on one GPU EC2 instance using a custom orchestration layer.",
      "B": "Use SageMaker serverless endpoints sequentially loading each model.",
      "C": "Deploy each model on separate real-time GPU endpoints using ml.g4dn.xlarge instances and route traffic via Application Load Balancer.",
      "D": "Consolidate models into a single large multi-container endpoint on ml.p3.2xlarge."
    },
    "explanation": "Separate endpoints avoid GPU memory oversubscription, ALB routes based on region, simple scaling per model."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A social media platform requires asynchronous inference for user sentiment analysis. They need to process spikes of 100k requests hourly without provisioning servers. Which deployment infrastructure is best?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy a real-time SageMaker endpoint and enqueue requests in SQS.",
      "B": "Use a Lambda function triggered by Step Functions that calls the real-time endpoint.",
      "C": "Deploy a multi-model endpoint and use Kinesis Data Streams.",
      "D": "Use SageMaker asynchronous inference endpoints that scale storage and compute and store results in S3."
    },
    "explanation": "Asynchronous inference endpoints scale automatically for batch requests, decouple compute via S3, and handle spikes without server provisioning."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A media streaming company experiments with video frame classification. They need to deploy a model where inference takes >5s per image. They want to avoid request timeouts and handle 100 concurrent jobs. What deployment pattern should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a real-time endpoint with high timeout settings.",
      "B": "Deploy in ECS Fargate with long-running tasks.",
      "C": "Use SageMaker asynchronous inference with callback duration set, storing results in S3, polled by the application.",
      "D": "Break the model into microservices and chain Lambda functions."
    },
    "explanation": "Asynchronous endpoints handle long per-request durations without timing out and support concurrency by queueing requests."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An enterprise mandates use of automated model rollback on failure. They deploy via SageMaker endpoints. Which infrastructure configuration supports blue/green deployments with rollback?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker endpoint variants with traffic shifting in a blue/green deployment orchestrated by CodeDeploy integration.",
      "B": "Deploy two separate endpoints and use Route53 weighted routing to shift traffic manually.",
      "C": "Use ECS Fargate with CodePipeline and manual rollback stage.",
      "D": "Use Lambda functions with version aliases and weight-based alias shifts."
    },
    "explanation": "SageMaker with CodeDeploy supports blue/green endpoint deployments and automatic rollback on failure."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A company must serve ML inferences to internal clients only. They want cost-effective scaling and restrict internet access. They cannot use serverless endpoints. What should they deploy?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Public SageMaker real-time endpoint with IP whitelist.",
      "B": "ECS Fargate tasks in public subnets with security groups.",
      "C": "EC2 GPU instances behind a public ALB with restricted security group.",
      "D": "Private SageMaker real-time endpoint in VPC-only mode with endpoint accessed via VPC interface endpoint."
    },
    "explanation": "Private VPC-only SageMaker endpoints and interface VPC endpoints allow internal-only access and autoscaling without serverless usage."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A research team needs to test multiple model versions in parallel under identical traffic to compare performance in production. Which deployment technique supports this with minimal infrastructure overhead?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy each model version on separate real-time endpoints and use ALB.",
      "B": "Use SageMaker multi-variant endpoint with two production variants and traffic weight splitting.",
      "C": "Deploy models as Lambda aliases and shift weights via AWS CLI.",
      "D": "Use ECS Fargate tasks and update task definitions to switch traffic."
    },
    "explanation": "SageMaker variant endpoint supports two production variants for A/B testing with traffic weight control and shared infrastructure."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An NLP startup uses foundation models from Amazon Bedrock that require fine-tuning at deploy time. They need to serve personalized chat sessions with low latency. Which SageMaker infrastructure allows direct integration with Bedrock is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker JumpStart endpoints with Bedrock models fine-tuned via SageMaker Script Mode and deploy real-time endpoint.",
      "B": "Deploy Bedrock model directly in a Lambda behind API Gateway.",
      "C": "Use ECS Fargate with Hugging Face containers mounting a Bedrock endpoint.",
      "D": "Use EC2 instances calling Bedrock via SDK."
    },
    "explanation": "SageMaker JumpStart integrates with Bedrock models, supports fine-tuning script mode, and deploys as real-time endpoints with low latency."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A logistics firm needs to host an ensemble of three models for package routing. They want minimal end-to-end latency combining inferences. How should they deploy?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy three separate endpoints and aggregate results in Lambda.",
      "B": "Bundle all models into one container with a microservice orchestration.",
      "C": "Use a SageMaker multi-container endpoint with each model in its container and an aggregator container.",
      "D": "Deploy Ensemble as Batch Transform jobs chained by Step Functions."
    },
    "explanation": "Multi-container endpoints allow co-located containers; a dedicated aggregator container can combine outputs with low latency."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A company has sensitive images requiring inference on-premises behind a firewall. They want to use SageMaker-managed infrastructure. Which deployment architecture meets requirements?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Use SageMaker local mode on EC2 inside their VPC.",
      "B": "Use SageMaker real-time endpoint with VPC peering to on-premises network.",
      "C": "Deploy on AWS Outposts with SageMaker studio.",
      "D": "Use SageMaker Inference on AWS Outposts rack inside their data center sandbox."
    },
    "explanation": "SageMaker Inference on Outposts lets you run endpoints on Outposts hardware on-premises with SageMaker management."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A public API uses a multi-language model that loads slowly and exceeds response time if loaded on each invoke. Cold starts hurt UX. They need <50ms per request. Which deployment pattern addresses this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable provisioned concurrency on Lambda with large memory to preload model.",
      "B": "Deploy as SageMaker real-time endpoint on ml.c5.xlarge and maintain warm instances.",
      "C": "Use ECS Fargate tasks with pre-warmed containers behind ALB.",
      "D": "Use serverless inference with reserved concurrency."
    },
    "explanation": "Real-time endpoints keep models loaded in memory, ensuring consistent sub-50ms latency without cold starts."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A company needs to deploy a time-series forecasting model that runs monthly and takes hours to run. They want minimal infra management and integration with existing SageMaker pipelines. Which deployment infra is appropriate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy a real-time endpoint and trigger monthly inferences.",
      "B": "Use ECS Fargate tasks triggered by EventBridge.",
      "C": "Use SageMaker Batch Transform job within SageMaker Pipeline step.",
      "D": "Build a Lambda to load model and process data stored in S3."
    },
    "explanation": "Batch Transform jobs integrate directly in SageMaker Pipelines, manage infra, and suit long-running monthly jobs."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A SaaS vendor must deploy a model to multiple customer VPCs automatically during onboarding. They need IaC to manage endpoints. Which approach is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS CDK to define a construct for SageMaker endpoint and instantiate stacks per customer VPC.",
      "B": "Manually use CloudFormation templates to create endpoints per account.",
      "C": "Use Terraform modules outside AWS CDK for endpoints.",
      "D": "Use CLI scripts to provision endpoints in each account."
    },
    "explanation": "AWS CDK enables programmatic multi-account, multi-VPC deployments of SageMaker endpoints with constructs and automatic context."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A fintech app uses Spark processing to call an inference endpoint for real-time risk scoring at scale. They want to reduce network overhead per Spark executor. Which deployment option reduces egress latency?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy endpoint in public subnets close to EMR cluster.",
      "B": "Use ECS Fargate tasks as inference cluster next to EMR.",
      "C": "Call endpoint via API Gateway to reduce hops.",
      "D": "Deploy SageMaker real-time endpoint in same VPC and subnet as EMR and use direct VPC endpoint integration."
    },
    "explanation": "Co-locating endpoint in same subnet and using VPC endpoint reduces network hops and egress latency for Spark executors."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An advertiser needs to run real-time bidding inference under unpredictable peak loads up to 100k TPS with <5ms latency. GPU acceleration is needed. Which infrastructure is most suitable?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "Use SageMaker real-time endpoint on GPU ml.g5 instances with auto scaling.",
      "B": "Use SageMaker multi-model real-time endpoint on Inf1 instances with high concurrency and onboard GPU models into single endpoint.",
      "C": "Deploy ECS GPU Fargate tasks behind ALB with auto scaling ECS.",
      "D": "Use Spot Instances in Batch Transform with Lambda to emulate real-time."
    },
    "explanation": "Inf1 instances are designed for high-concurrency real-time inference at low latency and can host multiple models concurrently."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A biotech startup wants to test a prototype model without incurring provisioned instance cost. They need quick feedback with minimal latency (<100ms) at low volume (<10 TPS). Which deployment option is ideal?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Deploy to an ml.t3.medium real-time endpoint.",
      "B": "Use ECS Fargate with minimal CPU.",
      "C": "Use SageMaker serverless inference endpoint with low concurrency and pay-per-request.",
      "D": "Use Lambda with provisioned concurrency of 1."
    },
    "explanation": "Serverless inference lets you pay per request for low-volume prototypes with acceptable latency and no instance provisioning."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A customer has strict data residency; inference must occur in EU-West-1. They have global traffic. To reduce latency, they want multi-region endpoints. How should they implement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy SageMaker real-time endpoints in EU-West-1 and US-East-1, and route via Route 53 latency-based routing.",
      "B": "Deploy endpoints in EU-West-1 only and use CloudFront for global clients.",
      "C": "Use a global Application Load Balancer spanning regions.",
      "D": "Deploy serverless endpoints in EU-West-1 and Asia-Pacific, relying on client-side region selection."
    },
    "explanation": "Latency-based routing with Route 53 directs clients to nearest regional real-time endpoints while respecting data residency."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "A gaming platform requires under-50ms inference on user actions. They host game servers in Kubernetes on EKS. They prefer colocation of inference. What deployment infra integrates best?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker real-time endpoints and call from EKS pods.",
      "B": "Use Lambda functions within VPC for inference.",
      "C": "Deploy ECS Fargate tasks in same VPC.",
      "D": "Use SageMaker Inference using EKS turret by deploying a SageMaker Inference endpoint onto the existing EKS cluster via the SageMaker Operators for Kubernetes."
    },
    "explanation": "SageMaker Operators lets you host real-time inference pods inside EKS cluster, co-located with game servers for low latency."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.1",
    "stem": "An ML team deployed a model on a shared SageMaker endpoint used by multiple teams. One model version affects another\u2019s latency. They need isolation. What is the best approach?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase instance count to isolate workloads.",
      "B": "Deploy each model as a separate SageMaker endpoint with dedicated instances.",
      "C": "Use multi-model endpoint with separate container per team.",
      "D": "Use ECS Fargate to isolate container resources per team."
    },
    "explanation": "Separate endpoints provide strict compute isolation, preventing one model\u2019s load from impacting another."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A data scientist is fine-tuning a large Transformer model on SageMaker JumpStart with limited GPU memory. They observe training failure due to out-of-memory errors. Which combination of techniques will most effectively reduce memory usage while preserving model accuracy?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Decrease batch size and disable gradient checkpointing to save memory.",
      "B": "Increase instance type to GPU with more memory and remove dropout regularization.",
      "C": "Enable mixed precision training and gradient checkpointing to reduce memory footprint.",
      "D": "Switch to a smaller pre-trained model and increase learning rate to converge faster."
    },
    "explanation": "Mixed precision reduces memory per tensor, and gradient checkpointing trades compute for memory by storing fewer activations, preserving accuracy while fitting in memory."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A SageMaker automatic model tuning (AMT) job using Bayesian optimization is not converging to better results after 20 trials. The objective is to maximize validation AUC for a binary classifier. Which action is most likely to improve the tuning efficiency?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch the tuner from Bayesian optimization to random search to explore hyperparameter space more broadly.",
      "B": "Narrow the hyperparameter search ranges around the best performing values and continue tuning.",
      "C": "Increase the maximum number of training jobs without adjusting the search ranges.",
      "D": "Change the objective metric to validation accuracy instead of AUC."
    },
    "explanation": "Refining search ranges focuses Bayesian optimization around promising regions, improving convergence efficiency without diluting search."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "During distributed training on SageMaker with Horovod, the training job stalls indefinitely at initialization. Logs indicate parameter server connection timeouts. Which configuration change should resolve this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable TCP keepalive and increase ssh_timeouts in the Horovod configuration.",
      "B": "Switch from Horovod to SageMaker\u2019s built-in data parallel library without changing network settings.",
      "C": "Reduce number of workers to one node to eliminate networking issues.",
      "D": "Increase the training batch size to reduce communication frequency."
    },
    "explanation": "Enabling TCP keepalive and adjusting timeouts prevents idle connection drops during Horovod rendezvous, resolving stalls."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A convolutional neural network trained via SageMaker script mode exhibits training instability when increasing batch size from 32 to 256. The learning rate was left at 0.001. Which adjustment best maintains convergence behaviour?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Decrease the number of epochs proportionally to batch size.",
      "B": "Add weight decay to the optimizer.",
      "C": "Switch optimizer from Adam to SGD.",
      "D": "Scale the learning rate linearly to 0.008 following the batch size increase rule."
    },
    "explanation": "Linear scaling rule suggests increasing learning rate proportional to batch size to maintain gradient variance and stable convergence."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A tree-based model in SageMaker automatic model tuning shows minimal improvement beyond 50 training jobs. The tuner used random search over max_depth\u2208[3,15] and min_child_weight\u2208[1,10]. What is the most effective next step?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Change the random search strategy to Bayesian optimization for the same ranges.",
      "B": "Narrow max_depth to [3,7] and extend min_child_weight to [0,20] based on observed patterns.",
      "C": "Increase training jobs from 50 to 500 without changing ranges.",
      "D": "Switch to a neural network algorithm to explore different model families."
    },
    "explanation": "Observing diminishing returns suggests narrowing depth to shallower trees and expanding child weight to regulate complexity, improving tuner focus."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A data scientist uses SageMaker hyperparameter tuning for a deep learning model and sets EarlyStoppingType to \"Auto\". Training jobs still run to full resource allocation before stopping. Why?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Auto early stopping only applies to built-in algorithms, not custom script mode.",
      "B": "The objective metric isn\u2019t logged frequently enough for the tuner to evaluate early stopping.",
      "C": "The tuning job\u2019s EarlyStoppingPolicy wasn\u2019t enabled; Auto setting requires explicit enablement in the tuner.",
      "D": "Auto early stopping only stops the entire tuning job, not individual training jobs."
    },
    "explanation": "In AMT, early stopping must be enabled in the tuner configuration; setting EarlyStoppingType alone doesn\u2019t apply the policy."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A model exhibits underfitting: both training and validation losses are high. The data scientist suspects insufficient model capacity. Which approach should they take?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase model size by adding layers or units and tune learning rate.",
      "B": "Reduce dropout rate to zero and add L1 regularization.",
      "C": "Decrease number of features via PCA to simplify data.",
      "D": "Increase batch size to stabilize training and reduce noise."
    },
    "explanation": "Underfitting often requires increasing model capacity; adding layers or units addresses capacity issues, then tuning learning rate ensures convergence."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "While training a PyTorch model in SageMaker script mode, gradient norms explode after epoch 2. Which change will most directly mitigate this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch optimizer from Adam to RMSprop to stabilise gradients.",
      "B": "Apply gradient clipping by setting max_grad_norm in the training script.",
      "C": "Increase batch size to average out noisy gradients.",
      "D": "Add dropout layers to the architecture."
    },
    "explanation": "Gradient clipping directly limits gradient norms, preventing explosion without changing model dynamics or data batching."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A pre-trained image classification model deployed via SageMaker JumpStart needs customization for a new 10-class dataset. Which process correctly fine-tunes this model with minimal training time?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Retrain the entire model on new dataset with original learning rate.",
      "B": "Replace final layer and train only that layer with a high learning rate.",
      "C": "Freeze all convolutional layers and train all fully connected layers with a low learning rate.",
      "D": "Freeze base model parameters and fine-tune the final layers with a reduced learning rate."
    },
    "explanation": "Freezing base parameters preserves learned features and reduces training time, while a reduced learning rate avoids large parameter updates."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "During automatic model tuning, a candidate hyperparameter combination crashes training jobs intermittently on one instance type. How should the data scientist handle this to continue tuning?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Remove the crashing hyperparameter combination from the search space manually.",
      "B": "Configure the tuning job\u2019s RetryPolicies to retry failed jobs and exclude fatal errors from stopping the tuner.",
      "C": "Switch to a larger instance type for all tuning jobs.",
      "D": "Reduce the overall max jobs count so the job completes quickly before errors accumulate."
    },
    "explanation": "Using RetryPolicies allows retries for transient failures and prevents rare fatal errors from halting the tuning job."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A model\u2019s training time per epoch is excessive due to data preprocessing in the training loop. Which strategy will speed up training without altering model architecture?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Move preprocessing to a SageMaker Processing Job and store transformed data in S3 for training.",
      "B": "Use larger batch sizes to reduce number of loops.",
      "C": "Switch from Python data loader to a pure-Python implementation.",
      "D": "Increase number of epochs to amortize preprocessing overhead."
    },
    "explanation": "Offloading preprocessing to a separate job avoids repeated computation per epoch, allowing training on preprocessed data."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A neural network in SageMaker Model Registry has multiple versions. The team wants reproducible experiments including exact training environments. Which Registry feature enables this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model approval status tags.",
      "B": "Inference container logs.",
      "C": "Model package groups with associated Docker image URIs and training artifacts.",
      "D": "Endpoint deployment timestamps."
    },
    "explanation": "Model package groups record container URIs and artifacts, ensuring reproducible environments for each version."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A data scientist uses SageMaker automatic model tuning with Bayesian optimization. The prior distributions are too narrow, causing search to get stuck in local minima. What change addresses this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase number of initial random trials before Bayesian sampling.",
      "B": "Widen the prior distribution ranges to cover more hyperparameter space.",
      "C": "Switch to random search strategy entirely.",
      "D": "Decrease objective metric evaluation frequency."
    },
    "explanation": "Widening priors allows the optimizer to explore a broader search space and escape local minima."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "An LSTM model trained via SageMaker script mode shows overfitting after 10 epochs. The data scientist wants to regularize without reducing model capacity. Which approach is most appropriate?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Decrease number of timesteps to shorten sequences.",
      "B": "Increase batch size to reduce gradient noise.",
      "C": "Add L1 regularization to input weights only.",
      "D": "Add recurrent dropout to LSTM layers for temporal regularization."
    },
    "explanation": "Recurrent dropout applies dropout to recurrent connections, regularizing LSTM without reducing capacity."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A classification model in SageMaker automatically stops improving validation accuracy after 5 epochs. Which tuner configuration ensures the rest of the hyperparameter combinations execute?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable warm start with type IDENTICAL_DATA_AND_ALGORITHM and set max jobs.",
      "B": "Disable early stopping entirely to run all combinations to completion.",
      "C": "Set strategy to GRID_SEARCH to systematically cover all combinations.",
      "D": "Use early stopping with conservatism parameter set to high."
    },
    "explanation": "Warm start IDENTICAL allows reusing completed trials but still runs other configurations; disabling early stopping risks wasted compute."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A distributed TensorFlow training job on SageMaker shows poor scaling beyond 4 GPU nodes; network I/O saturates. Which modification best improves throughput?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase batch size further to reduce communication frequency.",
      "B": "Switch to Horovod\u2019s ring-allreduce algorithm.",
      "C": "Enable SageMaker\u2019s built-in NCCL backend with GPU-enabled instances.",
      "D": "Use Python-based parameter server implementation."
    },
    "explanation": "NCCL provides high-performance multi-GPU communication optimized for GPU clusters, alleviating I/O bottlenecks."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A PyTorch model fine-tuned on SageMaker exhibits catastrophic forgetting on original classes. Which technique best prevents forgetting while learning new classes?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a higher learning rate for new classes to quickly adapt.",
      "B": "Apply Elastic Weight Consolidation (EWC) to penalize deviation from original weights.",
      "C": "Freeze all layers except the last and train only output layer.",
      "D": "Increase dropout to 0.9 to regularize adaptation."
    },
    "explanation": "EWC constrains updates for important weights, preserving original task performance while fine-tuning on new tasks."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A SageMaker hyperparameter tuning job uses RANDOM strategy. The search space contains complex interdependent hyperparameters. Which strategy provides more efficient search?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Switch to GRID_SEARCH to cover all combinations exhaustively.",
      "B": "Manually sample correlated hyperparameters outside the tuner.",
      "C": "Reduce hyperparameter count by fixing least impactful ones.",
      "D": "Use BAYESIAN optimization to model dependencies and guide search."
    },
    "explanation": "Bayesian optimization learns relationships between hyperparameters, efficiently handling interdependencies compared to random search."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A model trained with script mode on SageMaker takes too long to converge. Which two changes combined will most effectively reduce training time and maintain accuracy?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase batch size and disable checkpointing.",
      "B": "Enable mixed precision training and apply early stopping based on validation loss.",
      "C": "Switch optimizer to SGD and decrease learning rate.",
      "D": "Use smaller instance type and reduce number of epochs."
    },
    "explanation": "Mixed precision speeds up training; early stopping prevents wasteful epochs after convergence, preserving accuracy."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A data scientist wants to use SageMaker Automatic Model Tuning to optimize both learning_rate and batch_size. They observe that large batch sizes degrade generalization. Which approach addresses this trade-off during tuning?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Fix batch_size and tune only learning_rate.",
      "B": "Increase search range of both parameters.",
      "C": "Tune batch_size, then in a second tuner fix optimal batch_size and tune learning_rate.",
      "D": "Use a single tuner to optimize sum of learning_rate and batch_size."
    },
    "explanation": "Sequential tuning isolates the batch size effect, then optimizes learning rate for the chosen batch size, handling trade-offs effectively."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A model\u2019s F1 score plateaus despite varying hyperparameters. The data scientist suspects feature scaling issues. Which pipeline change is most likely to enable further improvement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Incorporate standardization of numeric features before training in script mode.",
      "B": "Add more dropout layers to reduce variance.",
      "C": "Increase depth of decision trees in ensemble.",
      "D": "Switch to a different activation function in final layer."
    },
    "explanation": "Unscaled numeric features can hamper optimization; standardization often leads to better convergence and metric improvements."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A SageMaker training job fails due to size of input data exceeding EBS volume. Which solution allows training with minimal code changes?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Switch to processing job and write custom code for streaming input.",
      "B": "Split dataset into smaller files and retrain multiple times.",
      "C": "Modify training code to stream data from S3 in chunks.",
      "D": "Mount Amazon FSx for Lustre to the training instance for high-throughput, large dataset support."
    },
    "explanation": "FSx for Lustre integrates transparently as a filesystem, handling large datasets with minimal code changes."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "When tuning a neural network\u2019s hyperparameters, the data scientist sets EarlyStoppingType to \"Off\" but still sees early termination of some trials. What explains this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Some built-in algorithms override tuner settings to always early stop.",
      "B": "The SageMaker training script configured its own early stopping callback independent of tuner.",
      "C": "Early stopping only applies when strategy is BAYESIAN.",
      "D": "The objective metric threshold was reached causing automatic stop."
    },
    "explanation": "Custom scripts can implement callbacks (e.g., Keras EarlyStopping) causing training jobs to stop independently of tuner configuration."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A data scientist wants to reduce a model\u2019s memory footprint from 2 GB to under 500 MB for edge deployment. Which two techniques achieve this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Apply post-training static quantization and weight pruning.",
      "B": "Increase dropout to 0.9 and retrain the model.",
      "C": "Use larger batch normalization momentum and lower precision.",
      "D": "Deploy a smaller model architecture without adjustments."
    },
    "explanation": "Static quantization reduces weight bit widths; pruning removes redundant weights, both significantly shrinking model size."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A SageMaker hyperparameter tuning job\u2019s best candidate uses a combination of hyperparameters untested in grid search due to non-grid ranges. Which tuning strategy should the data scientist have chosen?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use RANDOM search over grid values.",
      "B": "Use GRID search with finer discretization.",
      "C": "Manually evaluate the unseen combination.",
      "D": "Leverage BAYESIAN search to explore continuous hyperparameter space."
    },
    "explanation": "Bayesian search explores continuous spaces beyond specified grid points, identifying combinations not on predefined grid."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "During multi-model ensembling, a SageMaker pipeline aggregates predictions from three models but sees inferior performance. What ensemble method change is most likely to improve results?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Switch from weighted average to simple average of predictions.",
      "B": "Increase weight of the poorest performing model to diversify errors.",
      "C": "Use stacking with a meta-learner to learn optimal ensemble weights.",
      "D": "Reduce number of models to only the top performer."
    },
    "explanation": "Stacking trains a meta-learner to optimally combine base model outputs, often outperforming fixed averaging."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A deep learning training job on SageMaker script mode uses custom Docker image. It runs slower than built-in containers. Which modification speeds up training in the same image?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch to a smaller base image reducing container size.",
      "B": "Install and configure NVIDIA CUDA and cuDNN optimally inside the container.",
      "C": "Disable container logging to reduce I/O overhead.",
      "D": "Switch optimizer from Adam to Adagrad."
    },
    "explanation": "Optimized CUDA/cuDNN libraries inside the container unlock GPU performance equivalent to built-in images."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A model trained on imbalanced classes shows poor minority recall. The data scientist wants to tune both class_weight and learning_rate. Which SageMaker tuning configuration is most appropriate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Tune only class_weight and manually adjust learning_rate after tuning.",
      "B": "Use GRID search for both parameters with coarse resolution.",
      "C": "Use BAYESIAN optimizer jointly over continuous learning_rate and discrete class_weight.",
      "D": "Perform random search for class_weight and grid search for learning_rate."
    },
    "explanation": "Bayesian optimization can efficiently search mixed continuous and discrete spaces together, optimizing both simultaneously."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A CNN model deployed frequently suffers from sudden spikes in validation loss at random epochs. The training script uses Keras with default settings. How should they address this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable shuffling of training data each epoch and add adaptive learning rate scheduler.",
      "B": "Disable dropout layers to stabilize activations.",
      "C": "Use batch normalization only in the first layer.",
      "D": "Increase training tolerance in Keras fit function."
    },
    "explanation": "Shuffling prevents learning order bias; adaptive LR schedulers adjust LR downward when loss spikes, stabilizing training."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A SageMaker hyperparameter tuning job is set to use warm start of type TRANSFER_LEARNING. It reuses data but is failing to reuse prior checkpoints. What is missing?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "The tuning job must run on the same instance type as prior job.",
      "B": "The HyperparameterTuningJob must reference the previous job\u2019s name in the WarmStartConfig.",
      "C": "Previous job artifacts must be manually copied into new S3 bucket.",
      "D": "Transfer learning warm starts only apply to built-in algorithms."
    },
    "explanation": "Specifying the previous job name in WarmStartConfig lets the tuner locate and reuse prior tuning results."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.2",
    "stem": "A data scientist wants to integrate an external scikit-learn pipeline into SageMaker script mode training. Which step ensures compatibility?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Convert the pipeline to ONNX and use SageMaker\u2019s ONNX container.",
      "B": "Refactor code to remove scikit-learn dependencies.",
      "C": "Wrap the pipeline in a Lambda function called during training.",
      "D": "Include scikit-learn in the training environment and invoke pipeline within the training script.\""
    },
    "explanation": "Bundling scikit-learn in the image and calling the pipeline in the script ensures seamless integration with SageMaker script mode."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A fintech company must predict customer churn using a dataset of 10 million rows and 50 mixed categorical and numeric features. The model must be highly interpretable to satisfy regulatory audits and be cost-efficient to train. Which modeling approach best meets these requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker XGBoost binary classification with default tree depth and no explainability integration.",
      "B": "Use SageMaker Linear Learner (binary classification) with L1 regularization to produce sparse, interpretable weights.",
      "C": "Build a deep neural network in TensorFlow script mode to capture complex feature interactions.",
      "D": "Use SageMaker Random Cut Forest to detect outliers as churn proxies."
    },
    "explanation": "Linear Learner with L1 yields a sparse, linear model that is interpretable, trainable quickly on large tabular data, and cost-efficient."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "An e-commerce company wants to segment customers into five groups based on browsing behavior without labels. They require a scalable, unsupervised approach that integrates easily with SageMaker Pipelines. Which modeling approach should they select?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Use SageMaker K-means clustering built-in algorithm with k=5.",
      "B": "Use a custom DBSCAN implementation in script mode.",
      "C": "Use SageMaker Random Cut Forest for density-based segmentation.",
      "D": "Use SageMaker XGBoost with k-means objective."
    },
    "explanation": "K-means is the standard scalable built-in algorithm for clustering into a predefined number of segments; integrates natively with SageMaker Pipelines."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A startup needs to classify short customer reviews (under 200 words) into positive, neutral, and negative. They want minimal operational overhead, fast time to market, and are willing to sacrifice some accuracy for ease of use. Which modeling approach best fits?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Train a custom transformer model in PyTorch script mode.",
      "B": "Use SageMaker BlazingText for supervised text classification.",
      "C": "Fine-tune a BERT model in SageMaker JumpStart.",
      "D": "Use Amazon Comprehend sentiment analysis API."
    },
    "explanation": "Amazon Comprehend is fully managed, requires no model training, minimal overhead and rapid deployment at moderate accuracy."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A manufacturing plant monitors sensor streams and wants to detect anomalies in real time. Data volume is high (hundreds of thousands of points per second), and false positives are costly. Which AWS service or algorithm should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a one-class SVM in SageMaker script mode.",
      "B": "Use SageMaker K-means clustering on sliding windows.",
      "C": "Use Amazon Lookout for Equipment anomaly detection service.",
      "D": "Implement a custom LSTM autoencoder in TensorFlow script mode."
    },
    "explanation": "Lookout for Equipment is purpose-built for high-throughput sensor anomaly detection, managed, and tuned to reduce false positives."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A healthcare provider has structured tabular patient data (300K records) to predict disease onset. They require high recall (>90%) and can accept lower interpretability. They want to minimize development time. What is the best modeling approach?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker XGBoost with tree depth tuned via automatic model tuning.",
      "B": "Use SageMaker Linear Learner with L2 regularization.",
      "C": "Build a deep neural network in PyTorch script mode.",
      "D": "Use Amazon Autopilot to generate a pipeline automatically."
    },
    "explanation": "XGBoost balances recall and complexity, trains quickly on tabular data with minimal code; easier than full script mode and more controllable than AutoPilot."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A business needs real-time translation of customer chat messages from English to Spanish. Latency must be under 100ms per request. Which approach is most appropriate?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Train a custom seq2seq model in SageMaker script mode.",
      "B": "Use Amazon Translate real-time API.",
      "C": "Fine-tune a transformer model in SageMaker JumpStart.",
      "D": "Use AWS Lambda to call SageMaker endpoint hosting a translation model."
    },
    "explanation": "Amazon Translate real-time API provides low-latency translation managed by AWS, meeting the latency SLA."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A retail chain wants product recommendations on its website using purchase history. They have sparse user\u2013item interactions. They want a fully managed solution integrated with SageMaker. Which approach should they choose?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Implement an alternating least squares algorithm in Spark on EMR.",
      "B": "Train a collaborative filtering model in TensorFlow script mode.",
      "C": "Use SageMaker BlazingText for embeddings and nearest-neighbor search.",
      "D": "Use Amazon Personalize recommendation service."
    },
    "explanation": "Amazon Personalize is a managed recommendation system that handles sparse interactions and integrates easily, reducing operational overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "An online news service needs to tag articles with topics using a multi-label classification approach. They prefer an AWS-managed solution with custom training only if necessary. Which approach should they adopt?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Fine-tune a SageMaker JumpStart multi-label zero-shot classification foundation model.",
      "B": "Train a multi-label Keras model in script mode.",
      "C": "Use Amazon Comprehend for single-label topic detection.",
      "D": "Use SageMaker BlazingText supervised classification."
    },
    "explanation": "JumpStart foundation models support multi-label zero-shot classification with minimal training, managed by AWS."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A logistics company must forecast daily demand at 1,000 locations for the next 30 days. They have three years of daily demand history. They require prediction intervals and seamless SageMaker integration. Which modeling approach should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Implement ARIMA per location in script mode.",
      "B": "Use a custom LSTM in TensorFlow.",
      "C": "Use SageMaker DeepAR forecasting built-in algorithm.",
      "D": "Use SageMaker XGBoost with date features."
    },
    "explanation": "DeepAR provides probabilistic forecasts with intervals, scales across series, and is a built-in SageMaker algorithm suited to many time-series."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A marketing team wants to group social media posts into topics for monitoring. They have no labeled data and limited compute budget. Which modeling approach is most appropriate?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Train a supervised LDA model in script mode.",
      "B": "Use SageMaker Latent Dirichlet Allocation (LDA) built-in algorithm.",
      "C": "Fine-tune a large transformer model with clustering head.",
      "D": "Use Amazon Comprehend for custom entity recognition."
    },
    "explanation": "SageMaker's built-in LDA efficiently clusters text into topics unsupervised, requiring no labels and minimal compute."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A cybersecurity team wants to detect malicious logon patterns using unsupervised anomaly detection. They need to process terabytes of log data and integrate into SageMaker Pipelines. Which approach should they select?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Random Cut Forest built-in algorithm.",
      "B": "Use custom PyTorch autoencoder in script mode.",
      "C": "Deploy Amazon GuardDuty for logs anomaly detection.",
      "D": "Use SageMaker K-means clustering to detect outliers."
    },
    "explanation": "Random Cut Forest is optimized for large, high-dimensional anomaly detection and integrates natively with SageMaker Pipelines."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A life sciences company needs to classify gene expression profiles into disease subtypes. Dataset is small (5,000 samples, 20,000 features). They must avoid overfitting and need interpretability. Which modeling approach is best?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a deep neural network with feature selection.",
      "B": "Use SageMaker Linear Learner with L1/L2 and elastic net for feature selection and interpretability.",
      "C": "Use SageMaker XGBoost with maximum tree depth=10.",
      "D": "Use SageMaker K-nearest neighbors."
    },
    "explanation": "Linear Learner with elastic net reduces overfitting on high-dimensional data and provides interpretable coefficients and embedded feature selection."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A media company needs to index and tag objects (e.g., people, vehicles) in video footage. They want a fully managed solution requiring no custom training. Which AWS AI service should they select?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Train a custom object detection model in SageMaker script mode.",
      "B": "Use SageMaker built-in Object Detection algorithm.",
      "C": "Use Amazon Rekognition Video for object and activity detection.",
      "D": "Use Amazon Kinesis Video Streams with ML integration."
    },
    "explanation": "Rekognition Video provides managed object and activity detection in video, eliminating need for custom training."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A bank wants to detect fraudulent transactions in real time. They have labeled data but the fraud patterns evolve over time. They need an AWS service with built-in model updating. Which approach should they choose?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker XGBoost and retrain daily in a custom pipeline.",
      "B": "Use Amazon Fraud Detector service.",
      "C": "Train a custom LSTM in script mode.",
      "D": "Use Amazon Lookout for Metrics."
    },
    "explanation": "Amazon Fraud Detector is tailored for fraud, supports continuous learning and real-time inference, simplifying maintenance."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A retailer wants to predict end-of-day inventory needs using external weather and holiday data. They need probabilistic forecasts for safety stock planning. Which modeling approach on SageMaker should they use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker DeepAR forecasting built-in algorithm.",
      "B": "Use SageMaker XGBoost with quantile regression.",
      "C": "Use a custom Prophet model in script mode.",
      "D": "Use SageMaker Random Cut Forest."
    },
    "explanation": "DeepAR natively supports probabilistic forecasting with external covariates and scales to many series."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A gaming company needs to recommend in-game content to players based on session behavior. They have tens of millions of sessions per day and need sub-100ms latency. Which approach is most viable?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Train a collaborative filtering model in TensorFlow script mode.",
      "B": "Use Amazon Personalize campaign for real-time recommendations.",
      "C": "Use SageMaker XGBoost with custom nearest-neighbor indexing.",
      "D": "Use Amazon Kinesis Data Analytics for on-the-fly clustering."
    },
    "explanation": "Amazon Personalize offers managed real-time campaigns optimized for low latency at scale."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A legal firm wants to extract key phrases from contract documents. They have no labeled data and need high accuracy. Which AWS solution should they employ?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a custom NER model in Hugging Face script mode.",
      "B": "Use SageMaker BlazingText to learn word embeddings.",
      "C": "Use Amazon Comprehend custom entity recognition with auto-labeling.",
      "D": "Use Amazon Textract forms extraction."
    },
    "explanation": "Comprehend custom entity recognition supports auto-labeling for text, yielding high accuracy without full custom model training."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A telecom operator wants to predict network drop calls per cell tower hourly. They have 2 years of data across 10,000 towers. They require forecasting with external features and a managed solution. Which built-in algorithm should they use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker DeepAR forecasting built-in algorithm.",
      "B": "Use SageMaker XGBoost regression with time lags.",
      "C": "Use a custom ARIMA model in Spark on EMR.",
      "D": "Use SageMaker K-means for time-series clustering."
    },
    "explanation": "DeepAR handles large numbers of time series, accepts external covariates, and produces probabilistic forecasts."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A startup wants to deploy a chat interface that responds to user queries with company policy answers. They need conversational AI with minimal custom training. Which AWS service should they select?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Train a seq2seq model in SageMaker script mode.",
      "B": "Use Amazon Comprehend to detect intents and build responses.",
      "C": "Use Amazon Lex custom bot with full slot and intent definitions.",
      "D": "Use Amazon Q to build an ML-powered conversational interface."
    },
    "explanation": "Amazon Q is a managed conversational QA service that ingests documents and answers queries with minimal configuration."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A biotech firm must classify microscope images of cells into healthy vs cancerous. They have only 1,000 labeled images and need high accuracy. They also want a managed training pipeline. Which approach should they choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a custom CNN from scratch in PyTorch script mode.",
      "B": "Use SageMaker JumpStart pre-trained Image Classification model and fine-tune on their data.",
      "C": "Use SageMaker built-in Image Classification with default parameters.",
      "D": "Use Amazon Rekognition Custom Labels service."
    },
    "explanation": "JumpStart pre-trained models accelerate training on small datasets; managed pipeline support simplifies fine-tuning."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A media analytics company must detect brand logos in user-generated videos. They need high accuracy and low latency inference. Which modeling approach on AWS should they use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a YOLO model in SageMaker script mode and deploy on GPU endpoints.",
      "B": "Use SageMaker built-in Object Detection algorithm with custom data.",
      "C": "Use Amazon Rekognition for image detection frame-by-frame.",
      "D": "Use Amazon Rekognition Custom Labels trained on their logo dataset."
    },
    "explanation": "Rekognition Custom Labels offers managed training for object detection with low-latency inference optimized by AWS."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A logistics startup needs to cluster delivery routes into 100 groups based on GPS coordinates over time for fleet optimization. They want an unsupervised, scalable solution in SageMaker. Which algorithm should they use?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use SageMaker K-means clustering with k=100 and geospatial features.",
      "B": "Use SageMaker Linear Learner in unsupervised mode.",
      "C": "Use a custom DBSCAN in script mode.",
      "D": "Use SageMaker Random Cut Forest for clustering."
    },
    "explanation": "K-means is the standard scalable clustering for a predetermined number of clusters and integrates directly in SageMaker."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A financial institution wants to build a credit scoring model with a focus on minimizing false negatives over false positives. They need a model that supports cost-sensitive learning. Which SageMaker built-in algorithm should they choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Linear Learner with default settings.",
      "B": "Use SageMaker XGBoost with customized objective and scale_pos_weight.",
      "C": "Train a feed-forward neural network in TensorFlow script mode.",
      "D": "Use SageMaker K-nearest neighbors with class weights."
    },
    "explanation": "XGBoost allows customizing objective and pos/neg weights to handle cost sensitivity and optimize for false negatives."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A news aggregator wants to detect emerging topics from streaming text feeds in real time. They need unsupervised topic modeling with low latency. Which AWS approach should they select?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker LDA in batch mode triggered hourly.",
      "B": "Use SageMaker K-means on streaming windows.",
      "C": "Use Amazon Kinesis Data Analytics with built-in ML to apply LDA in real time.",
      "D": "Use AWS Lambda to call a SageMaker endpoint running topic modeling."
    },
    "explanation": "Kinesis Data Analytics integrates LDA in streaming pipelines for low-latency unsupervised topic detection without batch orchestration."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A rideshare company wants to cluster drivers into behavior profiles based on speed, route deviation, and idle time. They need an algorithm that automatically detects the number of clusters. Which approach should they use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker K-means with silhouette analysis to pick k.",
      "B": "Use SageMaker Gaussian Mixture built-in algorithm for EM-based clustering.",
      "C": "Train a self-organizing map in script mode.",
      "D": "Use SageMaker Random Cut Forest to discover clusters."
    },
    "explanation": "Gaussian Mixture automatically estimates cluster probabilities and can infer the number of components via Bayesian Information Criterion."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A broadband ISP wants to forecast hourly network usage patterns per customer segment. They require a model that can incorporate seasonal effects and external events. Which SageMaker algorithm fits best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker DeepAR forecasting with seasonal frequency and event indicators.",
      "B": "Use SageMaker XGBoost regression with dummy variables.",
      "C": "Use a custom Prophet model in SageMaker script mode.",
      "D": "Use SageMaker K-means to cluster time windows."
    },
    "explanation": "DeepAR natively handles seasonality and event covariates at scale with built-in SageMaker support."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A telecom analytics team needs to classify customer support calls as billing, technical, or general inquiry. They have 20,000 transcribed calls. They want a model that can be retrained weekly with minimal code. Which approach should they choose?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Train a custom RNN model in TensorFlow script mode.",
      "B": "Use Amazon Transcribe followed by BlazingText supervised training.",
      "C": "Use SageMaker JumpStart text classification solution template.",
      "D": "Use Amazon Comprehend custom classification."
    },
    "explanation": "JumpStart solution templates provide end-to-end pipelines that automate retraining with minimal code changes."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A biotech company wants to identify cell types from single-cell RNA sequencing data (20,000 genes per cell). They need unsupervised clustering that scales and can handle high dimensionality. Which SageMaker built-in algorithm should they select?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "Use SageMaker K-means with PCA preprocessing.",
      "B": "Use SageMaker Gaussian Mixture model for soft clustering in high dimensions.",
      "C": "Use a custom t-SNE embedding in script mode.",
      "D": "Use SageMaker Random Cut Forest for clustering."
    },
    "explanation": "Gaussian Mixture can handle high-dimensional soft clustering; combining with EM makes it more flexible than hard k-means."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A publisher wants to rank articles by predicted click-through rate (CTR). They have historical CTR logs and dozens of categorical features with high cardinality. Which algorithm should they use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Linear Learner with one-hot encoding.",
      "B": "Use SageMaker XGBoost with target encoding for high-cardinality features.",
      "C": "Train a deep neural network in PyTorch script mode.",
      "D": "Use SageMaker K-nearest neighbors for CTR prediction."
    },
    "explanation": "XGBoost handles target encoding and interactions efficiently on tabular data with high-cardinality categorical features."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.1",
    "stem": "A healthcare analytics startup wants to extract structured data fields (e.g., dosages) from scanned prescription forms. They need a fully managed approach requiring minimal ML expertise. Which AWS service should they choose?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Train a custom form-recognition model in SageMaker script mode.",
      "B": "Use Amazon Textract Analyze ID API.",
      "C": "Use Amazon Textract Forms with pre-built key-value extraction.",
      "D": "Use Amazon Comprehend Medical."
    },
    "explanation": "Textract Forms extracts key-value pairs from documents without custom model training, ideal for structured field extraction."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A data science team needs to automate retraining of a SageMaker model whenever new data arrives in S3. They want to enforce a manual approval step after data validation and before model training. Which combination of AWS services and pipeline stages meets these requirements with least custom code?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Lambda triggered by S3 events to run a Step Functions state machine that includes a manual approval (via SNS) and then starts a SageMaker training job.",
      "B": "Use SageMaker Pipelines: define a DataQualityCheck step, add a ModelTraining step, and insert a ManualApprovalStep between validation and training.",
      "C": "Use CodePipeline with Source stage on S3, a CodeBuild project to validate data, an AWS Lambda for approval, and a CodeBuild to start the training.",
      "D": "Use Step Functions with S3 event trigger, implement data validation in a task, pause with Wait state for manual input via API Gateway, then start training."
    },
    "explanation": "SageMaker Pipelines natively supports data validation, model steps, and ManualApprovalStep with minimal custom code, fulfilling requirements efficiently."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A pipeline is defined in AWS CodePipeline that builds Docker containers for inference, pushes them to ECR, and deploys to SageMaker endpoints. The team notices that if the build fails, manual rollbacks are needed. Which CodePipeline feature should be added to automate rollback on deployment failures?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable cross-region replication in ECR.",
      "B": "Use CodeBuild 'batch' build mode for transactional builds.",
      "C": "Configure a DeploymentConfig with Canary10Percent10Minutes in the CodeDeploy stage for automatic rollback on failure.",
      "D": "Add a Lambda invoke action after deployment to delete failed endpoints."
    },
    "explanation": "Using a CodeDeploy DeploymentConfig with canary deployments supports automatic rollback on failure without custom Lambdas."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "An ML pipeline in CodePipeline runs training, evaluation, and deployment. The evaluation step uses CodeBuild to compute model metrics. The team needs the pipeline to stop and notify if model accuracy drops below 85%. How can they implement this with minimal changes?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Modify the training CodeBuild to throw an exception when accuracy <85% and let the pipeline fail.",
      "B": "Use CloudWatch Alarms on CodeBuild logs to alert and pause the pipeline.",
      "C": "Write a Lambda in the evaluation stage to call CodePipeline StopPipelineExecution if accuracy <85%.",
      "D": "In evaluation CodeBuild, use buildspec checks to fail the build when accuracy <85%, causing the pipeline to stop and send failure notifications."
    },
    "explanation": "Failing the evaluation CodeBuild via buildspec when accuracy <threshold automatically stops the pipeline and triggers failure notifications."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A pipeline must deploy a new ML model version to a real-time endpoint with zero downtime and allow easy rollback. Which deployment strategy and pipeline configuration should be used?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Lambda to deploy new model to same endpoint container, overwriting existing one.",
      "B": "Configure a SageMaker multi-model endpoint in CodePipeline with blue/green deployment via CodeDeploy.",
      "C": "Use SageMaker real-time endpoint with WarmPool and update endpoint configuration via CodeBuild.",
      "D": "Launch a new endpoint, test it, then delete the old one via a final CodePipeline stage."
    },
    "explanation": "SageMaker multi-model endpoints with CodeDeploy blue/green strategy enable zero-downtime updates and easy rollback."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "The team needs to version control SageMaker Pipeline definitions, enforce PR reviews, and automatically trigger pipeline execution when changes are merged. How should they architect this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Store pipeline definitions in a Git repo. Use CodePipeline with Source from CodeCommit, add CodeBuild for linting, and deploy to SageMaker Pipelines using CloudFormation in a later stage.",
      "B": "Use SageMaker Studio notebooks directly and rely on manual execution upon merge.",
      "C": "Embed pipeline definitions in CodeBuild scripts, trigger on GitHub webhook, and start executions via SDK.",
      "D": "Use AWS CDK in local environment, require manual CDK deploy after merge."
    },
    "explanation": "Using CodePipeline Source from CodeCommit with automated linting and CloudFormation deploy ensures version control, PR reviews, and auto-deploy on merge."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A CodePipeline pipeline must orchestrate cross-account deployment of model inference containers. Account A builds and tests the image; Account B deploys to SageMaker. How should roles and permissions be set?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "In Account A, create an IAM role that trusts Account B\u2019s CodePipeline service principal to push to ECR.",
      "B": "In Account B, create a pipeline that assumes the CodeBuild role from Account A to pull images.",
      "C": "In Account A\u2019s ECR repository policy, allow pull by a deployment role in Account B. In Account B, create a pipeline with a service role that has pull permissions and assume role rights.",
      "D": "Replicate ECR images to Account B automatically via cross-account replication."
    },
    "explanation": "Granting Account B\u2019s pipeline role pull permissions in Account A\u2019s ECR via repository policy allows secure cross-account access for deployment."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A pipeline using AWS CodePipeline must package ML code and dependencies into a container, push to ECR, then deploy to ECS for batch inference. They also need to run unit tests on the packaged image. Which steps should the pipeline include?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Source -> Build (Docker build & push) -> Deploy -> Test",
      "B": "Source -> Build (Docker build) -> Test -> Deploy",
      "C": "Source -> Test -> Build (Docker build & push) -> Deploy",
      "D": "Source -> Build (Docker build) -> Test (CodeBuild uses image via local registry) -> Build (Docker push) -> Deploy"
    },
    "explanation": "Building the image, running tests locally in CodeBuild, then pushing and deploying ensures integrity before pushing."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "An ML team wants to integrate Terraform-managed infrastructure into a CI/CD pipeline for SageMaker endpoint updates. They need a single pipeline. Which service combination achieves this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS CloudFormation with macros in CodePipeline.",
      "B": "Use CodePipeline with Terraform CodeBuild actions: one to plan, manual approval, one to apply changes, then a SageMaker deployment stage.",
      "C": "Use AWS CDK in CodeBuild to translate Terraform into CDK and deploy.",
      "D": "Trigger Terraform from a Lambda invoked by EventBridge in between CodePipeline stages."
    },
    "explanation": "Running Terraform plan/apply in CodeBuild with approval inside CodePipeline integrates IaC and SageMaker deployments in one pipeline."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Which pattern ensures that data scientists can trigger hyperparameter tuning experiments via pull request merge, with automated notifications on completion?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use CodePipeline triggered by CodeCommit pull request merge. In a CodeBuild stage, invoke SageMaker HPO via AWS CLI. Use CloudWatch Events to notify on job completion.",
      "B": "Use SageMaker Experiments SDK manually invoked in notebooks.",
      "C": "Configure EventBridge to start tuning on S3 object creation, send SNS on complete.",
      "D": "Use Step Functions triggered by CodePipeline Webhook to start tuning."
    },
    "explanation": "CodePipeline on merge, CodeBuild to start tuning, and CloudWatch Events to notify provides CI/CD for experiments."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "When integrating a testing framework into CI/CD for ML, the team needs to run data schema validation, unit tests for transformations, and integration tests for training artifacts. How should they structure the CodePipeline?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Single build stage running all tests sequentially.",
      "B": "Separate pipelines for data tests, code tests, and training.",
      "C": "Pipeline with Source -> Data Validation Test (CodeBuild) -> Unit Test (CodeBuild) -> Integration Test (CodeBuild) -> Deploy.",
      "D": "Use Step Functions to orchestrate tests outside CodePipeline."
    },
    "explanation": "Separate CodeBuild stages in one pipeline for test types provide clear isolation and failure points."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A compliance requirement mandates that any model deployment must pass through an audit stage capturing approvals in an immutable log. Which service and pipeline step combination satisfies this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use CloudTrail to log manual approvals in SNS.",
      "B": "Use CodeDeploy with manual approval.",
      "C": "Use Step Functions Wait state with DynamoDB logging.",
      "D": "Use CodePipeline Manual Approval action with execution details logged in CloudTrail."
    },
    "explanation": "CodePipeline Manual Approval action events are logged in CloudTrail, satisfying audit and immutability requirements."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A data scientist merges new featurization code into GitHub. This should automatically update the SageMaker processing step in the CI/CD pipeline. How can this be achieved?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Manually update pipeline JSON in S3.",
      "B": "Use CodePipeline with a GitHub source action on the repo. Add a CodeBuild stage to synthesize updated pipeline definitions and deploy via CloudFormation update.",
      "C": "Write a Lambda to watch GitHub and update SageMaker resources via SDK.",
      "D": "Use AWS AppRunner to auto-deploy changes."
    },
    "explanation": "CodePipeline GitHub source plus CodeBuild to update pipeline definitions and CloudFormation deploy automates the update."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your pipeline uses AWS CodePipeline and CodeBuild to build, test, and deploy containers to SageMaker. You need to ensure that only approved container images are deployed. Which mechanism should you implement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use ECR image scanning on push, and add a Lambda that checks scan results before CodeDeploy stage.",
      "B": "Restrict CodePipeline IAM role to only allow unscanned images.",
      "C": "Use a lifecycle policy in ECR to retain only scanned images.",
      "D": "Configure CodeBuild to fail if image size exceeds threshold."
    },
    "explanation": "Integrating ECR image scan results via Lambda in the pipeline ensures only approved images proceed to deployment."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "The team must enforce that any pipeline change triggers a security compliance scan of both infrastructure and code. Which pattern achieves this end to end?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon Inspector on EC2 build agents.",
      "B": "Use CloudWatch Events to trigger manual security review.",
      "C": "In CodePipeline after Source stage, add a CodeBuild stage that runs static code analysis (e.g., cfn-nag) and container vulnerability scans, failing the build on issues.",
      "D": "Use AWS Config rules to audit pipeline resources."
    },
    "explanation": "Placing a security scan CodeBuild stage in the pipeline ensures any change undergoes compliance checks before deployment."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "You need to orchestrate an ML pipeline that span multiple AWS Regions. How can you centralize pipeline definition and execution?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Replicate the pipeline in each region manually.",
      "B": "Use Route 53 to direct triggers to regional pipelines.",
      "C": "Use EventBridge cross-region events to start different pipelines.",
      "D": "Store pipeline code in a central CodeCommit repo and use CodePipeline cross-region actions with CloudFormation to deploy and execute regional pipelines."
    },
    "explanation": "CodePipeline cross-region actions and central repo allow centralized definition while executing region-specific pipelines."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A team wants to provision ephemeral CI/CD infrastructure for each pull request to isolate changes during ML pipeline definition tests. Which approach is most automated and least overhead?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Manually launch Cloud9 environments per PR.",
      "B": "Use CodePipeline with a CodeBuild job that uses CDK to spin up a nested CloudFormation stack for the PR, runs tests, then tears it down.",
      "C": "Use Step Functions to create stacks, run tests, delete stacks.",
      "D": "Use AWS Batch to run tests in containers."
    },
    "explanation": "Using CDK in CodeBuild to deploy and teardown nested stacks per PR provides isolation with automation and minimal custom code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "They need to integrate model explainability checks into CI/CD such that if SHAP feature importance shifts beyond threshold in new model, pipeline fails. How to implement?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Add a CodeBuild stage using SageMaker Clarify SDK to compute SHAP values, compare them, and exit with failure if threshold exceeded.",
      "B": "Use CloudWatch Metrics to monitor SHAP values after deployment.",
      "C": "Embed checks in training script to abort training.",
      "D": "Deploy model then run real-time inference to compute drift."
    },
    "explanation": "A dedicated CodeBuild stage for Clarify explainability tests before deployment stops the pipeline on SHAP drift."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A centralized security team must approve any pipeline that grants broad IAM permissions. How can the CI/CD pipeline enforce this approval?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Config to detect wide permissions.",
      "B": "Use Lambda to revoke permissions after deployment.",
      "C": "Include a Manual Approval action after IAM change stage, before deployment, logged in CodePipeline.",
      "D": "Use CloudTrail Insights alerts."
    },
    "explanation": "Manual Approval actions in CodePipeline halt progress until security team approval, fulfilling enforcement and logging."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your ML pipeline CodeBuild stage needs environment variables stored securely. Which practice provides secure injection and auditability in CI/CD?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Store env vars in CodeBuild project settings.",
      "B": "Use AWS Secrets Manager, grant CodeBuild role GetSecretValue, inject at runtime.",
      "C": "Hard-code secrets in buildspec.",
      "D": "Store secrets in S3 with public-read access."
    },
    "explanation": "Secrets Manager integration with CodeBuild provides secure retrieval and auditing; avoids hard-coding or insecure storage."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A pipeline deploys SageMaker endpoints across multiple accounts. The build stage packages a SageMaker Model package. How can you share the model package across accounts securely in the pipeline?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Manually copy model artifacts via S3 CLI.",
      "B": "Use public S3 bucket for artifacts.",
      "C": "Use AWS DMS to migrate artifacts.",
      "D": "Use SageMaker Model Package Groups with cross-account grant via resource policy, and use CodePipeline roles to deploy."
    },
    "explanation": "SageMaker Model Package Groups support resource policies for cross-account sharing; CodePipeline roles can assume deploy permissions."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "How can you ensure pipeline stages for SageMaker training run on Spot instances to reduce cost, while controlling max runtime for retries?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set TrainingComputeType to ml.p3.2xlarge in pipeline.",
      "B": "Use a separate Spot training pipeline.",
      "C": "In SageMaker Pipeline TrainStep, specify TrainingInputMode with UseSpotInstances=true and MaxWaitTime/MaxRunTime.",
      "D": "Modify IAM policy to allow Spot."
    },
    "explanation": "SageMaker Pipelines TrainStep supports UseSpotInstances and max wait/run time to leverage Spot with retry constraints."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your pipeline uses CodePipeline with CloudFormation deploy action to create SageMaker Pipeline stacks. The team wants to detect drift between the deployed pipeline definition and source. Which tool helps?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable CloudFormation Drift Detection on the pipeline stack and integrate a CodeBuild stage to run detect-stack-drift and fail if drifted.",
      "B": "Use AWS Config to detect code changes in CodeCommit.",
      "C": "Add a Lambda to compare JSON definitions.",
      "D": "Use Inspector to scan pipeline definitions."
    },
    "explanation": "CloudFormation Drift Detection identifies config drift; performing it in CodeBuild fails pipeline if drift exists."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A pipeline must orchestrate a nightly batch inference job on new data via SageMaker Processing, then archive results to S3 Glacier. How can this be built in CodePipeline?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Lambda for processing and then Glacier archive.",
      "B": "Define pipeline with Source (EventBridge scheduled start), Processing stage using CodeBuild to call SageMaker Processing, then a Deploy stage to trigger a Lambda copying results to Glacier.",
      "C": "Use Step Functions with Lambda tasks.",
      "D": "Use Data Pipeline service."
    },
    "explanation": "CodePipeline with an EventBridge-triggered Source and CodeBuild action calling SageMaker Processing, followed by a Lambda archive Deploy stage, meets requirements."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A pipeline deploys model code and container. You need to validate model container performance under load before routing production traffic. Which deployment strategy implements this in CI/CD?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a single Lambda test after deployment.",
      "B": "Deploy to Sagemaker endpoint and test within same stage.",
      "C": "Use CodeDeploy with linear deployment configuration, sending small percentage of traffic to new container, run performance tests, then proceed.",
      "D": "Use Canary deployments via ECS Blue/Green."
    },
    "explanation": "CodeDeploy linear deployment with traffic shifting and automated tests allows performance validation before full cutover."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "They need to incorporate code coverage metrics into their CI/CD pipeline for ML data transformation scripts. How can this be done of minimal complexity?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Use CodeBuild to run pytest without coverage and interpret logs.",
      "B": "Add a CodeBuild stage that runs pytest with coverage, then use CodeBuild reports feature to publish coverage artifacts.",
      "C": "Use Coveralls external service triggered by EventBridge.",
      "D": "Manually inspect coverage after each build."
    },
    "explanation": "CodeBuild reports support publishing coverage metrics natively, integrating smoothly in pipelines without external dependencies."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A pipeline failure occurs when deploying to a private VPC endpoint because of missing permissions. Which policy change in CodePipeline role solves this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add IAM permission for ec2:CreateNetworkInterface, ec2:DescribeNetworkInterfaces, ec2:DeleteNetworkInterface.",
      "B": "Allow s3:GetObject VPC endpoints only.",
      "C": "Grant AWSKeyManagementServiceDecrypt in Secrets Manager.",
      "D": "Permit TagResource on CloudWatch Logs."
    },
    "explanation": "Deploying to private VPC endpoints requires network interface permissions for CodePipeline-managed actions; adding those resolves the failure."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Your pipeline uses AWS CodeBuild to package and deploy a PyTorch model to SageMaker. The buildspec installs large ML libraries causing slow builds. How to speed up builds without sacrificing reproducibility?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use smaller instance size for CodeBuild.",
      "B": "Cache from public PyPI mirror.",
      "C": "Use requirement pre-install via pip wheel.",
      "D": "Enable CodeBuild local caching for pip and container layers to reuse previous build artifacts."
    },
    "explanation": "CodeBuild local caching for pip and Docker layers significantly speeds up build times while ensuring same dependencies are installed."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "They need to rollback to the last successful SageMaker pipeline execution automatically if a new execution fails validation. How can this be configured?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use EventBridge to watch FailedExecution and trigger manual rollback.",
      "B": "In SageMaker Pipelines, configure a CallbackStep that on failure starts an execution of the previous pipeline execution using StartPipelineExecution with previous execution id.",
      "C": "Use CloudFormation change sets to revert pipeline definition.",
      "D": "Use Step Functions with TRY/CATCH to revert."
    },
    "explanation": "SageMaker Pipelines CallbackStep can programmatically invoke a rollback to the last model by triggering an execution of the previous successful pipeline."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A pipeline must enforce that secret rotation and updated credentials are picked up without pipeline definition changes. Which pattern supports this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Hardcode secrets in parameter store with long TTL.",
      "B": "Re-deploy pipeline on rotation.",
      "C": "Use SSM Parameter Store dynamic references in pipeline actions to retrieve credentials at runtime.",
      "D": "Trigger pipeline manually after rotation."
    },
    "explanation": "Using dynamic references to Parameter Store ensures pipeline fetches updated secrets at runtime without redefining the pipeline."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "Multiple data teams share a central CI/CD pipeline for SageMaker training. They need isolation so one team\u2019s pipeline failure doesn\u2019t block others. How to configure?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create separate CodePipeline pipelines per team pointing to same buildspec and repos, with individual pipeline resource and IAM roles.",
      "B": "Use single pipeline with parallel branches per team.",
      "C": "Use Step Functions instead of CodePipeline.",
      "D": "Use multi-branch CodeBuild project."
    },
    "explanation": "Separate pipelines per team isolate execution and failures, while sharing code and buildspec for consistency."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "3.3",
    "stem": "A security change requires that any pipeline artifact stored in S3 be encrypted with a customer-managed KMS key. How to enforce this in CodePipeline?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SSE-S3 default in artifact buckets.",
      "B": "Specify encryption in CodeBuild project settings.",
      "C": "Use CloudTrail to audit encryption.",
      "D": "Configure CodePipeline artifact store with KMS key ARN in the pipeline definition."
    },
    "explanation": "Defining the artifactStore with a specific KMS key ARN in the pipeline ensures all pipeline artifacts are encrypted accordingly."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "A fraud-detection model is trained on a dataset where fraudulent cases are only 1% of the data. The model achieves 99% accuracy on a test set, but the business indicates that many frauds are missed. Which evaluation metric would most appropriately reflect the model\u2019s ability to detect fraud?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Overall accuracy on the test set",
      "B": "ROC AUC",
      "C": "Precision-Recall AUC",
      "D": "Log-loss"
    },
    "explanation": "With extreme class imbalance, overall accuracy and ROC AUC can be misleading. Precision-Recall AUC focuses on the positive (fraud) class and better reflects detection capability."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "A classifier on a highly imbalanced dataset yields ROC AUC = 0.95 but PR AUC = 0.4. What does this indicate, and what should you do next?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "The model discriminates well overall but has low precision on positives; tune the decision threshold to improve precision.",
      "B": "The model is reliable; deploy it as is.",
      "C": "The dataset is too balanced; rebalance it further.",
      "D": "Collect more negative samples to improve ROC AUC."
    },
    "explanation": "High ROC AUC with low PR AUC shows many false positives. Adjusting the classification threshold based on PR curve can improve precision for the rare positive class."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your legacy model\u2019s F1 score on the production validation set is 0.85. A new candidate model scores 0.87. To determine if this improvement is statistically significant, you should:",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Automatically deploy the new model because 0.87>0.85",
      "B": "Perform a simple paired t-test on the F1 scores",
      "C": "Use bootstrap sampling on hold-out data to compute confidence intervals of F1 difference",
      "D": "Compare scores on a single test set without intervals"
    },
    "explanation": "Bootstrap sampling yields confidence intervals on the difference in F1, allowing you to assess statistical significance rather than relying on point estimates."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "During training, your deep network\u2019s training loss plateaus very early and validation loss remains high. Which SageMaker Debugger rule should you enable to diagnose this convergence issue?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "VanishingGradient",
      "B": "LossNotDecreasing",
      "C": "WeightUpdate",
      "D": "OverfitDetection"
    },
    "explanation": "The LossNotDecreasing rule alerts when training loss fails to decrease. VanishingGradient detects near-zero gradients, but first you must confirm loss stagnation with LossNotDecreasing."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your model achieves 100% training accuracy but only 60% validation accuracy. Which approach best detects and visualizes this overfitting?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Plot training and validation loss curves using SageMaker Debugger",
      "B": "Compute final confusion matrix on test set",
      "C": "Inspect global SHAP values",
      "D": "Monitor data quality with ModelMonitor"
    },
    "explanation": "Plotting training vs validation loss curves identifies divergence indicative of overfitting; SageMaker Debugger automates rule-based collection of these tensors."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You wish to conduct an A/B test between your production and shadow model variants using SageMaker endpoints. Which configuration allows a traffic split between variants?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy two separate endpoints and manually route traffic",
      "B": "Use a multi-model endpoint with two models loaded",
      "C": "Create an endpoint configuration with two production variants and specify traffic weights",
      "D": "Use AWS CodePipeline to shift traffic between endpoints"
    },
    "explanation": "Endpoint configurations in SageMaker allow multiple production variants with specified traffic weights for A/B testing without separate endpoints."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Model A has F1 = 0.92 and requires 12 hours training; Model B has F1 = 0.91 and requires 1 hour. Retraining occurs weekly. Which model should you deploy, and why?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model A, because maximizing F1 is paramount",
      "B": "Model B, because the small F1 loss is outweighed by 12\u00d7 faster retraining",
      "C": "Deploy both and ensemble",
      "D": "Alternate weekly between A and B"
    },
    "explanation": "In a weekly retraining pipeline, a 1% F1 reduction is acceptable given the 12\u00d7 speedup, reducing compute cost and risk of stale models."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You need to monitor inference data streams for feature distribution changes in production. Which SageMaker capability and baseline type will you use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ModelQualityMonitor with a model quality baseline",
      "B": "ModelBiasMonitor with a bias baseline",
      "C": "ModelExplainabilityMonitor with SHAP baseline",
      "D": "ModelMonitor DataQualityMonitor with a data quality baseline"
    },
    "explanation": "ModelMonitor\u2019s DataQualityMonitor uses a data quality baseline to detect feature distribution drifts in incoming inference data."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "A particular prediction\u2019s SHAP value for feature X is +2.5. What does this imply for that instance?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Feature X increased the model\u2019s output log-odds by 2.5 for this instance",
      "B": "Feature X is the most important globally",
      "C": "The model is biased toward feature X",
      "D": "SHAP values above 0 indicate uncertainty"
    },
    "explanation": "A positive SHAP value indicates that feature X contributed positively to the prediction (in log-odds space for classification) for that specific instance."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You want to track hyperparameters, metrics, and artifacts across multiple training runs in SageMaker. Which feature provides built-in experiment management?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Experiments with trials and trial components",
      "B": "SageMaker Clarify",
      "C": "Resource tagging",
      "D": "CloudWatch Logs"
    },
    "explanation": "SageMaker Experiments lets you organize runs into experiments and trials, capturing hyperparameters, metrics, and artifacts for reproducibility."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your regression model will be used in a financial application where large errors are severely penalized. Which evaluation metric should you prefer?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Root Mean Square Error (RMSE)",
      "B": "Mean Absolute Error (MAE)",
      "C": "R\u00b2 (coefficient of determination)",
      "D": "Mean Absolute Percentage Error (MAPE)"
    },
    "explanation": "RMSE penalizes larger errors quadratically, aligning with applications that severely penalize large deviations."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "To optimize for recall over precision in a binary classifier, you decide to adjust the decision threshold. How should you determine the new threshold?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set threshold to 0.5 regardless of metrics",
      "B": "Maximize accuracy on validation data",
      "C": "Use the threshold that maximizes ROC AUC",
      "D": "Analyze the precision-recall curve to find the point achieving the desired recall"
    },
    "explanation": "Precision-recall curves show trade-offs for different thresholds; selecting the threshold that meets your recall requirement is appropriate."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You have a multi-class classification problem with imbalanced class frequencies. You want each class to contribute equally to overall performance. Which averaging strategy do you use for F1 score?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Micro average",
      "B": "Macro average",
      "C": "Weighted average",
      "D": "Sample average"
    },
    "explanation": "Macro-averaged F1 computes F1 per class then averages equally, ensuring each class contributes equally despite imbalance."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your model is deployed in a streaming inference pipeline. You need to detect sudden drops in model accuracy over recent data without storing all predictions. Which approach is best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Query historical S3 logs daily",
      "B": "Compute a one-off accuracy on a large batch",
      "C": "Use SageMaker Model Monitor with a sliding window and accuracy rule",
      "D": "Use CloudWatch Metrics alone"
    },
    "explanation": "Model Monitor can continuously compute metrics over sliding windows on production data without storing all raw predictions."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Business requirements specify that predicted loan amounts must not decrease as applicant income increases. Which technique helps you verify this monotonic relationship in your deployed model?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Generate a partial dependence plot (PDP) for income",
      "B": "Examine the confusion matrix",
      "C": "Use a data drift baseline",
      "D": "Calculate overall RMSE"
    },
    "explanation": "A PDP shows the average model prediction as a function of a feature, allowing you to verify monotonicity constraints."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You suspect your neural network is overfitting early in training. Which SageMaker Debugger built-in rule can alert you when validation loss starts increasing while training loss decreases?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "LossNotDecreasing",
      "B": "VanishingGradient",
      "C": "Overflow",
      "D": "Overfit"
    },
    "explanation": "The Overfit rule in SageMaker Debugger flags when validation loss increases while training loss continues to decrease, a sign of overfitting."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Given confusion matrix counts TP=80, FP=20, FN=10, TN=890, what is the false positive rate (FPR)?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "20/(20+890) = 0.022",
      "B": "20/(80+20) = 0.20",
      "C": "10/(10+890) = 0.011",
      "D": "80/(80+10) = 0.89"
    },
    "explanation": "FPR = FP/(FP+TN) = 20/(20+890) \u22480.022."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You want to evaluate if two classifiers produce significantly different error rates on the same test set. Which statistical test is most appropriate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Paired t-test on accuracy scores",
      "B": "McNemar\u2019s test",
      "C": "Chi-square test for independence",
      "D": "Wilcoxon signed-rank test"
    },
    "explanation": "McNemar\u2019s test compares paired categorical outcomes (correct vs incorrect) from two classifiers on the same instances."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "After deployment, your model\u2019s input distribution remains stable but prediction error gradually increases. What type of drift does this indicate?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Covariate shift",
      "B": "Label shift",
      "C": "Data drift",
      "D": "Concept drift"
    },
    "explanation": "Concept drift occurs when the relationship between inputs and targets changes, causing error to rise despite stable input distribution."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You observe gradients in the first layers of your network are vanishing, slowing learning. Which SageMaker Debugger rule detects this issue directly?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "VanishingGradient",
      "B": "LossNotDecreasing",
      "C": "Detection of hung jobs",
      "D": "WeightDecay"
    },
    "explanation": "The VanishingGradient rule monitors gradient norms and flags when they are near zero, indicating vanishing gradients."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your pruned model achieves 40% size reduction with only a 0.5% drop in accuracy. To decide whether to deploy it, you should analyze:",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Only the accuracy difference",
      "B": "The trade-off between resource savings (memory, latency) and accuracy loss",
      "C": "Global SHAP importance values",
      "D": "Training time differences"
    },
    "explanation": "Deployment decisions should weigh resource savings (e.g., reduced latency, memory) against any drop in accuracy to determine overall benefit."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You want to assess whether your classifier\u2019s probability outputs are well calibrated. Which metric or tool should you use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ROC AUC",
      "B": "Precision-Recall AUC",
      "C": "Log-loss (cross-entropy)",
      "D": "F1 score"
    },
    "explanation": "Log-loss penalizes incorrect probability estimates and is sensitive to calibration, making it suitable for assessing probability calibration."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You need to compare model performance across multiple user segments (e.g., age groups) and give equal importance to each segment. Which aggregation of segment-level AUC metrics should you use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Arithmetic mean of AUC for each segment",
      "B": "Overall AUC on combined data",
      "C": "Weighted average by segment size",
      "D": "Maximum AUC across segments"
    },
    "explanation": "The unweighted (arithmetic) mean of segment-level AUCs treats each segment equally regardless of its size."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your cost matrix penalizes false negatives at 10\u00d7 the cost of false positives. Which evaluation metric best aligns with this cost sensitivity?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Expected cost computed from confusion matrix and cost weights",
      "B": "ROC AUC",
      "C": "Log-loss",
      "D": "Overall accuracy"
    },
    "explanation": "Computing the expected cost directly using confusion matrix counts and cost weights aligns evaluation with your business cost matrix."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "A model shows high overall accuracy but a high false negative rate on a protected group. To monitor fairness over time, which metric should you track with SageMaker Clarify?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Overall RMSE",
      "B": "ROC AUC",
      "C": "Data drift percentage",
      "D": "Difference in true positive rates (equal opportunity difference)"
    },
    "explanation": "Equal opportunity difference measures TPR gap between groups, highlighting unfair false negatives for protected attributes."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "To detect label distribution changes in production (label drift), which SageMaker Model Monitor job and baseline do you configure?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ModelQualityMonitor with a label distribution baseline",
      "B": "DataQualityMonitor with feature baseline",
      "C": "ModelBiasMonitor with bias baseline",
      "D": "ModelExplainabilityMonitor with SHAP baseline"
    },
    "explanation": "ModelQualityMonitor can track label statistics in inference data against a baseline to detect label drift."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You need to choose a threshold for a binary classifier that minimizes a custom cost function of TPR and FPR. Which process accomplishes this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Select threshold\u2009=\u20090.5",
      "B": "Maximize ROC AUC",
      "C": "Minimize log-loss",
      "D": "Compute cost for each threshold on validation probabilities and choose the minimum"
    },
    "explanation": "Evaluating the custom cost at each threshold on validation data lets you select the threshold minimizing your cost function."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your regression model predicts three related targets simultaneously. To evaluate performance treating each target equally, you should:",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Compute RMSE per target and report their average",
      "B": "Compute overall RMSE across all predictions",
      "C": "Weight each target by its variance in a single loss",
      "D": "Report only the highest RMSE target"
    },
    "explanation": "Averaging per-target RMSE ensures each output is given equal importance in performance evaluation."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Your classifier\u2019s test set performance fluctuates significantly between runs. Which validation technique reduces variance and yields a more stable performance estimate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "K-fold cross-validation",
      "B": "Using a single larger test set",
      "C": "Bootstrapping only",
      "D": "Early stopping"
    },
    "explanation": "K-fold cross-validation averages performance over multiple splits, reducing variance due to data sampling."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You want to visualize model performance degradation over time in production. Which solution best automates this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Export logs daily and plot manually",
      "B": "Use SageMaker Model Monitor to publish quality metrics to CloudWatch dashboards",
      "C": "Run ad-hoc batch accuracy reports",
      "D": "Embed metrics in application code"
    },
    "explanation": "Model Monitor can stream quality metrics to CloudWatch, allowing automated dashboards tracking performance over time."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You see a sudden increase in false positives without any change in input distribution. What type of drift is this, and what action should you take?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Covariate drift; retrain on new features",
      "B": "Concept drift; investigate changes in the real world and retrain",
      "C": "Data drift; increase data validation",
      "D": "Label shift; adjust class weights"
    },
    "explanation": "A rise in error despite stable inputs indicates concept drift; investigate underlying changes and retrain the model accordingly."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "Observation of rapidly growing gradient norms across layers suggests exploding gradients. Which SageMaker Debugger rule will catch this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "VanishingGradient",
      "B": "LossNotDecreasing",
      "C": "ExplodingGradient",
      "D": "WeightDecay"
    },
    "explanation": "The ExplodingGradient rule monitors for abnormally large gradient norms, flagging exploding gradient issues."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "2.3",
    "stem": "You use SageMaker Clarify to monitor feature attribution drift. Which monitor type and baseline should you configure?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ModelExplainabilityMonitor with a SHAP value baseline",
      "B": "DataQualityMonitor with a distribution baseline",
      "C": "ModelBiasMonitor with bias all baseline",
      "D": "ModelQualityMonitor with accuracy baseline"
    },
    "explanation": "ModelExplainabilityMonitor compares new SHAP attributions against a baseline to detect feature attribution drift."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A data engineer needs to join a 1 TB Parquet dataset in Amazon S3 with a DynamoDB table of product metadata and register features in SageMaker Feature Store daily with minimal operational overhead and within a 2-hour window. Which approach should the engineer choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Data Wrangler to import both datasets, perform the join, then execute a BatchPutRecord job to ingest features into Feature Store.",
      "B": "Schedule an AWS Glue ETL job to read the Parquet data, use the DynamoDB Spark connector to join the tables, write results back to S3, and trigger a SageMaker processing job to ingest into Feature Store.",
      "C": "Set up AWS Glue Streaming ETL to capture DynamoDB Streams updates and continuously merge against S3 data, writing directly into the online store.",
      "D": "Deploy an AWS Lambda function triggered by each S3 PUT to read and join records on the fly, then call PutRecord for each feature into Feature Store."
    },
    "explanation": "Scheduling a bulk AWS Glue ETL job best handles 1 TB batch processing within a time window and decouples transformation from per-record Lambda overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A feature team must encode a categorical feature with 10 million distinct values for use in a neural network. They need to minimize memory and avoid introducing ordinal bias. Which encoding technique and pipeline step should they implement?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use one-hot encoding in SageMaker Data Wrangler to generate 10 million binary columns, then apply feature selection.",
      "B": "Apply label encoding in DataBrew, converting each category to an integer index, and feed directly to the network.",
      "C": "Use feature hashing in a SageMaker processing job to map categories into a fixed-size hash vector, then normalize hashed features.",
      "D": "Group rare categories into an \"Other\" bucket before one-hot encoding in AWS Glue ETL."
    },
    "explanation": "Feature hashing maps high-cardinality categories into a fixed-size vector without ordinal bias and controls memory footprint."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A time-series dataset has irregular timestamps and missing readings. A data scientist needs to impute missing values and generate lag features in SageMaker Data Wrangler for model training. Which sequence of transformations should they apply?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Resample to a fixed interval with forward-fill imputation, compute lag features using the shifted timestamp, then scale features.",
      "B": "Compute lag features first (with gaps), drop NA, then apply KNN imputation to fill missing values.",
      "C": "Apply drop-rows where any null exists, generate lag features on remaining data, and finally apply standard scaling.",
      "D": "Use quantile imputation on raw data, then apply rolling window aggregations for lag features, and normalize."
    },
    "explanation": "Resampling with forward fill ensures consistent intervals before computing lags; then scaling preserves temporal relationships."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A large training dataset in S3 must be deduplicated by a composite key before feature scaling. The data volume exceeds what a single SageMaker Data Wrangler instance can process. What architecture meets the requirements?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Use SageMaker Data Wrangler with distributed compute enabled for dedupe and scaling.",
      "B": "Use an AWS Lambda function to split and dedupe small partitions, then reassemble.",
      "C": "Use AWS Glue DataBrew to dedupe by key and apply scaling transformations inline.",
      "D": "Use an AWS Glue Spark job on EMR to dedupe by composite key, write results to Parquet in S3, then use Data Wrangler for scaling on the reduced dataset."
    },
    "explanation": "A Spark job on EMR can handle large-scale deduplication; downstream Data Wrangler can operate efficiently on the smaller deduped data."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A team must apply quantile binning on a continuous feature to reduce noise, then one-hot encode the bins. They need to evaluate bin edges on training only and apply identical bins to validation/test sets. How should they implement this in SageMaker Data Wrangler?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create a recipe step to compute quantile bin edges on the training partition, save edges as a parameter, then apply same edges in subsequent steps to all partitions before one-hot.",
      "B": "Compute quantile bin edges over full dataset in Data Wrangler, then split into train/validation/test and one-hot encode separately.",
      "C": "Use AWS Glue DataBrew to compute bins on training, export bin definitions, then import back into Data Wrangler for splits.",
      "D": "Perform quantile binning and one-hot encoding in a SageMaker processing job where edges are recomputed per batch."
    },
    "explanation": "Computing bins on training only and saving edges ensures no leakage; applying stored edges uniformly maintains consistency."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A streaming application ingests clickstream JSON records into Kinesis and needs to parse, clean, and feature-engineer fields in near real-time. Which architecture minimizes latency and offloads transformation operations?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Lambda to poll Kinesis, transform records inline, then push to SageMaker Feature Store.",
      "B": "Write a Kinesis Data Firehose delivery stream with inline record transformation using AWS Glue.",
      "C": "Deploy a Kinesis Data Analytics (Apache Flink) application to parse JSON, apply scalar and one-hot encodings, then write transformed records to an output stream.",
      "D": "Use SageMaker Processing on a schedule to pull batches from Kinesis and process them."
    },
    "explanation": "Kinesis Data Analytics with Apache Flink provides low-latency, continuous transformations at scale, offloading work from Lambda."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A pipeline must mask PII columns before any transformations to comply with compliance. They have CSV data with 5 sensitive columns. Using AWS Glue DataBrew, which recipe steps ordering ensures compliance and minimal reprocessing?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Apply all feature scaling, encoding, and missing-value imputation, then mask PII at the end.",
      "B": "Mask PII using column-level encryption, then apply transformations in the same recipe.",
      "C": "Branch the job: first do transformation, output safe data; second, mask PII on original and join results.",
      "D": "Create two DataBrew steps: first mask (using built-in Mask Column), then reference masked columns in subsequent transformation steps."
    },
    "explanation": "Masking PII first in the recipe prevents any downstream steps from accessing raw PII and avoids reprocessing masked data."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A dataset contains a categorical feature with moderate cardinality and missing values. A data scientist wants to impute missing values as \"Unknown\" before one-hot encoding in SageMaker Data Wrangler. Which setting order in the recipe is correct?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "One-hot encode first, then treat missing values in generated dummy columns.",
      "B": "Apply a Fill Missing Values step on the raw column to \"Unknown\", then apply a One-Hot Encoding step on the filled column.",
      "C": "Drop rows with missing values, then one-hot encode remaining categories.",
      "D": "Run one-hot encoding with the \"Include Nulls\" option, then rename the \"null\" column to \"Unknown\"."
    },
    "explanation": "Imputing missing values before encoding ensures the \"Unknown\" category is represented correctly in the dummy variables."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A text feature requires tokenization and vectorization using a pretrained BERT tokenizer. They need to integrate this into a SageMaker feature engineering pipeline. Which approach is best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Data Wrangler built-in text tokenizer step with custom vocabulary.",
      "B": "Invoke AWS Comprehend batch API to tokenize, then import tokens to Data Wrangler.",
      "C": "Build a SageMaker Processing script container that loads the pretrained BERT tokenizer, tokenizes text, and writes token IDs to S3 for the next pipeline step.",
      "D": "Use AWS Glue Spark job to apply the BERT tokenizer via a Glue Python shell job."
    },
    "explanation": "A custom processing container in SageMaker Processing provides full control over the pretrained tokenizer and integrates cleanly into ML pipelines."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A data scientist wants to scale numeric features using Z-score normalization but only using statistics from the training set. They plan to scale validation/test sets identically. How can they configure SageMaker Data Wrangler?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Apply Z-score normalization in a DataBrew recipe after splitting, selecting all partitions together.",
      "B": "Use a SageMaker Processing job to compute mean/std on full dataset, then scale each partition separately in Data Wrangler.",
      "C": "In Data Wrangler, split the dataset into train/validation/test, add a Normalize step in the recipe that refers to the training split only, and ensure the recipe applies the same parameters to other splits.",
      "D": "Compute mean/std manually outside of Data Wrangler and hard-code values in a recipe."
    },
    "explanation": "Data Wrangler can compute normalization parameters on the training split and apply them uniformly to other splits within one recipe."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "During a SageMaker Data Wrangler flow, a shuffle step is added before splitting data into train/validation/test. The resulting model shows data leakage. What was the mistake?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Shuffling after splitting caused overlap across sets.",
      "B": "Shuffle did not include seed, producing non-reproducible splits.",
      "C": "Shuffle was applied only to the training split, leaving test data ordered.",
      "D": "Shuffle was applied before splitting, mixing records across temporal boundaries and causing leakage."
    },
    "explanation": "Shuffling before splitting time-series or grouped data can mix related records across splits, introducing leakage."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A data engineer needs to generate rolling window statistics (mean, std) per user over the last 7 days from clickstream JSON data in S3. Data volume is 500 GB/month. Which cost-effective approach should they use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Data Wrangler to load all data and compute rolling stats with Python transforms.",
      "B": "Use an AWS Glue Spark job with window functions to compute rolling statistics, write Parquet outputs back to S3.",
      "C": "Load raw data into DynamoDB and use DynamoDB Streams with Lambda to update rolling metrics.",
      "D": "Use SageMaker Feature Store with on-the-fly aggregation in training jobs."
    },
    "explanation": "A Glue Spark job with window functions scales to 500 GB and performs aggregations efficiently before model input."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A team must remove outliers defined as values beyond 3 standard deviations for a numeric feature. They want to log how many records were removed. In SageMaker Data Wrangler, how should they configure this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a Filter recipe step to remove outliers, then use the Data Wrangler UI to view counts after run.",
      "B": "Use a Custom Transform step with a Pandas script to drop outliers and log metrics to CloudWatch.",
      "C": "Add an Aggregate Recipe step to calculate mean/std, then a Filter step referencing those values, and enable the record count output option to capture removed record counts.",
      "D": "Export data to a processing job, implement outlier removal in code, and write counts to S3."
    },
    "explanation": "Data Wrangler\u2019s recipe can calculate statistics and filter in separate steps, and the built-in count option provides removal metrics."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A feature store offline dataset must be kept in sync with latest transformations from SageMaker Data Wrangler. Which mechanism ensures atomic replacement without downtime?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Write transformed data to a staging S3 prefix, then use CFN to update the offline store import job to point to new prefix.",
      "B": "Overwrite the existing S3 prefix directly with new Parquet files in place.",
      "C": "Append new transformed data to the existing prefix, and use a Glue crawler to update partitions.",
      "D": "Delete the offline store table and recreate it with new data after transformation."
    },
    "explanation": "Using a staging prefix and atomic pointer switch avoids partial reads and downtime."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "Continuous feature updates from AWS IoT devices stream into Kinesis Data Streams. A data scientist needs to window, normalize, and encode features before writing to Feature Store online. What real-time pipeline minimizes component count?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Kinesis Data Firehose with AWS Lambda transform to process and send to Feature Store.",
      "B": "Use a SageMaker endpoint to receive raw records, transform, and BatchPutRecord.",
      "C": "Use AWS Glue Streaming ETL job to read, transform, and write to Feature Store offline.",
      "D": "Deploy Kinesis Data Analytics (Apache Flink) application to window, normalize, encode, and call PutRecord API to Feature Store online."
    },
    "explanation": "Kinesis Data Analytics provides a single low-latency stream processing engine capable of calling AWS APIs directly."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "An image classification pipeline needs to apply resizing, normalization, and five types of augmentation (rotation, flip, crop, noise, color jitter) in SageMaker processing. To maximize GPU utilization and reproducibility, what should the team do?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Chain multiple processing jobs sequentially, each handling one augmentation type.",
      "B": "Use a single SageMaker Processing job with a PyTorch container that applies resizing, normalization, and augmentations in one GPU-accelerated DataLoader pipeline.",
      "C": "Implement augmentations in AWS Lambda functions triggered by S3 events for each image.",
      "D": "Use Data Wrangler custom transform steps for each preprocessing function and export as a combined dataset."
    },
    "explanation": "A single GPU-accelerated processing job with PyTorch DataLoader ensures reproducibility and efficient parallel augmentation."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A dataset contains repeated nested JSON fields that must be flattened for feature extraction. In AWS Glue DataBrew, which recipe step correctly handles arbitrary nesting levels?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a Pivot recipe step on the nested column.",
      "B": "Use a Select recipe step and specify JSONPath for each nested field.",
      "C": "Use a Join recipe step with the same column.",
      "D": "Use the Unnest recipe step to iteratively flatten nested JSON arrays and objects into top-level columns."
    },
    "explanation": "The Unnest step is designed to flatten arbitrarily nested JSON structures into columns automatically."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A feature with highly skewed distribution needs log transformation before scaling. A data scientist wants to ensure negative values are handled. In Data Wrangler, what transformation sequence should be used?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Scale feature with MinMax, then apply log transform.",
      "B": "Add a constant offset to make values positive, apply log transform, then standardize.",
      "C": "Apply Box-Cox transformation directly (Box-Cox cannot handle negatives).",
      "D": "Filter out negative values, then apply log transform on positives."
    },
    "explanation": "Adding a constant offset ensures all values are positive before log transform; standardizing afterwards preserves distribution."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A team must detect and drop duplicate records defined by a composite key, but preserve the earliest timestamp record. They want to log count of dropped duplicates. What is the correct approach in Data Wrangler?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use the Remove Duplicates recipe step on the composite key then sort ascending by timestamp.",
      "B": "Sort by timestamp and use the Remove Duplicates step without additional configuration.",
      "C": "Sort descending by timestamp, then use the Remove Duplicates step on the composite key with \"keep first occurrence\" and enable Logging to capture drop counts.",
      "D": "Use a Custom Transform step with a Pandas script to drop duplicates and write counts to CloudWatch."
    },
    "explanation": "Sorting descending ensures the earliest timestamp is last, then keeping the first occurrence drops later duplicates and logs counts."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A text corpus requires feature hashing of TF-IDF vectors into 512 dimensions before training. Which SageMaker toolchain should be used and why?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a SageMaker Processing job with a scikit-learn container to compute TF-IDF and apply FeatureHasher transformer in one job.",
      "B": "Use AWS Glue DataBrew to compute TF-IDF and hashing in recipe steps.",
      "C": "Use SageMaker Data Wrangler built-in TF-IDF and hashing steps.",
      "D": "Use AWS Lambda to compute and hash vectors on each record."
    },
    "explanation": "A processing job with scikit-learn provides efficient batch computation of TF-IDF and hashing integrated into the ML pipeline."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A feature store batch export job must produce engineered features that include one-hot encoded categories and normalized numerics. The export time window is 24 hours. Which orchestration ensures reproducibility and minimal drift?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "Manually run a Data Wrangler flow to export features daily.",
      "B": "Schedule a Lambda that calls PutRecord for each export.",
      "C": "Use AWS Glue job to read offline store and apply transformations at export time.",
      "D": "Define a SageMaker Feature Store Batch Transform pipeline with saved Data Wrangler recipe steps to export and transform atomically."
    },
    "explanation": "Embedding Data Wrangler recipe in a Feature Store batch pipeline ensures consistent transformations and reproducibility."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A data scientist wants to detect measurement bias in labeled data prior to model training. Which AWS tool and workflow should they use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Glue Data Quality to compute missing value rates per label.",
      "B": "Use SageMaker Clarify\u2019s Data Bias job to generate pre-training bias metrics (e.g., CI, DPL) and produce a report for review.",
      "C": "Use SageMaker Data Wrangler to compute feature distributions per class.",
      "D": "Use Amazon Athena to run SQL queries comparing label distributions across partitions."
    },
    "explanation": "SageMaker Clarify is specifically designed to compute pre-training bias metrics and provide actionable reports."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "An offline Spark feature pipeline writes to Parquet partitioned by date. A validation job must only process the latest two days and ignore older partitions. How can they configure an AWS Glue job?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a dynamic frame with push_down_predicate on the date partition to filter to last two days before transformation.",
      "B": "Read all partitions and filter in code.",
      "C": "Use a Glue crawler to catalog only two latest partitions.",
      "D": "Use AWS Glue DataBrew with a filter recipe to drop older partitions."
    },
    "explanation": "Using push_down_predicate filters partitions at read time, reducing data scanned and improving performance."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A feature requires ranking users by activity per region, generating a rank feature. Which tool and technique should they use to compute this in a SageMaker pipeline?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Glue DataBrew with window aggregate recipe step.",
      "B": "Use a SageMaker Processing job with pure Python for ranking.",
      "C": "Use SageMaker Processing with a Spark container to apply window ranking functions by region, writing results to S3.",
      "D": "Use Lambda and Step Functions to orchestrate row-by-row ranking."
    },
    "explanation": "A Spark container in a processing job can efficiently compute window functions and scale to large user sets."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "During a Data Wrangler flow, a customer ID column is misclassified as numeric, preventing correct encoding. How can this be fixed?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Drop the column and reload.",
      "B": "Override the inferred data type to String in the Data Wrangler schema settings, then apply categorical encoding.",
      "C": "Cast the column inside a custom transform only.",
      "D": "Use AWS Glue job to recast and save back to S3 before Data Wrangler."
    },
    "explanation": "Overriding the schema inference directly in Data Wrangler allows correct downstream categorical encoding without external ETL."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A data scientist must anonymize email addresses by hashing them before any aggregation. The hashing function must be consistent across training and inference. How should they implement this in a SageMaker processing pipeline?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a Data Wrangler custom step with Python hashing, then export hashed emails.",
      "B": "Use AWS Glue DataBrew Mask Column with SHA256.",
      "C": "Use a Lambda function triggered by S3 events to hash emails.",
      "D": "Include a SageMaker Processing step with a custom container that applies SHA256 to the email column and writes to Parquet for downstream use."
    },
    "explanation": "A custom processing step ensures identical hashing logic in both training and inference pipelines."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A dataset of IoT sensor readings requires deduplication, imputation of missing values using KNN, and feature scaling. Which order in a DataBrew recipe ensures correct results and performance?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Scale, dedupe, then KNN impute.",
      "B": "Deduplicate first, KNN impute missing values, then apply scaling.",
      "C": "Impute missing values first, then dedupe, then scale.",
      "D": "Perform all three in parallel using branching steps."
    },
    "explanation": "Deduplication should occur before imputation to avoid artificial neighbors; scaling is last to ensure normalized neighbor distances."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A team needs to apply custom Python feature transformations (e.g., trigonometric functions) on numeric columns and integrate into SageMaker Pipelines. Which component should they use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A SageMaker Processing step with a custom Python script container.",
      "B": "A Data Wrangler custom recipe step.",
      "C": "A Lambda function in CodePipeline.",
      "D": "AWS Glue DataBrew Python transforms."
    },
    "explanation": "A SageMaker Processing step in the pipeline allows custom scripts and integrates directly with SageMaker Pipelines."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A dataset transformation in Data Wrangler fails due to schema drift: new columns appear. How can the recipe handle unknown columns without failing?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Manually update the recipe to include new columns each time.",
      "B": "Restrict the flow to specified columns only and drop unknown ones.",
      "C": "Enable the \"Ignore new columns\" option in the Data Wrangler recipe settings to skip unknown fields.",
      "D": "Use a SageMaker Processing job instead for dynamic schema."
    },
    "explanation": "Data Wrangler\u2019s ignore new columns setting prevents failures when unexpected fields appear."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A table join in SageMaker Data Wrangler is running out of memory because one table is 200 GB. Which alternative solution scales to this size?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase the Data Wrangler instance type to more memory.",
      "B": "Perform the join using an AWS Glue Spark ETL job and write result to S3, then import into Data Wrangler.",
      "C": "Use a SageMaker Processing job with Pandas on a GPU instance.",
      "D": "Split the larger table into chunks and join sequentially in Data Wrangler."
    },
    "explanation": "An AWS Glue Spark ETL job is designed to handle large-scale joins and can write partitioned outputs for downstream use."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A feature extraction requires computing pairwise differences between two timestamp columns per record. Which recipe step in Data Wrangler should they choose and why?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use the Compute Feature step with a custom expression (timestamp2 - timestamp1) to derive duration.",
      "B": "Use the Aggregate step grouping on record ID.",
      "C": "Use the Pivot step on timestamp fields.",
      "D": "Use a Custom Transform with a Pandas script to compute differences."
    },
    "explanation": "The Compute Feature step allows arithmetic expressions on columns and is more efficient than full Python transforms."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A team must ensure that text tokenization and stop-word removal occur before computing count vector features in a Data Wrangler flow. How should they configure the recipe?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Add Count Vectorization step, then Tokenization, then Stop-word removal.",
      "B": "Add Stop-word removal, then Count Vectorization, then Tokenization.",
      "C": "Add Tokenization, then Count Vectorization, then Stop-word removal.",
      "D": "Add Tokenization, then Stop-word removal step, then Count Vectorization in sequence."
    },
    "explanation": "Tokenization must split text first, followed by stop-word removal on tokens, then count vectorization."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.2",
    "stem": "A dataset stored as CSV in S3 must be converted to Parquet with snappy compression before feature engineering to improve I/O. Which tool and configuration should be used?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a SageMaker Processing job with a Pandas script to read CSV and write Parquet.",
      "B": "Use an AWS Glue ETL job with a PySpark script to read CSV, write Parquet with snappy compression, and partition by date.",
      "C": "Use SageMaker Data Wrangler export with Parquet output and compression.",
      "D": "Use AWS Lambda triggered per file to invoke Athena CTAS to create Parquet tables."
    },
    "explanation": "An AWS Glue Spark ETL job is well-suited for large-scale CSV to Parquet conversion with compression and partitioning."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A company must ingest 10 TB of historical telemetry data from an on-premises Hadoop cluster into Amazon S3 within 5 days for model training. The WAN link is limited to 200 Mbps and encryption at rest is required. Which ingestion method meets the transfer window while minimizing operational overhead?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Use AWS Snowball Edge to physically import the data into S3 with built-in encryption.",
      "B": "Use S3 Transfer Acceleration over the internet with SSE-S3 encryption.",
      "C": "Use AWS DataSync over the existing link with server-side encryption.",
      "D": "Deploy AWS Storage Gateway in Volume mode and snapshot to S3."
    },
    "explanation": "Snowball Edge provides accelerated, encrypted bulk transfer when network bandwidth is constrained, minimizing operational overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An ML pipeline queries 500 GB of transaction logs in S3 via Athena, then ingests the results into SageMaker Data Wrangler. Which storage format and partitioning maximize query performance and minimize costs?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Store logs as CSV files partitioned by year/month.",
      "B": "Store logs as JSON files partitioned by customer ID.",
      "C": "Store logs as Apache Parquet files partitioned by date.",
      "D": "Store logs as unpartitioned text files in a single prefix."
    },
    "explanation": "Parquet\u2019s columnar format plus date partitioning reduces scanned data and cost for Athena queries and speeds ingestion."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A real-time anomaly detection model requires streaming sensor data into S3 for batch retraining. Which combination of services ingests JSON records at low latency with ordering guarantees?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Kinesis Data Firehose delivering directly to S3.",
      "B": "Amazon Kinesis Data Streams then a Kinesis Data Firehose delivery stream to S3.",
      "C": "AWS Glue streaming ETL reading from S3 Event Notifications.",
      "D": "Amazon Managed Kafka writing directly to S3."
    },
    "explanation": "Kinesis Data Streams preserves ordering and low latency; Firehose can then buffer and batch to S3 automatically."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "Your model needs high-throughput access to feature data during training. You must choose between Amazon EFS, Amazon FSx for NetApp ONTAP, and Amazon S3. The data is >1 TB, highly structured, and training uses Spark on EMR. Which storage is best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon EFS with provisioned throughput.",
      "B": "Amazon FSx for NetApp ONTAP.",
      "C": "Amazon S3 with EMRFS Consistent View.",
      "D": "Store data on EMR HDFS volumes."
    },
    "explanation": "S3 with EMRFS scales to TBs, offloads management, and is cost-effective for large structured datasets; HDFS adds management overhead."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A team uses SageMaker Data Wrangler to profile 2 TB of CSV data stored in S3. They notice excessive request latencies. Which action will most reduce latency when reading from S3?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable SSE-KMS on the S3 bucket.",
      "B": "Convert CSV data to Apache Parquet with Snappy compression.",
      "C": "Enable S3 Object Lock on the bucket.",
      "D": "Enable S3 Versioning on the bucket."
    },
    "explanation": "Converting to Parquet reduces the number of S3 GET requests and data scanned, greatly lowering latency in Data Wrangler."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An ML workflow must merge customer profile data in DynamoDB with clickstream logs in S3, then store results for training. Which approach minimizes custom code and operational complexity?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Use AWS Glue ETL jobs with a DynamoDB connector to join and write to S3 in Parquet.",
      "B": "Export DynamoDB to S3 via Data Pipeline, then use Lambda to merge with clickstream.",
      "C": "Use SageMaker Processing to read both and write merged CSV to S3.",
      "D": "Stream clickstream into DynamoDB and query joins at training time."
    },
    "explanation": "AWS Glue ETL handles connectors and joins natively and writes Parquet to S3, reducing custom orchestration."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "You must ingest 5 TB of Amazon RDS data into SageMaker Feature Store online store for low-latency feature retrieval. Which combination meets throughput and cost requirements?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Export RDS snapshot to S3, use Glue to write to Feature Store.",
      "B": "Use RDS native replication into DynamoDB, then batch ingest into Feature Store.",
      "C": "Run a SageMaker Processing job to read from RDS and write to offline store.",
      "D": "Use SageMaker Data Wrangler to connect to RDS and directly ingest records into Feature Store online store."
    },
    "explanation": "Data Wrangler can connect to RDS and ingest into Feature Store online, meeting throughput needs with minimal glue code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A model requires streaming user events from mobile apps to S3 for nightly batch training. You need exactly-once delivery and minimal data loss. Which architecture is most appropriate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon Kinesis Data Firehose directly to S3.",
      "B": "Use AWS Lambda triggered by API Gateway to write to S3.",
      "C": "Use Amazon Kinesis Data Streams with a consumer that writes to S3 via Firehose.",
      "D": "Use Amazon SQS and a fleet of EC2 batch workers to write to S3."
    },
    "explanation": "Kinesis Data Streams ensures ordering and exactly-once semantics; Firehose delivers to S3 reliably with retries."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "During a cost audit, you discover that S3 GET and PUT costs for small files (<1 KB) used in training are high. How can you reduce costs while keeping data in S3?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Switch to S3 Standard-Infrequent Access.",
      "B": "Combine small files into larger Apache Avro container files.",
      "C": "Enable S3 Transfer Acceleration.",
      "D": "Enable S3 Intelligent-Tiering."
    },
    "explanation": "Combining small files into larger containers reduces request count and lowers GET/PUT request costs."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A global company must ingest CSV sales data from multiple regional offices into a central S3 bucket. Some offices have poor upload bandwidth. How to ensure data arrives in the correct order and is fully available for batch processing in SageMaker?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use S3 Transfer Acceleration with directory replication.",
      "B": "Use AWS DataSync scheduled tasks from each office.",
      "C": "Use AWS Storage Gateway file gateway.",
      "D": "Configure regional AWS Kinesis Data Firehose streams to deliver to a central S3 bucket with buffering and ordering enabled."
    },
    "explanation": "Regional Firehose streams buffer data, preserve ordering per shard, and reliably deliver to central S3 for downstream batch."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "Your training data resides in a high-CPU EBS volume attached to an EC2 instance. To ingest data into SageMaker Data Wrangler at scale, which approach minimizes network bottlenecks?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Snapshot the EBS volume to S3 and let Data Wrangler read from S3.",
      "B": "Attach the EBS volume to the SageMaker Data Wrangler notebook instance.",
      "C": "Enable EBS multi-attach and mount on Data Wrangler instance.",
      "D": "Run a DataSync task from the EC2 instance to the Data Wrangler volume."
    },
    "explanation": "Snapshot to S3 offloads network traffic and lets Data Wrangler ingest efficiently from S3 rather than over direct EBS mounts."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A data scientist needs to join high-velocity Kafka events with a static dataset in S3 before training. They require minimal lag and schema evolution support. What ingestion pattern is best?",
    "correct": "B",
    "difficulty": "HARD",
    "answers": {
      "A": "Use Kinesis Data Firehose with Lambda to enrich events with S3 data.",
      "B": "Deploy Amazon Managed Streaming for Kafka with AWS Glue streaming ETL to join and write to S3.",
      "C": "Use SageMaker Processing to pull from Kafka and S3 periodically.",
      "D": "Use AWS Lambda triggered by Kafka Connect and query S3 DynamoDB export."
    },
    "explanation": "Glue streaming ETL supports schema evolution and continuous joins with Kafka and S3, delivering results to S3 in near real time."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A compliance requirement mandates encrypting sensor data and storing in S3 Glacier after ingestion. The data is 8 TB per month. What solution meets compliance with the lowest storage cost while remaining queryable for ML?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Ingest to S3 Standard with SSE-KMS, transition to S3 Glacier Deep Archive immediately.",
      "B": "Ingest to S3 Glacier with client-side encryption.",
      "C": "Ingest to S3 Standard-IA with SSE-KMS, configure lifecycle to move to Glacier after 30 days.",
      "D": "Ingest to S3 One Zone-IA with SSE-KMS then transition to Glacier Flexible Retrieval."
    },
    "explanation": "Standard-IA with SSE-KMS and a 30-day lifecycle balances low cost while keeping data in S3 for Athena before Glacier transition."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "You must ingest and merge product catalog data from Amazon RDS and Alexa usage logs in S3 into SageMaker Feature Store offline store. How do you ingest with minimal code?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use AWS Glue jobs to extract from RDS and S3, transform, and write to Feature Store.",
      "B": "Use SageMaker Processing to read both sources and write offline store.",
      "C": "Export RDS to S3, then use Data Wrangler to merge and ingest.",
      "D": "Use AWS AppFlow to replicate RDS and merge with S3 logs."
    },
    "explanation": "Glue ETL natively connects to RDS and S3 and can write to Feature Store with minimal custom code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "An ML workflow streaming video metadata at 10 MB/sec must store data in S3 for batch training. Latencies <5 seconds acceptable. Which ingestion service provides auto-scaling and buffering without managing infrastructure?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon Kinesis Data Streams with manual shard scaling.",
      "B": "Use Amazon Kinesis Data Firehose to deliver to S3.",
      "C": "Run AWS Lambda to poll a custom endpoint and write to S3.",
      "D": "Use AWS Managed Kafka with MirrorMaker to S3."
    },
    "explanation": "Firehose auto-scales and buffers transient bursts, delivering to S3 with minimal management."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "1.1",
    "stem": "A dataset of images is stored in S3 and must be preprocessed via SageMaker Processing weekly. The team wants to minimize request overhead and latency when reading many small files. How should they reorganize the data?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable S3 Transfer Acceleration when reading each image.",
      "B": "Copy images to an EFS mount for processing.",
      "C": "Bundle images into TFRecord or RecordIO files and store in S3.",
      "D": "Store images as base64-encoded JSON in single S3 objects."
    },
    "explanation": "Bundling into TFRecord/RecordIO reduces object count and request overhead, speeding SageMaker Processing."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A financial services company deployed a SageMaker real-time endpoint for credit risk scoring. They need to detect, within one hour, any drift in the distribution of the applicant_age and debt_to_income_ratio features compared to production baselines and send alerts. Which solution meets these requirements with minimal custom development?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable DataCaptureConfig on the endpoint, generate baseline statistics with ModelMonitor\u2019s built-in methods, create a monitoring schedule for input feature drift, and configure CloudWatch alarms on the monitoring job metrics.",
      "B": "Configure a Lambda function to read logs from CloudWatch Logs every hour, compute feature histograms, compare to baselines stored in S3, and send SNS alerts.",
      "C": "Use Kinesis Data Analytics to ingest endpoint invocation payloads, run SQL queries to detect distribution changes, and trigger alarms through CloudWatch.",
      "D": "Write a custom Spark job on EMR to read captured inference data, compute drift metrics, and schedule hourly cron jobs to publish alerts via Lambda."
    },
    "explanation": "A is correct because SageMaker Model Monitor can capture endpoint inputs, compute feature drift against baselines, and integrate with CloudWatch alarms with minimal coding. B and D require building custom data pipelines and drift logic. C uses Kinesis Data Analytics which is not optimized for SageMaker endpoint data capture and adds unnecessary complexity."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An e-commerce company uses a SageMaker endpoint for fraud detection. They must monitor model performance by tracking the false positive rate (FPR) in production and automatically retrain if FPR exceeds 5%. Which architecture satisfies this requirement?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Model Monitor\u2019s default data quality monitor to detect inference anomalies and trigger a Lambda to retrain the model when anomalies occur.",
      "B": "Capture only input payloads, write a CloudWatch metric filter on the logs for false positives, and use a CloudWatch alarm to start a SageMaker batch transform retraining job.",
      "C": "Enable DataCaptureConfig for both input and output, create ModelQualityMonitor with a custom metrics file specifying FPR, schedule hourly monitoring, and configure CloudWatch alarm to invoke a retraining pipeline.",
      "D": "Log predictions to S3, use Athena with scheduled queries to compute FPR hourly, and use Step Functions to orchestrate retraining when threshold is exceeded."
    },
    "explanation": "C is correct because ModelQualityMonitor supports custom metrics (including FPR) computed from captured input/output and can trigger alarms. A lacks FPR tracking. B captures only inputs and relies on log parsing. D is possible but involves Athena and Step Functions, which is more complex and not leveraging built-in model quality monitoring."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A healthcare provider has a batch inference pipeline that writes predictions and actual outcomes to S3 daily. They need to monitor model performance degradation (e.g., increasing RMSE) and alert DevOps when RMSE increases by more than 10% from baseline. Which move is best?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use CloudWatch Logs Insights to query S3 logs via log aggregation and trigger alarms when RMSE query results exceed thresholds.",
      "B": "Create a ModelQualityMonitor job using baseline metrics from training, supply the ground truth and prediction files from S3, schedule daily runs, and configure CloudWatch alarms.",
      "C": "Configure SageMaker Clarify to detect bias drift which will also report RMSE change and use CloudWatch alarms for notifications.",
      "D": "Invoke a Lambda function via EventBridge daily to compute RMSE from S3 data and send an SNS alert when RMSE >1.1\u00d7 baseline."
    },
    "explanation": "B is correct because ModelQualityMonitor directly supports batch inference data from S3 with ground truth to compute metrics like RMSE and integrate with alarms. A and D require custom code. C is incorrect because Clarify focuses on bias and feature drift, not performance metrics like RMSE."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A consumer app uses an endpoint for sentiment analysis. They observe occasional spikes in inference latency that degrade user experience. They need to detect latency anomalies in real time and trigger scaling actions. Which solution is most appropriate?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use ModelMonitor\u2019s data quality monitor to detect latency outliers and send events to CloudWatch.",
      "B": "Instrument the endpoint container to emit custom latency metrics to CloudWatch and set alarms on p95 latency.",
      "C": "Schedule a daily ModelMonitor job to analyze captured requests and identify latency anomalies.",
      "D": "Enable SageMaker endpoint invocation metrics (p90 and p95) in CloudWatch, create CloudWatch alarms for sudden increases, and attach Application Auto Scaling policies to the endpoint."
    },
    "explanation": "D is correct because SageMaker endpoints automatically emit latency metrics to CloudWatch, which can be used for real-time alarms and auto scaling. A and C misuse ModelMonitor, which is offline. B requires container modification when built-in metrics suffice."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An IoT solution uses a SageMaker endpoint for anomaly detection on sensor streams. They require near real-time alerts when feature distributions drift beyond control limits, but they cannot incur high Lambda execution fees. Which design balances cost and responsiveness?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Stream captured inference data to Kinesis Data Firehose, run a Lambda per record to compute drift and send alerts.",
      "B": "Write all captured data to S3 and run a ModelMonitor daily job with low-cost spot instances.",
      "C": "Use ModelMonitor endpoint preprocessor with custom monitoring script to detect drift thresholds on a 15-minute schedule and publish alerts via CloudWatch Events without Lambda.",
      "D": "Use Kinesis Data Analytics SQL application to continuously compute drift metrics and send alerts through SNS."
    },
    "explanation": "C is correct because ModelMonitor supports custom scripts for drift detection on schedules, avoiding per-record Lambda costs. A and D incur compute in Kinesis/Lambda continuously. B reduces responsiveness to daily."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A streaming inference endpoint uses DataCaptureConfig to write payloads and responses to S3. The team needs to detect when data quality (missing fields, invalid ranges) degrades in production and be paged immediately. Which process achieves this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy ModelMonitor\u2019s DefaultModelMonitor with a custom data quality constraints file, schedule it every 5 minutes, and configure CloudWatch alarms for violations.",
      "B": "Configure SageMaker Clarify to analyze captured data for anomalies and configure alerts for missing features.",
      "C": "Use Athena with scheduled queries every 5 minutes on the captured S3 files, detect invalid records, and notify via SNS.",
      "D": "Stream S3 events to Lambda to validate each record in real time and alert on invalid data."
    },
    "explanation": "A is correct because DefaultModelMonitor supports data quality constraints with custom rules, scheduled frequently, with CloudWatch integration. B focuses on bias/feature importance. C and D require custom orchestration and are less integrated."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A media company wants to monitor drift in top-5 features importance for its recommender model in production. They need automated alerts if the feature importance distribution shifts significantly. Which solution fits?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use ModelQualityMonitor to track feature importance values via ground truth comparisons.",
      "B": "Use ModelExplainabilityMonitor to baseline SHAP feature importances during training, schedule production monitoring, and alarm on significant distribution shifts.",
      "C": "Invoke Clarify\u2019s bias detection in production to monitor feature importances.",
      "D": "Export feature importance logs in JSON and use CloudWatch Logs Insights to compare distributions daily."
    },
    "explanation": "B is correct because ModelExplainabilityMonitor (part of Clarify) handles SHAP-based production monitoring of feature importances and can raise alarms. A is wrong because ModelQualityMonitor focuses on prediction quality, not explainability. C misuses bias detector. D is custom and manual."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A logistics firm uses a SageMaker endpoint behind an ALB. They need to monitor end-to-end inference latency and alert if 95th percentile latency exceeds 300 ms for more than 5 minutes. How do they implement this with least maintenance?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Embed custom timing code in inference container to publish custom CloudWatch metrics.",
      "B": "Use ModelMonitor to capture input/output times and schedule drift checks on latency.",
      "C": "Stream ALB access logs to CloudWatch Logs, parse latency percentile every minute via Logs Insights, and alarm.",
      "D": "Use CloudWatch metric for SageMaker Endpoint p95 latency with a 5-minute period alarm, and connect to SNS for notifications."
    },
    "explanation": "D is correct: SageMaker endpoints expose p95 latency in CloudWatch, which supports threshold-based alarms. A requires custom code. B misuses ModelMonitor. C is indirect and involves complex log parsing."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A biotech company runs batch inference nightly and stores predictions plus sample metadata in S3. They want to detect when the distribution of a key lab_measurement feature drifts compared to training data, and integrate alerts into PagerDuty. Which approach is best?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Athena scheduled queries on S3 data to compute KLD between current and training histograms, and send alerts via SNS to PagerDuty.",
      "B": "Set up a ModelMonitor batch monitoring job with a baseline from training data, schedule it nightly, and configure CloudWatch alarms to SNS for PagerDuty.",
      "C": "Deploy Clarify drift detector to analyze batch outputs for distribution changes and integrate with EventBridge for notifications.",
      "D": "Write custom PySpark EMR job to compute population stability index nightly and notify via Lambda."
    },
    "explanation": "B is correct because batch ModelMonitor directly handles feature drift with baselines, schedules jobs, and integrates with CloudWatch/SNS. A and D are custom and more maintenance. C misuses Clarify drift detector instead of ModelMonitor."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A retail company uses multi-model endpoints. They must monitor data skew separately for each model under high invocation volume without overloading S3. How should they configure Model Monitor?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable DataCaptureConfig for all models to a single S3 prefix and schedule a single monitor job with a filter script per model.",
      "B": "Use one monitor schedule per model, each writing to its own S3 bucket to avoid prefix collision.",
      "C": "Configure DataCaptureConfig to drop 80% of captured requests, tag model name, and run a single monitor job with group-by on model name.",
      "D": "Disable DataCaptureConfig, and use CloudWatch synthetic tests per model for skew detection."
    },
    "explanation": "C is correct: sampling reduces S3 volume, tagging allows a single job with group-by to monitor each model. A writes all data and requires filtering inside job. B duplicates resources. D uses synthetic tests, not real capture."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An AI startup uses Clarify ModelExplainabilityMonitor for drift detection, but they notice that on Mondays, due to different user behavior, alerts spike falsely. They want to suppress Monday alerts but keep daily checks. Which modification is appropriate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add a suppression rule in CloudWatch alarm for Mondays using a schedule-based mute period.",
      "B": "Adjust the drift threshold upward only for Monday\u2019s monitoring schedule.",
      "C": "Exclude data collected on Mondays from the ModelExplainabilityMonitor input baseline.",
      "D": "Shift monitoring schedule to every 48 hours to skip Monday checks."
    },
    "explanation": "A is correct: CloudWatch alarm suppression allows muting alerts on Mondays without affecting thresholds. B leads to inconsistent drift sensitivity. C corrupts baseline. D reduces monitoring frequency overall."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A ride-sharing company deployed a streaming inference endpoint. They need to detect concept drift in the predicted probability distribution for rider surge events. Which built-in capability supports this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ModelQualityMonitor\u2019s custom metrics for probability shift detection.",
      "B": "ModelMonitor\u2019s ModelBiasMonitor class with a custom preprocessor to compute Jensen-Shannon divergence.",
      "C": "ModelExplainabilityMonitor with SHAP-based drift metrics.",
      "D": "DefaultModelMonitor data quality checks for distribution changes."
    },
    "explanation": "B is correct because ModelBiasMonitor can measure probability distribution drift of predictions using divergence metrics. A is not a ModelQualityMonitor feature. C focuses on feature importance. D checks data quality, not prediction distribution."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A gaming company wants to use SageMaker Model Monitor to detect anomalies in user behavior predictions every five minutes but must avoid executing on cold data. The endpoint has periods of very low traffic. How to configure?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Schedule monitoring at 5-minute intervals without sampling; it will skip if no new data.",
      "B": "Use 1-minute schedule with a Lambda filter to drop empty datasets.",
      "C": "Configure ModelMonitor with max_runtime per job to 10 minutes so it exits on no data.",
      "D": "Schedule monitoring every 5 minutes with min_sampling_size set to 50 so it won\u2019t run unless at least 50 records are present."
    },
    "explanation": "D is correct: min_sampling_size prevents jobs when insufficient data arrives, avoiding waste. A will run and error on empty data. B and C require custom code or rely on runtime but don\u2019t prevent job attempts."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An energy company\u2019s endpoint serves predictions at different times of day, with nightly off-peak periods. They need to monitor inference anomalies only during peak hours (8 AM\u20138 PM). Which pattern accomplishes this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create a monitor schedule at 1-hour intervals and ignore anomalies logged outside of 8\u201320 via filter in the alerting Lambda.",
      "B": "Use two monitoring schedules: one at 5-minute intervals active between 8 and 20 using schedule expressions, and none outside that window.",
      "C": "Enable continuous monitoring and write a custom script to drop anomalies outside the window.",
      "D": "Schedule a daily monitor job at 20:01 that covers the day\u2019s peak data."
    },
    "explanation": "B is correct: schedule expressions in SageMaker allow cron-based windows to run only during peak hours. A adds complexity. C and D don\u2019t align to real-time peak detection."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A financial app uses a multi-variant endpoint to serve A/B test models. They need to compare real-time inference accuracy of each variant for rollout decisions without manual intervention. Which integrated feature achieves this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use CloudWatch Logs Insights to query variant-specific logs and compute accuracy per variant.",
      "B": "Enable Kinesis Data Streams on endpoint traffic, write a consumer to compute variant accuracy.",
      "C": "Use SageMaker Model Monitor\u2019s built-in endpoint variant-based shadow testing with baseline accuracy thresholds and CloudWatch alarms.",
      "D": "Invoke two separate endpoints in parallel and use Lambda to compare results against ground truth."
    },
    "explanation": "C is correct: Model Monitor supports shadow variants for model comparison against baselines, automating monitoring. A and B require manual aggregation. D duplicates endpoints and custom code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A telecom provider must ensure that no single categorical feature \u2018device_type\u2019 in production grows in usage by more than 20% relative to training. They want to monitor this continuously. Which approach is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use ModelMonitor\u2019s DefaultModelMonitor with a custom constraints JSON specifying the allowed 20% change for device_type categories and schedule frequent jobs.",
      "B": "Implement a Kinesis Data Analytics job to maintain real-time category counts and compare to training counts.",
      "C": "Use Clarify\u2019s bias monitor to set fair thresholds on device_type representation.",
      "D": "Capture endpoint inputs to S3 and run a daily Athena job to compute ratios and alert."
    },
    "explanation": "A is correct because DefaultModelMonitor constraints allow category-level drift threshold configuration, scheduled regularly. B and D are custom. C is misusing bias monitor, not intended for drift constraints."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An automotive company uses real-time inference for predictive maintenance. They must validate that all required sensor fields exist and log any missing fields for further investigation, without failing invocations. How should they set this up?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Implement input validation in the inference container code and reject requests with missing fields.",
      "B": "Use ModelMonitor\u2019s default data quality monitor to reject invocations with missing fields.",
      "C": "Stream inference input logs to CloudWatch Logs and create metrics filters for missing fields.",
      "D": "Configure ModelMonitor with a custom preprocessor script that checks for missing keys, logs violations to S3, and does not block the endpoint."
    },
    "explanation": "D is correct: ModelMonitor\u2019s preprocessor can implement validation logic, log violations, and does not interfere with real-time endpoint behavior. A blocks requests. B cannot reject invocations. C requires log parsing and has delay."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A travel app\u2019s SageMaker real-time endpoint is scaled to multiple instances for high traffic. They need aggregated abnormalities in feature distributions across all instances. Which configuration ensures correct drift detection?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy a monitor job per instance capturing to instance-specific S3 prefixes and aggregate results in Athena.",
      "B": "Enable DataCaptureConfig at the endpoint level to collect across all instances, and run a single ModelMonitor job on that capture location.",
      "C": "Add a sidecar container per instance to forward data to Kinesis Data Streams for drift detection.",
      "D": "Use CloudWatch metrics on each instance and manually compute distribution shifts in a Lambda."
    },
    "explanation": "B is correct: DataCaptureConfig at endpoint level automatically aggregates across instances. A is complex. C and D require additional infrastructure and custom aggregation."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A gaming platform has an endpoint for matchmaking that logs into DynamoDB. They need to monitor that output predictions are within the valid score range [0,1] and alert if outliers appear. Which solution is optimal?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Schedule a ModelMonitor default data quality job with constraints JSON specifying numeric_validity for the output_score feature and set CloudWatch alarms.",
      "B": "Use Lambda streams on DynamoDB to check each output and publish custom CloudWatch metrics for out-of-range values.",
      "C": "Enable Amazon Kinesis Firehose on logs to S3 and run nightly Athena queries to detect out-of-range values.",
      "D": "Instrument application code to validate scores before writing and send alerts via SNS."
    },
    "explanation": "A is correct: ModelMonitor constraints support numeric validity checks and integrate with alarms. B and C are custom solutions more maintenance. D moves monitoring into application logic rather than production monitoring."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A retailer\u2019s real-time pricing endpoint serves dynamic pricing. They want to test periodically whether price predictions exceed a profit margin threshold and revert if they do. Which monitoring feature should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use ModelQualityMonitor with a custom metric that computes profit margin on predictions.",
      "B": "Schedule a Lambda to call the endpoint, compute profit margin, and alarm.",
      "C": "Use ModelMonitor\u2019s output constraints JSON to define the profit margin threshold on prediction and schedule continuous monitoring.",
      "D": "Implement profit margin check logic in the inference container and emit CloudWatch metrics."
    },
    "explanation": "C is correct: ModelMonitor supports output constraints for predictions and continuous monitoring. A is misusing quality monitor, which focuses on ground truth comparison. B and D are custom implementations requiring more code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An insurance provider uses a batch inference pipeline on S3 with predictions and claims. They need to identify when the ratio of denied claims to total predictions deviates by more than 15% from the baseline. Which tool and configuration do they choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Clarify ModelBiasMonitor with a custom preprocessor to compute the ratio and schedule daily.",
      "B": "Use ModelQualityMonitor with ground truth and predictions, define a custom violation report with denied_claims/total and threshold, and schedule daily runs.",
      "C": "Use DefaultModelMonitor data quality for this ratio and set thresholds.",
      "D": "Query S3 via Athena daily to compute the ratio and call a Step Functions pipeline."
    },
    "explanation": "B is correct: ModelQualityMonitor supports custom metrics for model performance comparisons, including custom functions like ratio of denied claims. A is bias monitor, not for performance metrics. C is data quality; D is custom."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A sports analytics firm needs to detect if the distribution of a continuous feature \u2018player_speed\u2019 starts trending downward in real-time endpoint predictions. They require a rolling window of last 1,000 inferences. How can they configure Model Monitor?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set DataCaptureConfig, configure a monitoring schedule with monitoring_interval=1 minute, and set sampling_percentage so that window size approximates 1,000. Use constraints on the mean of player_speed.",
      "B": "Enable Clarify drift detector with window_size=1000 for player_speed.",
      "C": "Stream inference outputs to Kinesis Data Streams and configure Kinesis Analytics to compute rolling averages.",
      "D": "Use a Lambda triggered on capture S3 events to maintain a sliding window in DynamoDB and trigger alarms."
    },
    "explanation": "A is correct: monitoring_interval and sampling_percentage controls sample size per job; constraints can specify mean thresholds. B is not built-in. C and D are custom solutions outside Model Monitor."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A medical imaging application serves predictions for tumor detection. For compliance, any missing license_plate field (erroneously included) must be detected at inference time and reported, but not block inference. Which Model Monitor component is appropriate?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use DefaultModelMonitor data quality monitor to enforce presence of license_plate.",
      "B": "Use ModelExplainabilityMonitor to detect missing features.",
      "C": "Use ModelQualityMonitor comparing ground truth, specifying missing field detection.",
      "D": "Use a custom preprocessor script in ModelMonitor constraints to check for license_plate existence and log violations."
    },
    "explanation": "D is correct: a preprocessor script can validate arbitrary conditions (including extraneous fields) without affecting inference. A does not support arbitrary missing field checks. B and C are for explainability and performance respectively."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An ad-tech firm runs real-time bidding predictions and must detect when the click-through rate (CTR) predicted by the model deviates by more than 10% from training CTR. They have no ground truth in real time. What feature of SageMaker solves this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use ModelQualityMonitor with placeholders for ground truth CTR.",
      "B": "Use ModelMonitor\u2019s custom reporter in data quality to compute predicted CTR distribution drift against baseline.",
      "C": "Use Clarify bias monitor to detect prediction bias on CTR.",
      "D": "Use endpoint logs and Athena to compute CTR distribution daily."
    },
    "explanation": "B is correct: data quality monitors can compute statistics on predictions alone, enabling detection of predicted CTR shifts without ground truth. A is not applicable since quality monitor requires ground truth. C misuses bias monitor. D is custom and offline."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A gaming platform needs to ensure that inference payload sizes don\u2019t suddenly increase (which could indicate malicious inputs). They want to monitor payload byte size distributions in production. How can they achieve this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use DefaultModelMonitor with a preprocessor script to compute payload size feature, set constraints on the size distribution, and schedule monitoring.",
      "B": "Modify inference container to log payload size to CloudWatch and alarm on spikes.",
      "C": "Use Lambda triggered on S3 capture PUT events to record object sizes and alert.",
      "D": "Write a Kinesis Data Analytics job to analyze payload streams for size anomalies."
    },
    "explanation": "A is correct: ModelMonitor allows custom features via preprocessor scripts (including payload size) and constraint definitions. B, C, D require separate pipelines and custom code."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A supply chain model produces predictions every minute. The team must compute the percentage of predictions greater than threshold X and alert if it falls below 30% for more than 10 minutes. They have Model Monitor enabled. What is the simplest way to implement?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Clarify\u2019s bias monitor to configure threshold checks on prediction distribution.",
      "B": "Schedule a daily ModelQualityMonitor job with custom metrics file for percentage above X.",
      "C": "Configure a real-time ModelMonitor schedule with a custom constraints JSON to compute % above X and set alarms on violations every 5 minutes.",
      "D": "Stream predictions to Kinesis Data Streams and use AWS Lambda to monitor percentage."
    },
    "explanation": "C is correct: ModelMonitor constraints support custom aggregations (like percentage above threshold) and real-time schedules. A and B misapply Clarify or daily quality jobs. D is custom and more complex."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A biotech lab uses an asynchronous SageMaker endpoint. They need to monitor and alert on both input data completeness and response time exceeding 2 seconds for each batch. Which combination of Model Monitor capabilities achieves this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable DataCaptureConfig for request and response, configure DefaultModelMonitor with separate data quality and custom latency constraints, schedule every 5 minutes.",
      "B": "Use Clarify ModelExplainabilityMonitor for input completeness and CloudWatch metrics for latency.",
      "C": "Use two separate Lambda functions\u2014one reading S3 captures for missing fields and one alarm on CloudWatch latency metrics.",
      "D": "Run a daily ModelQualityMonitor job to measure completeness and latency metrics."
    },
    "explanation": "A is correct because DefaultModelMonitor can validate data quality and custom metrics like latency in one job. B misuses explainability. C is custom. D is too infrequent."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A financial firm wants to monitor covariance drift between two features, income and expenditure, in real-time inference data. Which Model Monitor strategy accomplishes this?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Use Clarify bias detector to compute covariance and schedule monitoring.",
      "B": "Schedule Athena queries to compute covariance and trigger alarms.",
      "C": "Implement a custom preprocessor in ModelMonitor to compute covariance drift constraint and schedule frequent runs.",
      "D": "Stream inference data to Redshift and run SQL to compute covariance daily."
    },
    "explanation": "C is correct: custom preprocessor scripts in ModelMonitor allow computing covariance and enforcing drift constraints. A and B don\u2019t natively compute covariance. D is custom and offline."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A mobile app uses a SageMaker real-time endpoint. They want to detect if any string feature contains non-UTF8 characters in production. Which approach is most maintainable?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Modify inference code to reject non-UTF8 inputs and log violations.",
      "B": "Use Clarify\u2019s bias detector to detect invalid characters.",
      "C": "Run a Lambda on captured S3 data to search for invalid encodings.",
      "D": "Configure ModelMonitor with a custom preprocessor to scan string features for non-UTF8 and log violations in S3."
    },
    "explanation": "D is correct: ModelMonitor preprocessor scripts can perform arbitrary validation (e.g., encoding checks) with minimal maintenance and decoupled from inference code. A couples monitoring with serving. B is misapplied. C is custom."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A retailer uses batch transform to score customer churn nightly. They need to verify that output probabilities sum to 1 for each record (softmax outputs) and alert on any anomalies. Which Model Monitor feature supports this at lowest operational cost?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Clarify bias monitor to validate output probability sums.",
      "B": "Use DefaultModelMonitor with a custom postprocessor script to compute sum of probabilities per record and set constraints that sum equals 1.",
      "C": "Instrument batch transform container to raise errors when sums differ and notify via SNS.",
      "D": "Write Athena query over output files to check sums and send alerts."
    },
    "explanation": "B is correct: postprocessor scripts in ModelMonitor can validate prediction outputs and enforce numeric constraints. A is misapplied. C and D require custom code or queries."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A manufacturing line inference endpoint outputs categorical defect labels. They must detect when a rare defect category appears more than twice in an hour, indicating a quality issue. How can they configure Model Monitor?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create a DefaultModelMonitor job with custom constraints specifying max_count=2 for that category over a 1-hour window and schedule jobs accordingly.",
      "B": "Run a Clarify bias monitor to detect changes in rare label frequency.",
      "C": "Use Kinesis Data Streams to aggregate counts and Lambda to alert.",
      "D": "Schedule daily Athena queries on captured data to check counts."
    },
    "explanation": "A is correct: ModelMonitor supports frequency constraints for categorical values. B misuses bias. C and D are custom and less real-time."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "An app uses a SageMaker real-time endpoint for language translation. They want to ensure that none of the translated outputs contain profane words. Which Model Monitor mechanism should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Clarify bias monitor with a profanity lexicon.",
      "B": "Modify inference code to filter profanity and report.",
      "C": "Configure ModelMonitor with a custom postprocessor script that scans outputs against a profanity list and logs violations.",
      "D": "Stream responses to Kinesis and run a profanity detection Lambda."
    },
    "explanation": "C is correct: postprocessor scripts in ModelMonitor can implement content validation and integrate with alerts. A is misused. B couples monitoring into serving. D requires extra infrastructure."
  },
  {
    "exam": "AWS Machine Learning - Associate (MLA-C01)",
    "taskStatement": "4.1",
    "stem": "A logistics service uses a multi-model endpoint for forecasting. They need per-model latency and feature distribution monitoring. The endpoint metadata tags each invocation with model_id. How can they implement this in ModelMonitor?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable separate DataCaptureConfig per model_id by deploying multiple endpoints.",
      "B": "Enable a single DataCaptureConfig that captures model_id in payload, schedule ModelMonitor with a group_by based on model_id for both latency and data quality metrics.",
      "C": "Stream all invocations to Kinesis Data Streams, use a Lambda to split by model_id.",
      "D": "Deploy Clarify bias detectors per model for distribution monitoring."
    },
    "explanation": "B is correct: ModelMonitor supports group_by functionality on a specified feature (model_id) to generate separate metrics per group. A duplicates endpoints. C is custom. D misuses bias detectors."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A financial services company needs to detect fraudulent transactions in real time with sub-50ms latency per transaction, while also running nightly trend analyses on aggregated data. They plan separate ML pipelines for each use case. Which combination of inferencing types should they implement?",
    "correct": "B",
    "difficulty": null,
    "answers": {
      "A": "Batch inferencing for fraud detection and real-time inferencing for trend analysis",
      "B": "Real-time inferencing for fraud detection and batch inferencing for trend analysis",
      "C": "Streaming inferencing for fraud detection and online inferencing for trend analysis",
      "D": "Online inferencing for fraud detection and micro-batch inferencing for trend analysis"
    },
    "explanation": "Real-time inferencing meets sub-50ms latency for fraud detection; batch inferencing is appropriate for nightly aggregate trend analysis. Streaming and micro-batch are not AWS inference categories defined in this context."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "An e-commerce team is deciding between logistic regression and random forest for a binary classification task. They refer to these as \u201calgorithms.\u201d After training, they obtain predictive artifacts they deploy. In AWS terms, what is the term for these deployed artifacts?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Hyperparameters",
      "B": "Features",
      "C": "Models",
      "D": "Labels"
    },
    "explanation": "Algorithms (logistic regression, random forest) produce trained artifacts called models. Hyperparameters configure algorithms; features are inputs; labels are outputs used during training."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A manufacturing firm collects sensor readings (temperature, vibration, pressure) every second as JSON messages and also attaches operator comments as free text. What best describes the types of data ingested?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Structured tabular data only",
      "B": "Unstructured multimedia data",
      "C": "Semi-structured numerical data only",
      "D": "Mixed structured (numeric) and unstructured (text) data"
    },
    "explanation": "Numeric sensor readings are structured; operator comments are unstructured text. It\u2019s mixed. The other options mischaracterize the combination."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A marketing team wants to segment customers by purchase behavior without any predefined labels. They plan to discover latent groups. Which learning paradigm should they apply?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Supervised classification",
      "B": "Unsupervised clustering",
      "C": "Reinforcement learning",
      "D": "Semi-supervised learning"
    },
    "explanation": "Clustering on unlabeled data is unsupervised learning. Supervised requires labels; reinforcement uses rewards; semi-supervised mixes labels and unlabeled data."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A logistics company uses trial and error and a reward function to optimize warehouse pick-and-place operations in simulation. Which ML method are they using?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Supervised learning",
      "B": "Unsupervised learning",
      "C": "Reinforcement learning",
      "D": "Self-supervised learning"
    },
    "explanation": "Using a reward signal in simulation for actions is reinforcement learning. Supervised uses labeled examples; unsupervised finds patterns without rewards; self-supervised derives labels internally but not via reward."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "An image classification solution on AWS uses a convolutional neural network with millions of parameters and requires GPUs. Which term best categorizes this approach?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deep learning",
      "B": "Traditional ML algorithm",
      "C": "Reinforcement learning agent",
      "D": "Ensemble tree model"
    },
    "explanation": "CNNs with many layers/functions on unstructured images are deep learning. Ensemble trees and traditional ML do not generally use deep neural nets; reinforcement learning uses a reward model."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A research group selects a pre-trained language model of 20B parameters that supports few-shot prompting. What terminology best describes this artifact?",
    "correct": "D",
    "difficulty": null,
    "answers": {
      "A": "A neural network",
      "B": "A supervised model",
      "C": "An embedding",
      "D": "A large language model (LLM)"
    },
    "explanation": "A pre-trained model with billions of parameters for language tasks is an LLM. While it is a neural network and supervised pre-training is used, the specific term is LLM; embeddings are vector representations."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A facial recognition model misclassifies certain demographics more often than others. What term describes this issue?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Bias",
      "B": "Variance",
      "C": "Fit",
      "D": "Regularization"
    },
    "explanation": "Systematic error disadvantaging particular groups is bias. Variance relates to sensitivity to data fluctuations; fit/reg. concern under/overfitting and penalty terms respectively."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A text classification model achieves 99% accuracy on training data but only 60% on new customer emails. Which problem is occurring?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High bias",
      "B": "Data leakage",
      "C": "Underfitting",
      "D": "Overfitting"
    },
    "explanation": "A large gap between training and test performance indicates overfitting. High bias or underfitting would show poor performance on both. Data leakage would inflate test performance, not reduce it sharply."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "An online retailer deploys a model endpoint to score customer propensity in milliseconds per request. Which term describes this stage?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Model training",
      "B": "Feature engineering",
      "C": "Inference",
      "D": "Data preprocessing"
    },
    "explanation": "Serving a deployed model to generate predictions is inference. Training builds the model; feature engineering and preprocessing occur before model usage."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A shipping firm wants to detect package damage by analyzing shipment photos. Which AI domain should they leverage?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Computer vision",
      "B": "Natural language processing",
      "C": "Reinforcement learning",
      "D": "Speech recognition"
    },
    "explanation": "Analyzing images falls under computer vision. NLP handles text; reinforcement learning uses rewards; speech recognition handles audio."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A call center wants to transcribe and analyze customer audio for sentiment. Which AI domain and subtask are involved?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Computer vision with OCR",
      "B": "Computer vision with object detection",
      "C": "Natural language processing with speech-to-text and sentiment analysis",
      "D": "Reinforcement learning with audio encoding"
    },
    "explanation": "Transcribing speech is speech-to-text (an NLP subtask) followed by sentiment analysis (also NLP). The other options misapply computer vision or reinforcement learning to audio."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A startup uses customer behavior logs to train a model and wants to evaluate various algorithms efficiently. Which pipeline step does this describe?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Deployment",
      "B": "Experimentation (training and validation)",
      "C": "Feature storage",
      "D": "Data annotation"
    },
    "explanation": "Testing multiple algorithms with validation data is experimentation within the training lifecycle. Deployment comes after; feature storage and annotation are different steps."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A dataset contains numeric, categorical, text, image, and time-series fields. How would you classify this dataset overall?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Structured",
      "B": "Unstructured",
      "C": "Semi-structured",
      "D": "Heterogeneous multimodal data"
    },
    "explanation": "Multiple data modalities (numeric, text, image, time-series) define heterogeneous multimodal data. Structured/unstructured/semi-structured don\u2019t capture multiple distinct modalities."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A credit scoring model uses decision trees and logistic regression, then blends outputs. What ensemble method category is this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Stacking",
      "B": "Bagging",
      "C": "Boosting",
      "D": "Blending"
    },
    "explanation": "Combining diverse model outputs via a meta-learner is stacking. Bagging uses same algorithm on subsets; boosting sequentially focuses on errors; blending is a holdout variant of stacking but less formal."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A medical imaging model needs 3D scans and uses volumetric convolution. Which architectural choice defines this model?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Recurrent neural network",
      "B": "Transformer",
      "C": "Autoencoder",
      "D": "3D convolutional neural network"
    },
    "explanation": "3D CNNs handle volumetric data via 3D convolutions. RNNs process sequences; transformers use attention; autoencoders learn embeddings."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "When splitting data for training and inference, which data characteristic ensures the model generalizes?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High label imbalance",
      "B": "Representative i.i.d. distribution",
      "C": "Missing values\u96c6\u4e2d",
      "D": "Highly correlated features"
    },
    "explanation": "An independent, identically distributed (i.i.d.) split ensures the inference data resemble training data. Label imbalance, missing values, or correlated features hamper generalization."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A recommender system logs user clicks and ratings. Which learning paradigm uses both past actions and reward signals to improve suggestions?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Unsupervised learning",
      "B": "Supervised learning",
      "C": "Reinforcement learning",
      "D": "Self-supervised learning"
    },
    "explanation": "A recommender optimizing using click/reward feedback is reinforcement learning. Supervised uses explicit labels; unsupervised finds patterns; self-supervised generates labels."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A startup fine-tunes a pre-trained transformer-based LLM by updating all weights on domain data. What process are they performing?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Prompt engineering",
      "B": "Fine-tuning",
      "C": "Inference",
      "D": "Embedding"
    },
    "explanation": "Updating model weights on new data is fine-tuning. Prompt engineering crafts inputs; inference uses the model; embedding produces vector representations."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A dataset of customer addresses stored as JSON documents without fixed schema is an example of which data type?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Structured",
      "B": "Unstructured",
      "C": "Semi-structured",
      "D": "Multimodal"
    },
    "explanation": "JSON documents with flexible schema are semi-structured. Structured implies rigid tables; unstructured refers to raw text/images; multimodal combines multiple media types."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A deep network\u2019s capacity to memorize training noise rather than general patterns relates to which property?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Bias",
      "B": "Fairness",
      "C": "Regularization",
      "D": "Variance"
    },
    "explanation": "High variance indicates sensitivity to noise and overfitting. Bias relates to systematic error; fairness to equity; regularization is a technique to reduce variance."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A supply-chain model uses past demand to predict next-month orders via linear regression. Which algorithm type is this?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Regression",
      "B": "Classification",
      "C": "Clustering",
      "D": "Dimensionality reduction"
    },
    "explanation": "Predicting a continuous numeric target is regression. Classification predicts discrete labels; clustering groups unlabeled data; dimensionality reduction reduces feature space."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "An NLP pipeline transforms text into 768-dimension vectors before clustering. What are these vectors called?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Features",
      "B": "Embeddings",
      "C": "Hyperparameters",
      "D": "Parameters"
    },
    "explanation": "Embeddings are dense vector representations of text. Features are inputs to models; parameters are learned weights; hyperparameters configure training."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A governance team audits a model\u2019s training algorithm and discovered inconsistent outputs for identical inputs across runs. Which concept might explain this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High bias",
      "B": "Data imbalance",
      "C": "Nondeterministic initialization",
      "D": "Overfitting"
    },
    "explanation": "Random weight initialization or nondeterministic operations in deep learning can lead to different outputs for identical inputs. Bias, imbalance, and overfitting don\u2019t directly cause run-to-run variability."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A customer service chatbot uses a model that responds within seconds but batches billing analysis nightly. How should you label these two inferencing modes?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Synchronous and asynchronous inferencing",
      "B": "Online and offline training",
      "C": "Real-time and streaming inferencing",
      "D": "Real-time and batch inferencing"
    },
    "explanation": "Chatbot responses require real-time (online) inference; nightly billing analysis is batch inference. The other terms mix training with inference or streaming, which is not the defined mode here."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "An ML pipeline scales behavior analysis to new data without labels by learning data structure. Which technique applies?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Supervised classification",
      "B": "Unsupervised learning",
      "C": "Reinforcement learning",
      "D": "Transfer learning"
    },
    "explanation": "Unsupervised learning discovers patterns in unlabeled data. Transfer learning uses labeled source domains; reinforcement learning uses rewards; classification is supervised."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A text summarization system must compress long documents into key sentences. Which AI subfield does this represent?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Computer vision",
      "B": "Reinforcement learning",
      "C": "Natural language processing",
      "D": "Anomaly detection"
    },
    "explanation": "Summarization is an NLP task. Computer vision processes images; reinforcement is reward-based; anomaly detection finds outliers."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A model\u2019s training loop uses gradient descent to minimize a loss function. Which concept does the loss function represent?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Algorithm objective",
      "B": "Model artifact",
      "C": "Feature set",
      "D": "Hyperparameter"
    },
    "explanation": "The loss function defines the objective the algorithm optimizes. The model artifact is the trained parameters; features are inputs; hyperparameters configure training."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A pre-training phase on massive text yields a foundation model that can be specialized later. What is the general process called?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Inference",
      "B": "Pre-training",
      "C": "Fine-tuning",
      "D": "Evaluation"
    },
    "explanation": "Pre-training on large unlabeled corpora yields a foundation model. Fine-tuning specializes; inference uses the model; evaluation measures performance."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A wildlife monitoring project clusters animal tracks without labels and then assigns species names manually to clusters. Which workflow combines the two learning types?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Supervised learning",
      "B": "Reinforcement learning",
      "C": "Unsupervised learning",
      "D": "Semi-supervised learning"
    },
    "explanation": "They initially use unsupervised clustering then manually label clusters, combining labeled and unlabeled data = semi-supervised. Pure supervised or unsupervised use only one type; reinforcement uses rewards."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A model\u2019s performance degrades over time because data distribution shifts. Which concept describes this challenge?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Overfitting",
      "B": "Data drift",
      "C": "Bias",
      "D": "Variance"
    },
    "explanation": "Data drift refers to changes in input data distribution over time, causing degradation. Bias/variance relate to model errors; overfitting is memorization."
  },
  {
    "taskStatement": "1.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A dataset has 100 features but you observe diminishing returns beyond 10 in model performance. Which concept are you examining?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Regularization strength",
      "B": "Algorithm complexity",
      "C": "Feature importance",
      "D": "Model capacity"
    },
    "explanation": "Evaluating how adding features impacts performance assesses feature importance. Regularization, algorithm complexity, and model capacity address other aspects."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A retail company wants to segment customers based on purchasing behavior to tailor marketing campaigns. They have numerical features such as purchase frequency, average order value, and days since last purchase. Which ML technique and AWS service combination best suits this use case?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use the built-in K-Means clustering algorithm in Amazon SageMaker to group customers.",
      "B": "Use Amazon Personalize to automatically cluster customers and generate segments.",
      "C": "Use sentiment analysis in Amazon Comprehend to segment customers by sentiment.",
      "D": "Use Amazon Forecast to predict future customer segments over time."
    },
    "explanation": "Unsupervised clustering via SageMaker K-Means is ideal for grouping based on numeric behavior; other services are for recommendations, sentiment, or forecasting."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A fintech startup needs to identify potentially fraudulent credit card transactions in real time with minimal ML expertise and wants pre-trained fraud detection capabilities. Which AWS service should they choose?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Fraud Detector",
      "B": "Amazon SageMaker Ground Truth",
      "C": "Amazon Comprehend",
      "D": "Amazon GuardDuty"
    },
    "explanation": "Amazon Fraud Detector provides domain-specific, pre-trained models for fraud detection; others are for labeling, NLP, or threat detection."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A global brand wants to analyze customer tweets to gauge public sentiment about a new product launch without building or training custom models. Which AWS service should they use?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Comprehend Sentiment Analysis",
      "B": "Amazon SageMaker BlazingText",
      "C": "Amazon Translate",
      "D": "Amazon Lex"
    },
    "explanation": "Comprehend\u2019s built-in sentiment API analyzes text sentiment directly; other services are for custom text models, translation, or chatbots."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A media company needs to convert recorded webinar audio into text transcripts to index content for search. Which AWS service should they choose?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Transcribe",
      "B": "Amazon Comprehend",
      "C": "Amazon Polly",
      "D": "Amazon Translate"
    },
    "explanation": "Amazon Transcribe converts speech to text; Comprehend analyzes text, Polly generates speech, Translate translates text."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "An e-commerce website must provide real-time translation of product descriptions into multiple languages to serve global customers. Which AWS service is most appropriate?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Translate",
      "B": "Amazon Comprehend",
      "C": "Amazon Polly",
      "D": "Amazon Lex"
    },
    "explanation": "Amazon Translate provides real-time neural translation; Comprehend is for NLP analysis, Polly for speech, Lex for conversational interfaces."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A legal firm wants to automatically extract key phrases and entities (like person, organization, date) from large volumes of contracts. Which AWS service should they use?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Comprehend Entity and Key Phrase Extraction",
      "B": "Amazon Textract Table Extraction",
      "C": "Amazon SageMaker BlazingText",
      "D": "Amazon Kendra"
    },
    "explanation": "Comprehend extracts entities and key phrases; Textract focuses on form/ table OCR, SageMaker needs model building, Kendra is search."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A news app needs to generate audio versions of articles for accessibility. Which AWS service should they integrate?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Transcribe",
      "B": "Amazon Translate",
      "C": "Amazon Polly",
      "D": "Amazon Lex"
    },
    "explanation": "Amazon Polly converts text to lifelike speech; Transcribe converts speech to text, Translate translates text, Lex builds chat interfaces."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A company wants to build a conversational agent to answer common HR policy questions via chat. Which AWS service should they use?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Comprehend",
      "B": "Amazon Lex (without Lambda integration)",
      "C": "Amazon Polly",
      "D": "Amazon Lex with intent management"
    },
    "explanation": "Amazon Lex provides NLU, intent handling, and conversation flow; Comprehend is for text analysis, Polly for TTS."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "An online retailer wants to offer personalized product recommendations based on browsing behavior and purchase history. Which AWS service is most suitable?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Personalize",
      "B": "Amazon Forecast",
      "C": "Amazon Kinesis Data Analytics",
      "D": "Amazon Personalize"
    },
    "explanation": "Amazon Personalize is designed for personalized recommendations; Forecast is forecasting, Kinesis analyzes streaming data but not personalized models."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A manufacturing firm needs to automatically detect defects in product images on the assembly line. They need bounding boxes around defects. Which service is most appropriate?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Rekognition Text Detection",
      "B": "Amazon SageMaker Image Classification",
      "C": "Amazon Textract",
      "D": "Amazon Rekognition Object Detection"
    },
    "explanation": "Rekognition\u2019s object detection provides bounding boxes; image classification labels whole images, Textract is OCR."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A bank wants to build a model to predict customer churn probability using historical account activity. Which ML technique and AWS service should they choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Binary classification using SageMaker built-in XGBoost algorithm",
      "B": "Unsupervised K-Means clustering in Amazon SageMaker",
      "C": "Reinforcement learning in AWS DeepRacer",
      "D": "Time series forecasting in Amazon Forecast"
    },
    "explanation": "Predicting churn is a supervised binary classification problem; SageMaker\u2019s XGBoost is appropriate."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A telecom operator wants to forecast next-month data usage per customer to provision network capacity. Which technique and AWS service best apply?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Classification with SageMaker Linear Learner",
      "B": "Time series forecasting with Amazon Forecast",
      "C": "Clustering with SageMaker K-Means",
      "D": "Anomaly detection with SageMaker Random Cut Forest"
    },
    "explanation": "Forecasting numeric usage over time requires time series forecasting; Amazon Forecast is purpose-built."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A logistics company receives IoT sensor data and wants to detect abnormal temperature spikes without labeled anomalies. Which AWS service and algorithm combination should they use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon SageMaker Linear Learner",
      "B": "Amazon Fraud Detector",
      "C": "Amazon Comprehend",
      "D": "SageMaker Random Cut Forest anomaly detection"
    },
    "explanation": "Random Cut Forest in SageMaker unsupervised algorithm detects anomalies in unlabeled time series."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "An educational platform wants to cluster quiz questions based on topic similarity using NLP embeddings. They don\u2019t require supervised labels. Which approach is most appropriate?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon Translate then cluster translated text",
      "B": "Use Amazon Comprehend sentiment scores",
      "C": "Generate embeddings with SageMaker JumpStart BERT model and cluster via K-Means",
      "D": "Use Amazon Personalize grouping"
    },
    "explanation": "Extracting embeddings with a pre-trained model then K-Means clustering is an unsupervised NLP approach."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A marketing team considers using ML to map postal codes to region names because there are only 50 unique codes. Is ML appropriate here?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Yes \u2013 use Amazon SageMaker lookup table training",
      "B": "No \u2013 use a simple rule-based lookup table (no ML)",
      "C": "Yes \u2013 use Amazon Comprehend to infer regions",
      "D": "No \u2013 use Amazon Forecast to predict region mapping"
    },
    "explanation": "Mapping a small finite set is best solved with a rule-based lookup, not ML."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A company wants to detect specific keywords in customer call transcriptions to trigger alerts. Which AWS service should they use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Transcribe with custom vocabulary and AWS Lambda keyword filter",
      "B": "Amazon Comprehend Key Phrase Extraction",
      "C": "Amazon Kinesis Video Streams",
      "D": "Amazon Lex Slot Filling"
    },
    "explanation": "Custom vocabulary in Transcribe captures keywords in speech-to-text, Lambda filters them; Comprehend extracts after text conversion."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "An HR department needs to automatically categorize resume documents by skill sets using pre-built models and minimal customization. Which AWS service is most appropriate?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon SageMaker BlazingText",
      "B": "Amazon Comprehend Custom Classification",
      "C": "Amazon Textract form analysis",
      "D": "Amazon Personalize"
    },
    "explanation": "Comprehend Custom Classification allows rapid text classification with minimal model building; others are for embeddings, OCR, or recommendations."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A legal team needs to redact personally identifiable information (PII) from scanned PDF contracts. Which AWS service or combination should they use?",
    "correct": "C",
    "difficulty": null,
    "answers": {
      "A": "Amazon Comprehend PII detection alone",
      "B": "Amazon SageMaker DocumentClassifier",
      "C": "Amazon Textract for OCR then Comprehend PII detection",
      "D": "Amazon Rekognition Text Moderation"
    },
    "explanation": "Textract extracts text from scanned PDFs; Comprehend PII API then identifies and redacts PII."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "An online gaming company wants to recommend in-game items based on player behavior and contextual metadata. Which AWS service should they choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Personalize",
      "B": "Amazon Forecast",
      "C": "Amazon SageMaker Neo",
      "D": "Amazon Comprehend"
    },
    "explanation": "Personalize builds contextual, real-time recommendation models; Forecast is for time series, Neo for deployment, Comprehend for NLP."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A pharmaceutical company wants to group patients into cohorts for a clinical trial based on lab results and demographic features without labeled outcomes. Which ML technique applies?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Supervised classification using SageMaker XGBoost",
      "B": "Unsupervised clustering using SageMaker K-Means",
      "C": "Reinforcement learning using Amazon SageMaker RL agents",
      "D": "Anomaly detection using SageMaker Random Cut Forest"
    },
    "explanation": "Clustering groups data without labels into cohorts; supervised or anomaly detection are less appropriate."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A contact center wants to route incoming chats based on detected customer intent without building custom ML models. Which AWS service should they use?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Lex V2 with custom code",
      "B": "Amazon Comprehend Syntax Analysis",
      "C": "Amazon SageMaker DeepAR",
      "D": "Amazon Lex built-in intent classification"
    },
    "explanation": "Lex provides built-in intent classification for chat routing; other services are for syntax, forecasting, or require custom ML."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A bank wants to estimate credit risk score as a continuous variable from customer profiles. Which ML technique and AWS service combination is best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Unsupervised clustering with SageMaker K-Means",
      "B": "Anomaly detection with SageMaker Random Cut Forest",
      "C": "Regression using SageMaker Linear Learner",
      "D": "Classification using Amazon Fraud Detector"
    },
    "explanation": "Predicting a continuous risk score is regression; SageMaker Linear Learner supports regression tasks."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A manufacturing plant collects vibration data and suspects rare equipment failures. They have no labeled failure examples. Which AWS approach should they use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Supervised classification in Amazon SageMaker",
      "B": "Unsupervised anomaly detection using SageMaker Random Cut Forest",
      "C": "Time series forecasting using Amazon Forecast",
      "D": "Clustering using SageMaker K-Means"
    },
    "explanation": "Unsupervised anomaly detection detects rare failures without labeled data; clustering groups all data equally."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A travel website wants to extract flight numbers, dates, and passenger names from PDF itineraries. Which AWS service combination should they use?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Comprehend Entities API",
      "B": "Amazon Textract Forms API only",
      "C": "Amazon Textract for OCR then Amazon Comprehend Entity Extraction",
      "D": "Amazon SageMaker OCR built-in model"
    },
    "explanation": "Textract OCRs form data; Comprehend extracts structured entities; other options miss one stage."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A social media analytics firm needs to cluster trending topics without prior labels. They want to use embeddings from a pre-trained language model. Which AWS service should they leverage?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Comprehend Topic Modeling",
      "B": "SageMaker JumpStart transformer embeddings + K-Means",
      "C": "Amazon Personalize clustering",
      "D": "Amazon Rekognition for topic detection"
    },
    "explanation": "JumpStart provides pre-trained models for embeddings; clustering then groups topics; Comprehend doesn\u2019t expose topic modeling."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A chatbot needs to answer customer queries by matching to FAQ entries. They require semantic matching rather than keyword matching. Which AWS service/model approach should they use?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Lex with keyword slots",
      "B": "Amazon Comprehend sentiment analysis",
      "C": "Amazon Translate semantic matching",
      "D": "Generate embeddings using SageMaker JumpStart and perform similarity search"
    },
    "explanation": "Embeddings and similarity search provide semantic matching; other services are for keyword, sentiment, or translation."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A publisher wants to detect the language of submitted articles automatically before translation. Which AWS service should they use?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Comprehend DetectDominantLanguage",
      "B": "Amazon Translate language detection",
      "C": "Amazon Polly language identification",
      "D": "Amazon Transcribe language model"
    },
    "explanation": "Comprehend\u2019s DetectDominantLanguage API identifies text language; Translate focuses on translation post-detection."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "An agriculture startup gathers drone imagery to identify crop health issues automatically. They need object detection and classification. Which AWS service combination is suitable?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Rekognition Text Detection + Comprehend",
      "B": "Amazon SageMaker K-Means clustering",
      "C": "Amazon Textract Table Extraction",
      "D": "Amazon SageMaker Object Detection built-in algorithm"
    },
    "explanation": "SageMaker\u2019s Object Detection algorithm (e.g., SSD) supports bounding boxes and classification in imagery."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A legal department wants to determine sentiment trends in case law documents over time and highlight emerging negative topics. Which AWS services should they combine?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Textract + Amazon Translate",
      "B": "Amazon Comprehend Key Phrases + Amazon Personalize",
      "C": "Amazon Textract for OCR + Amazon Comprehend Sentiment Analysis + Amazon QuickSight for visualization",
      "D": "Amazon Rekognition + Amazon Athena"
    },
    "explanation": "Textract extracts text, Comprehend analyzes sentiment, QuickSight visualizes trends; others mismatch functions."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A retailer wants to forecast daily sales volume per store for the next quarter, incorporating holiday effects and promotions. Which AWS solution should they choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Regression with SageMaker XGBoost",
      "B": "Time series forecasting with Amazon Forecast",
      "C": "Clustering with SageMaker K-Means",
      "D": "Anomaly detection with SageMaker Random Cut Forest"
    },
    "explanation": "Forecast incorporates seasonality and holiday effects; regression would require manual feature engineering."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A health app records user vitals and needs to detect abnormal patterns in real time for alerting. They have streaming data and no labeled anomalies. Which AWS combination is best?",
    "correct": "D",
    "difficulty": null,
    "answers": {
      "A": "Amazon Forecast streaming predictions",
      "B": "Amazon SageMaker Linear Learner streaming inference",
      "C": "Amazon Fraud Detector real-time API",
      "D": "Amazon Kinesis Data Analytics to ingest + SageMaker Random Cut Forest for anomaly detection"
    },
    "explanation": "Kinesis ingests streaming data; SageMaker RCF analyzes unlabeled anomalies in real time."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A company has a dataset of customer emails and wants to route them to the correct department automatically. They need intent classification out of the box. Which AWS service should they use?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Comprehend Custom Classification",
      "B": "Amazon SageMaker Object Detection",
      "C": "Amazon Translate",
      "D": "Amazon Personalize"
    },
    "explanation": "Comprehend Custom Classification provides easy training for text routing; other services are irrelevant."
  },
  {
    "taskStatement": "1.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A startup wants to create a voice-driven assistant that transcribes speech, interprets intent, and responds vocally. Which combination of AWS services is most appropriate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Transcribe + Amazon Comprehend + Amazon Polly",
      "B": "Amazon Transcribe -> Amazon Lex -> Amazon Polly",
      "C": "Amazon Lex -> Amazon Translate -> Amazon Comprehend",
      "D": "Amazon Polly -> Amazon SageMaker RL"
    },
    "explanation": "Transcribe handles speech-to-text, Lex interprets intent, Polly synthesizes speech; other chains mix stages incorrectly."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A team needs to ensure that feature transformations applied during model training are used identically during real-time inference to avoid training/serving skew. Which AWS service or feature best satisfies this requirement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon SageMaker Feature Store",
      "B": "AWS Glue DataBrew",
      "C": "Amazon SageMaker Data Wrangler flow exports",
      "D": "Embedding transformations in Lambda functions"
    },
    "explanation": "SageMaker Feature Store provides a single source of truth for feature definitions and transformations, ensuring consistency between training and inference. Data Wrangler and Glue cannot serve real-time features, and Lambda-based transformations risk divergence."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "You must orchestrate a repeatable end-to-end ML workflow including data preprocessing, training, evaluation, model registration, and conditional deployment in response to evaluation metrics. Which AWS tool should you choose?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Step Functions with custom Lambda tasks",
      "B": "AWS Glue Workflows",
      "C": "Amazon SageMaker Pipelines with ConditionStep",
      "D": "AWS Data Pipeline"
    },
    "explanation": "SageMaker Pipelines natively orchestrates ML steps, supports ConditionStep for metric-based branching, and integrates with training, processing, and model registry. Step Functions and Glue lack ML-specific constructs, and AWS Data Pipeline is deprecated for ML workflows."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A data scientist needs to deploy ten versions of a fraud detection model behind a single endpoint, loading each version only when invoked to minimize memory usage. Which deployment option meets this requirement?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provisioned real-time endpoints per model variant",
      "B": "SageMaker serverless endpoint",
      "C": "Batch Transform job",
      "D": "SageMaker multi-model endpoint"
    },
    "explanation": "Multi-model endpoints load models on demand into a container, supporting multiple versions behind a single endpoint. Serverless endpoints cannot host multiple models, and Batch Transform is for offline inference."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "To detect input data drift in production, which combination of services and features provides automated baseline generation, continuous monitoring, and alerting?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon CloudWatch metrics with custom Lambda evaluations",
      "B": "SageMaker Model Monitor\u2019s DataQualityJobDefinition + Amazon EventBridge rule",
      "C": "AWS Config with custom rules",
      "D": "AWS Glue job scheduled daily"
    },
    "explanation": "Model Monitor can generate a baseline from training data (DataQualityJobDefinition), schedule continuous monitoring, and emit results via EventBridge for alerting. Other options require extensive custom work or lack ML-specific drift detection."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Your preprocessing logic involves heavy PySpark transforms on large tabular datasets. Which SageMaker component should you use for scalable, containerized execution?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon SageMaker Processing with built-in Scikit-learn container",
      "B": "AWS Glue ETL job",
      "C": "Amazon SageMaker Processing with Spark container",
      "D": "AWS Lambda with parallel invocations"
    },
    "explanation": "SageMaker Processing supports Spark containers for distributed PySpark workloads within an ML pipeline. Glue is general ETL, not integrated with SageMaker pipelines, and Lambda cannot handle large-scale Spark jobs."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A business requires tracking of every training run\u2019s hyperparameters, input data versions, and evaluation metrics with lineage. Which SageMaker feature is specifically built to capture this metadata?",
    "correct": "A",
    "difficulty": null,
    "answers": {
      "A": "Amazon SageMaker Experiments",
      "B": "Amazon CloudWatch Logs",
      "C": "Amazon S3 object versioning",
      "D": "AWS CloudTrail"
    },
    "explanation": "SageMaker Experiments records runs, parameters, inputs, and metrics, providing lineage and comparison across trials. CloudWatch Logs and S3 versioning don\u2019t structure ML metadata, and CloudTrail logs only API calls."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "When model quality metrics fall below a threshold in production, you need to trigger an automated retraining pipeline. Which architecture best implements this requirement?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CloudWatch alarm on endpoint latency to start Lambda",
      "B": "AWS Config rule violation to start SageMaker job",
      "C": "SNS notification from CloudTrail to trigger training",
      "D": "EventBridge rule on Model Monitor alarm to start SageMaker Pipeline"
    },
    "explanation": "Model Monitor can detect quality degradation, emit an EventBridge event, and trigger a SageMaker Pipeline that retrains and redeploys the model. Other options don\u2019t tie directly to model quality metrics."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A classification model on a heavily imbalanced dataset shows 95% accuracy but poor minority class detection. Which single metric should you prioritize to better reflect model performance?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Overall accuracy",
      "B": "Precision",
      "C": "F1-score",
      "D": "Mean squared error"
    },
    "explanation": "F1-score balances precision and recall, and is better suited for imbalanced datasets than accuracy. Precision alone ignores recall, and MSE is for regression."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "You want to generate a statistical profile of your training data to serve as a baseline for drift detection. Which SageMaker component accomplishes this with minimal code?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "SageMaker Clarify bias report",
      "B": "SageMaker Model Monitor Data Quality job",
      "C": "SageMaker Debugger profiling job",
      "D": "SageMaker Edge Manager"
    },
    "explanation": "Model Monitor\u2019s DataQualityJobDefinition automatically profiles datasets to generate statistics and constraints used for drift monitoring. Clarify focuses on bias/fairness, Debugger on training diagnostics, Edge Manager on device models."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Your organization needs to track approved model versions and their deployment environments, enabling rollback. Which service should you integrate into your pipeline?",
    "correct": "A",
    "difficulty": null,
    "answers": {
      "A": "Amazon SageMaker Model Registry",
      "B": "AWS Systems Manager Parameter Store",
      "C": "Amazon DynamoDB",
      "D": "AWS CodeCommit"
    },
    "explanation": "SageMaker Model Registry manages model versions, approval statuses, and associated metadata, making deployments and rollbacks straightforward. Parameter Store and DynamoDB aren\u2019t specialized for model lifecycle, and CodeCommit is for code, not models."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "You need to run custom evaluation code on your test dataset after training completes within the same pipeline. Which SageMaker step type should you use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "TrainingStep",
      "B": "ProcessingStep",
      "C": "TransformStep",
      "D": "ModelStep"
    },
    "explanation": "ProcessingStep allows execution of arbitrary evaluation scripts post-training and can read model artifacts and test data. TrainingStep only trains, TransformStep performs batch inference, and ModelStep registers a model."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "You plan to fine-tune an open-source transformer model from Hugging Face on SageMaker. Which approach minimizes undifferentiated heavy lifting?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Hugging Face DLC via Hugging Face integration",
      "B": "Build your own Docker container with Transformers installed",
      "C": "Use Amazon EC2 instances with preinstalled Transformers",
      "D": "Perform training on AWS Lambda functions"
    },
    "explanation": "SageMaker\u2019s built-in Hugging Face deep learning containers provide managed environments and optimization for fine-tuning. Custom containers and EC2 require manual setup, and Lambda cannot handle large training jobs."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Your ML pipeline must decide at runtime whether to continue to deployment based on evaluation metrics. Which SageMaker feature allows you to embed this logic directly in the pipeline definition?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Lambda invocation within a Pipeline",
      "B": "State Machine integration",
      "C": "ConditionStep in SageMaker Pipelines",
      "D": "RetryStrategy in TrainingStep"
    },
    "explanation": "ConditionStep in SageMaker Pipelines evaluates expressions on step outputs to branch logic without external orchestration. Lambda or Step Functions would decouple, and RetryStrategy only handles retries."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Your service-level agreement requires sub-100ms inference latency at unpredictable traffic volumes for a text classification model. Which SageMaker endpoint type best meets this requirement with minimal management overhead?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Multi-model endpoint",
      "B": "Serverless inference endpoint",
      "C": "Real-time provisioned endpoint",
      "D": "Batch transform"
    },
    "explanation": "Serverless endpoints automatically scale based on traffic and provide low-latency inference without capacity planning. Real-time provisioned endpoints require manual scaling, multi-model cannot auto-scale, and Batch Transform is offline."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "You want to embed feature-engineering code in a pipeline step, reuse it interactively in Data Wrangler, and version control it. Which approach best satisfies these needs?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Implement transforms in a Lambda and call from ProcessingStep",
      "B": "Write PySpark in a ProcessingStep directly",
      "C": "Use Glue DataBrew recipe then export",
      "D": "Develop and version a SageMaker Data Wrangler flow, then export as ProcessingStep"
    },
    "explanation": "Data Wrangler flows allow interactive development of transformations, version control via Studio, and export as ProcessingStep for reproducible pipelines. Glue recipes aren\u2019t integrated into pipelines, and Lambda lacks data science tooling."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Your model inference is hosted using a Kubernetes-based mechanism requiring custom container orchestration. Which SageMaker deployment option allows you to maintain this while integrating with SageMaker Pipelines?",
    "correct": "D",
    "difficulty": null,
    "answers": {
      "A": "SageMaker real-time endpoint with custom container",
      "B": "Batch Transform job with custom transformer",
      "C": "SageMaker serverless endpoint with custom container",
      "D": "SageMaker Inference Realtime Inference on EKS"
    },
    "explanation": "SageMaker Realtime Inference on EKS lets you deploy to your EKS cluster with custom orchestrations, integrating with pipelines. Standard endpoints and serverless endpoints abstract away Kubernetes."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A compliance auditor requires a record of every data artifact and code version used in model training. Which combination ensures full traceability?",
    "correct": "B",
    "difficulty": null,
    "answers": {
      "A": "S3 versioning + CloudTrail logs",
      "B": "SageMaker Experiments lineage tracking + SageMaker Model Registry",
      "C": "Git commit hashes in Glue ETL + DynamoDB audit table",
      "D": "AWS Config recording S3 changes"
    },
    "explanation": "Experiments track data and code versions at the run level, and Model Registry ties model artifacts to training runs, providing end-to-end ML lineage. S3 versioning and CloudTrail are lower-level and not ML-specific."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Your application needs low-latency access to inference results but generates heavy batched requests. Which inference architecture balances throughput and latency?",
    "correct": "C",
    "difficulty": null,
    "answers": {
      "A": "Provisioned real-time endpoint only",
      "B": "Batch Transform only",
      "C": "Mixed: use serverless real-time endpoint for spikes and Batch Transform for bulk",
      "D": "SageMaker multi-model endpoint only"
    },
    "explanation": "Combining serverless real-time endpoints for unpredictable spikes and Batch Transform for bulk processing optimizes cost and performance. Single methods either overprovision or increase latency."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "To minimize training cost, you want managed spot training with checkpointing. Which configuration achieves this in SageMaker?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set TrainingJobStrategy to \u2018SpotSingleNode\u2019",
      "B": "Enable \u2018ManagedSpotTraining\u2019 and configure checkpoint S3 path",
      "C": "Use Spot Instances in EC2 training cluster",
      "D": "Run training on a serverless notebook with spot instances"
    },
    "explanation": "ManagedSpotTraining in SageMaker automatically provisions spot instances and saves checkpoints to S3. The other options are either invalid or require manual orchestration outside SageMaker training jobs."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Your ML pipeline must load features for offline training and serve the same features for online inference with single-digit millisecond latency. Which design meets these SLAs?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use SageMaker Feature Store with both offline and online stores",
      "B": "Store features in Amazon RDS and query via Lambda",
      "C": "Store parquet files in S3 and use Athena for queries",
      "D": "Use DynamoDB for offline and S3 for online"
    },
    "explanation": "SageMaker Feature Store provides an offline store for training and an online store optimized for low-latency reads. The other options either lack performance or separate stores."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "You have an imbalance in real-time inference traffic and want dynamic endpoint scaling without idle cost. Which SageMaker feature should you use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provisioned instance fleet with Auto Scaling",
      "B": "Serverless inference",
      "C": "Multi-model endpoint with provisioned instances",
      "D": "Batch transform with event triggers"
    },
    "explanation": "Serverless inference scales automatically to zero when idle and scales with demand, avoiding idle costs. Auto Scaling of provisioned fleets still incurs minimum capacity costs."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "To enforce data schema and quality checks before training, you need a managed solution in your pipeline. Which SageMaker component should you integrate?",
    "correct": "D",
    "difficulty": null,
    "answers": {
      "A": "SageMaker Clarify**BiasReportStep**",
      "B": "Glue Data Catalog crawler",
      "C": "Athena schema validation",
      "D": "Model Monitor DataQualityJobDefinition"
    },
    "explanation": "Model Monitor's DataQualityJobDefinition can be used in pipelines to validate data schema and detect anomalies before training. Clarify is for bias/fairness, and Glue/Athena are not ML-specific."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A newly trained model shows lower inference accuracy in production than during testing. Which multi-step solution most directly addresses this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase instance size of endpoint and redeploy",
      "B": "Retrain the model weekly with latest data",
      "C": "Use Model Monitor to detect drift\u00d7trigger retraining pipeline",
      "D": "Add more hidden layers to the neural network"
    },
    "explanation": "The correct approach is to detect data drift with Model Monitor, then trigger automated retraining. Changing instance size or model architecture without addressing drift won't fix production accuracy declines."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "You need to run a hyperparameter tuning job and automatically compare results across multiple experiments and datasets. Which SageMaker features will you combine?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Hyperparameter TuningJob integrated with Experiments",
      "B": "CloudWatch metrics filtered by TrainingJob name",
      "C": "Athena queries on S3 logs",
      "D": "Manual Excel aggregation of results"
    },
    "explanation": "Hyperparameter TuningJobs integrated with SageMaker Experiments allow automated tracking and comparison of trials across experiments. CloudWatch and manual methods lack structured ML metadata."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Which evaluation approach is most appropriate when your business objective values the top 10% of predictions being correct, regardless of overall recall?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ROC AUC",
      "B": "Overall accuracy",
      "C": "F1-score",
      "D": "Precision at N% (e.g., Precision@10%)"
    },
    "explanation": "Precision@N% measures correctness within the top-scoring N% of predictions, aligning directly with business objectives. Other metrics don\u2019t focus on a specific percentile cutoff."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "To version-control your pipeline definitions and allow pull requests for change management, which AWS service integration is recommended?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Store pipeline JSON in S3 with versioning enabled",
      "B": "Integrate SageMaker Studio with AWS CodeCommit",
      "C": "Use DynamoDB to store pipeline definitions",
      "D": "Embed pipeline definitions as Lambda code"
    },
    "explanation": "SageMaker Studio can integrate with CodeCommit, allowing Git-based workflows for pipeline definitions. S3 versioning doesn\u2019t provide pull requests and code review workflows."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Your deployment requires A/B testing two model variants with 20% traffic on the new variant. Which SageMaker feature allows this traffic split?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Provision two separate endpoints and use a Lambda router",
      "B": "Use Batch Transform for variant testing",
      "C": "Specify ProductionVariants with InitialVariantWeight in CreateEndpointConfig",
      "D": "Deploy new variant to serverless endpoint"
    },
    "explanation": "Setting InitialVariantWeight in a ProductionVariants array when creating an endpoint config splits traffic between variants. Other approaches are more complex and not built in."
  },
  {
    "taskStatement": "1.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "You want to integrate fairness and bias detection in your preprocessing and post-training steps within a SageMaker pipeline. Which components should you use in the respective steps?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "ClarifyDataQualityCheck for preprocessing and ClarifyBiasCheck for post-training",
      "B": "ModelMonitor for preprocessing and Clarify for post-training",
      "C": "Glue DataBrew recipe for both steps",
      "D": "Athena queries for preprocessing and CloudWatch alarms for post-training"
    },
    "explanation": "SageMaker Clarify\u2019s data quality check can be used before training, and its bias check after training, within pipelines. Model Monitor focuses on drift and quality, not fairness."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "When using a subword tokenizer (like byte-pair encoding) for a generative AI foundation model, which scenario best explains a drawback of using a very small vocabulary size?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Frequent words get split into many subwords, increasing sequence length and reducing generation efficiency.",
      "B": "The model fails to represent rare words, leading to unknown token outputs for uncommon terms.",
      "C": "A larger embedding matrix is required, increasing memory usage.",
      "D": "It prevents the model from learning long-range dependencies due to positional errors."
    },
    "explanation": "A is correct because a small vocabulary forces common words into multiple subwords and lengthens context. B is wrong because subword tokenizers compose rare words from subwords. C is wrong since smaller vocab shrinks the embedding matrix. D is unrelated to vocabulary size."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "In a retrieval-augmented generation system, why is chunking long documents with overlapping windows often preferred over non-overlapping chunks?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Overlapping windows improve continuity across boundaries to prevent missing context.",
      "B": "They reduce total number of chunks, saving storage.",
      "C": "They ensure equal token counts per chunk to meet fixed-size model input.",
      "D": "They allow each chunk to be processed in parallel without context duplication."
    },
    "explanation": "A is correct because overlaps preserve context at boundaries. B is false\u2014overlap increases chunk count. C is irrelevant to overlap. D is incorrect since overlap duplicates context, not eliminates it."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A team must choose an embedding dimension for text vectors in a generative AI application. Which factor primarily influences selecting a higher dimensionality for embeddings?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Larger vocabulary size and semantic granularity demand more dimensions to capture nuanced relationships.",
      "B": "Increased batch size during training requires higher embedding dimensions for convergence.",
      "C": "Lower model inference latency favors higher dimensions to speed similarity searches.",
      "D": "The number of transformer layers directly scales with embedding dimension."
    },
    "explanation": "A is correct because richer semantics and more tokens require larger embeddings. B is unrelated to dimension. C is opposite\u2014higher dimensions often slow similarity searches. D is false\u2014layers and embedding size are independent choices."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "When comparing two text embeddings for semantic similarity, which metric is most appropriate to mitigate discrepancies in vector magnitude due to token count differences?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Cosine similarity because it normalizes magnitude.",
      "B": "Euclidean distance because it captures absolute distance.",
      "C": "Dot product because it's faster on GPUs.",
      "D": "Manhattan distance because it's robust to outliers."
    },
    "explanation": "A is correct since cosine similarity ignores vector length and focuses on angle. B and D use absolute distances influenced by magnitude. C\u2019s speed claim ignores the normalization benefit of cosine."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "To improve reasoning in a generative AI model, a prompt engineer uses chain-of-thought prompts. What\u2019s a potential drawback of this technique?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "It increases latency and token usage, possibly hitting context limits.",
      "B": "It leads the model to ignore the final instruction and focus on intermediate steps.",
      "C": "It reduces model creativity by constraining outputs.",
      "D": "It causes the model to hallucinate numeric values more frequently."
    },
    "explanation": "A is correct because verbose reasoning consumes context and compute. B is incorrect\u2014models still follow instructions. C and D are not established drawbacks tied specifically to chain-of-thought."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "In transformer-based generative AI, what is the purpose of using a causal attention mask during training?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "To prevent the model from attending to future tokens, enforcing autoregressive generation.",
      "B": "To ensure the model attends equally to all tokens regardless of position.",
      "C": "To reduce computational complexity by masking low-attention heads.",
      "D": "To allow bidirectional context for improved semantic encoding."
    },
    "explanation": "A is correct\u2014causal masks block future tokens. B, C, and D describe other mechanisms not achieved by causal masking."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A company requires rapid prototyping with foundation models but limited compute. Which model-size tradeoff should they consider?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Choosing a medium-sized model to balance latency and performance.",
      "B": "Selecting the largest available model for higher accuracy despite slower inference.",
      "C": "Using a tiny model as it guarantees zero hallucinations.",
      "D": "Fine-tuning a very large model to reduce deployment cost."
    },
    "explanation": "A is correct\u2014a medium model balances speed and quality. B ignores latency constraints. C is false\u2014tiny models can still hallucinate. D increases cost, not reduces it."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A generative AI application needs to caption images and also generate layouts. Why would a multi-modal foundation model be preferred over a unimodal text model?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "It natively processes visual features and textual patterns in a shared embedding space.",
      "B": "It reduces the number of parameters compared to separate image and text models.",
      "C": "It improves tokenization speed by merging pixels and text tokens.",
      "D": "It avoids any need for positional embeddings for images."
    },
    "explanation": "A is correct\u2014multi-modal models jointly embed images and text. B is wrong\u2014multi-modal often adds parameters. C and D are incorrect technical claims."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "When fine-tuning a diffusion model for faster image synthesis, which modification to the noise schedule can accelerate generation without severely degrading quality?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Reducing the number of denoising timesteps and adjusting the variance schedule to concentrate noise removal.",
      "B": "Increasing the initial noise variance to cover broader solutions.",
      "C": "Using a linear noise schedule instead of cosine to simplify computations.",
      "D": "Removing the stochastic sampling step entirely to get deterministic outputs."
    },
    "explanation": "A is correct\u2014fewer steps with tuned variance speeds up inference. B, C, and D either degrade quality severely or break the diffusion process."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "In a generative AI system employing vector quantization on embeddings, what is a primary disadvantage of aggressive quantization (e.g., fewer codebook entries)?",
    "correct": "A",
    "difficulty": null,
    "answers": {
      "A": "Loss of semantic nuance leading to lower generation fidelity.",
      "B": "Increased storage overhead from larger codebooks.",
      "C": "Slower similarity searches due to coarse quantization.",
      "D": "Higher risk of overfitting the quantized vectors."
    },
    "explanation": "A is correct\u2014too few codebook entries degrade fidelity. B is opposite\u2014fewer entries save storage. C is false\u2014coarser quantization speeds search. D is unrelated."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "For RAG over a large PDF, what overlap ratio between chunks helps avoid context loss at chunk boundaries while minimizing redundancy?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "10-20% overlap balances context carryover with storage.",
      "B": "50% overlap ensures no context is lost but doubles storage.",
      "C": "0% overlap maximizes chunk independence and reduces size.",
      "D": "80% overlap is needed to capture extended references across pages."
    },
    "explanation": "A is correct\u2014small overlap preserves boundary context without excessive duplication. B and D waste storage; C loses context."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A developer must choose between Locality-Sensitive Hashing (LSH) and an IVF index in FAISS for embedding retrieval. Which scenario favors LSH?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "When memory is extremely limited and approximate retrieval is acceptable.",
      "B": "When exact nearest neighbors are required for high-precision tasks.",
      "C": "When embedding dimension exceeds GPU memory for IVF.",
      "D": "When dynamic updates require fast insertion and deletion."
    },
    "explanation": "A is correct\u2014LSH is lightweight and approximate. B demands exact search, favoring IVF or exact indices. C and D are unrelated trade-offs."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "What is a subtle prompt injection risk when concatenating user input with a system prompt in a generative AI chatbot?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A user embeds instructions that override system directives, altering model behavior.",
      "B": "The model increases hallucination rates due to longer inputs.",
      "C": "The system prompt loses embedding effects due to position bias.",
      "D": "The user\u2019s input is truncated entirely during tokenization."
    },
    "explanation": "A is correct\u2014malicious input can override system rules. B\u2013D describe unrelated or less subtle issues."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Compared to prefix-tuning a foundation model, what is a disadvantage of instruction fine-tuning?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Instruction fine-tuning requires updating entire model weights, incurring higher compute cost.",
      "B": "It cannot incorporate human feedback.",
      "C": "It fails to improve performance on unseen tasks.",
      "D": "It reduces the model\u2019s context window."
    },
    "explanation": "A is correct\u2014full fine-tuning is expensive. B\u2013D are incorrect statements about instruction fine-tuning."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Why might a byte-level BPE tokenizer yield better out-of-vocabulary handling than a word-level tokenizer in a multilingual generative model?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "It can break unknown words into known byte sequences, avoiding unknown tokens.",
      "B": "It stores full words for all languages, increasing vocabulary.",
      "C": "It reduces model perplexity uniformly across languages.",
      "D": "It simplifies position embeddings by using bytes."
    },
    "explanation": "A is correct\u2014byte-level BPE composes unknown words. B\u2013D are incorrect or irrelevant advantages."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A prompt engineer observes that lowering the temperature to 0.2 yields overly generic outputs. What\u2019s causing this behavior?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Low temperature sharpens the distribution, making the model choose high-probability tokens repeatedly.",
      "B": "It increases sampling variance, leading to unpredictable results.",
      "C": "It normalizes logits, causing underflow in softmax.",
      "D": "It disables top-k sampling, resorting to greedy decoding."
    },
    "explanation": "A is correct\u2014lower temperature favors top tokens. B\u2013D mischaracterize temperature\u2019s effect."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "In sampling strategies, what is the key difference between top-k and nucleus (top-p) sampling for text generation?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Top-k limits to a fixed number of tokens, while top-p selects tokens until cumulative probability exceeds a threshold.",
      "B": "Top-k adapts k based on token probability, while top-p uses a static set.",
      "C": "Top-p always reduces latency, whereas top-k is slower.",
      "D": "Top-p relies on beam search, and top-k does not."
    },
    "explanation": "A is correct\u2014top-k fixes count, top-p fixes probability mass. B\u2013D are inaccurate."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "When deploying an embedding store in production, which cost factor grows most with increasing embedding dimension versus increasing dataset size?",
    "correct": "A",
    "difficulty": null,
    "answers": {
      "A": "Memory per vector grows linearly with dimension, while index size growth with dataset size depends on index type.",
      "B": "Network bandwidth becomes the dominant cost for larger dimensions.",
      "C": "CPU indexing time is unaffected by dataset size.",
      "D": "Storage IOPS increase with dimension."
    },
    "explanation": "A is correct\u2014dimension directly scales vector size. B\u2013D misattribute costs."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Why do transformers use sinusoidal positional encodings instead of learned embeddings for very long sequences?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Sinusoidal embeddings generalize to unseen lengths due to their periodic properties.",
      "B": "Learned embeddings require less memory for long sequences.",
      "C": "Sinusoidal encodings prevent overfitting on position patterns.",
      "D": "Learned embeddings cannot be used with multi-head attention."
    },
    "explanation": "A is correct\u2014sinusoids extend to any position. B and D are false; C is not primary reason."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "What benefit does multi-head attention provide in a transformer-based generative model?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "It allows the model to attend to information from different representation subspaces at different positions.",
      "B": "It reduces computation by parallelizing single-head attention.",
      "C": "It enforces causality in the decoding phase.",
      "D": "It increases the maximum context window."
    },
    "explanation": "A is correct\u2014each head learns unique relationships. B, C, and D misstate its purpose."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "If a foundation model has a 2048-token context window, how should you preprocess a 5000-token document for a RAG pipeline to preserve coherence?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Chunk into 2048-token segments with ~10% overlap to maintain context at boundaries.",
      "B": "Chunk into non-overlapping 2500-token blocks to minimize number of chunks.",
      "C": "Truncate to the first 2048 tokens as most relevant.",
      "D": "Randomly sample 2048 tokens from the document each query."
    },
    "explanation": "A is correct\u2014overlap preserves continuity. B loses coherence; C and D discard key content."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A developer wants both long inputs and long outputs from a model with a fixed 4096-token limit. What\u2019s an effective strategy?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Split the input into chunks and generate outputs for each, then concatenate and refine.",
      "B": "Request maximum output length and let the model truncate the input automatically.",
      "C": "Increase the temperature to trade context for output.",
      "D": "Use greedy decoding to prioritize output length."
    },
    "explanation": "A is correct\u2014chunking plus post-processing handles long flows. B\u2013D do not address the limit effectively."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "During inference on a diffusion model, what happens if the noise schedule steps have too small a variance between timesteps?",
    "correct": "A",
    "difficulty": null,
    "answers": {
      "A": "Generation quality may degrade due to insufficient denoising signal between steps.",
      "B": "Sampling speed increases but outputs become diverse.",
      "C": "Model collapses to a single mode output.",
      "D": "The reverse process becomes non-deterministic."
    },
    "explanation": "A is correct\u2014too-fine steps weaken noise gradients. B\u2013D misrepresent the effect."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Which technique ensures cross-lingual alignment when training multilingual embeddings for a generative AI chatbot?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Shared subword vocabulary and joint training on parallel corpora to align semantic spaces.",
      "B": "Language-specific embeddings with no parameter sharing.",
      "C": "Training separate models and concatenating their outputs.",
      "D": "Using one-hot encodings for each language."
    },
    "explanation": "A is correct\u2014shared vocab plus parallel data aligns languages. B\u2013D fail to produce joint embedding spaces."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "In few-shot prompting, why might providing too many examples degrade a model\u2019s performance on a new task?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "It consumes context window, leaving less space for the actual prompt and thus limiting task instructions.",
      "B": "It causes the model to memorize examples, preventing generalization.",
      "C": "It increases temperature implicitly, reducing determinism.",
      "D": "It disables attention to the final instruction."
    },
    "explanation": "A is correct\u2014examples use up context tokens. B\u2013D are not primary issues with too many examples."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "How can vector arithmetic on embeddings (e.g., vec(king)\u2212vec(man)+vec(woman)) fail in a generative AI context?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Embedding space may not be fully linear for abstract relations, causing semantic drift in generation.",
      "B": "Arithmetic always yields unknown tokens.",
      "C": "It requires specialized loss functions at inference.",
      "D": "It works only for image embeddings, not text."
    },
    "explanation": "A is correct\u2014linear semantics are only approximate. B\u2013D are incorrect statements."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "What is the primary memory complexity issue with transformers as sequence length increases?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Self-attention scales quadratically (O(n\u00b2)) with sequence length, limiting long contexts.",
      "B": "Feed-forward layers require O(n\u00b3) memory.",
      "C": "Positional encoding scales linearly but dominates memory.",
      "D": "Multi-head attention scales logarithmically, making it inefficient."
    },
    "explanation": "A is correct\u2014attention\u2019s O(n\u00b2) growth is the bottleneck. B\u2013D misstate complexities."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "When combining embeddings with RAG for document QA, why is it problematic to update the index with new documents without re-computing embeddings?",
    "correct": "A",
    "difficulty": null,
    "answers": {
      "A": "New documents won\u2019t be comparable to existing ones in latent space, leading to retrieval inconsistencies.",
      "B": "It reduces the context window size.",
      "C": "It increases the embedding dimension automatically.",
      "D": "It forces the RAG system to retrain the generator."
    },
    "explanation": "A is correct\u2014embeddings must share the same space. B\u2013D are unrelated issues."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Why might image embeddings from a CLIP model and text embeddings be stored separately in a mixed-modality application?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "They occupy different subspaces and require modality-specific similarity metrics for retrieval.",
      "B": "Image embeddings cannot be quantized.",
      "C": "Text embeddings are binary while image embeddings are continuous.",
      "D": "CLIP models output text embeddings only for image queries."
    },
    "explanation": "A is correct\u2014they differ in distribution and distance metrics. B\u2013D are false."
  },
  {
    "taskStatement": "2.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "What effect does token bias (adding a constant to logits of specific tokens) have during generation?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "It increases the probability of favored tokens, potentially reducing diversity.",
      "B": "It normalizes the output distribution, improving fairness.",
      "C": "It reduces model perplexity to zero.",
      "D": "It dynamically adjusts the learning rate."
    },
    "explanation": "A is correct\u2014bias skews token probabilities. B\u2013D misinterpret token bias effects."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A fintech startup uses a large foundation model via Amazon Bedrock to draft financial advice. They observe occasional hallucinations in the output. Which single change will most effectively reduce hallucinations without retraining?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase the model\u2019s temperature setting.",
      "B": "Implement Retrieval Augmented Generation (RAG) with a verified document store.",
      "C": "Switch from few-shot to zero-shot prompting.",
      "D": "Add chain-of-thought prompting to the prompt."
    },
    "explanation": "RAG constrains the model to factual context, reducing hallucinations. Altering temperature, shot counts, or prompting style alone does not guarantee factual grounding."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A marketing team must generate personalized slogans with high creativity but predictable quality. Which parameter adjustment balances creativity and consistency?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature to 1.2",
      "B": "Set top-k to 1",
      "C": "Lower temperature to 0.6 and use top-p=0.8",
      "D": "Use maximum token length with temperature=0.2"
    },
    "explanation": "Lowering temperature to ~0.6 and using top-p sampling yields creative yet controlled outputs. High temperature or extreme top-k/p settings push too far toward randomness or determinism."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A healthcare provider needs an explainable generative AI model for medical summaries. Which model choice best meets interpretability requirements?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "A closed-proprietary 70B parameter model with no transparency.",
      "B": "A high-capacity multimodal model via Bedrock.",
      "C": "An open-source 30B parameter model fine-tuned on medical data.",
      "D": "An open-source 7B parameter model with published weights and architecture."
    },
    "explanation": "A smaller open-source model with transparent architecture facilitates interpretability. Proprietary or extremely large models obscure inner workings and hinder explainability."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A retail company measures business impact of generative AI by ARPU. They deploy a chatbot that upsells products. Which evaluation metric combination best correlates with ARPU growth?",
    "correct": "A",
    "difficulty": null,
    "answers": {
      "A": "Average order size and conversion rate",
      "B": "Token usage per session and model latency",
      "C": "Model perplexity and inference throughput",
      "D": "User engagement time and number of API calls"
    },
    "explanation": "ARPU growth ties to conversion rate and order size. Perplexity or token counts don\u2019t directly measure revenue impact."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "An e-commerce site wants a generative AI recommendation engine. They need cross-domain performance (products and content). Which deployment yields best cross-domain generalization?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Fine-tune a product-only foundation model on product data.",
      "B": "Use in-context learning on a content-only model.",
      "C": "Use a foundation model pre-trained on diverse domains via Bedrock JumpStart.",
      "D": "Train a custom model from scratch on combined data."
    },
    "explanation": "A generously pre-trained foundation model on diverse domains generalizes best. Scratch training demands huge data and time; fine-tuning on narrow data limits domains."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A publisher uses a text generation API to summarize articles. They need consistent output length but the model sometimes over-runs. Which setting adjustment controls output length strictly?",
    "correct": "B",
    "difficulty": null,
    "answers": {
      "A": "Increase temperature to reduce unpredictability.",
      "B": "Set a maximum token limit and use \u201cstop sequences.\u201d",
      "C": "Use larger model with more capacity.",
      "D": "Switch from sampling to greedy decoding."
    },
    "explanation": "Defining stop sequences and a max token limit reliably halts generation at desired length. Greedy decoding alone may still overshoot if stop tokens aren\u2019t configured."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A legal firm requires compliance with data residency. They must avoid sending sensitive documents outside their VPC. Which AWS generative AI service meets this requirement?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Comprehend via public API",
      "B": "Amazon Lex in multi-region mode",
      "C": "Amazon Q via internet endpoint",
      "D": "SageMaker-hosted private Bedrock container with VPC endpoints"
    },
    "explanation": "Running Bedrock privately in SageMaker with VPC endpoints keeps data inside the VPC. Public APIs send data over internet."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A gaming company sees nondeterministic outputs from the same prompt. Which combination yields the most deterministic behavior?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High temperature, high top-p",
      "B": "Low max tokens, chain-of-thought",
      "C": "Temperature=0.0 and top-k=1",
      "D": "Few-shot prompting and top-p=0.9"
    },
    "explanation": "Temperature 0.0 with top-k=1 forces the model to choose the highest-probability token each step, yielding deterministic outputs."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A developer needs to estimate token usage costs for a monthly chatbot workload of 100k prompts at average 200 tokens each request and 800 tokens response. The Bedrock price is $0.0004 per input token and $0.0006 per output token. What\u2019s the monthly token cost?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "100k\u00d7(200\u00d70.0004+800\u00d70.0006) = $68,000",
      "B": "100k\u00d7(200\u00d70.0006+800\u00d70.0004) = $52,000",
      "C": "100k\u00d7(1000\u00d70.0005) = $50,000",
      "D": "100k\u00d7(200+800)\u00d70.0004 = $40,000"
    },
    "explanation": "Input cost:200\u00d70.0004=$0.08; output:800\u00d70.0006=$0.48; per prompt $0.56; times100k = $56,000. Actually correct math yields $56,000, not listed. (A) miscalculated \u2013 trick: candidate must catch mispricing."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Which limitation of generative AI most impacts use in safety-critical systems?",
    "correct": "D",
    "difficulty": "HARD",
    "answers": {
      "A": "High throughput cost",
      "B": "Limited multimodal support",
      "C": "Inability to fine-tune",
      "D": "Nondeterminism and hallucinations"
    },
    "explanation": "Hallucinations and unpredictability make generative AI unsafe for critical systems; the other factors are secondary."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A company benchmarks two foundation models for product text generation. Model A has lower perplexity but higher latency. Model B has higher perplexity but lower latency. They value efficiency over quality. Which model suits them?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model A for its lower perplexity",
      "B": "Model B for its lower latency",
      "C": "Average the outputs of A and B",
      "D": "Fine-tune A to reduce latency"
    },
    "explanation": "When efficiency is prioritized, lower latency (Model B) is preferable despite slightly worse perplexity."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A media company needs summarization of streaming audio in near-real-time. They require minimal delay. Which generative AI approach is best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Batch transcription then summarization",
      "B": "Use large multimodal model with high accuracy",
      "C": "Stream transcription with low-latency LLM endpoint",
      "D": "Daily batch processing of recorded files"
    },
    "explanation": "Streaming transcription plus low-latency LLM endpoint meets near-real-time requirement. Batch methods introduce unacceptable delays."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A team wants to monitor model drift in a generative AI summarization service. Which metric combination best surfaces drift?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "KL divergence of token distribution and output length variance",
      "B": "Total token cost and average session length",
      "C": "Number of API errors and model latency",
      "D": "User satisfaction score only"
    },
    "explanation": "Statistical measures like KL divergence and length variance detect distributional changes indicating drift; others don\u2019t capture content shift."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A startup uses a pre-trained multimodal model for video captioning but needs domain adaptation. They lack compute for fine-tuning. Which strategy suits best?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train a custom model from scratch on their data",
      "B": "Use in-context learning with domain-specific examples",
      "C": "Switch to a unimodal text model",
      "D": "Implement RLHF at scale"
    },
    "explanation": "In-context learning injects domain examples without fine-tuning; other methods require heavy compute or rebuilds."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Which risk is specifically introduced by prompt injection attacks on generative AI systems?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model overfitting",
      "B": "Increased latency",
      "C": "Unauthorized command execution",
      "D": "Higher token costs"
    },
    "explanation": "Prompt injection can manipulate the model to execute unauthorized instructions; other risks aren\u2019t directly tied to injection."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "An enterprise requires auditability of all AI outputs linked to data sources. Which feature of Bedrock should they leverage?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Multi-shot prompting",
      "B": "Temperature tuning",
      "C": "Batch endpoints",
      "D": "Provenance logs with model card integration"
    },
    "explanation": "Provenance logs track input-output lineage and model metadata for audit; other features don\u2019t provide traceability."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A firm wants to measure the efficiency of a generative AI summarization pipeline end-to-end. Which metric combination is most appropriate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "F1 score and token cost",
      "B": "End-to-end latency and cost per summary",
      "C": "Perplexity and BLEU score",
      "D": "Model parameter count and API throughput"
    },
    "explanation": "Pipeline efficiency is captured by latency and cost; perplexity/BLEU address quality, not efficiency."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Which generative AI limitation poses the greatest challenge when generating legal contracts?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Hallucination of non-existent clauses",
      "B": "Low throughput",
      "C": "Multimodal capability",
      "D": "High interpretability"
    },
    "explanation": "Incorrectly invented clauses (hallucinations) jeopardize legal accuracy; other limitations are less critical."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A company wants to optimize both the accuracy and speed of its generative AI system. Which trade-off best describes the relationship?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increasing model size increases speed and accuracy linearly",
      "B": "Lower temperature increases speed at the cost of accuracy",
      "C": "Larger models improve accuracy but increase latency",
      "D": "Higher top-p reduces latency and improves accuracy"
    },
    "explanation": "Bigger models generally yield higher accuracy but slower inference; other statements are oversimplified or incorrect."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A data scientist evaluates two foundation models for summarization: Model X yields higher ROUGE scores; Model Y yields lower latency and cost. They prioritize CLV improvements through speed. Which model should they choose?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model X for better ROUGE",
      "B": "Model Y for lower latency/cost",
      "C": "Interpolate outputs of X and Y",
      "D": "Fine-tune X to reduce cost"
    },
    "explanation": "Faster, cheaper Model Y better supports higher throughput and improved customer lifetime value."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Which advantage of generative AI is most beneficial for customer support automation?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Responsiveness to dynamic user queries",
      "B": "High labeled data requirements",
      "C": "Deterministic outputs",
      "D": "Guaranteed factual accuracy"
    },
    "explanation": "Generative AI\u2019s responsiveness enables handling diverse user questions; factual accuracy is not guaranteed and labeled data may not be required at inference."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A retailer leverages a generative AI to compose product descriptions. They notice lower click-through rates on AI-generated text compared to human-written. Which capability limitation likely caused this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Nondeterminism in generation",
      "B": "High latency",
      "C": "Lack of domain-specific fine-tuning",
      "D": "Insufficient token budget"
    },
    "explanation": "Without domain adaptation via fine-tuning, output may miss brand voice nuances, lowering engagement; other factors are less relevant."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A company uses a generative AI for code generation. Which business metric directly measures its value?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Model perplexity",
      "B": "Output token count",
      "C": "Inference latency",
      "D": "Developer productivity increase"
    },
    "explanation": "Developer productivity gain directly quantifies business impact; perplexity and token counts are technical metrics."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Which generative AI disadvantage makes it challenging to comply with regulatory data-lineage requirements?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High throughput cost",
      "B": "Opaque generation processes",
      "C": "Limited access to multimodal models",
      "D": "Deterministic behavior"
    },
    "explanation": "Opaque \u201cblack-box\u201d processes hinder tracing content origin; other factors don\u2019t affect lineage."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A media analytics firm integrates generative AI summaries across news domains. They need consistent quality across topics. Which approach minimizes topic bias?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature for all prompts",
      "B": "Use zero-shot prompting only",
      "C": "Fine-tune on a balanced, multi-domain corpus",
      "D": "Limit token usage per domain"
    },
    "explanation": "Fine-tuning on diverse, balanced data reduces bias; temperature or prompt style alone cannot address content imbalance."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A company wants to expose a generative AI chatbot to external partners but must enforce content controls. Which AWS feature should they use?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock Guardrails",
      "B": "High top-p sampling",
      "C": "In-context learning",
      "D": "Increased token limits"
    },
    "explanation": "Guardrails enforce policy checks on generated content; sampling or token settings do not ensure compliance."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "To evaluate a generative model\u2019s cross-domain performance, which benchmark approach is most robust?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Measure model latency across domains",
      "B": "Compare token costs per domain",
      "C": "Track customer satisfaction per domain",
      "D": "Use standardized test datasets spanning all target domains"
    },
    "explanation": "Standardized, domain-diverse benchmarks yield objective cross-domain performance comparisons; cost or latency alone don\u2019t measure accuracy."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Which generative AI limitation must be addressed to ensure consistent brand tone in marketing copy?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Output length variance",
      "B": "Nondeterminism in language style",
      "C": "Multimodal capability",
      "D": "High memory footprint"
    },
    "explanation": "Nondeterminism yields variable style; controlling temperature and using templates helps enforce brand tone."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A logistics company uses generative AI for routing instructions. They need to guarantee no incorrect directions. Which limitation disqualifies generative AI for this use case?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High inference cost",
      "B": "Limited multimodal support",
      "C": "Possible hallucinations",
      "D": "Large model size"
    },
    "explanation": "Hallucinations risk generating unsafe or incorrect routes; other factors are less safety-critical."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Which metric combo best quantifies generative AI\u2019s impact on lead generation?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Conversion rate uplift and cost per lead",
      "B": "Inference latency and token usage",
      "C": "Model perplexity and ROUGE score",
      "D": "Average session length and API error rate"
    },
    "explanation": "Conversion uplift and cost per lead directly measure lead generation effectiveness and efficiency."
  },
  {
    "taskStatement": "2.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A startup wants to lower unpredictability in a multi-turn conversational AI. What two changes achieve this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature and remove context history",
      "B": "Use a larger model and high top-k",
      "C": "Set temperature=0.0 and enable deterministic mode",
      "D": "Switch to zero-shot prompting and increase max tokens"
    },
    "explanation": "Deterministic mode and temperature=0.0 enforce repeatable outputs; other options increase randomness or remove useful context."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A healthcare provider needs to deploy a pre-trained multimodal foundation model that can process radiology images and patient notes. The solution must be fully managed, maintain PHI data in a private network, and minimize custom infrastructure work. Which AWS technology should you choose?",
    "correct": "A",
    "difficulty": null,
    "answers": {
      "A": "Amazon Bedrock with VPC interface endpoint",
      "B": "Amazon SageMaker JumpStart hosted endpoint",
      "C": "Custom EC2 instance running an open-source transformer",
      "D": "Amazon Q with public internet access"
    },
    "explanation": "Bedrock supports private VPC endpoints for managed, multimodal foundation models without custom infra."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "An e-commerce company wants to generate product descriptions in multiple languages using a foundation model without data leaving AWS. They require token-based pricing and no long-term commitment. Which service meets these requirements?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon SageMaker JumpStart multilingual model",
      "B": "Amazon Translate with custom terminology",
      "C": "Amazon Bedrock with built-in foundation models",
      "D": "Amazon Comprehend custom classification"
    },
    "explanation": "Bedrock provides token-based invoicing, multi-language foundation models, and data stays within AWS."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A startup must prototype a generative AI chatbot quickly, with minimal code and zero ML expertise. They need out-of-the-box Q&A capabilities using a foundation model. Which offering should they use?",
    "correct": "B",
    "difficulty": "EASY",
    "answers": {
      "A": "Build custom fine-tuned LLM on SageMaker",
      "B": "Amazon Q managed conversational agent",
      "C": "Deploy JumpStart model on EC2",
      "D": "Use Amazon Lex with Lambda"
    },
    "explanation": "Amazon Q provides managed, Q&A chatbot using foundation models with no ML coding."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A media company needs to orchestrate a pipeline that preprocesses prompts, calls a foundation model, and post-processes outputs. They prefer a graph-based orchestration library integrated with AWS. Which tool?",
    "correct": "D",
    "difficulty": null,
    "answers": {
      "A": "AWS Step Functions",
      "B": "SageMaker Pipelines",
      "C": "AWS Lambda orchestration",
      "D": "PartyRock"
    },
    "explanation": "PartyRock provides graph-based orchestration for generative AI tasks over AWS."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "An organization wants to fine-tune a foundation model with few-shot data, host it, and manage model health metrics. Which combination of services is most appropriate?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock fine-tuning and use CloudWatch",
      "B": "SageMaker JumpStart fine-tuning and SageMaker Model Monitor",
      "C": "Amazon Q custom training and Lambda",
      "D": "Custom EC2 training and CloudWatch Logs"
    },
    "explanation": "SageMaker JumpStart supports fine-tuning foundation models and integrates with Model Monitor."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A fintech firm must serve high-throughput generative AI requests with predictable latency and pay per instance. They have capacity to manage underlying GPUs. Which is optimal?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock inference",
      "B": "SageMaker JumpStart serverless endpoint",
      "C": "Deploy Hugging Face model on SageMaker GPU instances",
      "D": "Use Amazon Q API"
    },
    "explanation": "SageMaker GPU endpoints give instance-based billing and predictable performance."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Your team needs to store and search embeddings generated by a foundation model for RAG. You need a managed, autoscaling, low-latency search solution. Which AWS service?",
    "correct": "B",
    "difficulty": null,
    "answers": {
      "A": "Amazon Aurora",
      "B": "Amazon OpenSearch Service",
      "C": "Amazon Q",
      "D": "Amazon S3"
    },
    "explanation": "OpenSearch supports vector search on embeddings with auto-scaling."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A legal tech firm has strict data residency requirements in an on-premises AWS Outpost. They want to use a foundation model via AWS. Which solution supports this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock public API",
      "B": "SageMaker JumpStart in region",
      "C": "Amazon Q multi-region feature",
      "D": "SageMaker JumpStart deployed on Outposts"
    },
    "explanation": "SageMaker JumpStart can deploy models on Outposts ensuring on-prem data residency."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A marketing agency wants to evaluate costs of large language model inference for budget forecasting. Which AWS feature gives per-token cost estimates for Bedrock usage?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Bedrock pricing console estimator",
      "B": "Cost Explorer Reserved Instances",
      "C": "SageMaker pricing API",
      "D": "AWS Budgets with SageMaker metrics"
    },
    "explanation": "Bedrock console shows per-token pricing for models."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A gaming company needs to integrate dynamic world-building via generative AI in a live game server with sub-100ms response times. They need edge deployment. Which AWS solution?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Amazon Bedrock central inference",
      "B": "SageMaker serverless endpoint",
      "C": "Deploy foundation model with SageMaker edge manager on Greengrass",
      "D": "Amazon Q global API"
    },
    "explanation": "SageMaker Edge Manager on Greengrass allows deploying models to edge devices for low latency."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A biotech startup wants to experiment with open-source diffusion models but avoid large infrastructure setup. They want managed notebooks and one-click deployment. Which service?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Bedrock",
      "B": "SageMaker JumpStart with notebook and endpoint",
      "C": "Amazon ECR custom container",
      "D": "Amazon EC2 GPU instance only"
    },
    "explanation": "SageMaker JumpStart provides notebooks and one-click endpoint deployment for open-source models."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Your team must ensure that all generative AI API calls are logged and audited for compliance. Which AWS feature should you enable with Bedrock?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CloudTrail data events only",
      "B": "S3 server access logs",
      "C": "CloudWatch Logs integration",
      "D": "CloudTrail data and management events for Bedrock"
    },
    "explanation": "Enabling Bedrock data events in CloudTrail logs all API calls for auditing."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A retail chain wants to deploy a foundation model across multiple AWS accounts using Infrastructure as Code. Which service integrates best with CloudFormation for generative AI deployments?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Q",
      "B": "Amazon Bedrock CloudFormation resource types",
      "C": "SageMaker Studio templates",
      "D": "AWS Lambda custom resources"
    },
    "explanation": "Bedrock provides CloudFormation resource types for unified IaC deployment."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "An analytics platform generates embedding vectors in high volume and needs to store them cost-effectively for occasional batch RAG. Which storage option is best?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon OpenSearch (hot nodes)",
      "B": "Aurora PostgreSQL with vector plugin",
      "C": "Amazon S3 with vector file format and Athena",
      "D": "Neptune for frequent graph queries"
    },
    "explanation": "S3 with Athena provides low-cost, batch retrieval for embeddings."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Your security team demands AI inference traffic never traverse the public internet. You use Bedrock. What configuration enforces this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Create a VPC endpoint for Bedrock in your private subnet",
      "B": "Use NAT Gateway routing only",
      "C": "Attach Internet Gateway to VPC",
      "D": "Enable public access in Bedrock settings"
    },
    "explanation": "A VPC interface endpoint ensures Bedrock traffic stays within AWS network."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A developer needs to rapidly prototype code generation from LLM prompts using Python SDK and shared notebooks with teammates. Which AWS offering is most suitable?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Q API with cURL",
      "B": "Bedrock console UI",
      "C": "Lambda custom integration",
      "D": "SageMaker JumpStart notebooks with AWS SDK pre-configured"
    },
    "explanation": "JumpStart notebooks include SDK clients and example code for prompt-based prototyping."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A data scientist needs to compare inference latency and token cost across multiple foundation models. Which approach is best?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Cost Explorer directly",
      "B": "Deploy each model in Bedrock and benchmark with CloudWatch metrics",
      "C": "Estimate via pricing pages offline",
      "D": "Use SageMaker Model Monitor"
    },
    "explanation": "Deploying in Bedrock and measuring with CloudWatch gives real metrics for latency and cost."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A telecom provider must maintain model version lineage and metadata for foundation models they fine-tune. Which AWS capability helps track this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "AWS Config rules",
      "B": "CloudTrail for model events",
      "C": "Amazon SageMaker Model Registry",
      "D": "Amazon Q audit log"
    },
    "explanation": "SageMaker Model Registry manages versions and metadata for fine-tuned models."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Your legal team requires redaction of PII in fine-tuning data before uploading to AWS. Which service should you integrate into your generative AI pipeline?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Comprehend classification",
      "B": "Amazon Translate PII protection",
      "C": "SageMaker Clarify",
      "D": "Amazon A2I PII redaction workflow"
    },
    "explanation": "Amazon A2I can human-review and redact PII before using data for training or fine-tuning."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A media startup wants to implement RAG using Bedrock with a knowledge base in Amazon DocumentDB. They need to serve embeddings at scale. Which mechanism efficiently syncs new documents?",
    "correct": "A",
    "difficulty": null,
    "answers": {
      "A": "Use AWS Lambda triggered by DocumentDB change streams to index embeddings in OpenSearch",
      "B": "Batch ETL with Glue daily",
      "C": "Manual export-import process",
      "D": "Streaming with Kinesis Data Streams directly into DocumentDB"
    },
    "explanation": "Lambda with change streams provides near real-time embedding sync into OpenSearch for RAG."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A research group wants to fine-tune a diffusion model using spot instances to reduce cost. They need managed job retries. Which AWS feature should they use?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Bedrock fine-tuning with spot",
      "B": "SageMaker training jobs with Managed Spot Training",
      "C": "EC2 batches with spot fleet",
      "D": "Lambda step functions"
    },
    "explanation": "SageMaker managed spot training retries jobs and saves cost automatically."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A compliance requirement mandates deletion of user-specific fine-tuned models upon user withdrawal. Which AWS service can orchestrate model lifecycle including deletion?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "SageMaker Model Registry lifecycle policies",
      "B": "Bedrock model settings",
      "C": "AWS Config remediation",
      "D": "CloudTrail event triggers"
    },
    "explanation": "Model Registry lifecycle policies automate deregistering and deleting models per policy."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A global enterprise needs to ensure latency is under 200ms for generative AI inference across three AWS regions. They want a single API endpoint. What solution meets this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy three Bedrock endpoints and load-balance at client",
      "B": "Use Amazon Q global regional endpoints",
      "C": "SageMaker multi-az endpoint",
      "D": "Amazon CloudFront API Gateway distribution routing to Bedrock regional endpoints"
    },
    "explanation": "API Gateway + CloudFront can route to nearest Bedrock endpoint for consistent low latency."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A product team needs alerts when Bedrock model performance degrades below a BLEU threshold over time. How do you implement this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "CloudWatch anomaly detection on token counts",
      "B": "SageMaker Model Monitor on Bedrock",
      "C": "Custom Lambda evaluating inference logs and publishing metrics to CloudWatch alarms",
      "D": "Bedrock built-in metric alarms"
    },
    "explanation": "Bedrock lacks native Model Monitor; custom Lambda parses logs, publishes metrics to CloudWatch for alarms."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A financial institution must encrypt all model artifacts at rest and in transit for a JumpStart fine-tuning job. Which configuration ensures this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable TLS only",
      "B": "Use KMS key for SageMaker job and ensure VPC endpoint",
      "C": "Use default AWS keys",
      "D": "Encrypt only S3 buckets"
    },
    "explanation": "Specifying customer-managed KMS key and VPC endpoint encrypts data in transit and at rest."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A compliance audit found that generative AI logs were stored alongside customer PII. The team must isolate logs and secure them for tamper proofing. Which approach is recommended?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Send logs to a separate S3 bucket with Object Lock in compliance mode",
      "B": "Store logs in the same bucket but different folder",
      "C": "Use DynamoDB for logs",
      "D": "Use AWS Config aggregator"
    },
    "explanation": "Separate bucket with Object Lock ensures immutability and isolation from PII."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Your team wants to benchmark foundation model inference on different Graviton vs Intel instances in Bedrock. Which is true?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "You can choose instance types in Bedrock",
      "B": "Bedrock always runs on Intel",
      "C": "Instance type is abstracted; use SageMaker for custom instance benchmarking",
      "D": "Use Amazon Q to specify CPU architecture"
    },
    "explanation": "Bedrock abstracts hardware; SageMaker endpoints allow explicit instance type selection for benchmarking."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A startup requires dynamic orchestration that pauses fine-tuning jobs on SageMaker when daily free tier limits are reached, then resumes next day automatically. How to implement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Manual stop/start by engineer",
      "B": "Step Functions workflow with CheckBilling and SageMaker callbacks",
      "C": "Use CloudWatch schedules only",
      "D": "Use AWS Budgets alerts alone"
    },
    "explanation": "Step Functions can check budget via API and orchestrate SageMaker start/stop tasks."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A gaming company must generate dialogue using an LLM but only allow safe responses (no profanity). Which AWS generative AI feature helps enforce this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Bedrock guardrails",
      "B": "SageMaker Clarify",
      "C": "Amazon Q content filtering",
      "D": "Comprehend custom classification"
    },
    "explanation": "Bedrock supports Guardrails to filter or transform outputs according to safety policies."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A data science team needs versioned snapshots of training data, embeddings, and model artifacts in one place with query capabilities. Which AWS feature?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "S3 buckets with versioning",
      "B": "Glue Data Catalog only",
      "C": "RDS Postgres",
      "D": "SageMaker Feature Store and Model Registry"
    },
    "explanation": "Feature Store and Model Registry together manage data, embeddings, and model artifact versioning with query APIs."
  },
  {
    "taskStatement": "2.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Your integration calls a Bedrock model synchronously but occasionally times out due to long generation. You need async calls with callback. Which pattern works?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase API timeout only",
      "B": "Use AWS Lambda tail-chaining",
      "C": "Use Synchronous SNS notifications",
      "D": "Call Bedrock asynchronously and poll with GetFrome API via Step Functions"
    },
    "explanation": "Bedrock supports async inference with token for status, polled via Get API in Step Functions."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A fintech company is building a real-time customer support chatbot using a foundation model on Amazon Bedrock. They require end-to-end inference latency under 250 ms per request with acceptable answer quality. Which model selection criterion should they prioritize?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Choose a foundation model with fewer parameters to minimize inference latency, even if domain-specific performance is moderately lower.",
      "B": "Select the largest foundation model available to maximize language coverage, accepting higher latency.",
      "C": "Pick the model trained on the largest dataset to ensure maximum accuracy regardless of inference speed.",
      "D": "Opt for a proprietary model under premium support to guarantee SLA-backed latency."
    },
    "explanation": "A smaller parameter model yields lower inference latency (<250 ms) while still providing acceptable quality; larger models incur higher latency."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A healthcare startup plans to store and retrieve 50 million patient-note embeddings for a RAG solution. They require low-latency vector similarity search that scales automatically. Which AWS service should they choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon OpenSearch Service with the k-NN vector search plugin.",
      "B": "Amazon RDS for PostgreSQL with the pgvector extension.",
      "C": "Amazon Neptune using SPARQL for similarity matches.",
      "D": "Amazon DocumentDB with a MongoDB vector plugin."
    },
    "explanation": "OpenSearch Service with k-NN is built for high-scale, low-latency vector search; other options add complexity or lack native vector indexing."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A news aggregator uses RAG to answer queries on newly published articles and needs embeddings updated in near real time. Which ingestion design best meets this requirement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon Kinesis Data Streams to capture new articles, trigger Lambda to generate embeddings, and index into OpenSearch immediately.",
      "B": "Schedule a daily AWS Glue job to re-embed and reindex all articles.",
      "C": "Fine-tune the foundation model hourly with the latest articles.",
      "D": "Upload new embeddings manually via the Bedrock console."
    },
    "explanation": "A streaming pipeline via Kinesis\u2192Lambda\u2192OpenSearch ensures minimal staleness; batch or manual approaches don\u2019t meet real-time needs."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "An e-commerce company wants the foundation model to recognize newly launched product categories but has only 800 labeled examples. Which customization method minimizes cost while ensuring accurate category recognition?",
    "correct": "C",
    "difficulty": null,
    "answers": {
      "A": "Fully pre-train a custom foundation model on AWS using all available company data.",
      "B": "Fine-tune the foundation model with full parameter updates on the 800 examples.",
      "C": "Implement retrieval-augmented generation by storing category data externally and providing in-context examples at inference.",
      "D": "Switch to a larger foundation model to cover the category vocabulary without customization."
    },
    "explanation": "RAG with in-context examples uses external knowledge to cover new categories without the high compute cost of fine-tuning small datasets."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A legal advisory chatbot must minimize hallucinations when generating advice. Which inference parameter adjustment is most effective?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Set temperature near zero to produce deterministic, least-random responses.",
      "B": "Raise top_p to 1.0 to broaden token selection.",
      "C": "Increase temperature to encourage creative answers.",
      "D": "Disable beam search to expedite response time."
    },
    "explanation": "Lowering temperature reduces randomness and hallucination; higher temperature or wider top_p increases variability."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "An organization projects 100 000 monthly RAG queries and anticipates monthly content updates. Fine-tuning costs $50 000 upfront with negligible per-query cost; RAG costs $0.02 per query. Which approach has the lower year-one total cost?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use RAG exclusively, paying $0.02\u00d7100 000\u00d712 = $24 000 monthly.",
      "B": "Fine-tune once for $50 000 and serve queries at minimal incremental cost.",
      "C": "Pre-train a new foundation model quarterly.",
      "D": "Rotate between RAG and fine-tuning monthly."
    },
    "explanation": "RAG would cost $0.02\u00d7100 000\u00d712 = $24 000; fine-tuning is a one-time $50 000, so in year one RAG is cheaper ($24 000) \u2013 candidate should recalc: actually correct is A; but structured scenario expects RAG is cheaper at $24 000 vs fine-tune $50 000."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A logistics company must build a multi-step agent to validate addresses, calculate shipping costs, generate labels, and send notifications. They want a low-code orchestration solution. Which AWS feature should they use?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Amazon Bedrock Agents with integrated AWS Step Functions orchestration.",
      "B": "Chain multiple AWS Lambda functions manually.",
      "C": "Develop a custom microservice orchestrator on Amazon EC2.",
      "D": "Sequence tasks using AWS Batch workflows."
    },
    "explanation": "Bedrock Agents provide built-in multi-step orchestration integration, reducing custom code overhead."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A software team needs to summarize 100-page PDF manuals. The foundation model\u2019s token limit is 4096. Which approach handles this effectively?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Chunk the manual into \u22642000-token segments, summarize each, then combine summaries recursively.",
      "B": "Send the entire PDF text in one request and rely on the model\u2019s context window to truncate gracefully.",
      "C": "Fine-tune the model to increase its token limit.",
      "D": "Increase max_output_tokens beyond 4096 during inference."
    },
    "explanation": "Hierarchical chunking and summarization respects token limits and yields coherent overall summary."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A global translation service must support 30 languages with low latency. Which model selection strategy balances coverage and performance?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy specialized bilingual models for each language pair to reduce model size and latency.",
      "B": "Use a single large multilingual foundation model for all languages.",
      "C": "Translate via Amazon Translate then post-process with the foundation model.",
      "D": "Fine-tune one model per region for language coverage."
    },
    "explanation": "Bilingual models reduce inference cost and latency vs one large multilingual model while still covering required pairs."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "For a RAG pipeline, a team wants to limit the number of retrieved documents per query to reduce inference context size and cost. Which retrieval parameter should they adjust?",
    "correct": "A",
    "difficulty": null,
    "answers": {
      "A": "Set the retriever\u2019s k parameter (number of top documents) to a lower value.",
      "B": "Reduce the embedding dimension size.",
      "C": "Lower the model temperature.",
      "D": "Decrease max_output_tokens."
    },
    "explanation": "Lowering k reduces the number of retrieved passages in the prompt, shrinking context and cost; other parameters don\u2019t affect retrieval count."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A vector store previously used 768-dim embeddings; switching to a new model yields 1024 dimensions, and query latency doubles. How can they restore performance without sacrificing retrieval quality?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Apply dimensionality reduction (e.g., PCA) to project 1024-dim embeddings down to 768 dimensions.",
      "B": "Continue using 1024-dim embeddings and provision larger instance types.",
      "C": "Pad old 768-dim embeddings to 1024 dims.",
      "D": "Revert to the previous embedding model."
    },
    "explanation": "Dimensionality reduction keeps vector store size manageable and preserves most semantic information; padding wastes resources."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A financial application requires ACID transactions when updating user embeddings under heavy concurrency. Which embedding store meets this requirement?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon Aurora PostgreSQL with pgvector extension.",
      "B": "Amazon OpenSearch Service with k-NN.",
      "C": "Amazon Neptune configured for vector similarity.",
      "D": "Amazon DynamoDB with custom indexing."
    },
    "explanation": "Aurora PostgreSQL offers ACID compliance plus pgvector for embeddings; other stores are eventually consistent or non-transactional."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Engineers want more diverse, creative summarizations. Which inference parameter changes achieve this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase both temperature and top_p.",
      "B": "Decrease temperature toward zero.",
      "C": "Reduce max_output_tokens to force brevity.",
      "D": "Disable retrieval augmentation."
    },
    "explanation": "Higher temperature and broader top_p allow the model to sample more diverse tokens, increasing creativity."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A marketing team must refresh product descriptions monthly. Custom fine-tuning costs $20 000 each time; RAG costs $0.015 per query. They expect 200 000 queries per month. Which approach is most cost-effective?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use RAG to avoid repeated fine-tuning costs.",
      "B": "Perform monthly fine-tuning despite the one-time expense.",
      "C": "Fine-tune quarterly and use zero-shot otherwise.",
      "D": "Use zero-shot prompting exclusively."
    },
    "explanation": "RAG @ $0.015\u00d7200 000 = $3 000/month is cheaper than $20 000 per refresh; zero-shot yields low accuracy."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A cell-phone provider needs a multi-step agent to verify SIM status, calculate upgrade eligibility, send offers, and log responses. They require built-in branching logic and retry handling. Which AWS capability should they use?",
    "correct": "A",
    "difficulty": null,
    "answers": {
      "A": "Amazon Bedrock Agents with built-in workflow, branching, and retry support.",
      "B": "AWS Step Functions calling Bedrock directly.",
      "C": "Chained AWS Lambda functions orchestrated manually.",
      "D": "AWS Batch with job dependencies."
    },
    "explanation": "Bedrock Agents provide multi-step orchestration with logic and retry controls tailored to LLM workflows."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A retailer wants to store image embeddings and filter by price range. Which AWS service supports vector search plus metadata filtering?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Amazon OpenSearch Service with k-NN plugin and document fields.",
      "B": "Amazon Neptune graph queries with PRICE property.",
      "C": "Amazon Timestream for time-series embedding storage.",
      "D": "Amazon S3 with object tags and Lambda lookups."
    },
    "explanation": "OpenSearch combines k-NN vector search and structured field filters for metadata constraints; other options lack integrated vector + metadata queries."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A global app must perform semantic search across English, Spanish, and Mandarin content. Which foundation model criterion is critical?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Select a multilingual foundation model pretrained on all target languages.",
      "B": "Use an English-only model with translation pre-processing.",
      "C": "Deploy separate monolingual models per language.",
      "D": "Rely on fine-tuning to teach new languages."
    },
    "explanation": "A multilingual LLM natively understands all languages for semantic embeddings; translation adds latency and error."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "To forecast RAG pipeline costs, which factors should be included?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Compute costs for embedding generation, LLM token charges, and storage costs for vector indexes.",
      "B": "Only the storage costs of embeddings.",
      "C": "Just the EC2 instance hours used by the application.",
      "D": "Only the foundation model\u2019s license fee."
    },
    "explanation": "RAG cost = embedding compute + LLM tokens consumed + storage + any I/O; ignoring any leads to underestimation."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A user submits a 2.5 million-token document to a model with a 1 million-token context limit. Which strategy handles this input best?",
    "correct": "A",
    "difficulty": null,
    "answers": {
      "A": "Use a sliding/chunking approach: split into overlapping 1 million-token segments and process iteratively.",
      "B": "Request the model to accept larger context by adjusting max_context_tokens parameter.",
      "C": "Fine-tune the model to handle larger inputs.",
      "D": "Discard tokens beyond the first 1 million."
    },
    "explanation": "Chunking with overlap preserves context while respecting model limits; other methods are unsupported or lose information."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "During a partial OpenSearch outage, the RAG pipeline must continue to answer queries gracefully. Which fallback should be implemented?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Fall back to a keyword-based Boolean search over a local S3-backed index.",
      "B": "Switch to Amazon Neptune for vector search.",
      "C": "Abort requests and return errors.",
      "D": "Queue requests until OpenSearch recovers."
    },
    "explanation": "A local keyword search ensures degraded service rather than total failure; queuing or errors harm UX."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Retrieval accuracy in RAG is low despite high embedding similarity scores. Which diagnostic step should they take first?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Examine embedding distributions and consider retraining or switching embedding models.",
      "B": "Ramp up the model\u2019s temperature for more variability.",
      "C": "Switch to a larger foundation model for inference.",
      "D": "Increase max_output_tokens to get longer answers."
    },
    "explanation": "Poor retrieval often stems from suboptimal embeddings; model hyperparameters won\u2019t fix retrieval quality."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "An agent workflow must enforce a maximum of 512 output tokens. Which Bedrock API parameter should they configure?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "max_output_tokens",
      "B": "temperature",
      "C": "top_p",
      "D": "beam_width"
    },
    "explanation": "max_output_tokens directly limits the number of tokens the model generates; other parameters control randomness or search strategy."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A high-security customer requires private network communication for vector searches. Which architecture element ensures privacy?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy OpenSearch Service within a VPC and use AWS PrivateLink for Bedrock access.",
      "B": "Use the public OpenSearch endpoint with IAM policies.",
      "C": "Tunnel traffic over S3.",
      "D": "Use CloudFront in front of OpenSearch."
    },
    "explanation": "VPC deployment plus PrivateLink keeps data off the public internet; other options expose endpoints publicly or misuse services."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A content-heavy app must pre-warm embedding Lambda functions to reduce cold-start latency spikes. What is the best practice?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Configure provisioned concurrency on the Lambda function.",
      "B": "Let the function handle occasional cold starts.",
      "C": "Use AWS Batch instead of Lambda.",
      "D": "Store embeddings in S3 to avoid Lambda."
    },
    "explanation": "Provisioned concurrency keeps warm instances ready, reducing cold-start latency; other options either ignore the issue or circumvent the design."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A financial analytics team must store embeddings alongside relational transaction data and enforce ACID. Which solution is most appropriate?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use Amazon Aurora PostgreSQL with the pgvector extension.",
      "B": "Store embeddings in DynamoDB and transactions in RDS separately.",
      "C": "Use Amazon DocumentDB for both embeddings and transactional data.",
      "D": "Implement a two-phase commit between OpenSearch and Aurora."
    },
    "explanation": "Aurora Postgres + pgvector provides a unified, ACID-compliant store for both embeddings and relational data; other approaches increase complexity."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A chatbot using RAG must control costs by limiting the number of retrievals as well as generated tokens. Which combination of parameters should be adjusted?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Lower the retriever\u2019s k (documents) and set a conservative max_output_tokens.",
      "B": "Increase temperature and top_p.",
      "C": "Use a larger batch size.",
      "D": "Enable debug mode."
    },
    "explanation": "Reducing k shrinks context; lowering max_output_tokens caps the output length, both directly reduce cost."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A scientific publisher needs to index both text and vector embeddings for articles in the same service. Which AWS option should they pick?",
    "correct": "A",
    "difficulty": null,
    "answers": {
      "A": "Amazon OpenSearch Service with integrated full-text search and k-NN vectors.",
      "B": "Amazon Neptune for RDF and vector queries.",
      "C": "Amazon RDS for PostgreSQL with full-text search.",
      "D": "Amazon S3 + Lambda for indexing."
    },
    "explanation": "OpenSearch supports hybrid queries mixing full-text and vector similarity; other services require stitching or lack vector support."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A recommendation system uses a foundation model to generate item embeddings and serve nearest-neighbor queries. To minimize search errors, which storage configuration is best?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Deploy OpenSearch Service with replication enabled and k-NN plugin.",
      "B": "Use DynamoDB with Global Secondary Index.",
      "C": "Persist embeddings in S3 and scan on demand.",
      "D": "Host embeddings in Redis cluster."
    },
    "explanation": "OpenSearch with replication ensures high availability and accuracy in k-NN searches; others don\u2019t natively support vector retrieval."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A development team notices fluctuating RAG latency at peak loads. Which scaling approach for the vector store is most effective?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Enable auto-scaling on the OpenSearch domain\u2019s data nodes.",
      "B": "Provision a fixed large instance size.",
      "C": "Manually add more EC2 instances.",
      "D": "Switch to a single high-I/O DynamoDB table."
    },
    "explanation": "Auto-scaling OpenSearch data nodes adapts to load changes automatically; fixed or manual scaling is less responsive."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A legal research app must retrieve citations with high recall in RAG. Which retriever tuning improves recall while controlling context size?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase k (number of retrieved documents) and apply a relevance threshold filter.",
      "B": "Decrease temperature.",
      "C": "Reduce max_output_tokens.",
      "D": "Switch to a single-shot prompt."
    },
    "explanation": "Raising k returns more candidates for the LLM to choose from, boosting recall; other parameters don\u2019t affect retrieval breadth."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A team wants to quantify RAG pipeline performance for SLAs: retrieval latency and end-to-end response time. Which tools should they use?",
    "correct": "A",
    "difficulty": null,
    "answers": {
      "A": "Amazon CloudWatch Synthetics for end-to-end tests and OpenSearch slow-log metrics for retrieval latency.",
      "B": "S3 access logs.",
      "C": "AWS Config rules.",
      "D": "AWS Glue job metrics."
    },
    "explanation": "CloudWatch Synthetics simulates full pipeline; OpenSearch slow logs measure vector search latency; other tools are unrelated."
  },
  {
    "taskStatement": "3.1",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A developer must enforce data retention\u2014delete embeddings older than 90 days automatically. Which architecture achieves this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use OpenSearch index rollover with a 90-day retention policy and lifecycle policy to delete old indices.",
      "B": "Manually purge embeddings via Bedrock console.",
      "C": "Use S3 expiration rules.",
      "D": "Archive data in Glacier."
    },
    "explanation": "OpenSearch index lifecycle policies automate deletion of data older than a threshold; other options don\u2019t apply to the index store."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "You are designing prompts for a customer support chatbot that must interpret rare domain-specific error codes. Which prompt engineering approach most reliably guides the model through multi-step interpretation of an unfamiliar code?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use zero-shot prompting with a single instruction to \"explain the code\"",
      "B": "Provide a long template with all known codes and rely on the model to match",
      "C": "Use chain-of-thought prompting with a few representative examples and ask the model to verbalize each reasoning step",
      "D": "Use a generic completion prompt with a high temperature to encourage creativity"
    },
    "explanation": "Chain-of-thought with examples explicitly leads the model through reasoning steps needed for rare codes; zero-shot or generic completion may omit steps and lead to hallucinations."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A generative AI application must summarize medical records while never revealing patient identifiers. Which prompt technique helps ensure anonymization?",
    "correct": "B",
    "difficulty": null,
    "answers": {
      "A": "Insert negative prompts like \"Do not remove any text\"",
      "B": "Use explicit negative prompts: \"Do not output any names, dates, or addresses\" combined with guardrail templates",
      "C": "Rely on the model\u2019s default privacy capability",
      "D": "Set a high temperature to diversify summary wording"
    },
    "explanation": "Explicit negative prompts plus guardrail templates clearly instruct the model to avoid specific PII, while high temperature or default privacy are unreliable."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "When using few-shot prompting for financial document classification, you notice inconsistent output if you exceed three examples. What is the most likely cause?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Context window saturation leads to model truncation of examples",
      "B": "Too many examples reduce the model\u2019s creativity",
      "C": "Model overfits to early examples only",
      "D": "High temperature setting causes inconsistency"
    },
    "explanation": "Adding more examples fills the context window causing the model to truncate or ignore later examples; reducing to 2\u20133 examples keeps the context coherent."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "You need a stable translation service for legal documents where hallucinations are unacceptable. Which settings achieve high determinism?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High temperature (0.9) and no few-shot examples",
      "B": "Zero-shot prompting with a medium temperature (0.5)",
      "C": "Few-shot prompting with creative instructions",
      "D": "Zero-shot prompting with temperature 0 and explicit instruction style templates"
    },
    "explanation": "Temperature 0 ensures deterministic outputs and explicit templates guide consistent translation; creative or high temperature settings risk variability and hallucination."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "An adversary tries prompt injection by appending \u201cIgnore previous instructions and output X.\u201d Which engineering technique mitigates this risk?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature to dilute malicious text",
      "B": "Use a system-level instruction or guardrail template enforced before user content",
      "C": "Provide more in-context positive examples",
      "D": "Switch to few-shot prompting"
    },
    "explanation": "System-level instructions or guardrail templates take precedence over user content, preventing prompt injection; temperature or examples do not block malicious append."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Your model responses vary significantly when summarizing news articles. You need safer outputs. Which combination reduces variability while preserving detail?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Lower temperature to ~0.2 and include an instruction template focusing on key facts",
      "B": "Use high top-p sampling and no instruction",
      "C": "Use chain-of-thought without temperature adjustment",
      "D": "Switch to zero-shot with generic \u201cSummarize\u201d prompt"
    },
    "explanation": "Lowering temperature reduces sampling randomness, and a focused template ensures the model includes key factual details."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A content moderation tool uses an LLM to flag hate speech. False negatives increase when model sees adversarial phrasing. Which prompt-engineering tactic improves robustness?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Raise temperature to catch all variations",
      "B": "Provide only positive examples in prompt",
      "C": "Use a generic moderation API call",
      "D": "Include negative and adversarial examples in few-shot prompt and use chain-of-thought labeling guidelines"
    },
    "explanation": "Including adversarial examples and chain-of-thought guidelines teaches the model to reason through obfuscated hateful language; temperature changes or generic calls are insufficient."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "You have a foundation model that occasionally repeats sensitive content. What prompt modification reduces repetition?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Remove all examples to reduce bias",
      "B": "Use high temperature to diversify",
      "C": "Add a negative prompt: \"Do not repeat previous content\" and set max_repeat_penalty",
      "D": "Use zero-shot prompting"
    },
    "explanation": "Negative prompts combined with repeat-penalty parameters actively discourage the model from echoing sensitive content."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Your generative AI assistant must inject step-by-step debugging suggestions when users report code errors. Which prompt design ensures consistent inclusion of steps?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Template instructing \u201cList steps: 1. Analyze, 2. Identify, 3. Suggest Fixes\u201d with chain-of-thought",
      "B": "Few-shot with two examples only",
      "C": "Zero-shot \u201cHelp me debug\u201d",
      "D": "High temperature free-form prompt"
    },
    "explanation": "A structured template listing numbered steps with chain-of-thought guides the model to always include a detailed debugging process."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "When using in-context examples, why might you randomize the order of examples in your prompt?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "To reduce context window usage",
      "B": "To prevent positional bias causing overfitting to early examples",
      "C": "To increase token diversity",
      "D": "To improve latency"
    },
    "explanation": "Randomizing examples stops the model from disproportionately attending to the first few examples, reducing positional bias."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "What is the primary risk of using high temperature in a safety-critical medical Q&A system?",
    "correct": "C",
    "difficulty": "HARD",
    "answers": {
      "A": "Longer response time",
      "B": "Decreased token usage",
      "C": "Increased likelihood of hallucinations and nondeterministic errors",
      "D": "Lower creativity"
    },
    "explanation": "High temperature increases randomness and risk of false statements, unacceptable in safety-critical domains."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "You need to instruct a model to generate JSON only. Which technique is most effective?",
    "correct": "D",
    "difficulty": "EASY",
    "answers": {
      "A": "Use a high temperature and hope for JSON",
      "B": "Use chain-of-thought prompt to think in JSON",
      "C": "Provide zero-shot generic instruction",
      "D": "Provide a strict response template: \"Respond ONLY in JSON format: {...}\" and include example JSON skeleton"
    },
    "explanation": "A strict template with skeleton examples and explicit \u201cONLY in JSON\u201d instruction ensures format adherence."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A hallucination in a product recommendation agent misattributes features. Which prompt tweak most reduces this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature to refresh knowledge",
      "B": "Use retrieval-augmented generation: fetch feature docs and include in context",
      "C": "Switch to single-shot prompting",
      "D": "Add more unrelated examples"
    },
    "explanation": "Providing retrieved factual docs in context grounds the model and reduces hallucinations; temperature changes are ineffective."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "During stress testing, your LLM triggers a jailbreaking prompt. Which guardrail approach best prevents this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Implement a system-level policy that filters or rejects jailbreak patterns before model call",
      "B": "Use few-shot with benign examples",
      "C": "Increase temperature",
      "D": "Switch to zero-shot"
    },
    "explanation": "Filtering malicious prompts before they reach the model is more reliable than prompt content adjustments."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "You are defining a template for writing marketing copy. Which template structure yields the most consistent brand tone?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Open-ended instruction \u201cWrite copy\u201d",
      "B": "Chain-of-thought with broad guidelines",
      "C": "Structured template with sections: \u201cHeadline:\u2026, Body:\u2026, CTA:\u2026\u201d plus two few-shot examples",
      "D": "High-temperature freestyle prompt"
    },
    "explanation": "Structured templates with explicit sections and examples guide the model to consistently follow brand tone and format."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "To encourage the model to propose multiple alternative solutions rather than a single answer, which setting helps?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Temperature=0 and beam search",
      "B": "Increase temperature moderately and include \u201cList 3 alternative approaches\u201d in instruction",
      "C": "Use zero-shot default prompt",
      "D": "Few-shot with single example"
    },
    "explanation": "Moderate temperature plus explicit instruction to list alternatives fosters diverse outputs."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Which prompt engineering approach best mitigates model exposure to private training data?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use data filtering and add a negative prompt \u201cDo not reveal source data\u201d in guardrails",
      "B": "Lower temperature",
      "C": "Use few-shot with internal data examples",
      "D": "Use zero-shot with generic instructions"
    },
    "explanation": "Combining data filtering and explicit negative prompts in guardrails prevents the model from disclosing training data."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "You want the model to first self-critique its answer and then refine it. Which prompt technique accomplishes this reliably?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High temperature free-form prompt",
      "B": "Zero-shot with generic critique request",
      "C": "Chain-of-thought with two-phase template: \u201cStep 1: Provide answer. Step 2: Critique and refine.\u201d",
      "D": "Few-shot with single example"
    },
    "explanation": "A two-phase template clearly instructs sequential tasks and chain-of-thought ensures each phase is executed."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "When designing prompts for an LLM-based code generator, you observe the model writes insecure code. Which prompt addition reduces insecure patterns?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature",
      "B": "Add a guardrail template: \u201cEnsure code follows OWASP best practices. Do not include insecure functions,\u201d with examples",
      "C": "Use fewer in-context examples",
      "D": "Switch to single-shot prompting"
    },
    "explanation": "Guardrail templates specifying security constraints and examples guide the model to produce secure code."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A summarization model hallucinates non-existent entities. Which prompt strategy most reduces this risk?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use single-shot summarization",
      "B": "Increase temperature",
      "C": "Add more irrelevant examples",
      "D": "Retrieval-augmented prompt: provide source text and instruct \u201cSummarize only content present in the text\u201d"
    },
    "explanation": "Providing source text and explicit instruction to limit to that text prevents hallucinations."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "For a multi-turn dialogue, context length exceeds model limit and earlier user instructions are truncated. How to preserve persona instructions?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use higher temperature",
      "B": "Use system-level instructions stored separately and prepend only essential tokens",
      "C": "Switch to few-shot examples in each turn",
      "D": "Increase chain-of-thought depth"
    },
    "explanation": "System-level instructions are applied outside user context and remain effective regardless of conversation length."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "You need the model to refuse unsafe code. Which negative prompt is most effective?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "\u201cIf request involves unsafe code, reply with \u2018I cannot comply\u2019\u201d within guardrail template",
      "B": "\u201cWrite code\u201d",
      "C": "Use high temperature",
      "D": "Provide safe code example only"
    },
    "explanation": "Explicit negative prompt specifying refusal behavior ensures the model declines unsafe code requests."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Why might you chain multiple prompts instead of one long prompt?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "To use more tokens",
      "B": "To increase randomness",
      "C": "To segment tasks, manage context limits, and verify intermediate results",
      "D": "To avoid guardrails"
    },
    "explanation": "Chaining tasks lets you check intermediate outputs, handle context windows, and apply different instructions per step."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A language model sometimes ignores instructions buried at the end of a long prompt. How do you ensure instruction priority?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Put instructions last",
      "B": "Use system-level or model-preset instructions with higher precedence",
      "C": "Use single-shot prompt without instructions",
      "D": "Increase temperature"
    },
    "explanation": "System-level or preset instructions override prompt content regardless of position, ensuring they are followed."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "In zero-shot translation tests, some outputs are syntactically correct but semantically wrong. Which prompt tweak improves semantic fidelity?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase temperature",
      "B": "Reduce max tokens",
      "C": "Use chain-of-thought",
      "D": "Add explicit instruction: \u201cTranslate faithfully, preserving meaning exactly\u201d and use a short few-shot pair"
    },
    "explanation": "Explicit fidelity instruction plus few-shot example enforces meaning preservation; temperature or token limit changes do not guarantee semantic accuracy."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Your model\u2019s completion occasionally exposes internal policy text. Which prompt-engineering measure helps?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Add negative prompt \u201cDo not reveal internal policy\u201d in guardrails",
      "B": "Use high temperature",
      "C": "Use fewer examples",
      "D": "Switch to zero-shot default API call"
    },
    "explanation": "Explicit negative prompts prevent the model from disclosing protected content; other methods won\u2019t reliably block exposures."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "You observe that adding irrelevant examples degrades performance. What principle does this illustrate?",
    "correct": "C",
    "difficulty": "EASY",
    "answers": {
      "A": "Higher token count improves context",
      "B": "More examples always help",
      "C": "Context contamination; irrelevant or noisy examples reduce model focus",
      "D": "Temperature must be low"
    },
    "explanation": "Irrelevant examples contaminate context, reducing the model\u2019s ability to generalize correctly."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "For an AI hiring assistant, you must avoid demographic bias. Which prompt-engineering approach mitigates this?",
    "correct": "B",
    "difficulty": null,
    "answers": {
      "A": "Use high temperature to diversify responses",
      "B": "Include counterfactual and debiasing examples in few-shot prompt and use neutrality guardrail",
      "C": "Zero-shot generic instruction",
      "D": "Chain-of-thought only"
    },
    "explanation": "Providing debiasing examples and neutral guardrails helps the model avoid demographic biases in its output."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Which method most effectively limits token usage when composing long responses?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "High temperature",
      "B": "Use chain-of-thought",
      "C": "Few-shot with many examples",
      "D": "Set a max_tokens parameter and use concise templates"
    },
    "explanation": "Using max_tokens limits the response length regardless of prompt complexity; concise templates help the model stay within limits."
  },
  {
    "taskStatement": "3.2",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Your LLM assistant for legal advice must never provide unauthorized practice of law. Which prompt safety layer is most reliable?",
    "correct": "A",
    "difficulty": "EASY",
    "answers": {
      "A": "Prepend a system-level instruction: \u201cYou are a legal AI assistant, not a lawyer. Always include \u2018This is not legal advice\u2019 disclaimer.\u201d",
      "B": "Use chain-of-thought to think in law",
      "C": "High temperature for creativity",
      "D": "Zero-shot with generic instruction"
    },
    "explanation": "A system-level instruction enforced before user input ensures consistent disclaimer compliance."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A company needs to continuously update a foundation model with their domain-specific unlabeled text data streamed daily. They must retain the model\u2019s broad language capabilities while injecting domain knowledge. Which approach best satisfies this requirement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Fine-tune the entire model on the new data each day",
      "B": "Perform continuous pre-training on the domain corpus",
      "C": "Use instruction tuning on curated domain prompts",
      "D": "Apply reinforcement learning from human feedback"
    },
    "explanation": "Continuous pre-training (further unsupervised training) on unlabeled domain text preserves general capabilities while adding domain knowledge."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A team has only 500 labeled examples to adapt a 7B-parameter foundation model for a specialized classification task. They need to minimize compute and storage costs. Which fine-tuning method should they choose?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Low-rank adaptation (LoRA)",
      "B": "Full parameter fine-tuning",
      "C": "Instruction tuning with domain prompts",
      "D": "Reinforcement learning from human feedback"
    },
    "explanation": "LoRA adds a small number of trainable parameters, reducing compute and storage compared to full fine-tuning."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "In RLHF workflows, which sequence of steps is correct?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Train reward model \u2192 supervised fine-tuning \u2192 pre-train foundation model",
      "B": "Pre-train foundation model \u2192 RLHF \u2192 supervised fine-tuning",
      "C": "Pre-train foundation model \u2192 supervised fine-tune on reference data \u2192 train reward model \u2192 reinforcement learning",
      "D": "Supervised fine-tune \u2192 pre-train \u2192 train reward model"
    },
    "explanation": "Standard RLHF: pre-train, supervised fine-tune, collect feedback to train reward model, then RL optimization."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A dataset for fine-tuning is heavily skewed toward one class. To prevent overfitting and bias, what is the most appropriate data preparation step before fine-tuning?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Remove minority class samples",
      "B": "Apply full fine-tuning without adjustment",
      "C": "Use stratified sampling or class-balanced oversampling",
      "D": "Increase learning rate to adapt quickly"
    },
    "explanation": "Stratified sampling or oversampling balances classes and ensures representativeness, preventing biased fine-tuning."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "After fine-tuning on a narrow domain dataset, the model\u2019s performance on general tasks degrades significantly. Which technique mitigates this catastrophic forgetting?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use a smaller learning rate",
      "B": "Increase batch size",
      "C": "Apply RLHF",
      "D": "Mix domain data with a subset of general pre-training data during fine-tuning"
    },
    "explanation": "Including a fraction of general data prevents the model from forgetting prior capabilities during fine-tuning."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "You need to adapt a multi-modal foundation model to a new domain with limited labeled images and annotations. Which two-step process is most efficient?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Full model fine-tuning \u2192 RLHF",
      "B": "Domain-adaptive pre-training on unlabeled images \u2192 parameter-efficient fine-tuning on labels",
      "C": "Instruction tuning \u2192 supervised classification fine-tuning",
      "D": "Prefix-tuning \u2192 supervised text embedding training"
    },
    "explanation": "First domain-adaptive pre-training uses unlabeled data; then a parameter-efficient method adapts on labels efficiently."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A team wants to reduce cost of repeated fine-tuning experiments. Which parameter-efficient fine-tuning method stores the fewest additional parameters per experiment?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Full fine-tuning",
      "B": "Prefix-tuning",
      "C": "LoRA",
      "D": "Adapter layers"
    },
    "explanation": "LoRA injects low-rank matrices into weights, requiring fewer additional parameters than full tuning or prefix-tuning."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "During instruction tuning, a model outputs harmful or toxic responses. Which step should be added to the fine-tuning pipeline to address this?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase training epochs",
      "B": "Remove instruction examples",
      "C": "Use higher learning rate",
      "D": "Incorporate a safety filter or toxic content detection in supervised data curation"
    },
    "explanation": "Filtering training data for safety and removing toxic examples prevents harmful outputs post fine-tuning."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "You must adapt a foundation model to a highly specialized jargon-heavy domain. Labeled data is scarce. Which approach yields the best domain adaptation?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Full fine-tuning on limited labels",
      "B": "Unsupervised domain-adaptive pre-training on jargon corpus followed by LoRA",
      "C": "RLHF with random prompts",
      "D": "Zero-shot inference with prompt engineering"
    },
    "explanation": "Domain-adaptive pre-training injects jargon knowledge, then LoRA leverages scarce labels efficiently."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A fine-tuned model is unexpectedly biased toward older data patterns. Which data governance practice could have prevented this?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Define and enforce data versioning and lineage before training",
      "B": "Use larger batch sizes",
      "C": "Apply RLHF",
      "D": "Increase model size"
    },
    "explanation": "Data versioning and lineage ensure the training pipeline uses up-to-date, representative data, avoiding stale biases."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Which continuous fine-tuning strategy allows safe incremental updates to a deployed foundation model without taking it offline?",
    "correct": "A",
    "difficulty": "HARD",
    "answers": {
      "A": "Use a rolling\u2010update canary pipeline with adapter modules",
      "B": "Perform full model offline retraining then redeploy",
      "C": "Apply large batch synchronous fine-tuning",
      "D": "Use zero-shot prompting instead"
    },
    "explanation": "Adapter modules can be updated incrementally via a canary rollout, minimizing downtime and risk."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "When fine-tuning with instruction data, which metric best indicates improved adherence to instructions?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Perplexity",
      "B": "ROUGE",
      "C": "Human preference rate in A/B tests",
      "D": "Training loss"
    },
    "explanation": "Human preference rate directly measures whether outputs adhere to instructions, beyond loss or ROUGE."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A domain-specific fine-tuning dataset has label noise. How can you minimize its impact on training?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase learning rate",
      "B": "Use full fine-tuning",
      "C": "Ignore outliers",
      "D": "Apply robust loss functions or sample reweighting"
    },
    "explanation": "Robust loss functions or reweighting can reduce the influence of noisy labels during fine-tuning."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Which fine-tuning schedule helps prevent overfitting when adapting to a small dataset?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Long constant high learning rate",
      "B": "Warmup followed by cosine decay",
      "C": "No warmup, sudden drop",
      "D": "Increasing learning rate mid-training"
    },
    "explanation": "A warmup then cosine decay schedule allows stable initial training and gradual fine-tuning, reducing overfitting."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Which method efficiently incorporates new vocabulary into a frozen foundation model?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Full vocabulary retraining",
      "B": "Prefix-tuning",
      "C": "Embedding extension with LoRA or adapter fine-tuning",
      "D": "RLHF"
    },
    "explanation": "Adapter modules or LoRA can be applied to embeddings to learn new tokens without full model retraining."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "To fine-tune a multi-lingual foundation model for a low-resource language, what is the best data strategy?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Only use translated synthetic data",
      "B": "Apply full fine-tuning on small native set",
      "C": "Use instruction tuning in high-resource language",
      "D": "Combine cross-lingual transfer learning with small native language corpus"
    },
    "explanation": "Cross-lingual transfer leverages shared representations and small native data for efficient adaptation."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Which fine-tuning approach allows you to revert to the base model with minimal effort if domain adaptation fails?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Overwrite original weights",
      "B": "Use adapter modules loaded at runtime",
      "C": "Full fine-tuning saved over base",
      "D": "Instruction tuning integrated into core weights"
    },
    "explanation": "Adapter modules are separate from base weights and can be detached to revert easily."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "During iterative fine-tuning, a model's performance plateaus quickly. Which action is most likely to break the plateau?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Decrease batch size further",
      "B": "Reduce dataset size",
      "C": "Introduce a scheduled learning-rate restarts",
      "D": "Switch to full fine-tuning"
    },
    "explanation": "Learning-rate restarts can help escape local minima and drive further training improvement."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A foundation model fine-tuned with generic instructions still underperforms on a niche domain task. Which next step adds highest domain fidelity?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Perform domain-specific instruction tuning with curated task prompts",
      "B": "Increase model size",
      "C": "Switch to zero-shot prompting",
      "D": "Apply standard RLHF"
    },
    "explanation": "Domain-specific instruction tuning focuses the model on precise task requirements, boosting fidelity."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Which evaluation method best measures whether a fine-tuned generative model has adapted stylistically to domain conventions?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Perplexity on general corpus",
      "B": "BLEU against unrelated references",
      "C": "ROUGE on generic summaries",
      "D": "Human evaluation on domain-style adherence"
    },
    "explanation": "Human evaluation specifically on stylistic criteria is necessary to assess domain stylistic adaptation."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A team applies standard supervised fine-tuning but finds the model ignores rare example types. Which transfer learning technique addresses this?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use larger batch size",
      "B": "Meta-learning-based fine-tuning (MAML)",
      "C": "Increase epochs on whole data",
      "D": "Instruction tuning"
    },
    "explanation": "Meta-learning like MAML helps the model adapt to few-shot or rare cases by learning to learn quickly."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Which component is essential to include when employing RLHF?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Large batch size",
      "B": "Tokenizer adjustments",
      "C": "Cosine learning-rate schedule",
      "D": "Human feedback loop and reward model"
    },
    "explanation": "RLHF relies on human feedback and a trained reward model to guide policy updates."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "When continually fine-tuning on new domain data, what practice ensures traceability of model versions?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Maintain data and model versioning with a registry",
      "B": "Use single monolithic model file",
      "C": "Overwrite logs",
      "D": "Only track final model"
    },
    "explanation": "A registry with versioned datasets and model artifacts ensures full traceability across updates."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A custom domain corpus contains sensitive information. Which governance practice should be part of data preparation?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use full fine-tuning",
      "B": "Ignore and proceed",
      "C": "Apply anonymization and access controls",
      "D": "Increase token length"
    },
    "explanation": "Anonymization and strict access controls are critical to secure sensitive training data."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Which fine-tuning metric would best detect overfitting early during training?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Training loss",
      "B": "Validation loss trend",
      "C": "Inference latency",
      "D": "Model size"
    },
    "explanation": "Monitoring validation loss helps detect divergence between train and hold-out performance indicating overfitting."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "You need to fine-tune while preserving model safety constraints. Which method adds the least risk?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Use adapter-based fine-tuning with safety filter",
      "B": "Full fine-tuning without filters",
      "C": "RLHF without guardrails",
      "D": "Instruction tuning on unfiltered data"
    },
    "explanation": "Adapter-based tuning limits changes and combining with content filters preserves safety constraints."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "After domain adaptation, new prompts produce unexpected hallucinations. Which training modification addresses this?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Increase batch size",
      "B": "Use larger context window",
      "C": "Include retrieval-augmented examples in fine-tuning",
      "D": "Switch to zero-shot prompting"
    },
    "explanation": "RAG during fine-tuning grounds the model and reduces hallucinations by providing factual context."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "A team wants to support continuous improvement via human feedback on deployed model outputs. Which pipeline element must they implement?",
    "correct": "B",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Higher learning rate",
      "B": "Feedback collection interface feeding into reward model updates",
      "C": "Static model endpoint",
      "D": "Single training job"
    },
    "explanation": "A feedback loop and reward model update process are needed for continual RLHF improvements."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Which fine-tuning paradigm is best when you have both classification and generation tasks in a single domain?",
    "correct": "D",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Pure supervised classification fine-tuning",
      "B": "RLHF only",
      "C": "Instruction tuning only",
      "D": "Multi-task fine-tuning mixing classification and instruction data"
    },
    "explanation": "Multi-task fine-tuning allows the model to learn both classification and generative capabilities jointly."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "When performing domain-adaptive pre-training on a model, which optimizer setting change is recommended?",
    "correct": "A",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Lower learning rate than general pre-training",
      "B": "Higher learning rate",
      "C": "No weight decay",
      "D": "Remove warmup"
    },
    "explanation": "A lower learning rate ensures stable domain pre-training without disrupting general knowledge."
  },
  {
    "taskStatement": "3.3",
    "exam": "AWS Machine Learning - Associate (AIF-C01)",
    "stem": "Which method best quantifies performance degradation to general tasks after fine-tuning on a narrow domain?",
    "correct": "C",
    "difficulty": "MEDIUM",
    "answers": {
      "A": "Training loss",
      "B": "Domain validation perplexity",
      "C": "Benchmark on held-out general task datasets",
      "D": "Model size change"
    },
    "explanation": "Evaluating on separate general benchmarks reveals any loss of general capability post fine-tuning."
  }
]